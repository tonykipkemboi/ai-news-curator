[
  {
    "title": "DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids",
    "url": "https://arxiv.org/abs/2601.10715v1",
    "source": "arxiv",
    "summary": "We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiti",
    "full_text": null
  },
  {
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "url": "https://arxiv.org/abs/2601.10712v1",
    "source": "arxiv",
    "summary": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Task Formulation\n2.2 Agentic Reinforcement Learning\n\n\n\n3 Our Approach: MatchTIR\n\n3.1 Reward Modeling\n3.2 Dual-Level Advantage Estimation\n3.3 Policy Optimization\n\n\n\n4 Experiments\n\n4.1 Experimental Setups\n4.2 Experimental Results\n4.3 Ablation Study\n4.4 Robustness to Task Complexity\n4.5 Tool-Use Efficiency and Accuracy\n4.6 Hyper-parameter Analysis\n\n\n\n5 Related Work\n\n5.1 Tool-Integrated Reasoning\n5.2 Fine-Grained Credit Assignment\n\n\n6 Conclusion\nA Benchmark Details\nB More Implementation Details\n\nC More Experiments\n\nC.1 Analysis on Advantage Estimation\nC.2 Analysis on Cost Matrix Construction\nC.3 Multi-Turn vs. Expanded Single-Turn\n\n\nD Case Study\nE Prompt\n\n\n\n\n\nMatchTIR: Fine-Grained Supervision for \nTool-Integrated Reasoning via Bipartite Matching\n\n\n\nChangle Qu1,\nSunhao Dai1,\nHengyi Cai2,\nJun Xu1,\nShuaiqiang Wang2,\nDawei Yin2\n1Gaoling School of Artificial Intelligence, Renmin University of China;\n2Baidu Inc. \n{changlequ,sunhaodai,junxu}@ruc.edu.cn,\ncaihengyi@ict.ac.cn,\nwangshuaiqiang@baidu.com, yindawei@acm.org\n\n\n\nAbstract\nTool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions.\nHowever, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory.\nThis coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios.\nTo address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation.\nSpecifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards.\nFurthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns.\nExtensive experiments on three benchmarks demonstrate the superiority of MatchTIR.\nNotably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks.\nOur codes are available at¬†https://github.com/quchangle1/MatchTIR.\n\n\n\nMatchTIR: Fine-Grained Supervision for \nTool-Integrated Reasoning via Bipartite Matching\n\n\n\n\nChangle Qu1,\nSunhao Dai1,\nHengyi Cai2,\nJun Xu1,\nShuaiqiang Wang2,\nDawei Yin2\n\n1Gaoling School of Artificial Intelligence, Renmin University of China;\n2Baidu Inc.\n\n{changlequ,sunhaodai,junxu}@ruc.edu.cn,\ncaihengyi@ict.ac.cn,\n\nwangshuaiqiang@baidu.com, yindawei@acm.org\n\n\n\n\n\n1 Introduction\n\nFigure 1: Comparison of reward and advantage assignment strategies for multi-turn TIR. (a) and (b) show traditional methods where all reasoning steps share the same reward and advantage. (c) and (d) illustrate our MatchTIR framework using hard and soft assignment to derive turn-level rewards and distinct advantages.\n\n\nTool-Integrated Reasoning (TIR) has established itself as a critical paradigm for enhancing the capabilities of large language models¬†(LLMs) by enabling them to interact with external tools during the reasoning process¬†Gou et al. (2024).\nBy leveraging external tools, TIR allows LLMs to overcome limitations of static parametric knowledge¬†Jin et al. (2025); Zheng et al. (2025c), perform precise computations¬†Wang et al. (2024); Das et al. (2024), and engage dynamically with external environments¬†Qin et al. (2024); Qu et al. (2024, 2025a).\nThrough multi-turn interactions, agents interleave reasoning, tool execution, and feedback acquisition, refining their trajectory toward solving complex real-world tasks.\n\n\nTo equip agents with such sophisticated tool-integrated reasoning capabilities, recent research has largely adopted reinforcement learning with verifiable rewards (RLVR) methods Chang et al. (2025); Jiang et al. (2025); Lin and Xu (2025), such as Group Relative Policy Optimization (GRPO) Shao et al. (2024).\nWhile early RLVR formulations primarily relied on sparse outcome-based rewards¬†Li et al. (2025), subsequent works have incorporated trajectory-level signals¬†Qian et al. (2025); Zeng et al. (2025b) to provide denser supervision.\nHowever, as shown in Figure¬†1, even these trajectory-level formulations typically assign a uniform advantage value to every turn within a trajectory.\nThis ‚Äúone-size-fits-all‚Äù credit assignment fails to distinguish critical reasoning steps from redundant or erroneous tool calls, leading to inefficient optimization and consequently hindering the model from learning precise and efficient tool-use strategies.\n\n\nSeveral studies have attempted to introduce fine-grained rewards via intrinsic signals or external reward models¬†Wang et al. (2025a); Zhang et al. (2025c).\nHowever, external reward models are inherently susceptible to bias and hallucination, while Monte Carlo estimation¬†Dong et al. (2025) incurs prohibitive computational costs and high variance, especially in long-horizon tasks.\nMoreover, many of these efforts are confined to restricted tools such as search engines, where intermediate steps are characterized by high semantic variance due to the fact that multiple different queries may be equally valid, making it difficult to establish a unique ground truth for reliable step-level supervision.\nIn contrast, general TIR scenarios naturally expose structured and verifiable signals, such as tool names, parameter names, and parameter contents, which enable explicit evaluation of the correctness of the tool use at each turn.\nThis motivates our approach to formulate turn-level reward assignment as a bipartite matching problem between predicted and ground-truth tool interactions.\n\n\nIn this paper, we propose MatchTIR, a framework designed to assign distinct, precise advantages to individual turns within a reasoning trajectory.\nWe formulate turn-level credit assignment as a bipartite matching problem, constructing a weighted bipartite graph based on similarity scores across tool names, parameter names, and parameter contents to align predicted calls with ground-truth references.\nBased on this formulation, we introduce both hard and soft credit assignment strategies to derive dense turn-level rewards:\nthe hard variant enforces ‚Äúone-to-one‚Äù matching, while the soft variant allows ‚Äúone-to-many‚Äù alignment.\nBoth strategies produce dense and precise turn-level rewards, enabling effective supervision of intermediate tool interactions.\nTo ensure the model optimizes for both local accuracy and global success, MatchTIR synthesizes turn-level rewards with final outcome-based rewards.\nFurthermore, we introduce a dual-level advantage estimation mechanism that harmonizes local turn-level rewards with global trajectory-level outcomes.\nSpecifically, the trajectory-level advantage aggregates all rewards across the entire path to evaluate global quality, while the turn-level advantage captures the tt-th turn‚Äôs specific contribution via discounted accumulation of all subsequent rewards.\nThe policy is finally optimized using the GRPO objective with our integrated dual-level advantages.\n\n\nExtensive experiments on both in-domain and out-of-domain benchmarks demonstrate the effectiveness and robustness of MatchTIR.\nIn summary, our main contributions are as follows:\n\n\n‚àô\\bullet¬†We identify the uniform credit assignment problem in existing TIR methods as a key bottleneck preventing efficient optimization.\n\n\n‚àô\\bullet¬†We propose MatchTIR, which formulates turn-level credit assignment as a bipartite matching problem and introduces hard and soft matching strategies to provide dense, turn-level supervision.\n\n\n‚àô\\bullet¬†Extensive experiments on three benchmarks demonstrate the effectiveness of MatchTIR, with particularly strong gains on long-horizon scenarios, validating its robustness and generalizability.\n\n\n\n\n2 Preliminaries\n\nIn this section, we first present the task formulation of TIR, and then introduce the reinforcement learning framework adopted in this work.\n\n\n\n2.1 Task Formulation\n\nFormally, given a user query qq and a tool set ùíØ={t1,t2,‚Ä¶,tn}\\mathcal{T}=\\{t_{1},t_{2},\\ldots,t_{n}\\} consisting of nn available tools, the goal of the agent is to generate an interaction trajectory œÑ={s1,s2,‚Ä¶,sT}\\tau=\\{s_{1},s_{2},\\ldots,s_{T}\\} to solve qq, where TT denotes the number of interaction turns.\nSpecifically, each interaction turn si=(ni,ci,oi)s_{i}=(n_{i},c_{i},o_{i}) is represented as a triplet, where nin_{i} is the natural language reasoning in ii-th turn, cic_{i} denotes the set of invoked tools from ùíØ\\mathcal{T} along with their specific parameter names and parameter contents, and oio_{i} is the observation returned by the environment.\nThe trajectory terminates when the agent generates a final answer response (where cT=‚àÖc_{T}=\\emptyset and oT=‚àÖo_{T}=\\emptyset) or reaches the pre-defined maximum turn limit LL.\n\n\nGoal. As discussed in ¬ß1, TIR is inherently a long-horizon, multi-turn process.\nHowever, existing outcome- or trajectory-level reward formulations assign uniform credit across an entire trajectory, failing to distinguish between effective and unnecessary tool invocations.\nThis results in inefficient credit assignment and suboptimal tool-use behaviors.\nTherefore, our goal is to assign precise turn-level rewards, enabling fine-grained credit assignment that incentivizes effective tool usage contributing to the final solution while penalizing redundant or erroneous actions at each turn.\n\n\nFigure 2: The illustration of our proposed MatchTIR. (a) illustrates the process of multi-turn TIR, where the policy LLM interacts with external tools over multiple turns and receives an outcome reward. (b) shows turn-level reward modeling by matching predicted and golden tool calls using hard or soft assignment strategies. ("
  },
  {
    "title": "High-accuracy and dimension-free sampling with diffusions",
    "url": "https://arxiv.org/abs/2601.10708v1",
    "source": "arxiv",
    "summary": "Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.\n  More precisely, prior works have s",
    "full_text": null
  },
  {
    "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
    "url": "https://arxiv.org/abs/2601.10707v1",
    "source": "arxiv",
    "summary": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly ",
    "full_text": null
  },
  {
    "title": "Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication",
    "url": "https://arxiv.org/abs/2601.10705v1",
    "source": "arxiv",
    "summary": "We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed a",
    "full_text": null
  },
  {
    "title": "Grounding Agent Memory in Contextual Intent",
    "url": "https://arxiv.org/abs/2601.10702v1",
    "source": "arxiv",
    "summary": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrie",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methods\n\n2.1 Contextual Intent Schema\n\n2.2 Contextual Intent Construction\n\n2.2.1 Thematic Scope Induction.\n2.2.2 Taxonomic Event Labeling.\n\n2.2.3 Key Entity Type Extraction.\n\nCoreference Resolution via Structural Alignment.\n\n\n2.2.4 Memory Snippet Construction\n\n\n2.3 Intent-Aware Retrieval\n\n\n\n3 CAME-Bench\n\n3.1 Benchmark Curation\n\n3.2 Evaluation Questions and Metrics\n\nMetrics.\n\n\n\n\n\n4 Experiments\n\nBenchmarks.\nBaselines\nImplementation details.\n\n\n\n5 Results and Analysis\n\nOverview.\nAnalysis of Baselines.\n\n5.1 Ablation Study\n\nThematic Scopes Effectively Reduce Context Noise.\nGranularity Trade-off.\nResolution Grounds Entities.\n\n\n\n\n\n6 Related Works\n\nMemory-Augmented Agent Systems\nLong-Context Agentic Benchmarks\n\n\n7 Conclusion\n\nA Benchmark Construction and Implementation Details\n\n\nA.1 Curation Pipeline Details\n\nPhase I: Closed-World Environment Construction.\nPhase II: Symbolic Storyboard Planning.\nPhase III: Storyboard-Conditional Trajectory Generation.\nPhase IV: Pragmatic Refinement.\n\n\n\nA.2 Domain Specifications\n\nA.2.1 Travel Planning Domain\nA.2.2 Debate Domain\n\n\n\nA.3 Ground Truth Generation and Metrics\n\nGround Truth Answer Generation.\nEvaluation Metric.\n\n\n\nA.4 Implementation Details\n\nModels used.\nPrompts, label ontologies, and hyperparameters.\nDeterminism.\n\n\nA.5 Benchmark Statistics.\n\nA.6 Quality Assurance and Verification\n\nAutomatic Verification.\nHuman Verification.\nInstructions Provided to Annotators.\n\n\nA.7 Example Trajectories and Evaluation Questions\n\n\n\nB Verification of Structural Alignment and Coreference Resolution\n\nB.1 Metric Definition\n\nB.2 Implementation Details\n\nTravel Planning Domain.\nDebate Domain.\n\n\nB.3 Results\n\n\nC Baseline Descriptions\n\nD Error Analysis\n\n\nD.1 Question-Side Label Selection\n\nMethodology.\nOverall Findings.\nLabel-Type Behavior.\nImplications.\n\n\n\nD.2 Failure Modes in Question-Time Label Selection\n\nMethodology.\nNon-Inducible Label Selection.\nGranularity Mismatch.\nImplications.\n\n\n\n\n\nE LLM Prompt Specifications\n\nE.1 Prompts Used by STITCH\nE.2 Prompts for Answer Generation and Evaluation\n\n\nF Benchmark Trajectory Showcase\nG Debate Trajectory Showcase\n\n\n\n\n\nGrounding Agent Memory in Contextual Intent\n\n\n\nRuozhen Yang1*‚ÄÉYucheng Jiang2*‚ÄÉYueqi Jiang1\nPriyanka Kargupta1‚ÄÉYunyi Zhang1‚ÄÉJiawei Han1\n1University of Illinois Urbana-Champaign‚ÄÉ2Stanford University\n{ruozhen2, hanj}@illinois.edu‚ÄÉyuchengj@cs.stanford.edu\n\n\n\nAbstract\nDeploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step‚Äôs intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.\nFor evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.111The codebase and benchmark are available at https://contextual-intent.github.io/.\n\n\n\nGrounding Agent Memory in Contextual Intent\n\n\n\n\nRuozhen Yang1*‚ÄÉ‚ÄäYucheng Jiang2*‚ÄÉ‚ÄäYueqi Jiang1\n\nPriyanka Kargupta1‚ÄÉYunyi Zhang1‚ÄÉJiawei Han1\n\n1University of Illinois Urbana-Champaign‚ÄÉ2Stanford University\n\n{ruozhen2, hanj}@illinois.edu‚ÄÉyuchengj@cs.stanford.edu\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are deployed in long-horizon tasks Liu et al. (2024) that require agents to track interleaved goals, resolve references to prior information, and coordinate actions over extended trajectories. Such settings arise across diverse applications, including extended human‚Äìagent dialogues (Jiang et al., 2024), deep research workflows (Shao et al., 2024), and autonomous tool-augmented environments (Xie et al., 2024).\n\n\nFigure 1: The Challenges of Long-Horizon Agentic Memory. We identify four capabilities required for robust agentic memory: (A) Incremental Memory Revision (tracking state changes over time); (B) Context-Aware Factual Recall (distinguishing semantically similar facts by context); (C) Context-Aware Multi-Hop Reasoning (resolving implicit references across distracting turns); and (D) Context-Aware Information Synthesis.\n\n\nTo address the complexity of these environments, recent works (e.g., Pan et al., 2025; Xu et al., 2025; Edge et al., 2024) have introduced agentic memory mechanisms that organize context online and retrieve relevant information at inference time. This retrieval-centric view aligns with the notion of retrieval cues in cognitive science (Tulving and Thomson, 1973; Craik and Lockhart, 1972): structured indices that determine which stored context information is accessed. In long-horizon settings, where many steps are semantically similar but contextually distinct, retrieval is bottlenecked by cue quality. Effective agentic memory must construct cues that reliably surface the right context amidst noisy, interference-prone history.\n\n\nHowever, existing agentic memory systems remain misaligned with the demands of long-horizon, goal-oriented reasoning in Figure¬†1. Memory compression techniques Sarthi et al. (2024); Pan et al. (2025) summarize semantically or temporally adjacent segments but often fail to link interleaved, persistent goal contexts across non-adjacent segments (Figure¬†1(C)), complicating state tracing (Figure¬†1(A)) and information synthesis (Figure¬†1(D)). Similarly, knowledge-graph-based memories (Edge et al., 2024; Xu et al., 2025) encode entity-level relations but often lack explicit goal- or episode-level disambiguation, so repeated mentions of the same entities can correspond to distinct latent goals (Figure¬†1(B)). While long-context LLMs (OpenAI, 2025; Comanici et al., 2025) can attend to extended trajectories, they become ineffective once trajectories exceed context limits and incur substantial overhead for real-time retrieval.\n\n\nTo address these limitations, we propose STITCH (¬ß2), an agentic memory system that models the latent intent underlying each step of a task trajectory. Grounded in Event Structure Theory (Zacks and Tversky, 2001), STITCH is motivated by the hypothesis that robust recall requires organizing experience by (i) the superordinate goal context and (ii) recurring action categories. We instantiate contextual intent, a structured retrieval cue induced online without a fixed ontology:\n(1) thematic scope (¬ß¬†2.2.1), a stable segment label tracking the current goal (e.g., ‚ÄúDay 2 Itinerary‚Äù or ‚ÄúModel Optimization‚Äù);\n(2) event type (¬ß¬†2.2.2), an action label capturing the operation performed (e.g., ‚ÄúRebuttal‚Äù, ‚ÄúHyperparameter Tuning‚Äù, ‚ÄúPrice Inquiry‚Äù); and\n(3) key entity types (¬ß¬†2.2.3), the entity classes that determine which attributes are relevant in-context (e.g., Metric, Hyperparameter, Price, Rating).\nTogether, these cues explicate contextual information otherwise ambiguous in the raw trajectory: thematic scope links non-adjacent, goal-consistent steps for multi-hop reasoning, state tracing, and synthesis (Figure¬†1(C, A, D)), while event and entity typing disambiguate repeated mentions by anchoring facts to functional roles and expected attributes, improving recall under interference (Figure¬†1(B)). At inference time, STITCH filters and prioritizes memory snippets by structural compatibility, suppressing semantically similar but context-mismatched content (¬ß¬†2.3).\n\n\nA parallel gap lies in how we evaluate long-horizon memory: existing benchmarks rarely test whether models can retrieve the right fact in the right context. In realistic agentic settings, success is not determined solely by finding semantically relevant text, but by verifying that retrieved information matches the contextual constraints under which it was stated or requested (e.g., a hotel ‚Äúprice‚Äù is only meaningful when conditioned on dates, location, and room type). Yet current evaluations often sidestep this requirement. First, many benchmarks segment a long trajectory into largely independent mini-episodes or topic blocks (Wu et al., 2024), which makes retrieval effectively local and reduces the need to track global state across interleaved goals. Second, they enforce strict turn-taking between a user and an assistant (Wu et al., 2024; Maharana et al., 2024), where queries are typically answered immediately after they are posed; this adjacency masks the harder case where requests are interleaved, deferred, and resolved only after several intervening steps.\n\n\nTo address these evaluation gaps, we introduce CAME-Bench (Context-Aware Agent Memory Evaluation Benchmark) (¬ß3), which targets context-aware retrieval in long, goal-oriented trajectories. CAME-Bench stresses this capability along three axes: interleaved, non-turn-taking interaction structure; multi-domain coverage; and controlled difficulty via diverse question types and length-stratified subsets. These choices make dispersed, often implicit constraints necessary for answering and reveal a complementary failure mode obscured in existing evaluations where strong baselines are near-saturated.\n\n\nOur contributions are threefold:\n\n\n1.\n\nWe propose STITCH, an intent-aware, domain-agnostic agentic memory system that induces contextual intent online and uses intent compatibility to filter"
  },
  {
    "title": "Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis",
    "url": "https://arxiv.org/abs/2601.10701v1",
    "source": "arxiv",
    "summary": "Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work",
    "full_text": null
  },
  {
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "url": "https://arxiv.org/abs/2601.10700v1",
    "source": "arxiv",
    "summary": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that ser",
    "full_text": null
  },
  {
    "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
    "url": "https://arxiv.org/abs/2601.10696v1",
    "source": "arxiv",
    "summary": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.10696v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.10696v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 15 Jan 2026]\n    Title:The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load\n    Authors:Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu            View a PDF of the paper titled The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load, by Han Jiang and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&#39; prior expertise and interaction strategies through prompting.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.10696 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.10696v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.10696\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yao Xiao [view email]          [v1]\n        Thu, 15 Jan 2026 18:52:59 UTC (857 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load, by Han Jiang and 3 other authorsView PDF\n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n         "
  },
  {
    "title": "Data-driven stochastic reduced-order modeling of parametrized dynamical systems",
    "url": "https://arxiv.org/abs/2601.10690v1",
    "source": "arxiv",
    "summary": "Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, w",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n2.1 Problem statement\n2.2 Priors and variational approximations\n2.3 Evidence lower bound (ELBO)\n2.4 Reparametrized ELBO\n2.5 Amortized SVI with continuous-time encoding\n2.6 Gradient estimation\n2.7 Point estimates of model parameters\n2.8 Probabilistic ROM predictions\n\n\n3 Results\n\n4 Discussion\n\nData/code availability\nAcknowledgments\n\n\n\nA PNODE/PNSDE details\n\nPNODE formulation\nPNSDE formulation\n\n\n\nB Summary of parametrized terms\n\nThe variational encoder\nThe variational decoder\nThe SDE prior\nThe amortized variational distribution\n\n\n\nC Details of Example Problems\n\n\nC.1 Reaction-Diffusion System in 2D\n\nArchitecture and Training\nInterpretable dynamics\n\n\n\nC.2 Forced, parametrized Burgers‚Äô equation\n\nArchitecture and training\n\n\n\nC.3 Fluid flow with control input\n\nArchitecture and training\n\n\n\n\n\n\n\n\n\nData-driven stochastic reduced-order modeling of \nparametrized dynamical systems\n\n\nAndrew F. Ilersich, Kevin Course, Prasanth B. Nair‚Ä†‚Ä†andrew.ilersich@mail.utoronto.ca, kevin.course@mail.utoronto.ca, prasanth.nair@utoronto.ca\nUniversity of Toronto Institute for Aerospace Studies, \n4925 Dufferin Street, Toronto, ON M3H 5T6\n\n\n\nAbstract\nModeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.\n\n\n\n1 Introduction\n\nNumerical simulation of complex dynamical systems is computationally demanding, particularly in applications requiring multiple simulation runs under varying parameter settings and forcing conditions such as optimal design, parameter estimation, and control.\nModel order reduction is a powerful approach to address this challenge, wherein a reduced-order model (ROM) over a lower-dimensional latent space is constructed to serve as a computationally efficient surrogate/emulator of the full-order model (FOM). Arguably, the earliest work on this topic dates back to Fox and Miura in 1971¬†[28], who proposed projection schemes for accelerating structural design calculations. There is now a rich body of literature on the theoretical and computational aspects of projection-based ROMs for a broad class of parametrized operator problems¬†[11].\n\n\nA major limitation of projection-based ROM approaches is that they require direct access to the FOM. In many practical applications, the underlying FOM may be inaccessible, too complex to work with, or entirely unknown with the trajectory observations arising from experimental studies. This has motivated the development of data-driven ROM algorithms capable of learning directly from observed FOM or experimental trajectories, circumventing the need for explicit knowledge of the governing equations.\n\n\nMost of the existing ROM approaches, whether projection-based or data-driven, proceed in two steps: first constructing forward and approximate inverse mappings between the full-order state and a low-dimensional latent state, then formulating equations governing the latent dynamics.\nThe effectiveness of dimensionality reduction in this first step is fundamentally linked to the Kolmogorov nn-width of the solution manifold, which quantifies the best possible approximation of the dynamics in a low-dimensional subspace¬†[49]. For problems with rapidly decaying Kolmogorov nn-width, linear dimensionality reduction techniques such as proper orthogonal decomposition (POD) have proven highly effective¬†[5, 6, 11, 48, 43, 58]. However, many complex systems exhibit solution manifolds with slowly decaying nn-width, motivating the use of nonlinear dimensionality reduction approaches¬†[60, 61, 62, 38, 35, 42, 9, 53], basis adaptation algorithms¬†[31], and gradient-based reduction methods¬†[8].\nProjection-based methods leverage the FOM in the second step¬†[10, 11, 38, 35], whereas data-driven approaches employ a non-intrusive approach involving the minimization of an appropriate loss function over training trajectories to estimate the ROM parameters¬†[6, 61, 48, 43, 29, 24].\n\n\nEven though the two-step approach has been applied successfully in many applications, it is worth noting that for certain datasets and latent mappings, the latent state trajectories learned in the first step can cross each other. In such scenarios, it is impossible to represent the dynamics with a system of differential equations¬†[25]. Some studies approach ROM by simultaneously learning the state space mappings and the ROM dynamics, which avoids this issue¬†[39, 15, 2].\n\n\nChampion et al.¬†[15] proposed a data-driven ROM method that simultaneously learns the state space mapping along with a sparse identification of nonlinear dynamics (SINDy) ROM. Similar to the approach presented in this work, the SINDy method also does not require a forward solver during training. However, this is achieved by assuming that the time-derivative of the state is available or can be computed using numerical differentiation. In the latter case, the accuracy of the ROM can deteriorate if the measurement noise variance is high. In contrast, the proposed approach requires only the state measurements to infer a ROM.\n\n\nIn the present work, we develop an amortized stochastic variational inference (SVI) scheme for learning ROMs across parameter spaces and forcing conditions using only FOM trajectory data. Our end-to-end methodology simultaneously infers a variational autoencoder for state space mapping and a system of continuous-time stochastic differential equations (SDEs) governing the latent dynamics. The present work advances data-driven ROM in the following ways: (1)¬†by learning SDE-based ROMs, our approach can address challenging problems with stochastic dynamics (see, for example,¬†[33, 30, 52]), (2)¬†the end-to-end learning approach developed here circumvents the technical challenges associated with two-step methods described earlier, and (3)¬†by adopting a stochastic modeling and inference framework, our approach enables quantification of prediction uncertainty, which is critical for downstream decision-making applications such as optimal design and control.\n\n\nAlthough learning SDEs in a variational inference setting offers several theoretical advantages, its implementation poses significant computational challenges. Traditional approaches require integrating the latent SDE using a forward solver for each training trajectory to calculate the evidence lower bound and its gradients¬†[3, 4, 40]. The need for a forward solver in the training loop, which is a well-known known bottleneck in training neural differential equations in general¬†[34], can make the training cost prohibitive for high-dimensional states and large-scale datasets. Moreover, parameter updates during the inference process tends to increase the stiffness of the underlying differential equations, precipitating a blow-up in the cost per iteration.\n\n\nWe address these challenges by leveraging a reparametrization trick recently proposed by Course and Nair¬†[20] that allows us to train the ROM without integrating it, which not only reduces the training cost, but decouples it from the SDE stiffness. Combined with an amortization strategy¬†[19], our approach enables further reductions in training cost by defining variational distributions over short partitions of the time-domain. We present numerical studies on three test cases to demonstrate the efficacy of the proposed ROM approach in comparison to existing approaches. In the final test case, we consider ROM of a controlled fluid flow problem with 105,600105,600 states, to demonstrate the scalability of our approach.\n\n\n\n\n2 Methodology\n\nFigure 1: Graphical overview of the proposed stochastic ROM approach. The ROM, which we train by stochastic variational inference, consists of three modules: a probabilistic encoder, a latent SDE, and a probabilistic decoder. Making a prediction with the ROM starts with parameters Œº\\mu, forcing f‚Äã(t)f(t), and initial condition u0u_{0}. We encode the initial condition to obtain the corresponding latent z0z_{0}. We then solve the latent SDE to obtain realizations of the trajectory z‚Äã(t)z(t). Decoding the sampled trajectories yields the stochastic ROM prediction of the FOM QoI, u‚Äã(t)u(t), that can be postprocessed for the mean prediction and statistical error bars.\n\n\n\n2.1 Problem statement\n\nConsider a parametrized spatio-temporal quantity of interest (QoI): u~:Œ©√ó[0,T]√ó‚Ñ±Œº√ó‚Ñ±f‚Üí‚Ñù\\widetilde{u}:\\Omega\\times[0,T]\\times\\mathcal{F}_{\\mu}\\times\\mathcal{F}_{f}\\to\\mathbb{R}, either computed using a FOM or obtained through experimental measurements over the time interval [0,T][0,T]. Here, Œ©‚äÇ‚Ñùm\\Omega\\subset\\mathbb{R}^{m} denotes the spatial domain, while ‚Ñ±Œº‚äÇ‚ÑùNŒº\\mathcal{F}_{\\mu}\\subset\\mathbb{R}^{N_{\\mu}} and ‚Ñ±f‚äÜL2‚Äã(Œ©√ó[0,T];‚Ñù)\\mathcal{F}_{f}\\subseteq L^{2}(\\Omega\\times[0,T];\\mathbb{R}) represent the parameter space and the space of forcing functions, respectively. Our training dataset compri"
  },
  {
    "title": "On the origin of neural scaling laws: from random graphs to natural language",
    "url": "https://arxiv.org/abs/2601.10684v1",
    "source": "arxiv",
    "summary": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this p",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Our contributions\n\n\n\n2 Neural scaling laws\n\n2.1 Methodology\n\n\n\n3 Random walks, nn-grams, and random graphs\n\n\n3.1 Random graphs: Erd√∂s-Renyi and Barab√°si-Albert ensembles\n\n3.1.1 Erd√∂s-Renyi graphs\n3.1.2 Biased random walks: dialing into power laws\n3.1.3 Scale-free graphs: Barab√°si-Albert ensemble\n\n\n\n\n\n4 Dialing up complexity: from random graphs to natural language\n\n4.1 Scaling laws for learning language bigrams\n4.2 Scaling laws for learning TnL-Language\n\n\n\n5 Revisiting scaling laws for natural language modeling\n\n5.1 Importance of the irreducible loss\n5.2 2d Chinchilla fit is relatively poor\n5.3 Recovering basic scaling law results with 2 layer transformers and short context lengths\n5.4 Scaling laws with Œº\\muP: improved parameter efficiency?\n\n\n6 Additional related work\n7 Discussion\n8 Acknowledgments\nA Experimental details\n\nB Fitting scaling laws\n\nB.1 L‚Äã(N)DL(N)_{D} and L‚Äã(D)NL(D)_{N} fits\nB.2 Confidence intervals\nB.3 Fitting L‚Äã(N,D)L(N,D)\nB.4 Fitting compute optimal scaling laws, LoptL_{\\text{opt}}, NoptN_{\\text{opt}}, DoptD_{\\text{opt}}\n\n\n\nC Sample error baseline\n\nC.1 Mean squared error\nC.2 Cross-entropy loss\nC.3 Application to graph random walk\n\n\n\nD Additional scaling law results\n\nD.1 Number of bigrams vs. count: most bigrams appear only a few times\nD.2 44 layer transformer trained on language (Fineweb-edu) with Standard Parameterization\nD.3 Barab√°si-Albert graph with 8K nodes, 50K edges\nD.4 Erd√∂s-Renyi graphs with 50K nodes and 2M edges, Œ∫=0,1\\kappa=0,1\n\n\n\n\n\n\n\n\n1]Meta Superintelligence Labs, FAIR\n2]Department of Physics, University of Maryland, College Park and Joint Quantum Institute\n3]Axiom Math\n\\contribution[*]Equal contribution\n\\contribution[‚Ä†]Work done at Meta\n\nOn the origin of neural scaling laws: \nfrom random graphs to natural language\n\n\nMaissam Barkeshli\n\n‚ÄÉ‚ÄÉ\nAlberto Alfarano\n\n‚ÄÉ‚ÄÉ\nAndrey Gromov\n\n[\n\n[\n\n[\n\n\n\nAbstract\nScaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing\ndata, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erd√∂s-Renyi and scale-free Barab√°si-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 100, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.\n\n\n\n1 Introduction\n\nOne of the most important lessons in modern deep learning is the steady improvement in model capabilities as additional compute resources and data are effectively leveraged (sutton2019bitter). This was partially quantified through the characterization of neural scaling laws (cortes1993learning; hestness2017deep; kaplan2020scaling; henighan2020scaling; hoffmann2022training), which demonstrate that across many vision and natural language tasks, the test loss decreases predictably as a simple power law over many orders of magnitude in number of model parameters NN, dataset size DD, and amount of compute CC. The discovery of neural scaling laws has had significant impact in practice for language model pretraining. It allows practitioners to determine how to optimally scale the model size and dataset size with compute (kaplan2020scaling; hoffmann2022training; chowdhery2022palm; grattafiori2024llama3; yang2024qwen2; yang2025qwen25; liu2024deepseekv2; jiang2024mixtral; tian2025moeScaling). It also provides a way to benchmark algorithmic breakthroughs in architectures, optimization, and data.\n\n\nThese empirical results have led to significant theoretical work in trying to understand the origin of neural scaling laws. Specifically, why is there a power law decrease in the test loss over many orders of magnitude in NN, DD, and CC, and what sets the exponents of the power laws? A clear answer to this question may be of significant practical value, since if we understand what sets the exponents, we might understand the extent to which they can be increased, thus increasing the asymptotic efficiency of deep learning methods.\n\n\nA popular suggestion has been that the power law scaling in the test loss originates from power laws that are already present in the dataset itself. For example, it is well-known that the frequency of words in a corpus of text follows Zipf‚Äôs law, with many other power laws having also been characterized in natural language corpora (piantadosi2014zipf; altmann2016statistical). Natural images also exhibit power laws in their spectra (ruderman1994statistics; maloney2022solvable). Many theoretical works have shown that in linear or kernel regression, power laws in the test loss do in fact originate from power laws in the data (or in features defined in terms of the data) (bordelon2020; bahri2021explaining; Spigler_2020; maloney2022solvable; lin2024scaling; paquette2024fourplus3; bordelon2024dynamical). More generally, if we assume that models need to learn a discrete set of tasks to achieve a particular value of test loss, and if these tasks are distributed with power law weighting, then a power law in the test loss follows (michaud2024quantizationmodelneuralscaling; ren2025emergence).\n\n\nThe above theories based on linear models with mean square error (MSE) loss are rather far from the setting of auto-regressive sequence modeling with cross-entropy loss. Consequently it is not clear to what extent they are representative of the neural scaling laws seen in natural language modeling. A potentially fruitful approach would be to study sequence modeling with datasets of tunable complexity, where one can systematically dial down from a realistic limit to a highly simplified limit, and track the change in the neural scaling law behavior. An additional benefit of datasets with tunable complexity is that they could potentially allow more appropriate comparisons of models across different scales: a proper comparison of a small model to a large model should also appropriately increase the complexity of the dataset. Complexity of the dataset in turn is not measured simply in terms of size of the dataset, but also in terms of the degree of correlations, and hierarchical, compositional structure (cagnetta2024deep).\n\n\nTo this end, we propose to study sequence modeling of random walks on graphs. Graphs and their generalizations ‚Äì hypergraphs and hierarchical graphical structures ‚Äì can capture a large portion of many kinds of data of interest, including many features of language, games, and formal mathematics. In particular, walks on graphs can correspond to generating from nn-gram models (Sec. 3), providing a simplified model of language. On the other hand, at a more abstract level, walks on graphs can be used to model stepwise inference and chain of thought reasoning (khona2024understandingstepwiseinferencetransformers; Besta_2024).\n\n\nFigure 1: Scaling laws for 2-layer transformers trained on next token prediction on unbiased random walks on an Erd√∂s-Renyi graph, with 88K nodes and 5050K edges. Neither the random walks nor the graph exhibits any power laws. Data in bottom two plots are fit to Eq. 2. Mean Œ±D¬Ø=1.028\\overline{\\alpha_{D}}=1.028 with standard deviation 0.1290.129. Mean Œ≤N¬Ø=0.749\\overline{\\beta_{N}}=0.749 with standard deviation 0.0140.014. Average MSE for L‚Äã(N)DL(N)_{D} 1d power law fits is 1.07√ó10‚àí81.07\\times 10^{-8}, compared to 7.03√ó10‚àí87.03\\times 10^{-8} for best exponential fit. Average MSE for L‚Äã(D)NL(D)_{N} 1d power law fits is 8.86√ó10‚àí88.86\\times 10^{-8}, compared to 2.13√ó10‚àí62.13\\times 10^{-6} for best exponential fit. Brackets indicate 95% confidence intervals obtained from bias-corrected and accelerated bootstrap method (see Appendix B for details). \n\n\n\n1.1 Our contributions\n\nFigure 2:  Left: Mean exponents Œ±D¬Ø\\overline{\\alpha_{D}} and Œ≤N¬Ø\\overline{\\beta_{N}} for all experiments reported in this paper. Legend is in the format &lt;dataset&gt; (model). Language refers to Fineweb-edu. Œ±D¬Ø\\overline{\\alpha_{D}} and Œ≤N¬Ø\\overline{\\beta_{N}} are averages of the best fit exponents Œ±D\\alpha_{D} and Œ≤N\\beta_{N} over DD and NN. Error bars indicate standard deviation of the best fits Œ±D\\alpha_{D}, Œ≤N\\beta_{N} across different NN and DD respectively. Right: Œ±D¬Ø\\overline{\\alpha_{D}} and Œ≤N¬Ø\\overline{\\beta_{N}} for 2-layer transformer experiments on language, T1L, T2L, and language bigrams, demonstrating monotonic evolution of Œ±D¬Ø\\overline{\\alpha_{D}} with approximate entropy of the dataset, along with relative stability of Œ≤N¬Ø‚âà0.5\\overline{\\beta_{N}}\\approx 0.5.\n\n\n\nOur main contributions are as follows.\n\n\n‚Ä¢\n\nWe demonstrate that transformers trained to perform next-token prediction on random walks from random graphs, such as Erd√∂s-Renyi and Barab√°si-Albert graphs, exhibit neural scaling laws. In particular, we empirically demonstrate the first example of scaling laws even when the input data has no power law structures at al"
  },
  {
    "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
    "url": "https://arxiv.org/abs/2601.10681v1",
    "source": "arxiv",
    "summary": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.10681v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.10681v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 15 Jan 2026]\n    Title:Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems\n    Authors:Amir Khurshid, Abhishek Sehgal            View a PDF of the paper titled Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems, by Amir Khurshid and 1 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.10681 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.10681v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.10681\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Amir Khurshid Mr [view email]          [v1]\n        Thu, 15 Jan 2026 18:43:19 UTC (727 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems, by Amir Khurshid and 1 other authorsView PDF\n      view license\n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will a"
  },
  {
    "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
    "url": "https://arxiv.org/abs/2601.10679v1",
    "source": "arxiv",
    "summary": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background on HRM\n\n2.1 Forward Pass\n\n2.2 Deep Supervision &amp; One-step Gradient\n\n2.2.1 Reasoning Depth Scaling\n2.2.2 The Role of Fixed Point Property\n\n\n2.3 Evaluation of HRM\n\n\n\n3 Failure on Extremely Simple Puzzles: Violation of Fixed Points\n\n3.1 Violation of Fixed Point Assumption\n3.2 One-step Gradient Postpones Acquirement of Stability\n3.3 Restoring Fixed Points via Data Augmentation\n\n\n\n4 Reasoning Modes of HRM: ‚ÄòGuessing‚Äô instead of Reasoning\n\n4.1 Mean-field Analysis: Scaling Laws of Loss Curves\n4.2 Single-Sample: ‚ÄúGrokking‚Äù Along Segments\n4.3 Four Reasoning Modes of HRM\n4.4 Spurious Fixed Points as Misleading Attractors\n\n4.5 Escaping the Trap\n\n4.5.1 Perturbation in the Input Space\n4.5.2 Perturbation in the Model Parameter Space\n\n\n4.6 Reasoning or Guessing?\n\n\n\n5 Spurious Fixed Points\n\n5.1 Rival Attractor\n5.2 Plausible Local Minima\n\n\n6 Conclusion and Discussion\nA Related Work\n\nB Technical Details of HRM\n\nB.1 Sub-segment unfolding\nB.2 Initialisation and reset\nB.3 ACT implementation\nB.4 Mapping to the main-text notation\n\n\n\n\n\n\n\nAre Your Reasoning Models Reasoning or Guessing? \nA Mechanistic Analysis of Hierarchical Reasoning Models\n\n\nZirui Ren\n\n‚ÄÉ‚ÄÉ\nZiming Liu\n\n\n\nAbstract\nHierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) ‚ÄúGrokking‚Äù dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM ‚Äòguesses‚Äô the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be ‚Äúguessing‚Äù instead of ‚Äúreasoning‚Äù. Leveraging this ‚Äúguessing‚Äù picture, we propose three strategies to scale HRM‚Äôs guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models ‚Äúreason‚Äù.\n\nHierarchical Reasoning Model, Machine Learning\n\n\n\n1 Introduction\n\nCurrent large language models (LLMs) are based on the transformer architecture¬†(Vaswani et al., 2017). Despite their success, they still struggle in reasoning-intensive tasks. For example, ¬†Wang et al. (2025a) showed that even the best language models achieve 0% success rate in solving hard Sudokus and mazes. In an effort to build models competent for these reasoning tasks, which generally require systematic System-2 thinking for humans, massive work has been done to prolong the reasoning process of LLMs, i.e., the intermediate outputs before reaching the final answer. It either relies on chain-of-thought (CoT) prompting¬†(Wei et al., 2022; Chen et al., 2025a) or fine-tuning with reinforcement learning¬†(Guo et al., 2025; Wen et al., 2025; Yue et al., 2025), both of which perform reasoning at the token level, limiting the potential of deep networks¬†(Turpin et al., 2023; Helwe et al., 2021).\n\n\nAlternatively, latent-space reasoning models¬†(Hao et al., 2024; Wang et al., 2025a) rise as a new paradigm of reasoning depth scaling. Among them, the hierarchical reasoning model (HRM)¬†(Wang et al., 2025a) achieves extraordinary accuracy on various reasoning-intensive tasks, outperforming LLM reasoners by a significant margin.\n\n\nTo understand the secret sauce of the success of HRM and reveal its potential failure modes, we closely inspect the reasoning patterns of HRM, focusing on the Sudoku-Extreme dataset¬†(Wang et al., 2025a). To our surprise, we identify three counterintuitive facts:\n\n\n\nFigure 1: A lone unknown cell exposes the fixed-point violation. HRM secretly guesses fixed points, no matter they are true or not. Multiple fixed points exist in the latent space; escaping them via data augmentation, input and model bootstrapping boosts accuracy from 54.5% to 96.9%.\n\n\n\n\n‚Ä¢\n\nFailure on extremely simple puzzles, due to violation of fixed points (Section 3): HRM could fail on a puzzle with only one unknown cell, due to a theory-practice mismatch. HRM theory assumes the fixed point property, i.e., the ability to maintain stability after finding the solution; however, we find that this property breaks down in practice. Luckily, we find that a simple fix suffices ‚Äì data augmentation.\n\n\n\n‚Ä¢\n\n‚ÄúGrokking‚Äù dynamics in recursion, due to HRM ‚Äúguessing‚Äù instead of ‚Äúreasoning‚Äù (Section 4): When approaching a puzzle, HRM does not incrementally refine the answer at each recursive step. Instead, it typically gets completely perplexed (error remains high and flat for many steps), and then ‚Äúgroks‚Äù (error drops to zero in one step). We hypothesize that the recursion (outermost loop) of HRM serves as a way of scaling ‚Äúguessing‚Äù attempts for a plausible latent state, challenging the common belief that recursive reasoning boosts performance by incremental refinement.\n\n\n\n‚Ä¢\n\nReasoning ‚Äúgets lost‚Äù in the latent space, due to multi-stability of reasoning landscape (Section 5): Closely inspecting the reasoning trajectory in the latent space, we are able to classify reasoning modes of HRM, among which the most interesting failure mode is when it ‚Äúgets lost‚Äù, i.e., lingering around some misleading attractive point.\nWe show that these false attractors can be interpreted as local optima of a heuristic error metric measuring the number of conflicts. This trap discourages HRM from further exploring the latent space, postponing or precluding the encounter of the ‚Äòtrue‚Äô fixed point. It turns out to be the central factor that caps HRM at suboptimal accuracy.\n\n\n\n\n\nAll insights above imply that HRM appears to be ‚Äúguessing‚Äù instead of ‚Äúreasoning‚Äù. In order to get better performance, the ‚Äúguessing‚Äù picture points to a new scaling axis ‚Äì guess attempts, in addition to model and data. We thus propose three methods to scale guessing attempts: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). Combining all methods, our Augmented HRM is able to boost the accuracy from 54.5% to 96.9% for Sudoku-Extreme, surpassing vanilla HRM¬†(Wang et al., 2025a) and its existing variants such as the Tiny Recursive Model¬†(Jolicoeur-Martineau, 2025). Scientifically, our findings provide new insights into how reasoning models could ‚Äúreason‚Äù.\n\n\n\n\n2 Background on HRM\n\nHRM is a recursive latent-space model in the sense that in each forward pass, the inputs and hidden states are recurrently passed through a single module several times. Each call of this module is named a segment, and the recursive loop of segments is named the outer loop.111The original HRM architecture uses separate H-module and L-module, each forwarded a few times within one segment. However, ablation studies have shown that this is not the core factor of superior performance¬†(Ge et al., 2025; Jolicoeur-Martineau, 2025). It is also not relevant to our analysis of reasoning trajectories in following sections. Regarding these facts, in this paper we obscure this inner structure of segments. This is merely a simplification of notation and does not alter the model architecture; see Appendix¬†B for details of this simplification, allowing us to focus mainly on the outer loop.\n\n\n\n2.1 Forward Pass\n\nWe formalize the (simplified) forward pass of HRM as follows. The input sequence is mapped to its embedding x~\\tilde{x} by the input network fIf_{I}:\n\n\n\nx~=fI‚Äã(x;Œ∏I)\\tilde{x}=f_{I}(x;\\theta_{I})\n\n(1)\n\n\nIn this paper, we focus on the Sudoku-Extreme dataset. The input samples xx‚Äôs are sudoku puzzles, formatted as 9√ó99\\times 9 sequences containing integer tokens ranging from 11 to 99, together with a special &lt;blank&gt; token representing masked cells.\n\n\nAs a latent-space model, HRM keeps a latent state ziz^{i}, deterministically initialized as z0z^{0}. The HRM segment ‚Ñ±\\mathcal{F} takes the input embedding and the current latent state as input, and computes the next latent state:\n\n\n\nzi+1=‚Ñ±‚Äã(zi,x~;Œ∏)z^{i+1}=\\mathcal{F}(z^{i},\\tilde{x};\\theta)\n\n(2)\n\n\nAfter all TT segments, the prediction vector is extracted from the terminal latent state:\n\n\n\ny^=fO‚Äã(zT;Œ∏O)\\hat{y}=f_{O}(z^{T};\\theta_{O})\n\n(3)\n\n\n\n\nIn practice, when HRM has already reached a plausible solution, the remaining segments are redundant. Correspondingly, an adaptive computation time (ACT) mechanism¬†(Graves, 2016) is introduced to decide whether to halt the computation after each segment.\n\n\n\n\n2.2 Deep Supervision &amp; One-step Gradient\n\n\n2.2.1 Reasoning Depth Scaling\n\nThe core training technique to achieve reasoning depth scaling is deep supervision¬†(Wang et al., 2025a). Each forward pass corresponds to only one ground truth label, while the number of segments can be arbitrarily scaled. This mismatch makes the loss signal sparse compared to reasoning depth.\n\n\nDeep supervision addresses this issue by computing the loss for the latent state ziz^{i} and the associated output y^i=fO‚Äã(zT,x~;Œ∏O)\\hat{y}^{i}=f_{O}(z^{T},\\tilde{x};\\theta_{O}) of each segment. Formally,\n\n\n\nLi=l‚Äã(y^i,y)L^{i}=l(\\hat{y}^{i},y)\n\n(4)\n\n\nHowever, the computation required for one back propagation through time (BPTT)¬†(Rumelhart et al., 1986; Werbos, 1990) of all these segment losses scales at Œò‚Äã(T)\\mathrm{\\Theta}(T), and deep supervision at each s"
  },
  {
    "title": "Single-Stage Huffman Encoder for ML Compression",
    "url": "https://arxiv.org/abs/2601.10673v1",
    "source": "arxiv",
    "summary": "Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along wit",
    "full_text": null
  },
  {
    "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies",
    "url": "https://arxiv.org/abs/2601.10660v1",
    "source": "arxiv",
    "summary": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Datasets and Tasks\n\n3.1 Winning Arguments\n3.2 The Topics Winning Arguments Dataset\n3.3 Anthropic/Persuasion\n3.4 Persuasion for Good\n\n\n\n4 Methodology\n\n4.1 Step 1: Strategy-Aware Reasoning\n4.2 Step 2: Strategy Scoring from Reasoning\n4.3 MS-PS-AVG: Zero-Shot Aggregation\n4.4 MS-PS-MLP: Learning a Persuasion Function\n\n\n\n5 Experimental Setup\n\n5.1 Language Models\n5.2 Direct Comparison\n5.3 Independent Scoring Baselines\n\n5.4 Experiments on Winning Arguments\n\n5.4.1 Results and Discussion\n\n\n5.5 Validating MS-PS Scores on Human Annotations\n5.6 Experiments on Anthropic and Persuasion for Good\n5.7 Topic-Based Evaluation\n\n\n6 Conclusions and Future Works\n\nA Implementation Details of TWA Topic Modeling\n\nA.1 Motivation for Four Topics\nA.2 Text Preprocessing\nA.3 Topic Modeling and Balanced Clustering\nA.4 Impact of Balanced Clustering\nA.5 Topic Labeling\nA.6 Reproducibility\n\n\nB TWA Topic Statistics\nC Strategy Analysis Prompt\nD Strategy Scoring Prompt\nE Rephrasing Strategy\n\nF Feature Design for MS-PS-MLP\n\nAverage.\nVariance.\nEntropy.\n\n\n\nG Implementation Details for LLM Experiments\n\nG.1 Hardware Setup\nG.2 Preprocessing and Input Construction\nG.3 Model Wrappers\nG.4 Decoding strategy and stability\nG.5 Error Handling\n\n\n\nH Direct Comparison Analysis\n\nH.1 Direct Comparison Prompt\nH.2 Positional Bias in Direct Comparison\n\n\nI Perturbation-Based Prompting\nJ Validation of the Persuasiveness Scale Choice\n\nK Independent Scoring Prompts\n\nIndependent Scoring\n+ Context\n+ Explanation\n+ Context + Explanation\n\n\nL Inter-model Agreement on Strategy Scores\n\nM Validation of Strategy-Based Reasoning\n\nPS-Simple.\nCoT-Simple.\n\n\nN Controlling for Input Token Length\n\nO MS-PS-MLP Grid Search and Best Hyperparameters\n\nLLaMA-3.1-8B\nGemini-1.5-Flash-02\nGemini-2.0-Flash\nGemma-3-12B\nOpenAI-o3\n\n\nP Strategy Score Distributions\n\nQ Validating Strategy Scoring with MS-PS\n\nQ.1 Single-Prompt Classification\n\nQ.2 MS-PS Scoring\n\nStep 1 ‚Äì Persuasion Analysis Prompt:\nStep 2 ‚Äì Scoring Prompt:\n\n\nQ.3 Results\n\n\n\nR Extension to Other Datasets\n\nR.1 Anthropic\nR.2 Persuasion for Good\n\nR.3 Prompts for Anthropic Experiments\n\nBaseline-1\nBaseline-2\nMS-PS Strategy Scoring\n\n\n\nR.4 Prompts for Persuasion for Good Experiments\n\nBaseline-1\nBaseline-2\nMS-PS Strategy Scoring\n\n\n\n\nS Strategy Impact Across Topics\nT LLMs Used in Experiments\n\n\n\n\n\nDetecting Winning Arguments \nwith Large Language Models and Persuasion Strategies\n\n\n\nTiziano Labruna1,\nArkadiusz Modzelewski1,2,\nGiorgio Satta1,\nGiovanni Da San Martino1\n\n1University of Padua,\n2Polish-Japanese Academy of Information Technology\n\n\n\n\n\nAbstract\nDetecting persuasion in argumentative text is a challenging task with important implications for understanding human communication.\nThis work investigates the role of persuasion strategies‚Äîsuch as Attack on reputation, Distraction, and Manipulative wording‚Äîin determining the persuasiveness of a text.\nWe conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good.\nOur approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies.\nResults show that strategy-guided reasoning improves the prediction of persuasiveness.\nTo better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.\n\n\n\nDetecting Winning Arguments \nwith Large Language Models and Persuasion Strategies\n\n\n\n\nTiziano Labruna1,\nArkadiusz Modzelewski1,2,\nGiorgio Satta1,\nGiovanni Da San Martino1\n\n1University of Padua,\n2Polish-Japanese Academy of Information Technology\n\n\n\n\nFigure 1: Overview of the MS-PS framework. Each of the two input messages is independently analyzed by a language model across six persuasion strategies. For each strategy, the model first generates an explanation assessing the presence of the strategy, followed by a 1‚Äì10 persuasiveness score. In the MS-PS-AVG variant (a), the more persuasive message is identified as the one with the higher average score. In the MS-PS-MLP variant (b), each message is represented by a feature vector consisting of the six individual scores plus their average, variance, and entropy, which is fed to a trained MLP classifier to predict which message is more persuasive.\n\n\n\n1 Introduction\n\nIn an era where online discussions shape public opinion, understanding what makes certain arguments more persuasive and ultimately successful is becoming increasingly important. The significance of this problem has been widely recognized, inspiring extensive research across multiple disciplines Saenger et¬†al. (2024); Hoeken et¬†al. (2012); Maio et¬†al. (2014); van Eemeren and Grootendorst (2015). Identifying arguments that effectively change opinions is particularly relevant for political debate analysis, decision-making support, and applications in computational social science Popkin (1991); Ziems et¬†al. (2024).\n\n\nPrevious research on predicting winning arguments‚Äîi.e., those that successfully convince a reader to adopt a certain opinion‚Äîhas mainly focused on assessing argument quality, evaluating convincingness through annotated argument pairs, and analyzing interaction dynamics Habernal and Gurevych (2016); Gleize et¬†al. (2019); Tan et¬†al. (2016a). Recent studies have also examined how large language models (LLMs) detect persuasive arguments Rescala et¬†al. (2024); Ziems et¬†al. (2024). To our knowledge, no prior work has explored the role of specific persuasion strategies in identifying winning arguments. In this study, we address this gap by investigating whether particular persuasion strategies serve as discriminative signals for winning arguments, moving beyond general linguistic or interaction patterns.\n\n\nTo investigate the usefulness of persuasion strategies in winning arguments detection, we use the Winning Arguments (WA) dataset Tan et¬†al. (2016b), collected from the Change My View subreddit, an online forum on Reddit where users propose discussion topics and present and evaluate arguments.\nTo enable more granular, topic-aware analysis, we further introduce the Topics Winning Arguments (TWA) dataset, a topic-annotated extension of WA.\nTWA organizes discussions by topic, providing a finer-grained examination of argument persuasiveness.\n\n\nMotivated by the Persuasion-Augmented Chain of Thought (PCoT) method for strategy-aware reasoning in disinformation detection Modzelewski et¬†al. (2025), we propose Multi-Strategy Persuasion Scoring (MS-PS), adopting a well-established taxonomy of six persuasion strategies, previously used in NLP research Dimitrov et¬†al. (2024); Piskorski et¬†al. (2023a, b).\nMS-PS leverages LLMs to perform structured, strategy-specific reasoning: for each persuasion strategy, the model first generates a textual analysis and then assigns a numerical persuasiveness score, resulting in one score per strategy for each message. These scores are used in two ways: (i) averaging across strategies to select the message with the higher mean, and (ii) training a multilayer perceptron that predicts the more persuasive message from the LLM-generated scores (see Figure 1). While averaging the scores provides a simple system to assess how the use of persuasion strategies correlates with persuasiveness, the multilayer perceptron aims to capture more nuanced interplay between multiple strategies and their impact on persuasiveness.\nUnlike PCoT, which injects all strategies jointly and focuses on their detection, our approach fully disentangles strategies by using independent prompts for each one, produces continuous strategy-specific scores, and learns a non-linear aggregation of these signals. These design choices reduce cross-strategy interference and yield a more interpretable and expressive representation of persuasion.\nWe validate MS-PS both against human annotations and on two additional datasets: Anthropic/Persuasion Durmus et¬†al. (2024) and Persuasion for Good Wang et¬†al. (2019), demonstrating its effectiveness across diverse domains.\n\n\nOur main contributions are:\n(I) we are the first to exploit information about persuasion strategies in a learning model with the goal of detecting winning arguments; (II) we introduce MS-PS, a zero-shot framework producing strategy-specific persuasiveness scores usable both directly and as features for supervised models; (III) the release of the TWA dataset, a topic-annotated extension of Winning Arguments enabling fine-grained, topic-aware analysis; (IV) extensive validation of MS-PS across datasets, showing robustness, effectiveness on contamination-free data, and scalability beyond binary tasks.\n\n\n\n\n2 Related Work\n\nEarly work on persuasion detection used lexical, syntactic, and sentiment features combined with rule-based or classical ML techniques¬†Tan et¬†al. (2016a); Hidey et¬†al. (2017); Lukin et¬†al. (2017). These methods offered useful signals but lacked robustness and struggled to capture discourse-level strategies.\n\n\nThe advent of LLMs enabled richer, context-aware representations and led to strong gains via fine-tuning and in-context learning for persuasion tasks¬†Bassi et¬†al. (2024); Li et¬†al. (2024a); Nayak and Kosseim (2024). LLMs have been used to produce interpretable features¬†Li et¬†al. (2024b), to support multi-label and multilingual persuasion classification¬†Purificato et¬†al. (2023); Hromadka et¬†al. (2023); Roll and Graham (2024), and to model belief change¬†Hoang et¬†al. (2025).\n\n\nPrior works have also applied LLMs to detecting manipulative content such as propaganda¬†Sprenkamp et¬†al. (2023); Hasanain et¬†al. (2024); these studies show models like GPT-4 can match state-of-the-art systems but still face "
  },
  {
    "title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution",
    "url": "https://arxiv.org/abs/2601.10657v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse,",
    "full_text": "\n\n\n\n1 Introduction\n2 Motivation\n\n3 Progress-Aware Consistent Evolution (ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits)\n\n3.1 Hierarchical Context Management (ùô∑ùô≤ùôº\\mathop{\\mathtt{HCM}}\\limits)\n3.2 Momentum-Based Backtracking (ùôºùô±ùô±\\mathop{\\mathtt{MBB}}\\limits)\n3.3 Self-Adaptive Collaborative Evolution (CE)\n\n\n\n4 Experiments\n\n\n4.1 Evolutionary Framework Comparison\n\n4.1.1 Symbolic Regression\n4.1.2 Kernel Bench\n\n\n4.2 Automated Engineering in Complex Environments\n4.3 Ablation studies\n\n\n5 Related Works\n6 Conclusion\nA Failure Analysis\n\nB Method Details\n\nB.1 Notation Table\n\nB.2 Context Management\n\nB.2.1 Decoupling Generation and Selection\nB.2.2 Context Pruning\nB.2.3 Prompt Templates\n\n\nB.3 Action Weighting\n\n\n\nC Experiment Details\n\nC.1 Benchmark Task Selection\nC.2 LLM-SR\n\nC.3 More KernelBench Results\n\nC.3.1 Kernel List\nC.3.2 Head to Head Comparison\n\n\n\n\n\nD Discovered Kernels\n\n\nD.1 BatchNorm\n\n\nD.2 Conv3d Divide Max GlobalAvgPool BiasAdd Sum\n\n\nD.3 Conv3d Max LogSumExp ReLU\n\n\nD.4 ConvTranspose2d BiasAdd Clamp Scaling Clamp Divide\n\n\nD.5 GELU\n\n\nD.6 Matmul with large K dimension\n\n\nD.7 Max pooling 2D\n\n\nD.8 MLP\n\n\nD.9 RMSNorm\n\n\nD.10 Softmax\n\n\nD.11 VGG16\n\n\nD.12 Mean Reduction over a dimension\n\n\nD.13 RNN\n\n\nD.14 BMM InstanceNorm Sum ResidualAdd Multiply\n\n\nD.15 AlexNet\n\nD.16 LayerNorm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits: Enabling Long-Horizon Progress-Aware Consistent Evolution\n\n\nMinghao Yan\n\n‚ÄÉ‚ÄÉ\nBo Peng\n\n‚ÄÉ‚ÄÉ\nBenjamin Coleman\n\n‚ÄÉ‚ÄÉ\nZiqi Chen\n\n‚ÄÉ‚ÄÉ\nZhouhang Xie\n\n‚ÄÉ‚ÄÉ\nZhankui He\n\n‚ÄÉ‚ÄÉ\nNoveen Sachdeva\n\n‚ÄÉ‚ÄÉ\nIsabella Ye\n\n‚ÄÉ‚ÄÉ\nWeili Wang\n\n‚ÄÉ‚ÄÉ\nChi Wang\n\n‚ÄÉ‚ÄÉ\nEd H. Chi\n\n‚ÄÉ‚ÄÉ\nWang-Cheng Kang\n\n‚ÄÉ‚ÄÉ\nDerek Zhiyuan Cheng\n\n‚ÄÉ‚ÄÉ\nBeidou Wang\n\n\n\nAbstract\nLarge Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process.\nWe identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits), a framework designed to robustly govern the agent‚Äôs context and search dynamics, to address these challenges.\nùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits combines hierarchical context management (ùô∑ùô≤ùôº\\mathop{\\mathtt{HCM}}\\limits) with pruning to address context pollution; momentum-based backtracking (ùôºùô±ùô±\\mathop{\\mathtt{MBB}}\\limits) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (ùô≤ùô¥\\mathop{\\mathtt{CE}}\\limits), allowing agents to balance internal refinement with cross-trajectory collaboration.\nWe demonstrate that ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, and surpassing the record on Modded NanoGPT.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) are increasingly used by evolutionary processes to optimize challenging scientific and engineering problems (Novikov et al., 2025; Romera-Paredes et al., 2024; Lange et al., 2024; Cheng et al., 2025b; Lange et al., 2025). They transform evolutionary search by replacing the rigid, random operators of classical algorithms (such as mutation and crossover) with intelligent, context-aware reasoning¬†(Novikov et al., 2025; Romera-Paredes et al., 2024; Lange et al., 2025). Unlike traditional Evolutionary Algorithms (EAs) that evaluate an extensive number of weakly-guided candidates (&gt;106&gt;10^{6} samples)¬†(Fogel, 1988; Holland, 1992), LLM-driven agents leverage in-context evolution history to perform iterative refinement¬†(Novikov et al., 2025). By treating the history as a dynamic knowledge base, these agents can theoretically learn from failures and perform meta-reasoning, shifting the paradigm toward sample-efficient, knowledge-guided optimization¬†(Zhai et al., 2025; Lange et al., 2025).\n\n\nOur work aims to use these intelligent search priors to unlock state-of-the-art performance in complex research and engineering tasks (Shojaee et al., 2024; Ouyang et al., 2025a; Jordan et al., 2024). However, this new LLM-in-the-loop paradigm introduces significant instability, preventing the search from consistently leveraging the LLM‚Äôs reasoning capabilities¬†(Xia et al., 2025; Kim et al., 2025).\nRather than steadily improving, these systems suffer from high variance (¬ß2), often failing to produce reliable improvements due to the combined stochasticity of the LLM and the search process¬†(Comanici et al., 2025; Renze, 2024).\n\n\nDespite many successes in applying LLM-based evolutionary search to diverse tasks¬†(Novikov et al., 2025; Lange et al., 2025), we lack a systematic and principled understanding of how to improve the evolution scaffold, often relying on ad hoc designs. In this work, we aim to answer the central research question:\nHow should we build an agent scaffold for an LLM-driven evolutionary search process?\n\n\nWe identify three core challenges that hinder the performance of modern LLM-assisted evolutionary agents (¬ß2):\nFirst, Context Pollution overwhelms the agent history with failed hypotheses due to reward sparsity¬†(Liu et al., 2025a), which degrades the quality of generated ideas¬†(Anthony et al., 2025; Zhu et al., 2025).\nSecond, Mode Collapse occurs when the agent fails to balance exploration and exploitation, leading to stagnation in local minima¬†(Zhang et al., 2025b).\nThird, Weak Collaboration hampers parallel search efficiency because current frameworks lack adaptive mechanisms to transfer knowledge among concurrent processes¬†(Romera-Paredes et al., 2024).\n\n\nFigure 1: We show the overall workflow of ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits. More details about each module can be found in Figure¬†2.\n\n\nFigure 2: This figure demonstrates the core components of ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits. We decouple idea generation from idea selection to enable easy hierarchical management of idea memory (¬ß3.1). We also design momentum-based self-adaptive backtracking (¬ß3.2) and crossover sampling mechanisms (¬ß3.3) to foster long-horizon reasoning in evolutionary search and escaping local minima.\n\n\nIn this paper, we introduce Progress-Aware Consistent Evolution (ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits) (Figure¬†1), a framework that addresses these challenges through a principled, systematic approach. ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits directly tackles the identified challenges via three components (Figure¬†2): First, we introduce a Hierarchical Context Management (ùô∑ùô≤ùôº\\mathop{\\mathtt{HCM}}\\limits) module to decouple idea generation from selection and mitigate Context Pollution by employing a hierarchical idea memory with context pruning (¬ß3.1). Second, we develop Momentum-based Backtracking (ùôºùô±ùô±\\mathop{\\mathtt{MBB}}\\limits) to combat Mode Collapse, providing a hard escape mechanism that enables the system to break free from local minima (¬ß3.2). Third, we develop Self-adaptive Collaborative Evolution Sampling (ùô≤ùô¥\\mathop{\\mathtt{CE}}\\limits) to resolve Weak Collaboration; this policy unifies parallel evolution processes by efficiently balancing internal backtracking (deep exploration) with external crossover (knowledge transfer) (¬ß3.3).\n\n\nWe empirically demonstrate that ùôøùô∞ùô≤ùô¥ùöüùöòùöïùöüùöé\\mathop{\\mathtt{PACEvolve}}\\limits achieves state-of-the-art results, significantly outperforming existing methods on a diverse suite of benchmarks, including Symbolic Regression (LLM-SR), KernelBench, and Modded NanoGPT. Our contributions are summarized as follows:\n\n\n‚Ä¢\n\nWe introduce Hierarchical Context Management (ùô∑ùô≤ùôº\\mathop{\\mathtt{HCM}}\\limits), a mechanism that decouples idea generation from selection and applies context pruning. This addresses the challenge of context pollution by ensuring the agent maintains a high signal-to-noise ratio in its evolutionary history, incentivizing diverse idea generation.\n\n\n\n‚Ä¢\n\nWe develop a unified search control policy enabling Momentum-Based Backtracking (ùôºùô±ùô±\\mathop{\\mathtt{MBB}}\\limits) and Self-Adaptive Collaborative Evolution (ùô≤ùô¥\\mathop{\\mathtt{CE}}\\limits). By monitoring search momentum, this policy dynamically balances the trade-off between deep internal exploration (via backtracking) and external knowledge transfer (via crossover), effectively preventing mode collapse across parallel search processes.\n\n\n\n‚Ä¢\n\nWe demonstrate state-of-the-art empirical performance, significantly outperforming existing methods on diverse and complex benchmarks, including Symbolic Regression (LLM-SR) and KernelBench, and surpassing prior records on Modded NanoGPT.\n\n\n\n\n\n\n\n2 Motivation\n\nTraditional evolutionary algorithms rely on a fixed set of operators, such as mutation and crossover. In contrast, LLM-based search can perform intelligent, context-aware operations, rewriting entire solutions based on a rich prompt that includes past experimental history.\n\n\nExisting evolutionary agent scaffolds follow an execution-and-reflection paradigm, in which an LLM operates a closed loop comprising idea sampling, execution, feedback collection, and reflection¬†(Novikov et al., 2025; Lange et al., 2025).\nThough promising, evolutionary agents still yield sub-optimal performance when applied to critical scientific and coding challenges, such as symbolic regression¬†(Shojaee et al., 2024) and kernel design¬†(Ouyang et al., 2025a; Liao et al., 2025). As an example, consider the three independent evolution trajectories on symbolic regression in Figure¬†3. We observe that if the evolutionary search cannot quickly find a low-NMSE solution, it is unlikely to discover better solutions later. We hypothesize this is due to summarized experiment"
  },
  {
    "title": "Multi-Property Synthesis",
    "url": "https://arxiv.org/abs/2601.10651v1",
    "source": "arxiv",
    "summary": "We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boo",
    "full_text": null
  },
  {
    "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs",
    "url": "https://arxiv.org/abs/2601.10645v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\t",
    "full_text": null
  },
  {
    "title": "Adjusted Similarity Measures and a Violation of Expectations",
    "url": "https://arxiv.org/abs/2601.10641v1",
    "source": "arxiv",
    "summary": "Adjusted similarity measures, such as Cohen's kappa for inter-rater reliability and the adjusted Rand index used to compare clustering algorithms, are a vital tool for comparing discrete labellings. These measures are intended to have the property of 0 expectation under a null distribution and maximum value 1 under maximal similarity to aid in interpretation. Measures are frequently adjusted with ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 General Adjustment for Chance\n\n2.1 Adjustment of an Index\n2.2 Common Null Models and Normalizations\n2.3 Impacts of the Choice of Generalized Normalization Sm‚Äãa‚ÄãxnS_{max}^{n}\n\n\n\n3 General Adjustment as an Operator\n\n3.1 Favorable Properties Through Linear Families\n3.2 Violations of Intended Properties\n3.3 Examples of Violation\n\n\n\n4 Conclusion\n\n4.1 Future Work\n\n\nA Proposition 3.7 parts (3.), (4.), and (5.) allowing different conventions cc for when the maximal index value is expected.\n\n\n\n\n\nAdjusted Similarity Measures and a Violation of Expectations\n\n\nWilliam L. Lippitt and Edward J. Bedrick and Nichole E. Carlson\n\n\n\nAbstract\nAdjusted similarity measures, such as Cohen‚Äôs kappa for inter-rater reliability and the adjusted Rand index used to compare clustering algorithms, are a vital tool for comparing discrete labellings. These measures are intended to have the property of 0 expectation under a null distribution and maximum value 1 under maximal similarity to aid in interpretation. Measures are frequently adjusted with respect to the permutation distribution for historic and analytic reasons. There is currently renewed interest in considering other null models more appropriate for context, such as clustering ensembles permitting a random number of identified clusters. The purpose of this work is two‚Äîfold: (1) to generalize the study of the adjustment operator to general null models and to a more general procedure which includes statistical standardization as a special case and (2) to identify sufficient conditions for the adjustment operator to produce the intended properties, where sufficient conditions are related to whether and how observed data are incorporated into null distributions. We demonstrate how violations of the sufficient conditions may lead to substantial breakdown, such as by producing a non-positive measure under traditional adjustment rather than one with mean 0, or by producing a measure which is deterministically 0 under statistical standardization.\n\n\nKeywords: similarity indices, correction for chance, Cohen‚Äôs kappa, adjusted Rand index, statistical standardization\n\n\n\n1 Introduction\n\nSuppose we have two categorical variables we wish to compare, such as diagnostic assessments using two different assessment techniques or a ground truth labelling versus labels generated through unsupervised clustering or predictive modelling. We might use an index, that is a function of the joint empirical distribution of the two variables, to quantify their similarity. A plethora of indices have been proposed in the literature for different contexts, such as Cohen‚Äôs\nŒ∫\\kappa\n[Cohen, 1960] assessing agreement when variables have the same set of possible values, variations of the Rand index [Rand, 1971, Hubert and Arabie, 1985] and Mutual Information [Shannon, 1948, Romano et al., 2016, 2014] when variables assign cluster membership, and Goodman and Kruskal‚Äôs Œ≥\\gamma [Goodman and Kruskal, 1979] measuring rank correlation when variables are ordinal. Efforts have been made to understand relationships between indices and their properties [Albatineh et al., 2006, meil«é2005comparing, Warrens, 2013, Brusco et al., 2021, Arinik et al., 2021] in support of identifying redundancies and unwanted behavior as well as matching applied contexts with appropriate indices.\n\n\nOf long-standing interest is the adjustment of indices for ‚Äòrandom chance.‚Äô This is accomplished through subtraction of an expected value with respect to a null model used to describe expected similarity due to ‚Äòrandom chance‚Äô followed by normalization by a maximum [Scott, 1955, Cohen, 1960, Hubert and Arabie, 1985, Krippendorff, 1987, Davenport Jr and El-Sanhurry, 1991, Albatineh et al., 2006, Vinh et al., 2009, Warrens, 2013]. Adjusted indices are thus designed to take values near 0 when compared variables are unassociated and maximal, optimal value 1, properties which support ease of interpretation and comparison [Vinh et al., 2009, Albatineh et al., 2006, Warrens, 2013]. Adjustment thus requires specification of a null model and a normalization.\n\n\nIt is standard to consider the permutation model, also known as the hypergeometric model, as the null model for random chance [Hubert and Arabie, 1985, Albatineh et al., 2006, Vinh et al., 2009, Romano et al., 2016]. Other models have been considered for specific contexts [Jeub et al., 2018, Sundqvist et al., 2023, Cohen, 1960, Krippendorff, 1987, Scott, 1955, Li et al., 2024], such as multinomial distributions featuring independence of variables and clustering ensembles allowing variation in the number of clusters. More broadly, Krippendorff [1987] elucidates how different modelling assumptions lead to substantially different interpretations for what an index measures and Gates and Ahn [2017] show the null model used in adjustment can have a substantial impact on conclusions. As such, null models must be chosen carefully for context.\n\n\nIn this work, we generalize foundational studies of the properties of the operation of adjustment for chance [Albatineh et al., 2006, Warrens, 2013] which traditionally focused on the permutation and related models and specific normalizations. We consider a generalized adjustment with respect to any null model and valid choice of normalization. Generalized adjustment thus also covers statistical standardization, recently considered by Romano et al. [2014, 2016] for clustering, as a special case.\n\n\nThrough generalized study of adjustment, we contribute the following:\n\n\n‚Ä¢\n\nsufficient conditions for the adjustment operator to exhibit desired properties, such as:\n\n\n‚Äì\n\nadjusted indices having mean 0 under the null distribution,\n\n\n\n‚Äì\n\nstandardized indices also having variance 1 under the null distribution, and\n\n\n\n‚Äì\n\nthe adjustment operator being idempotent (no need for repeated adjustment);\n\n\n\n\n\n\n‚Ä¢\n\ncharacterization of sufficient conditions in terms of whether and how the null distribution is data-driven; and\n\n\n\n‚Ä¢\n\ndemonstration of breakdown of desired properties when sufficient conditions are not met using a classic data-driven null model.\n\n\n\nIn this way, we provide concrete guidelines for future methodological research into adjustment of indices using more general models by providing clear conditions under which favorable properties are guaranteed and highlighting the need for a more careful theoretical treatment when those conditions are not met.\n\n\nThe paper is organized as follows. We formally introduce indices and discuss the general procedure for adjusting indices with respect to a general normalization and null model in Section 2. Then we extend previous work on families of indices under adjustment, identifying sufficient conditions for favorable properties in Section 3. We provide two example index-model pairs in violation of the sufficient conditions which respectively are (a) non-positive after traditional adjustment for expectation and maximum rather than mean 0 and (b) identically 0 after statistical standardization. Section 4 contains the discussion.\n\n\n\n\n2 General Adjustment for Chance\n\nSuppose we have two categorical variables we wish to compare where variable labels may or may not coincide. We might use an index to quantify similarity. Formally:\n\n\n\nDefinition 2.1 (Index SS).\n\n\nLet xx and yy be two categorical variables observed for the same k=1,‚Ä¶,Nk=1,...,N observations such that each xkx_{k} takes one of II possible values and each yky_{k} takes one of JJ possible values. Define marginal counts u,vu,v with ui=#‚Äã{k:xk=i},vj=#‚Äã{k:yk=j}u_{i}=\\#\\{k:x_{k}=i\\},v_{j}=\\#\\{k:y_{k}=j\\}, and joint count nn with ni‚Äãj=#‚Äã{k:xk=i,yk=j}n_{ij}=\\#\\{k:x_{k}=i,y_{k}=j\\}. An index SS for xx and yy is a real-valued function S‚Äã(n)S(n).\n\n\n\nBroadly, an index SS is a real function of contingency tables, that is joint count distributions nn, sometimes for nn of a specific size (e.g., I=J=2I=J=2 for comparing binary variables) and sometimes of any size (e.g., I,J‚àà‚ÑïI,J\\in\\mathbb{N} for comparing cluster labels) [Warrens, 2013]. In practice, the index SS is desired to monotonically measure strength of similarity in some sense between xx and yy [Krippendorff, 1987]. We give two examples which give rise to many popular measures through adjustment:\n\n\n\nDefinition 2.2 (Raw proportion pp of agreement).\n\n\nIn the context of Definition 2.1, suppose xx and yy take values in the same indexed set of I=JI=J possible values. The raw proportion pp of agreement between xx and yy is given by p‚Äã(n)=‚àëini‚ÄãiN.p(n)=\\sum_{i}\\frac{n_{ii}}{N}.\n\n\n\n\nDefinition 2.3 (Proportion qq of pair agreement).\n\n\nWith the notation of Definition 2.1, let ww be a count of a variable observed NN times taking values in a set ‚Ñõ\\mathcal{R} (e.g., w=uw=u and ‚Ñõ={1,‚Ä¶,I}\\mathcal{R}=\\{1,...,I\\} or w=nw=n and ‚Ñõ={1,‚Ä¶,I}√ó{1,‚Ä¶,J}\\mathcal{R}=\\{1,...,I\\}\\times\\{1,...,J\\}). The proportion qq of pairs of observations assigned the same value is given by q‚Äã(w)=‚àër‚àà‚Ñõ(wr2)(N2).q(w)=\\frac{\\sum_{r\\in\\mathcal{R}}\\binom{w_{r}}{2}}{\\binom{N}{2}}\\ .\n\n\n\nWhile many similarity measures for continuous variables take expected value 0 in the presence of no association, this often is not the case for basic indices assessing shared information or association between categorical variables. In the cases of p‚Äã(n)p(n) and q‚Äã(n)q(n), values of 0 indicate observations generally unlikely to have resulted at random from independent variables xx and yy. As such, these indices are typically adjusted through subtraction of an expected value and normalization by a maximum value to impose the properties that 0 indicates a level of observed association expected from unassociated variables and 1 indicates maximal association observable. In this way, more familiar association measures can be built, such as Cohen‚Äôs Œ∫\\kappa [Cohen, 1960], Scott‚Äôs œÄ\\pi [Scott, 1955], and Dice‚Äôs coefficient [Dice, 1945] from p‚Äã(n)p(n) and the adjusted Rand index A‚ÄãR‚ÄãIARI [Hubert and Arabie, 1985] and other pair-counting measures for clustering [Albatineh et al., 2006] from q‚Äã(w)q(w).\n\n\n\n2.1"
  },
  {
    "title": "STEM: Scaling Transformers with Embedding Modules",
    "url": "https://arxiv.org/abs/2601.10639v1",
    "source": "arxiv",
    "summary": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\nMixture-of-Experts (MoE).\nHash-layer Mixture-of-Experts.\nScaling the number of experts.\nPer Layer Embedding.\n\n\n\n3 Method\n\n3.1 STEM\n\n3.2 Insights\n\nKey‚Äìvalue memory view of FFNs.\nSTEM design choice.\n3.2.1 Better Information Storage Capacity\n3.2.2 Knowledge Specificity &amp; Interpretability\n\n3.2.3 Efficiency\n\nTraining efficiency.\nInference efficiency.\n\n\n\n3.2.4 VRAM and Communication Savings\n\nPrefetching cost.\n\n\n3.2.5 Context-length Adaptive Parameter Usage\n\n\n3.3 Knowledge Editing with STEM\n3.4 System Implementation\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setting\n\nDatasets.\nModels.\nEvaluations.\nTraining details.\n\n\n4.2 Experimental Results\n4.3 Downstream Evaluation Results\n\n4.4 Ablation Studies\n\n4.4.1 Impact of STEM Layer Count\n4.4.2 Impact of STEM Placement\n4.4.3 Up-projection with additive embedding\n\n\n\n\n\n5 STEM Characteristics\n\n5.1 Large Angular Spread of STEM Embeddings\n5.2 Interpretability of STEM Models\n\n\n6 Related Works\n7 Conclusion\n\n8 Appendix\n\n8.1 Additional Benchmarks\n8.2 Additional Long-context Evaluation\n\n8.3 Additional Architecture Ablation Study\n\n8.3.1 STEM‚Ä†\n\n\n\n\n\n\n\n\n\n\n‚Ä†]Carnegie Mellon University\n¬ß]Meta AI\n\\contribution[‚Ä°]Work done during internship at Meta AI\n\\contribution[¬∂]Work done at Meta AI\n\\contribution[*]Co-supervisors\n\\contact\\contact\\contact\\contact\\contact\\contact\\contact\\contact\n\nSTEM: Scaling Transformers with Embedding Modules\n\n\nRanajoy Sadhukhan\n\n‚ÄÉ‚ÄÉ\nSheng Cao\n\n‚ÄÉ‚ÄÉ\nHarry Dong\n\n‚ÄÉ‚ÄÉ\nChangsheng Zhao\n\n‚ÄÉ‚ÄÉ\nAttiano Purpura-Pontoniere\n\n‚ÄÉ‚ÄÉ\nYuandong Tian\n\n‚ÄÉ‚ÄÉ\nZechun Liu*\n\n‚ÄÉ‚ÄÉ\nBeidi Chen*\n\n[\n\n[\n\nrsadhukh@andrew.cmu.edu\n\nrick.caos@gmail.com\n\nharryd@andrew.cmu.edu\n\ncszhao@meta.com\n\nattiano@meta.com\n\nyuandong.tian@gmail.com\n\nzechunliu@meta.com\n\nbeidic@andrew.cmu.edu\n\n\n\nAbstract\nFine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity.\nMore interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation.\nIn addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ‚àº\\sim3‚Äì4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while\nproviding better interpretability, better training stability and improved efficiency.\n\n\n\\metadata\n[Github]https://github.com/Infini-AI-Lab/STEM\n\\metadata[Website]https://infini-ai-lab.github.io/STEM\n\n\n\n1 Introduction\n\n\n\n\n(a) \n\n\n\n\n\n(b) \n\n\n\n\n\n(c) \n\n\n\nFigure 1: (a) Validation PPL vs. training tokens for 1B STEM vs. dense; (b) Needle-in-a-Haystack at 8k/16k/32k; (c) STEM layer: embedding tables offloaded to CPU and token-indexed ones are prefetched to GPU.\n\n\nSparse computation is a key mechanism for realizing the benefits predicted by parameter-scaling laws (kaplan2020scalinglawsneurallanguage; hoffmann2022trainingcomputeoptimallargelanguage) without proportionally increasing per-token compute. In particular, Mixture-of-Experts (MoE) (shazeer2017outrageouslylargeneuralnetworks; artetxe2022efficientlargescalelanguage; fedus2022switchtransformersscalingtrillion) models have been adopted in several frontier LLMs (qwen3technicalreport; qwen3max; dai2024deepseekmoeultimateexpertspecialization) because they raise parametric capacity at roughly constant activated FLOPs by sparsely activating a small subset of experts per token. Recent work (boixadsera2025powerfinegrainedexpertsgranularity; he2024mixturemillionexperts; dbrx2024; dai2024deepseekmoeultimateexpertspecialization) further advocate for finer-grained sparsity that employs large number of micro-experts to achieve better expressivity, enhanced knowledge storing capacity, and favorable efficiency metrics.\n\n\nHowever, finer granularity introduces nontrivial challenges in both optimization and systems. On the training side, even large fraction of experts can remain under-trained (huang2025ultrasparsememorynetwork) due to a highly non-uniform routing and result in training instability. While load-balancing objectives (shazeer2017outrageouslylargeneuralnetworks; fedus2022switchtransformersscalingtrillion; lepikhin2020gshardscalinggiantmodels) can address these issues, they may interfere with the primary objective if not carefully tuned (dai2024deepseekmoeultimateexpertspecialization; qiu2025demonsdetailimplementingload; go2025moetuneroptimizedmixtureexpert). On the systems side, increasing the number of experts typically raises the number of all-to-all messages while shrinking message sizes, degrading bandwidth utilization and amplifying communication overhead (huang2024toward; li2025speculativemoecommunicationefficient). Finer granularity can also reduce parameter-access locality and degrade kernel efficiency when expert subnetworks become too small for dense linear-algebra kernels to reach high occupancy, yielding suboptimal end-to-end performance. Finally, these large-scale fine-grained sparse networks are far from interpretable. It is difficult to understand the roles of each micro-expert.\nTo harness the full potential of fine-grained sparsity, we require: (a) stable optimization, (b) broad expert utilization (each micro-expert learns useful representations), and (c) negligible expert-retrieval latency and communication overhead. Additionally, we would like our sparse architecture to be (e) more interpretable and ensure that we are using all the micro-experts in a more transparent manner.\n\n\nWe identify static sparsity as a potential solution to achieve these desired properties.\nStatic sparsity keeps the compute path predictable (no runtime routing latency), enables prefetch and CPU offloading (removing the need for inter-node communication).\nRecently, static sparsity via token-indexed routing has emerged as a promising direction (roller2021hashlayerslargesparse; gemma3n2024) with strong performance guarantees. Additionally, the token-indexed nature makes this static sparsity more interpretable as each micro-expert can correspond to a given token ID.\nHowever, such token-based selection strategy lacks context adaptivity. If applied naively, it can reduce the expressivity of the model and degrade quality despite more parameters. Our ablation study in sec. 4.4.2 highlights the criticality of selecting the suitable module for sparsification.\n\n\nBased on these observations, we introduce STEM, a static, token-indexed, fine-grained mechanism that replaces only the up-projection in gated FFNs with a token-specific vector retrieved from a layer-local embedding table. The gating and down-projection paths are preserved and shared across tokens. We observe that STEM achieves,\n\n\nBetter Training Stability: Despite being extremely sparse, STEM does not exhibit any training instability issues as usually seen in MoE models. Figure 5(a) shows that unlike MoE models, STEM does not exhibit any loss spikes.\n\n\nImproved Performance with Larger Knowledge Capacity: STEM learns a representation space for the embeddings that is conducive to better information storage. The learned embeddings exhibit a large angular spread (i.e., low pairwise cosine similarity), which reduces representational interference and improves addressability of the parametric memory. As a result, it effectively increases the distinct ‚Äúslots‚Äù available for storing and retrieving information. In our downstream evaluation benchmark, STEM consistently outperforms the dense baseline on knowledge-intensive tasks like, ARC-Challenge (allenai:arc), and OpenBookQA (OpenBookQA2018) by large margins (‚àº\\sim9‚Äì10%) and the improvement usually increases with more STEM layer inclusion.\n\n\nInterpretability features: Because each STEM embedding in every layer is tied to a specific token ID, individual ‚Äúmicro-experts‚Äù have clear, token-level semantics. This structure not only makes their role more interpretable, but also gives STEM models a surprising degree of controllability: by simply swapping the STEM table index (illustrated in Fig. 3) while leaving the input text unchanged, we can systematically steer the model‚Äôs output distribution. Such interventions highlight how much factual knowledge is localized in these embeddings and how modular, editable, and attributable that knowledge becomes.\n\n\nImproved Long-context Inference: During long-context inference, STEM activates more distinct parameters as sequence length grows, yielding test-time capacity scaling. As shown in Figure 1(b), the benefits strengthen with context: on Needle-in-a-Haystack (NIAH) (kamradt_needle_2024), the gap over the dense baseline increases from 8.4% to 13%.\n\n\nTraining and Inference-time efficiency: STEM reduces both FLOPs as well as parameter loading cost by eliminating one-third of the parameters in FFN layers. Consequently, it is strictly more efficient during both computation-intensive training and prefilling, as well as in memory-intensive decoding.\n\n\nWe benc"
  },
  {
    "title": "Classification Imbalance as Transfer Learning",
    "url": "https://arxiv.org/abs/2601.10630v1",
    "source": "arxiv",
    "summary": "Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generati",
    "full_text": null
  },
  {
    "title": "Parametric RDT approach to computational gap of symmetric binary perceptron",
    "url": "https://arxiv.org/abs/2601.10628v1",
    "source": "arxiv",
    "summary": "We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \\emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \\emph{satisfiability} ($Œ±_c$) -- \\e",
    "full_text": null
  },
  {
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "url": "https://arxiv.org/abs/2601.10611v1",
    "source": "arxiv",
    "summary": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstrea",
    "full_text": null
  },
  {
    "title": "Procedural Fairness in Multi-Agent Bandits",
    "url": "https://arxiv.org/abs/2601.10600v1",
    "source": "arxiv",
    "summary": "In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equa",
    "full_text": "\n\n\n\n1 Introduction\n2 Multi-Agent Multi-Armed Bandits\n3 Related Work\n\n4 Fairness in Multi-Agent Multi-Armed Bandits\n\n4.1 Procedural Fairness\n4.2 Equality Fairness\n4.3 Utilitarian Fairness\n\n\n\n5 Algorithms\n\n5.1 Procedural Fairness\n5.2 Equality Fairness\n5.3 Utilitarian Fairness\n\n\n\n6 Theoretical Results\n\n6.1 Impossibility Results\n6.2 Procedural Fairness and the Core\n6.3 Lack of Pareto Dominance Between Fairness Concepts\n\n\n\n7 Experiments\n\n7.1 Real World Example\n\n\n\n8 Discussion &amp; Conclusions\n\n8.1 Fairness as Legitimacy\n8.2 Tradeoffs as Design Choices\n8.3 Robustness of Procedural Fairness\n8.4 Broader Applications of Procedural Fairness\n8.5 Future Work\n\n\n\nA Examples\n\nA.1 Fair Policies\nA.2 Procedural Score\n\n\n\nB Theoretical Results\n\nB.1 Hoeffding Mean Concentration\nB.2 Uniform-in-Time Hoeffding Mean Concentration\nB.3 Randomized Exploration Bound\nB.4 Arm Counts and Adjustment Bounds\nB.5 Exploitation Phase Bound\nB.6 Full Procedural Fairness Regret Bound\nB.7 Procedural Fairness Guarantees\nB.8 Fractional Pull Bound\nB.9 Equality Fairness Bound\nB.10 Utilitarian Fairness Bound\nB.11 Procedural Fairness and Equality Fairness Impossibility\nB.12 Procedural Fairness and Utilitarian Fairness Impossibility\nB.13 Utility-Based Nash Welfare Not In Procedural Core\nB.14 Procedural Fairness Implies Procedural Core\nB.15 Pareto Incomparability\n\n\n\n\n\n\n\nProcedural Fairness in Multi-Agent Bandits\n\n\nJoshua Caiata\n\nUniversity of Waterloo\n\n\nCarter Blair\n\nHarvard University\n\n\nKate Larson\n\nUniversity of Waterloo\n\n\n\nAbstract\nIn the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper\nargues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.\n\n\n\n1 Introduction\n\nFrom the Magna Carta to the words that open constitutions and charters around the world, we have long understood that dignity and fairness require more than simply providing a good outcome. To be fair requires treating each person as an equal, entitled to a voice in the decisions that govern their lives. Yet, in the multi-agent systems we build today, this truth is too often forgotten [15, 13, 14]. Fairness is almost always reduced to optimizing for a specific outcome: the sum of utilities, the balancing of welfare, or the smoothing of inequality, echoing the consequentialist tradition of judging actions by their aggregate results [26, 27]. While these notions of fairness may provide elegance and tractability, they miss the very essence of what we consider to be fair. They consider what is gained by a set of decisions, not how they were decided. This is an imposition of values from outside of the system, rather than respecting the very agency from within.\n\n\nThis paper begins from a new conviction, that fairness in multi-agent systems must be grounded not in optimal outcomes, but in the principle of equal voice. To guarantee this is to honour the dignity of participation in multi-agent decision-making; to ignore it is to risk building systems that sacrifice legitimacy, that is, whether the decisions themselves are perceived as rightful and acceptable, for the sake of efficiency. This principle reflects a contractualist view of fairness, which holds that a decision is only legitimate if it cannot be reasonably rejected by those subject to it [23]. We call this principle procedural fairness.\n\n\nThis moral insight is not merely philosophical. Extensive evidence in psychology and economics shows that people consistently value fair process‚Äìeven if that means outcomes are less than ideal [1, 16, 28]. For example, Lind and Tyler [16] recount an example of a woman whose traffic ticket case was dismissed; however, she still left the courtroom angry because she felt she had compelling evidence and the judge never heard her argument. In fact, many people reported feeling the same way, despite being handed the best possible outcome from the court. This same dynamic appears in collective allocation systems like participatory budgeting, where communities not only want good projects but a voice in which projects are chosen.\n\n\nYet existing approaches in multi-agent learning overwhelmingly reduce fairness to outcomes such as utilitarianism, Nash welfare, or inequality. While they capture important values, they impose fairness as an external criterion rather than letting it arise from the agents themselves. What is missing in the literature is a framework that gives agents themselves an equal share of decision-making power. Inspired by Rawls‚Äô notion of pure procedural justice [22], we formalize procedural fairness in MA-MABs, a framework where each action (pulling an arm) produces potentially different rewards for each agent, sampled from potentially different distributions. This framework naturally captures both the allocation of benefits and the distribution of decision-making power in a simple and easy-to-understand way.\n\n\nTo situate procedural fairness, we compare it, both theoretically and empirically, with two other notions of fairness in multi-agent systems: equality fairness, where outcomes are distributed so that agents receive as equal outcomes as possible, and utilitarian fairness: decisions maximize aggregate welfare, prioritizing total benefit.\n\n\nOur central claim is that procedural fairness deserves recognition alongside traditional notions of fairness like Nash welfare, inequality, and utilitarianism, not as an alternative, but as a principle of legitimacy. We now outline our main contributions:\n\n\n\n\n‚Ä¢\n\nWe define procedural fairness formally in MA-MABs, and compare it to utilitarian, equality fairness, and Nash welfare.\n\n\n\n‚Ä¢\n\nWe prove impossibility results: fairness notions are fundamentally incompatible, showing that fairness requires normative choices.\n\n\n\n‚Ä¢\n\nWe design algorithms for learning fair policies with sublinear regret guarantees.\n\n\n\n‚Ä¢\n\nWe show that procedurally fair policies lie in the core, ensuring stability against coalitional deviation.\n\n\n\n‚Ä¢\n\nWe empirically evaluate our methods across a variety of settings, and show that procedural fairness balances efficiency and equality while preserving legitimacy.\n\n\n\n\n\n\n\n2 Multi-Agent Multi-Armed Bandits\n\nLet NN represent the number of agents in a multi-arm multi-agent bandit setting, and let ùí¶={1,‚Ä¶,K}\\mathcal{K}=\\{1,...,K\\} be the set of arms where K=|ùí¶|K=|\\mathcal{K}| represents the number of arms. Further, let P=(p1,‚Ä¶,pK)P=(p_{1},\\ldots,p_{K}) represent a policy where each element, pkp_{k}, represents the probability that arm k‚ààùí¶k\\in\\mathcal{K} is pulled in any given turn. Note that 0‚â§pk‚â§1,‚àÄpk‚ààP0\\leq p_{k}\\leq 1,\\forall p_{k}\\in P, and that ‚àëi=1Kpi=1\\sum_{i=1}^{K}p_{i}=1. We, at times, abuse notation and refer to KK as the set of arms.\n\n\nWhen an arm is pulled, all agents receive some reward drawn from a distribution. Agents will not necessarily receive the same reward, and distributions may vary from agent to agent and from arm to arm. We let Œº‚àó‚àà‚ÑùN√óK\\mu^{*}\\in\\mathbb{R}^{N\\times K} represent the agents‚Äô true reward means, where Œºi,k‚àó\\mu^{*}_{i,k} represents the mean reward agent ii receives when arm kk is pulled. Additionally, we let Œº^t\\hat{\\mu}^{t} denote the agents‚Äô reward estimates at time tt, where Œº^i,kt\\hat{\\mu}^{t}_{i,k} is the estimate at time tt of the reward agent ii receives when arm kk is pulled. We let œÉi,k\\sigma_{i,k} represent the standard deviation of rewards agent ii receives when arm kk is pulled.\nFinally, let FiF_{i} be the set of agent ii‚Äôs favourite arms, Fi={j‚ààùí¶‚à£Œºi,j‚àó=maxk‚ààùí¶‚Å°Œºi,k‚àó}F_{i}=\\{j\\in\\mathcal{K}\\mid\\mu^{*}_{i,j}=\\max_{k\\in\\mathcal{K}}\\mu^{*}_{i,k}\\} where Œºi,j‚àó\\mu^{*}_{i,j} represents the reward agent ii receives when arm jj is pulled.\n\n\nIn all instances, we assume that each true reward mean is strictly bounded between 0 and 1, i.e., 0&lt;Œºi,k‚àó&lt;1,‚àÄi‚àà{1,‚Ä¶,N},‚àÄk‚àà{1,‚Ä¶,K}0&lt;\\mu^{*}_{i,k}&lt;1,\\quad\\forall i\\in\\{1,\\dots,N\\},\\ \\forall k\\in\\{1,\\dots,K\\}, and that drawn rewards are in [0,1][0,1].\n\n\nWe next define the following concepts since they are foundational for our framework.\n\n\n\nDefinition 1 (Utility).\n\n\nThe utility of an agent i under a policy P=(p1,p2,..,pk)P=(p_{1},p_{2},..,p_{k}) is defined as the agent‚Äôs expected utility Ui‚Äã(P)=‚àëk=1Kpk‚ÄãŒºi,k‚àóU_{i}(P)=\\sum_{k=1}^{K}p_{k}\\mu^{*}_{i,k}.\n\n\n\n\nDefinition 2 (Decision Share).\n\n\nThe decision share of agent ii under a policy P=(p1,‚Ä¶,pK)P=(p_{1},\\dots,p_{K}) is defined as the total probability assigned to the agent‚Äôs favourite arm(s): D‚ÄãSi‚Äã(P)=‚àëk‚ààFipkDS_{i}(P)=\\sum_{k\\in F_{i}}p_{k}, where FiF_{i} is the set of agent ii‚Äôs favorite arms, defined as Fi={j‚ààùí¶‚à£Œºi,j‚àó=maxk‚ààùí¶‚Å°Œºi,k‚àó}F_{i}=\\{j\\in\\mathcal{K}\\mid\\mu^{*}_{i,j}=\\max_{k\\in\\mathcal{K}}\\mu^{*}_{i,k}\\}.\n\n\n\n\nDefinition 3 (Utility-Based Nash Welfare).\n\n\nThe utility-based Nash welfare is defined similarly to [13]: ‚àèi=1N‚àëk=1Kpk‚ÄãŒºi,k‚àó\\prod_{i=1}^{N}\\sum_{k=1}^{K}p_{k}\\mu^{*}_{i,k} where pk‚àà(p1,‚Ä¶,pK)p_{k}\\in(p_{1},...,p_{K}), which is some policy. This follows the traditional notion of Nash welfare found in the literature.\n\n\n\n\nDefinition 4 (Decision-Share-Based Nash Welfare).\n\n\nNash welfare is defined as ‚àèi=1N‚àëk‚ààFipk\\prod_{i=1}^{N}\\sum_{k\\in F_{i}}p_{k} wher"
  },
  {
    "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
    "url": "https://arxiv.org/abs/2601.10591v1",
    "source": "arxiv",
    "summary": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of ",
    "full_text": null
  },
  {
    "title": "Be Your Own Red Teamer: Safety Alignment via Self-Play and Reflective Experience Replay",
    "url": "https://arxiv.org/abs/2601.10589v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have achieved remarkable capabilities but remain vulnerable to adversarial ``jailbreak'' attacks designed to bypass safety guardrails. Current safety alignment methods depend heavily on static external red teaming, utilizing fixed defense prompts or pre-collected adversarial datasets. This leads to a rigid defense that overfits known patterns and fails to generalize to",
    "full_text": null
  },
  {
    "title": "Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders",
    "url": "https://arxiv.org/abs/2601.10588v1",
    "source": "arxiv",
    "summary": "Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistenc",
    "full_text": "\n\n\n\n\n\nSearching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders\n\n\nI. K. Kominis\n\nikominis@uoc.gr\n\nSchool of Science, Zhejiang University of Science and Technology, Hangzhou 310023, China\n\nDepartment of Physics and Institute of Theoretical and Computational Physics, University of Crete, Heraklion 70013, Greece\n\n‚ÄÉ‚ÄÉ\nC. Xie\n\nCollege of Life and Environmental Sciences, Hangzhou Normal University, Hangzhou 311121, China\n\n‚ÄÉ‚ÄÉ\nS. Li\n\nSchool of Automation and Electrical Engineering, Zhejiang University of Science and Technology, Hangzhou 310023, China\n\n‚ÄÉ‚ÄÉ\nM. Skotiniotis\n\nQuantum Thermodynamics and Computation Group, Departamento de Electromagnetismo y F√≠sica de la Materia, Universidad de Granada, 18071 Granada, Spain\n\nInstituto Carlos I de F√≠sica Te√≥rica y Computacional, Universidad de Granada, 18071 Granada, Spain\n\n‚ÄÉ‚ÄÉ\nG. P. Tsironis\n\nDepartment of Physics and Institute of Theoretical and Computational Physics, University of Crete, Heraklion 70013, Greece\n\nJohn A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA 02138, USA\n\n\n\nAbstract\nWhether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.\n\n\nThe possibility that quantum effects play a functional role in the brain has long been a subject of debate [1]. Much of this discussion emphasized the (non)-sustainability of long-lived quantum coherence or entanglement in neural systems under physiological conditions [2, 3]. In recent years, related discussions have resurfaced [4, 5], motivated in part by growing evidence that quantum coherence can survive in biological processes, most notably in photosynthetic light harvesting and magnetoreception [6, 7, 8], and can confer functional advantages [9, 10]. These developments, together with the extreme multiscale complexity of neural systems and the persistent challenge of explaining consciousness in physical terms [11, 12, 13], have renewed interest in whether analogous mechanisms could arise in the brain [14, 15, 16, 17, 18, 19]. Nonetheless, readily testable proposals remain scarce.\n\n\nHere we depart from the traditional strategy of seeking quantum coherence in specific biophysical substrates. Instead, we adopt an information-theoretic perspective inspired by recent developments in machine learning [20, 21] and quantum machine learning [22, 23, 24], shifting the focus from microscopic physical realizations to the structure, efficiency, and cost of information processing itself. The extreme complexity of the brain, together with the vast volume of information it must process under strict energetic constraints [25], strongly suggests the necessity of efficient mechanisms for compression and representation. In machine learning, autoencoders provide a standard framework for formalizing such compression, mapping high-dimensional inputs into a low-dimensional latent space from which the original inputs are reconstructed. The requirement of faithful reconstruction forces the latent variables to distill the essential structure of the input data [30]. If neural systems were to exploit quantum effects at all, it would be plausible for them to arise in encoding and compression tasks, where quantum thermodynamic resources might offer advantages to the management of information, energy and entropy bottlenecks [31, 32].\n\n\nFigure 1: (a) An autoencoder maps high-dimensional inputs xx to a low-dimensional latent representation and reconstructs outputs yy through multiple decoder settings Œ∏\\theta. The latent variables are treated as unobserved degrees of freedom probed indirectly via observable decoding marginals. Nonclassicality in the latent space reflects the impossibility to account for all marginal output statistics (different decoder settings Œ∏\\theta) with a unique positive latent distribution. (b) Analogy between Bell nonlocality tests and nonclassicality tests in the latent space of autoencoders. \n\n\nIn this work we introduce a purely statistical test of nonclassicality for latent representations learned by autoencoders, and address the question: given a collection of observable reconstruction statistics obtained under different decoder settings, can all corresponding marginals be realized as projections of a single positive latent probability distribution, or is such a joint classical description impossible? Conceptually, we adopt the operational logic of Bell or contextuality tests [33, 34, 35, 36, 37, 38, 39], with decoder settings playing the role of measurement contexts and latent variables acting as hidden variables (Fig.¬†1). We note for completeness that there are numerous works on quantum autoencoders, which posit intrinsically quantum latent subsystems by construction [40, 41, 42, 43, 44, 45, 46, 47]. Instead, the present work provides a Bell-type hypothesis test for the adequacy of classical latent-variable explanations themselves, entirely independent of any assumed quantum implementation.\n\n\nIn more detail, consider data samples xx drawn from a distribution pdata‚Äã(x)p_{\\mathrm{data}}(x), an encoder E:x‚Ü¶zE:x\\mapsto z, and a decoder D:z‚Ü¶yD:z\\mapsto y. The encoder maps high-dimensional inputs xx into a typically lower-dimensional latent variable zz, which is intended to capture a compressed representation of the data sufficient for reconstruction. A generic autoencoder is trained to minimize a reconstruction error of the form ùîºx‚Äã[‚Ñì‚Äã(x,D‚Äã(E‚Äã(x)))]\\mathbb{E}_{x}[\\ell(x,D(E(x)))], where ‚Ñì‚Äã(x,y)\\ell(x,y) is a loss function. While any fixed encoder EE induces a conditional distribution p‚Äã(z‚à£x)p(z\\mid x), the reconstruction objective generally admits many distinct encoders, and hence many distinct latent representations, achieving comparable loss. As a result, the associated aggregate latent distribution, p‚Äã(z)=‚à´p‚Äã(z‚à£x)‚Äãpdata‚Äã(x)‚Äãùëëxp(z)=\\int p(z\\mid x)\\,p_{\\mathrm{data}}(x)\\,dx, is not uniquely specified by the learning objective. This underdetermination motivates the question of which additional, ensemble-level restrictions on latent representations can be meaningfully imposed and, crucially, tested against observable data.\n\n\nA seemingly innocuous such restriction is the assumption that all observable statistics across readout contexts (to be defined shortly) arise from a single positive latent distribution. Although this assumption appears natural and is widely taken for granted, it constitutes a foundational constraint that we here scrutinize. Concretely, in a classical latent-variable model the conditional distribution of observable outputs yy, given an externally specified readout context Œ∏\\theta, is assumed to arise from a single positive latent distribution p‚Äã(z)‚â•0p(z)\\geq 0, according to\n\n\n\np‚Äã(y‚à£Œ∏)=‚à´ùëëz‚Äãp‚Äã(y‚à£z,Œ∏)‚Äãp‚Äã(z)p(y\\mid\\theta)=\\int dz\\,p(y\\mid z,\\theta)\\,p(z)\n\n(1)\n\n\nAs in contextuality tests, classicality is not a property of statistics obtained in a single context, but of their joint consistency across multiple contexts with one underlying latent distribution.\n\n\nOperationally, experiments provide access to samples of the outputs yy for each context Œ∏\\theta, from which the conditional distributions p‚Äã(y‚à£Œ∏)p(y\\mid\\theta) can be estimated. To formulate a finite-dimensional consistency test, we consider the probabilities p‚Äã(yk‚à£Œ∏j)p(y_{k}\\mid\\theta_{j}) of observing outcome yky_{k} under measurement context Œ∏j\\theta_{j}, with j=1,‚Ä¶,Jj=1,\\dots,J and k=1,‚Ä¶,Kk=1,\\dots,K. The J√óKJ\\times K array containing the probabilities p‚Äã(yk‚à£Œ∏j)p(y_{k}\\mid\\theta_{j}) is concatenated (flattened) into a single vector ùê©‚àà‚ÑùJ‚ÄãK\\mathbf{p}\\in\\mathbb{R}^{JK}. Finally, the space of latent distributions is also discretized, yielding a coefficient vector ùê∞‚àà‚ÑùN\\mathbf{w}\\in\\mathbb{R}^{N}, with ‚àëiwi=1\\sum_{i}w_{i}=1, and where NN is independent of the number of measured contexts or outcomes.\n\n\nThe decoder induces a linear map on latent distributions, represented in the finite-dimensional setting as ùê©=A‚Äãùê∞\\mathbf{p}=A\\,\\mathbf{w}, where the matrix A‚àà‚Ñù(J‚ÄãK)√óNA\\in\\mathbb{R}^{(JK)\\times N} is fully determined by the decoder and the chosen set of readout settings {Œ∏j}\\{\\theta_{j}\\}. The set of classically admissible decoding statistics forms the convex polytope ùíû={A‚Äãùê∞‚à£ùê∞‚â•0,‚àëiwi=1}\\mathcal{C}=\\{A\\mathbf{w}\\mid\\mathbf{w}\\geq 0,\\ \\sum_{i}w_{i}=1\\}. Now, we introduce a linear nonclassicality witness [49, 50, 51, 52] as a real vector ùêú‚àà‚ÑùJ‚ÄãK\\mathbf{c}\\in\\mathbb{R}^{JK} defining the scalar statistic S‚Äã(ùê©)=ùêú‚ãÖùê©S(\\mathbf{p})=\\mathbf{c}\\cdot\\mathbf{p}. Also define Scl=maxi‚Å°ùêú‚ãÖùêöiS_{\\mathrm{cl}}=\\max_{i}\\mathbf{c}\\cdot\\mathbf{a}_{i}, where ùêöi\\mathbf{a}_{i} denotes the iith column of the matrix AA. We observe that for any classical model ùê©=A‚Äãùê∞\\mathbf{p}=A\\mathbf{w}, that is, when wi‚â•0w_{i}\\geq 0, it follows that S‚Äã(ùê©)=‚àëiwi‚Äã(ùêú‚ãÖùêöi)‚â§(maxi‚Å°ùêú‚ãÖùêöi)‚Äã‚àëiwi=SclS(\\mathbf{p})=\\sum_{i}w_{i}\\,(\\mathbf{c}\\cdot\\mathbf{a}_{i})\\leq\\left(\\max_{i}\\mathbf{c}\\cdot\\mathbf{a}_{i}\\right)\\sum_{i}w_{i}=S_{\\mathrm{cl}}. Hence, we use Œî‚Äã(ùêú)=ùêú‚ãÖùê©‚àíScl‚Äã(ùêú)\\Delta(\\mathbf{c})=\\mathbf{c}\\cdot\\mathbf{p}-S_{\\mathrm{cl}}(\\mathbf{c}) to quantify nonclassicality. Since Œî‚Äã(ùêú)\\Delta(\\mathbf{c}) depends on the choice of ùêú\\mathbf{c}, we define Œî‚ãÜ=max‚Äñùêú‚Äñ2=1‚Å°Œî‚Äã(ùêú)\\Delta^{\\star}=\\max_{\\|\\mathbf{c}\\|_{2}=1}\\Delta(\\mathbf{c}). A positive value Œî‚ãÜ&gt;0\\Delta^{\\star}&gt;0 certifies noncla"
  },
  {
    "title": "Adversarial Evasion Attacks on Computer Vision using SHAP Values",
    "url": "https://arxiv.org/abs/2601.10587v1",
    "source": "arxiv",
    "summary": "The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibilit",
    "full_text": null
  },
  {
    "title": "Combinatorial Optimization Augmented Machine Learning",
    "url": "https://arxiv.org/abs/2601.10583v1",
    "source": "arxiv",
    "summary": "Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operation",
    "full_text": null
  },
  {
    "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
    "url": "https://arxiv.org/abs/2601.10581v1",
    "source": "arxiv",
    "summary": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API ",
    "full_text": "\n\n\n\n1 Introduction\n2 GeneGPT\n3 Reproducibility of GeneGPT\n4 GenomAgent\n5 Experiments\n6 Final Remarks and Future Work\n\n\n\n\n\n\n\nLLM\nLarge Language Model\nMCP\nModel Context Protocol\nQA\nQuestion Answering\nNL\nNatural Language\n\n11institutetext: University of Padua, Italy 22institutetext: Aalto University, Finland\nFrom Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA\n\n\nKimia Abedini\n\n‚ÄÉ‚ÄÉ\nFarzad Shami\n\n‚ÄÉ‚ÄÉ\nGianmaria Silvello\n\n\n\nAbstract\nComprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic  Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.\n\nhttps://kimia-abedini.github.io/Genom-Agent/\n\n\n\n1 Introduction\n\nLarge Language Models have shown remarkable potential in QA tasks and have recently gained traction in genomic QA applications¬†[12, 1].\nA notable and widely cited example is GeneGPT¬†[10], which currently represents the state-of-the-art for genomic QA tasks by successfully augmenting LLMs with external domain-specific APIs through in-context learning¬†[3] and tool integration [19].\nGeneGPT operates as a single-agent architecture[15] where an LLM is guided through carefully constructed prompts containing API documentation and examples, with inference managed sequentially through a single forward loop of API calls and result processing. Despite its effectiveness in achieving high accuracy on genomic benchmarks, GeneGPT‚Äôs architecture exhibits several limiting characteristics that constrain its scalability and adaptability. The system‚Äôs rigid dependency on specific API formats makes it fragile when interfacing with evolving tools, while its reliance on extensive context windows can lead to attention dilution and reduced focus on the original query¬†[13, 9]. Furthermore, the sequential processing approach struggles with multi-turn conversations¬†[11] where context drift becomes problematic, and the stop-token mechanisms for API call extraction lack the robustness needed for integration with newer LLMs.\n\n\nIn response to these limitations and building upon recent advances in multi-agent LLM systems [4], we propose a novel multi-agent architecture that addresses these efficiency bottlenecks through specialized agent coordination and dynamic task decomposition.\nWe first conduct a GeneGPT reproducibility study and adapt the system to more recent LLMs to identify key limitations. Second, we introduce GenomAgent, a multi-agent framework that extends GeneGPT‚Äôs capabilities.\nExperimental results show GenomAgent achieves an average performance score of 0.93 (+12% over GeneGPT‚Äôs 0.83) while reducing computational costs by 79% ($2.11 vs. $10.06 total) across the GeneTuring benchmark¬†[8].\n\n\nThe remainder of the paper is organized as follows: Section¬†1 reviews GeneGPT, Section¬†3 details GeneGPT replication, Section¬†3 describes GenomAgent, Section¬†5 presents the experiments, and Section¬†6 provides some final remarks.\n\n\n\n\n2 GeneGPT\n\nGeneGPT[10] is a domain-specific system that enhances LLMs by integrating a tool-augmented architecture to connect  Natural Language (NL) queries with structured genomics databases. It utilizes in-context learning¬†, enabling the LLM to dynamically generate and execute API calls to external resources, thus allowing real-time data retrieval and synthesis. This approach overcomes the limitations of static knowledge repositories in pre-trained models and demonstrates the extended utility of LLMs in specialized fields by ensuring access to up-to-date, structured data, while retaining their NLP capabilities for scientific QA.\n\n\nGeneGPT employs a specialized prompting strategy that leverages the code completion capabilities of LLMs. It is based on OpenAI Codex[6], and the prompt structure includes task instructions, relevant API documentation for E-utils and BLAST¬†[17, 2, 7], in-context learning examples, and the target question. GeneGPT uses the special symbol ‚Äú‚Üí\\rightarrow‚Äù as a stop token to identify API calls. When the LLM generates text containing this symbol, the system: (1) extracts the URL using a regex pattern; (2) executes the API call; and (3) appends the API result to the prompt. The model then continues generation, repeating steps 1-3 for any additional API calls, until the termination token ‚Äú\\n\\n\\backslash\\texttt{n}\\backslash\\texttt{n}‚Äù is detected.\nThen, the LLM generates the final answer using the retrieved results and in-context understanding of the examples.\n\n\nGeneGPT was developed in four configurations: full, slim, turbo, and lang. In full settings, the system incorporates complete API documentation and four examples, while slim uses only two examples. The turbo configuration replaces Codex¬† with GPT-3.5-turbo-16k, and lang implements the ReAct framework¬†[22]. The system was evaluated on nine tasks in the GeneTuring benchmark. Based on the experimental results, GeneGPT achieves state-of-the-art performance with an average performance score of 0.83, which substantially outperforms baselines as Bing Chat (0.44), BioMedLM¬†[14] (0.08), and GPT-3 (0.16).\n\n\nGeneGPT performance was assessed across multiple evaluation metrics designed for different tasks within the GeneTuring benchmark. These include exact match accuracy for nomenclature tasks, recall for association tasks, and task-specific scoring for alignment tasks. While individual task metrics employ different evaluation criteria and cannot be directly compared inter-task due to varying task complexity and requirements, all metrics are normalized in [0,1][0,1], enabling uniform interpretation. For comparative analysis, following an established approach in multi-task evaluation¬†[20, 21], we report a macro-averaged performance score computed as the arithmetic mean across all task-specific metrics, providing a singular measure of overall system accuracy while acknowledging that this aggregate metric represents a simplified view of the system‚Äôs diverse capabilities across heterogeneous genomics QA tasks.\n\n\n\n\n3 Reproducibility of GeneGPT\n\nTo understand GeneGPT‚Äôs operational principles and identify improvement opportunities, we conducted a reproducibility study. The original system relied on code-davinci-002 and GPT-3.5-turbo-16k, which were deprecated in 2023 and 2024, respectively111https://platform.openai.com/docs/deprecations.\nWe selected GPT-4o-mini as the replacement model due to its performance, cost efficiency, and current stability.\nWe implemented two compatible configurations: turbo and lang. The original paper for the lang setting mentions only LangChain as the orchestration framework without detailing its implementation. Due to substantial changes and deprecation in favor of LangGraph222https://langchain-ai.github.io, we opted for LangGraph for this configuration.\nWe preserve GeneGPT‚Äôs core design based on the stop-token interaction mechanism.\n\n\nTable 1: Results of the reproducibility of GeneGPT on the GeneTuring Benchmark.\n\n\n\n\n\nModel\nNomenclature\nGenomicLocation\nFunctionalAnalysis\nSequenceAlignment\n\n\n\n\n\nGene\nAlias\n\n\n\n\nName\nConv.\n\n\n\n\nSNP\nAssoc.\n\n\n\n\nGene\nLoc.\n\n\n\n\nSNP\nLoc.\n\n\n\n\nDisease\nAssoc.\n\n\n\n\nProtein\nGenes\n\n\n\n\nDNA to\nHuman\n\n\n\n\nDNA to\nSpecies\n\n\n\n\nGeneGPT Turbo\n0.64\n1.00\n0.96\n0.54\n0.98\n0.63\n0.96\n0.42\n0.88\n\n\nReproduced\n0.68\n0.98\n0.90\n0.54\n0.92\n0.56\n0.80\n0.07\n0.62\n\n\nRelative diff\n6.25%\n-2.00%\n-6.25%\n0.00%\n-6.12%\n-11.11%\n-16.67%\n-83.33%\n-29.55%\n\n\nGeneGPT Lang\n0.76\n0.02\n0.90\n0.54\n0.74\n0.39\n0.90\n0.06\n0.54\n\n\nReproduced\n0.76\n0.92\n1.00\n0.72\n1.00\n0.76\n1.00\n0.31\n0.54\n\n\nRelative diff\n0.00%\n4500%\n11.11%\n33.33%\n35.14%\n94.87%\n11.11%\n416.67%\n0.00%\n\n\n\n\n\n\nDuring the reproduction process, we encountered two main challenges. First, GPT-4o-mini did not consistently follow the URL generation format required by GeneGPT‚Äôs extraction pipeline. We addressed this by explicitly prompting the model to use the desired format.\nSecond, the original implementation used context truncation to avoid exceeding length limits, which hindered HTML data extraction by discarding critical information. We removed this limit with GPT-4o-mini‚Äôs larger context window.\nUnlike the original system‚Äôs single-token outputs, the reproduced system often requires manual extraction from multi-sentence responses before automatic evaluation.\n\n\nFor the reproducibility analysis, we employ the GeneTuring Benchmark, which encompasses 12 distinct tasks, each comprising 50 question-answer pairs. We approached 9 of these GeneTuring tasks, replicating the original GeneGPT paper. These selected tasks are grouped into four main subcategories: (1) nomenclature inquiries, focusing on gene aliases and name transformations; (2) genomic location inquiries, examining the positioning of genes and SNPs and their interrelations; (3) functional analysis inquiries, investigating aspects such as gene-disease associations and the genes responsible for protein coding; and (4) sequence alignment inquiries, which involve mapping DNA sequences to the human genome and comparing them across various species.\n\n\nTable¬†1 presents the reproduced results. Our reproduced system consistently shows improvements in the lang setting; these gains show that correct implementation of ReAct architecture with newer models can increase performance. However, in turbo settings, we observed high variation and degradation as a result of the non-compatibility of stop-token processing with general-purpos"
  }
]