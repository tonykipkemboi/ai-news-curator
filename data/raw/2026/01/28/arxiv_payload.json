[
  {
    "title": "Evaluation of Oncotimia: An LLM based system for supporting tumour boards",
    "url": "https://arxiv.org/abs/2601.19899v1",
    "source": "arxiv",
    "summary": "Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and e",
    "full_text": "\n\n\n\n1 Introduction\n\n2 ONCOTIMIA tool description\n\n2.1 System architecture\n2.2 Data ingestion and ETL processes\n2.3 Lung cancer form schema\n\n\n3 Materials and methods\n4 Results\n5 Conclusions\n\n\n\n\n\nEvaluation of ONCOTIMIA: an LLM‚Äëbased system for supporting tumour Boards \n\n\nLuis Lorenzo1,\nMarcos Monta√±a-M√©ndez1,\nSergio Figueiras1,\nMiguel Boubeta1,‚àó,\nCrist√≥bal Bernardo-Casti√±eira1,‚àó\n1 Innovation Department, Bah√≠a Software SLU, Ames (A Coru√±a), Spain \n‚àó Corresponding author(s):\nmiguel.boubeta@bahiasoftware.es, cristobal.bernardo@bahiasoftware.es\n\n\n\nAbstract\nMultidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80%80\\% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.\n\n\nKeywords‚ÄÇGenAI  ‚ãÖ\\cdot\nLLMs  ‚ãÖ\\cdot\nVector database  ‚ãÖ\\cdot\nEmbeddings  ‚ãÖ\\cdot\nRAG  ‚ãÖ\\cdot\nTumour boards  ‚ãÖ\\cdot\nLung cancer form autocompletion\n\n\n\n1 Introduction\n\nIn recent years, advances in transformer-based architectures have firmly established LLMs as a foundational technology in biomedical informatics. Early developments in general-purpose LLMs (e.g., GPT-3 and its successors) revealed emergent capabilities in clinical summarisation, question answering, report generation, coding support and contextual reasoning (Brown et al., 2020). Subsequent work has shown that domain-adapted models, such as BioGPT (Luo et al., 2022), BioMedLM, PubMedBERT (Gu et al., 2021) and Med-PaLM (Singhal et al., 2023), achieve expert-level performance on diverse medical reasoning benchmarks. An expanding evidence base further demonstrates that LLMs can reliably extract salient clinical information and support guideline-informed recommendations when deployed with appropriate safeguards. Recent prospective evaluations in hospital settings indicate that LLM-assisted clinical documentation can meaningfully reduce clinician workload while maintaining high linguistic quality (Bracken et al., 2025; Nori et al., 2023). Nevertheless, these systems continue to require rigorous oversight owing to risks of hallucinations, incomplete contextualisation and occasional misinterpretation of clinical guidelines.\n\n\nIn medicine, multidisciplinary management (MDM) offers cancer patients the advantage of having specialists from different medical fields collaboratively involved in treatment planning. This approach is usually implemented through multidisciplinary clinics, such as breast units, where various experts assess patients, perform physical examinations, request and conduct diagnostic tests efficiently, and jointly evaluate potential treatment options. MDM is also conducted through multidisciplinary tumour board (MDTB) meetings, which are structured sessions in which all relevant patient information is collected, and key specialists convene to discuss the diagnosis and management of cancer patients (El Saghir et al., 2014). However, studies show that clinicians spend significant effort managing and analysing information within electronic health records (EHRs), reducing the time for direct patient care, especially in high-complexity fields such as oncology (Arndt et al., 2017). MDTBs are particularly affected because each case requires the preparation of standardised summaries, structured staging information, pathology details, radiological interpretations, biomarker data, allergy profiles and records of prior treatment, demands that are specially challenging in settings with limited staff resources. The fragmentation of data across narrative notes, laboratory systems, pathology platforms and PACs often results in manual information retrieval and redundant re-entry of variables into MDTB case forms.\n\n\nAutomating pieces of this workflow is therefore both operationally compelling and clinically relevant. Early efforts using traditional natural language processing (NLP) methods demonstrated benefit in extracting structured information from radiology or pathology reports (Wang et al., 2018). The transition from rule-based NLP to LLM-enabled generative systems, in addition to extracting information, also allows it to be synthesised into coherent drafts aligned with medical guidelines.\n\n\nAutocompletion in clinical documentation has emerged as a promising application of LLMs. Initial experiments in general EHR contexts have shown that LLMs can assist with automatic drafting of the main complaints, suggesting phrasing for assessment and plan sections, and generating templated texts conditioned on structured inputs Ayers et al. (2023). These systems have demonstrated that LLM-driven autocompletion improves efficiency and reduces repetitive typing. However, studies explicitly examining autocompletion for oncology MDTB forms remain extremely limited. To date, most published work in oncology has focused on information extraction (e.g., stage from clinical notes) or summarisation of radiology reports. Some recent pilot studies have explored the use of LLMs to generate oncology case summaries or harmonise staging descriptions Chen et al. (2025), but standardised autocompletion of MDTB forms, particularly in lung cancer, has not yet been rigorously evaluated in prospective settings. This represents a critical evidence gap given the structured, repetitive and data-dense nature of these forms and their importance in treatment decision making.\n\n\nLung cancer has been one of the earliest and most active oncology domains for AI research due to the abundance of imaging, molecular data and clinical texts. NLP and deep learning models have been applied to staging extraction, biomarker result interpretation, automatic radiology summarisation and automated assessment of eligibility for targeted therapy or clinical trials (Esteva et al., 2019; Hu et al., 2021; Aldea et al., 2025).\n\n\nIn this work, our objective is to describe and technically evaluate the performance of ONCOTIMIA, a modular and secure LLM-based tool that integrates RAG and a rule‚Äëdriven adaptive form model to automate the completion of lung cancer tumour board forms. We assess its performance in a realistic tumour board setting, using synthetic but clinically representative cases. Six state‚Äëof‚Äëthe‚Äëart LLMs are evaluated in terms of form‚Äëcompletion accuracy and end‚Äëto‚Äëend latency. Through this study, we aim to provide empirical evidence on the technical and operational feasibility of GenAI‚Äëassisted autocompletion within oncology workflows, and to demonstrate its potential to reduce documentation burden while maintaining data quality.\n\n\nThe following Section 2 introduces the ONCOTIMIA platform, outlining its architecture, data ingestion pipeline, and the design of lung cancer data schema. Section 3 summarises the methodology for generating medical data records, the RAG workflow and the selection criteria for LLMs. Section 4 reports the performance evaluation results, and Section 5 concludes by highlighting key findings, limitations, and directions for future work.\n\n\n\n\n2 ONCOTIMIA tool description \n\nONCOTIMIA is a modular system that integrates generative AI to support tumour board workflows and reduce documentation burden. Its core functionality focuses on the automatic autocompletion of standardised tumour board forms and the generation of structured patient summaries from heterogeneous clinical sources. The system also incorporates information retrieval and RAG‚Äëassisted reasoning modules to facilitate case preparation; these features are intended to support review and do not replace clinical judgment. In this section, we present the system architecture and clinical data ingestion process and the definition of lung cancer form used in tumour boards committee workflows.\n\n\n\n2.1 System architecture\n\nThe architecture of ONCOTIMIA has been conceived as a modular, scalable, and secure infrastructure designed to enable the seamless integration of GenAI into oncology workflows. Its design adheres to the principles of interoperability, traceability, maintainability, and controlled evolution, thereby ensuring long-term sustainability in complex clinical environments undergoing continuous technological transformation. From a conceptual standpoint, the ONCOTIMIA architecture (see Figure 1 for more details) is structured around a set of interconnected yet decoupled modules that operate in a coordinated manner through standardised interfaces and secure communication protocols. This modular arrangement enables the independent evolution of system components and the incremental incorporation of new functionalities without compromising overall system stability.\n\n\nFigure 1: ONCOTIMIA architecture. (i) Data ingestion layer, (ii) storage subsystem, (iii) backend services, (iv) LLM abstraction layer a"
  },
  {
    "title": "Self-Distillation Enables Continual Learning",
    "url": "https://arxiv.org/abs/2601.19897v1",
    "source": "arxiv",
    "summary": "Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SF",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nOff-policy versus On-policy Learning.\nInverse Reinforcement Learning.\nContext Distillation.\n\n\n\n3 Self-Distillation Fine-Tuning\n\n3.1 Self-Distillation as Inverse RL\n3.2 Validating the ICL Assumption\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setting\n\nEvaluation.\nBaselines.\n\n\n\n4.2 On-policy learning leads to better generalization\n\nResults.\n\n\n\n4.3 Learning without forgetting\n\nResults.\n\n\n4.4 Effect of model size\n\n4.5 Training reasoning models without reasoning data\n\nResults.\n\n\n\n4.6 What drives the improvement in performance?\n\nResults.\n\n\n\n\n\n5 Discussion and Limitations\n\nRelationship to on-policy RL.\nComputational Costs.\nLearned Artifacts.\nRequirements for Model Capability.\nFuture Work.\n\n\n\nA Additional Ablations\n\n\nA.1 Estimating the KL gradient\n\nToken-level (partial) estimator\nFull analytic per-token estimator.\nRao-Blackwellized estimator.\n\n\n\nA.2 The Importance of Demonstration-Conditioned Context\n\nResults.\n\n\nA.3 Choice of teacher model\n\n\n\nB Training and Evaluation details\n\nB.1 Training Details\nB.2 Evaluation Details\n\nB.3 Dataset Details\n\nScience Q&amp;A.\nTool Use.\nMedical.\nKnowledge Acquisition.\n\n\n\n\n\n\n\n\n\nSelf-Distillation Enables Continual Learning\n\n\n\nIdan Shenfeld1‚Äâ2 ‚ÄÉMehul Damani1 ‚ÄÉJonas H√ºbotter3 ‚ÄÉPulkit Agrawal1‚Äâ2\n‚ÄÑ1MIT\n‚ÄÑ2Improbable AI Lab\n‚ÄÑ3ETH Zurich \n\nCorrespondence to idanshen@mit.edu.\n\n\nAbstract\nContinual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations. Code and Datasets are available at http://idanshenfeld.com/SDFT.\n\n\n\n1 Introduction\n\nFigure 1: \nSupervised Fine-Tuning¬†(SFT) is commonly used to learn from expert demonstration datasets, but its off-policy nature leads to catastrophic forgetting of general capabilities.\nWe introduce Self-Distillation Fine-Tuning (SDFT), which turns expert demonstrations into on-policy learning signals by using a demonstration-conditioned version of the model as its own teacher. In this way, SDFT enables true continual learning with the model improving on new tasks as they arise without regressing existing capabilities.\n\n\nFoundation models have achieved remarkable success in recent years, powering AI applications across language, vision, robotics, and beyond. However, despite their impressive capabilities, today‚Äôs AI systems remain static after deployment. While they can adapt their behavior at inference time through mechanisms such as retrieval or prompting, they do not update their parameters to acquire new skills, internalize new knowledge, or improve from experience. To enable the next generation of foundation models, we must solve the problem of continual learning: enabling AI systems to keep learning and improving over time, similar to how humans accumulate knowledge and refine skills throughout their lives (Hassabis et¬†al., 2017; De¬†Lange et¬†al., 2021).\n\n\nA growing body of recent work has highlighted the importance of on-policy learning for continual learning. When models learn from data generated by their current policy, they exhibit substantially reduced catastrophic forgetting compared to off-policy alternatives (Shenfeld et¬†al., 2025; Chen et¬†al., 2025). To date, most successful on-policy approaches have been developed in the context of reinforcement learning (RL), where feedback is provided through an explicit reward function. However, in many real-world settings such rewards are unavailable or difficult to specify. Instead, learning typically proceeds from datasets of expert demonstrations. The dominant paradigm in this regime is supervised fine-tuning (SFT), which trains the model to imitate expert actions under a fixed, offline data distribution. While simple and scalable, SFT is inherently off-policy, and prior work has shown that sequential SFT can lead to poor generalization and severe catastrophic forgetting when models are adapted to new tasks or domains (Kirkpatrick et¬†al., 2017; Li &amp; Hoiem, 2017). This tension raises a fundamental challenge for continual learning: how can we obtain the benefits of on-policy learning when only demonstrations are available?\n\n\nFigure 2: (Left) SDFT leverages a model‚Äôs in-context learning ability to generate on-policy training signals.\nFor each query xx, the model acts in two roles. A student that is conditioned only on the query P=œÄ(‚ãÖ|x)P=\\pi(\\cdot|x) and the teacher, which is the same model conditioned on an expert demonstration cc, producing a demonstration-aware distribution Q=œÄ(‚ãÖ|x,c)Q=\\pi(\\cdot|x,c). Training minimizes the reverse KL divergence between the student and teacher, yielding on-policy updates. (Right) Conditioning the model on the expert demonstrations creates a teacher with an output distribution that is substantially closer to the base model, while maintaining the same new-task accuracy.\n\n\nThe challenges of off-policy learning can, in principle, be overcome by first learning a reward function from demonstrations (i.e., Inverse Reinforcement Learning or IRL), and then performing on-policy RL (Ng et¬†al., 2000; Abbeel &amp; Ng, 2004).\nWhile IRL is conceptually elegant, effectively recovering rewards typically requires strong priors over the reward structure, which has limited its practical adoption to settings where such assumptions are justified, such as RLHF (Peng et¬†al., 2018; Stiennon et¬†al., 2020).\n\n\nRather than inferring an explicit reward function, we propose Self-Distillation Fine-Tuning (SDFT), an on-policy distillation (Ross et¬†al., 2011; Agarwal et¬†al., 2024) framework for learning directly from demonstrations. SDFT relies on the observation that large pretrained models exhibit strong in-context learning‚Äîthe ability to adapt their behavior when conditioned on examples, without parameter updates (Brown et¬†al., 2020). We exploit this property by using the same model in two roles: a teacher, conditioned on both the task input and an expert demonstration, and a student, conditioned only on the task input. Training distills the teacher‚Äôs predictions into the student on trajectories generated by the student itself, yielding on-policy updates that incorporate information from demonstrations without explicit reward inference or offline imitation.\n\n\nWe evaluate SDFT in two continual learning settings: skill learning, where demonstrations are used to improve performance on a task, and knowledge acquisition, where new information must be incorporated into the model. Across both settings, SDFT provides stable on-policy updates that enable learning while substantially reducing catastrophic forgetting compared to supervised learning. Consistent with prior work on on-policy learning (Ross et¬†al., 2011; Chu et¬†al., 2025), SDFT also improves generalization both in-distribution and out-of-distribution, making it beneficial even in settings where retaining prior capabilities is not the primary objective. In a sequential learning experiment involving three distinct skills, SDFT enables a single model to acquire each skill in turn while preserving performance on previously learned skills as well as on unrelated, pre-existing capabilities ‚Äî demonstrating that continual learning from demonstrations is possible.\n\n\n\n\n2 Related Work\n\nOff-policy versus On-policy Learning.\n\nA long line of work highlights the advantages of on-policy learning, i.e., training on trajectories induced by the model itself, over off-policy learning. The seminal result of Ross et¬†al. (2011) shows that off-policy imitation learning suffers from compounding errors at inference time, as the learned policy drifts away from the states covered in the demonstrations, errors accumulate rapidly, a failure mode that on-policy algorithms avoid by continually training under their own state distribution. More recent empirical studies reinforce this distinction. Models fine-tuned with on-policy RL have been shown to generalize better beyond the training distribution (Agarwal et¬†al., 2024; Han et¬†al., 2025; Chu et¬†al., 2025; Li et¬†al., 2025) and transfer more effectively to related tasks (Huan et¬†al., 2025) than models trained purely off-policy. In continual learning settings, on-policy updates also reduce catastrophic forgetting when adapting to new tasks (Shenfeld et¬†al., 2025; Lai et¬†al., 2025). These findings collectively motivate our goal - to enable on-policy learning from demonstrations, thereby retaining the benefits of on-policy RL while avoiding the need for explicit reward engineering.\n\n\n\nInverse Reinforcement Learning.\n\nIRL (Ng et¬†al., 2000) provides a classical solution to the problem faced in many RL settings: the agent must learn a policy when no explicit reward function is available, only demonstrations. Rather than cloning the expert‚Äôs actions, IRL seeks to infer the underlying reward for which those demonstrations would be optimal. This perspective avoids the issues of off-policy imitation learning, since the inferred reward can support on-policy updates (Xu et¬†al., 2020). While this idea has deep theoretical appeal, t"
  },
  {
    "title": "Post-LayerNorm Is Back: Stable, ExpressivE, and Deep",
    "url": "https://arxiv.org/abs/2601.19895v1",
    "source": "arxiv",
    "summary": "Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at s",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminary\n\n2.1 Highway Networks\n2.2 Residual Networks\n2.3 Post-LN: The Original Transformer Formulation\n2.4 Pre-LN: The Modern Standard in LLMs\n2.5 Hybrid Variants: Interpolating Between Post-LN and Pre-LN\n\n\n\n3 KEEL\n\n3.1 Implementation Details\n3.2 Instability of Post-LN LLMs: A Gradient Perspective\n3.3 KEEL: Stabilizing Deep LLMs\n3.4 KEEL is a Post-LN Architecture\n\n\n\n4 Discussions\n\n4.1 KEEL vs. DeepNorm\n\n4.2 Design Evolution: From Naive Scaling to KEEL\n\nObservation:\nObservation:\n\n\n4.3 Depth-wise Test-Time Training\n\n\n\n5 Experiments\n\n5.1 Stability Analysis\n5.2 Optimal Learning Rate\n5.3 Scalability Analysis: Performance at Depth Scaling\n5.4 Scalability Analysis: Performance at Data Scaling\n5.5 Deeper vs. Wider\n5.6 Experimental Setup\n5.7 Main Results\n\n\n6 Conclusion\n7 Limitation and Future Work\n8 Layer Redundancy in Deep LLMs\n9 Discrepancy Between Training Loss and Downstream Evaluation\n10 Model Configuration\n\n\n\n\n\n\n\\contribution\n[*]Equal Contribution\n\\contribution[‚Ä†]Corresponding authors\n\nPost-LayerNorm Is Back: Stable, ExpressivE, and Deep\n\n\nChen Chen\n\n‚ÄÉ‚ÄÉ\n  Lai Wei\n\nByteDance Seed\n\nlaiwei.future@bytedance.com\n\n\n(January 27, 2026)\n\nAbstract\nLarge language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity.\nIn contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths.\nWe revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks.\nWe present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks.\nKeel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.\n\n\n\\correspondence\nLai Wei at \n\n\n\n\n\n(a) Training Stability\n\n\n\n\n(b) Expressiveness\n\n\n\n\n(c) Depth Scaling\n\n\n\nFigure 1: KEEL enables stable, expressive, and deep LLM training.\n(a) Training Stability: Keel maintains smooth convergence at aggressive learning rates, while Pre-LN exhibits severe instability under the same configuration.\n(b) Expressiveness: Keel demonstrates superior performance across all capability domains, particularly in Math &amp; Code (+16.5%).\n(c) Depth Scaling: Keel consistently outperforms Pre-LN across all depths (64-1024 layers).\nTogether, these results demonstrate that Keel‚Äôs architectural improvements enable stable optimization of ultra-deep networks with enhanced learning efficiency and model expressiveness.\n\n\n\n\n\n1 Introduction\n\nLarge language model (LLM) progress has been driven primarily by scaling: bigger models, longer context windows, and larger training corpora. Yet these conventional scaling axes are beginning to show diminishing returns. Width scaling saturates quickly, context scaling grows increasingly expensive, and parameter growth alone does not unlock qualitatively new behaviors. As a result, LLM scaling is hitting a wall, and there is increasing interest in architectural directions that can deliver more expressivity per parameter.\n\n\nDepth scaling offers a promising path forward. In principle, deeper networks can represent exponentially richer functions and support more hierarchical reasoning. However, current LLM architectures struggle to capitalize on depth. Training becomes increasingly unstable at extreme depths, and even when optimization succeeds, depth scaling delivers substantially worse returns than width scaling under current architectures.\n\n\nThe placement of Layer Normalization (LN), which is a seemingly simple architectural choice, has an enormous effect on depth scaling. The original Transformer architecture used Post-LayerNorm (Post-LN), but modern LLMs overwhelmingly adopt Pre-LayerNorm (Pre-LN) [gpt3, llama]. Pre-LN stabilizes early training by normalizing each sublayer‚Äôs input, preventing the divergence commonly seen in deep Post-LN networks [xiong2020layernorm]. However, Pre-LN introduces its own structural limitations: it weakens gradient propagation and reduces the effective contribution of deeper layers [nguyen2019transformers]. As models grow deeper, this results in poor depth scaling and representational expressivity, limiting the potential of depth as a new scaling axis.\n\n\nIn contrast, Post-LN maintains large gradient signals in deeper layers, which can support superior depth scaling. Yet its training instability has made it unsuitable for LLM-scale models. When the residual output and transformed features are summed and then normalized, gradients in LayerNorm can exhibit extreme variability, especially in deep regimes [xiong2020layernorm]. Previous attempts to revive Post-LN, such as DeepNorm [deepnorm], Admin [admin], and more recent hybrid normalization strategies [mixln, hybridnorm], mitigate some failure modes but do not fundamentally resolve the gradient pathologies of Post-LN LLMs, nor do they demonstrate reliable behavior at the depths needed to break the scaling limits of today.\n\n\nTo understand the root cause of these instabilities, we formally analyze the gradient dynamics of Post-LN. We derive bounds on the backward signal and show that the ResNet-style residual path is the primary source of gradient vanishing. These issues arise not from normalization itself, but from the way that residual and transformed activations are mixed before normalization.\n\n\nMotivated by this analysis, we consider a small yet impactful architectural change: replace the ResNet-style residual branch with a simplified Highway-style connection, and re-express its gradient dynamics under the same theoretical framework. Our results show that this Highway-style pathway provides provable control of gradient magnitudes, allowing signals to propagate through depth without vanishing. Crucially, it maintains the inter-layer coupling that makes Post-LN expressive, while suppressing the unstable mixing that previously made Post-LN difficult to train.\n\n\nGuided by these findings, we introduce Keel, a Post-LN architecture that incorporates a lightweight Highway-style gated connection [highway]. The gate dynamically balances carry and transform signals, regulating both forward and backward information flow. This simple modification stabilizes Post-LN at scale, enabling it to realize its expressivity advantages without special initialization or customized residual scaling.\n\n\nEmpirically, Keel delivers substantial gains in depth scalability and model performance, effectively addressing the training stability issues often associated with traditional deep architectures. By integrating a Highway-style pathway with a revived Post-LN configuration, Keel enables robust training at depths exceeding 1000 layers.111In this paper, unless otherwise specified, ‚Äúlayer‚Äù refers to the total count of residual connections, which includes both the Attention and Feed-Forward Network (FFN) layers. While standard Post-LN or Pre-LN architectures often exhibit severe instability when subjected to aggressive learning rates, Keel maintains smooth convergence, suggesting a more well-conditioned optimization landscape.\n\n\nThis newfound stability does not come at the cost of representational power. Instead, Keel demonstrates superior expressiveness across the entire depth spectrum ranging from 64 to 1024 layers. These gains are particularly pronounced in specialized capability domains such as Math and Code, where the model achieves a +16.5% performance increase over Pre-LN baselines. Collectively, these results suggest that the architectural improvements in Keel facilitate a more efficient learning process, allowing for the stable optimization of ultra-deep networks. By breaking the conventional depth-scaling barrier, this approach establishes a practical and powerful framework for the next generation of large language model scaling.\n\n\n\n\n2 Preliminary\n\nIn this section, we review the architectural components relevant to depth scaling and gradient propagation in deep learning. We revisit Highway Networks and Residual Networks, and then summarize the Post-LN and Pre-LN normalization schemes that define the operational behavior of modern Transformer blocks.\n\n\n\n2.1 Highway Networks\n\nHighway Networks [highway] were introduced as an early mechanism for training very deep feed-forward architectures. Their key idea is to allow hidden layers to adaptively regulate how much of their input is transformed versus directly carried forward.\nGiven an input ùê±\\mathbf{x} and transformation ùêÖ‚Äã(ùê±)\\mathbf{F}(\\mathbf{x}), a Highway layer computes:\n\n\n\nùê±l+1=ùêì‚Äã(ùê±l)‚äôùêÖ‚Äã(ùê±l)+ùêÇ‚Äã(ùê±l)‚äôùê±l,\\mathbf{x}_{l+1}=\\mathbf{T}(\\mathbf{x}_{l})\\odot\\mathbf{F}(\\mathbf{x}_{l})+\\mathbf{C}(\\mathbf{x}_{l})\\odot\\mathbf{x}_{l},\n\n(1)\n\n\nwhere ùêì‚Äã(ùê±l)\\mathbf{T}(\\mathbf{x}_{l}) is a typically trainable gate, ùêÇ‚Äã(ùê±l)\\mathbf{C}(\\mathbf{x}_{l}) is typically set to 1‚àíùêì‚Äã(ùê±l)1-\\mathbf{T}(\\mathbf{x}_{l}) and ‚äô\\odot denotes elementwise multiplication.\nThis gating mechanism ensures that gradients can bypass transformations when necessary, preventing gradient attenuation.\n\n\n\n\n2.2 Residual Networks\n\nResidual Networks [resnet] replace the Highway gate with a fixed identity path, computing:\n\n\n\nùê±l+1=ùê±l+ùêÖ‚Äã(ùê±l).\\mathbf{x}_{l+1}=\\mathbf{x}_{l}+\\mathbf{F}(\\mathbf{x}_{l}).\n\n(2)\n\n\n\n\nThis unconditional ski"
  },
  {
    "title": "M-SGWR: Multiscale Similarity and Geographically Weighted Regression",
    "url": "https://arxiv.org/abs/2601.19888v1",
    "source": "arxiv",
    "summary": "The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes \"near\" and \"related\" remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial",
    "full_text": null
  },
  {
    "title": "AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability",
    "url": "https://arxiv.org/abs/2601.19886v1",
    "source": "arxiv",
    "summary": "The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized ac",
    "full_text": null
  },
  {
    "title": "SONIC: Spectral Oriented Neural Invariant Convolutions",
    "url": "https://arxiv.org/abs/2601.19884v1",
    "source": "arxiv",
    "summary": "Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these li",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.19884v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2601.19884v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 27 Jan 2026]\n    Title:SONIC: Spectral Oriented Neural Invariant Convolutions\n    Authors:Gijs Joppe Moens, Regina Beets-Tan, Eduardo H. P. Pooch            View a PDF of the paper titled SONIC: Spectral Oriented Neural Invariant Convolutions, by Gijs Joppe Moens and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.\n    \n\n    \n    \n              \n          Comments:\n          10 pages, 4 figures. Accepted at ICLR 2026\n        \n\n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.19884 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.19884v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.19884\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Eduardo Pooch [view email]          [v1]\n        Tue, 27 Jan 2026 18:51:11 UTC (10,017 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled SONIC: Spectral Oriented Neural Invariant Convolutions, by Gijs Joppe Moens and 2 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n    "
  },
  {
    "title": "RHSIA: Real-time Hemodynamics Surrogation for Non-idealized Intracranial Aneurysms",
    "url": "https://arxiv.org/abs/2601.19876v1",
    "source": "arxiv",
    "summary": "Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically. This is because CFD requires specialized expertise and is time-consuming and low throughput, making it difficult to support clinical trials. A deep learning model that m",
    "full_text": null
  },
  {
    "title": "Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection",
    "url": "https://arxiv.org/abs/2601.19871v1",
    "source": "arxiv",
    "summary": "Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Tra",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n\n2 Methods\n\n2.1 Reflective Translation\n2.2 Masking to Reduce Copying\n\n\n3 Datasets\n\n4 Experimental Setup and Evaluation\n\n4.1 Baselines and Prompting Strategies\n4.2 Metrics\n\n\n5 Results\n\n6 Data Analysis\n\n6.1 First- vs. Second-Pass Improvements\n6.2 Effect of Confidence Thresholding\n6.3 Statistical Significance Testing\n\n\n7 Discussion\n8 Limitations and Future Work\n9 Code and Data Availability\n\nA Prompt Templates\n\nA.1 Baseline Translation Prompts\nA.2 Few-Shot Translation Prompts\nA.3 Brief Reasoning (Chain-of-Thought-Style) Prompts\n\n\n\n\n\n\n\nReflective Translation for Low-Resource Machine Translation via Structured Self-Reflection\n\n\nNicholas Cheng‚Äâ\nIndependent Researcher\n\n\n\nAbstract\nLow-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation (MT) due to limited parallel corpora and scarce linguistic resources. Recent work on large language models (LLMs) suggests that self-reflection‚Äîthe ability of a model to critique and revise its own outputs‚Äîcan improve reasoning quality and factual consistency. Building on this idea, this paper presents Reflective Translation, a prompting framework in which an LLM internally evaluates and corrects its own translations through structured, multi-round prompting to improve semantic fidelity.\nThe method is evaluated using GPT-3.5 and Claude Haiku 3.5 on English‚ÄìisiZulu and English‚ÄìisiXhosa sentence pairs drawn from OPUS-100 and NTREX-African. Translation quality is assessed using BLEU and COMET. Across settings, second-pass translations improve consistently relative to first-pass outputs. This paper further introduces a reflection-augmented dataset consisting of (source, draft, critique, revision) tuples, enabling reproducible analysis of reflective behavior. Overall, the results suggest that reflection-based prompting is a lightweight, model-agnostic approach for improving MT quality in under-resourced languages without fine-tuning or additional labeled data.\n\n\n\n1 Introduction\n\nMachine Translation (MT) enables users to exchange information across languages without human intermediaries. The effectiveness of MT depends on linguistic accuracy, semantic faithfulness, and contextual consistency. Large language models (LLMs) have recently shown strong translation performance without task-specific fine-tuning (Brants et al., 2007; Moslem et al., 2023). However, a substantial gap remains in low-resource settings (Robinson et al., 2023; Haddow et al., 2022), where limited parallel data can lead to hallucinations, omissions, and distortions (Wang and Sennrich, 2020).\n\n\nAn emerging line of work studies self-reflection‚Äîprompting models to critique and refine their own outputs‚Äîas a mechanism to improve generation quality. Iterative prompting frameworks such as Reflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023), and Chain-of-Verification (Creswell and Shanahan, 2023) demonstrate that structured self-evaluation can improve factuality and consistency. Related approaches incorporate reflection signals through training or translation pipelines (Li et al., 2023; Wang et al., 2024).\n\n\nThis paper investigates whether reflection can be applied as an inference-time correction step to improve translation faithfulness without fine-tuning or new labeled data. Translation is treated as constrained reasoning: the target sentence must preserve the meaning of the source. To operationalize this, the proposed framework generates an initial translation, produces a structured self-critique that identifies typical translation errors (mistranslation, omission, semantic distortion), and then produces a revised translation guided by the critique.\n\n\nContributions.\n\n\n\n‚Ä¢\n\nThis paper proposes a reflection-guided prompting framework for MT in which models generate and act on structured self-assessments to improve translation faithfulness.\n\n\n\n‚Ä¢\n\nThe framework is evaluated on OPUS-100 and NTREX-African for English‚ÄìisiZulu and English‚ÄìisiXhosa across two LLMs (GPT-3.5 and Claude Haiku 3.5).\n\n\n\n‚Ä¢\n\nA reflection-augmented dataset of (source, draft, critique, revision) tuples is released to support reproducibility and future analysis.\n\n\n\n\n\n\n\n\n2 Methods\n\n\n2.1 Reflective Translation\n\nReflective Translation is a prompting pipeline that guides a model to self-review its own translations and produce improved outputs via structured feedback. For each source sentence, GPT-3.5 (OpenAI, 2023) and Claude Haiku 3.5 (Anthropic, 2024) generate a first-pass translation. A structured reflection is then produced to identify key errors and provide concise corrective guidance. The model uses this reflection to generate a second-pass translation.\n\n\nEach reflection consists of:\n\n\n1.\n\nError identification: key mistranslations, omissions, or distortions.\n\n\n\n2.\n\nHigh-level fixes: reusable corrective instructions (e.g., preserve named entities; fix tense/aspect; repair agreement).\n\n\n\n3.\n\nCritical content: phrases or semantic constraints that must be preserved.\n\n\n\n\n\n\n\n2.2 Masking to Reduce Copying\n\nTo reduce leakage from the reflection into the second translation, salient content words are extracted and masked using the Rapid Automatic Keyword Extraction (RAKE) algorithm (NLTK implementation) (Rose et al., 2010). Extracted phrases are replaced with a &lt;MASK&gt; token so the model must apply the critique semantically rather than copy text verbatim.\n\n\nFigure 1: Overview of the reflective translation framework: first-pass translation ‚Üí\\rightarrow structured reflection (with masking) ‚Üí\\rightarrow second-pass translation.\n\n\n\n\n\n3 Datasets\n\nEvaluation uses OPUS-100 (Tiedemann, 2012) and NTREX-African (Nekoto et al., 2023). The experiments focus on isiZulu and isiXhosa. OPUS-100 provides broad multilingual parallel data; NTREX-African provides curated evaluation sets for African languages. The experiments use OPUS-100 for English‚ÄìisiZulu and NTREX-African for English‚ÄìisiXhosa.\n\n\n\n\n4 Experimental Setup and Evaluation\n\n\n4.1 Baselines and Prompting Strategies\n\nBaseline performance uses GPT-3.5 and Claude Haiku 3.5 without fine-tuning. Three prompting strategies are evaluated within the same reflective pipeline:\n\n\n‚Ä¢\n\nBaseline (zero-shot) translation.\n\n\n\n‚Ä¢\n\nChain-of-thought-style prompting (Wei et al., 2022) (brief internal reasoning instruction).\n\n\n\n‚Ä¢\n\nFew-shot prompting (Brown et al., 2020) with in-context examples.\n\n\n\n\n\n\n\n4.2 Metrics\n\nTranslation quality is assessed using BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020). BLEU measures n-gram overlap with a brevity penalty; COMET is a learned metric designed to better track semantic adequacy.\n\n\n\n\n\nBLEU=B‚ÄãP‚ãÖexp‚Å°(‚àën=1Nwn‚Äãlog‚Å°pn),B‚ÄãP={1if¬†‚Äãc&gt;re1‚àír/cif¬†‚Äãc‚â§r\\text{BLEU}=BP\\cdot\\exp\\left(\\sum_{n=1}^{N}w_{n}\\log p_{n}\\right),\\quad BP=\\begin{cases}1&amp;\\text{if }c&gt;r\\\\\ne^{1-r/c}&amp;\\text{if }c\\leq r\\end{cases}\n\n\n\n\n\n\nCOMET‚Äã(x,y)=fŒ∏‚Äã(x,y)\\text{COMET}(x,y)=f_{\\theta}(x,y)\n\n\n\n\n\n\n\n\n5 Results\n\nReflective translation improves second-pass outputs across prompting strategies. Figures¬†2 summarizes the scores for the first vs. second attempt by a prompt strategy, showing consistent gains, particularly in COMET.\n\n\nFigure 2: First vs. second attempt translation quality by prompting strategy. (a) COMET. (b) BLEU.\n\n\nThreshold ablation shows that stricter thresholds reduce coverage but can yield larger average gains among refined samples (Figure¬†3).\n\n\nFigure 3: Threshold ablation: average BLEU and COMET before vs. after reflection across thresholds.\n\n\n\n\n6 Data Analysis\n\n\n6.1 First- vs. Second-Pass Improvements\n\nAcross both metrics, second-pass outputs improve relative to first-pass translations. COMET improvements are typically larger and more stable than BLEU improvements, suggesting that reflective translation primarily improves semantic adequacy rather than only lexical overlap. This pattern is consistent with the separation observed in Figure¬†2, which summarizes first- vs. second-attempt scores by prompting strategy, and Figure¬†3, which shows the effect of confidence thresholding.\n\n\n\n\n6.2 Effect of Confidence Thresholding\n\nThreshold-based filtering shapes the trade-off between coverage and per-sentence improvement. Higher thresholds reduce the number of sentences eligible for refinement but increase the average improvement among refined samples, consistent with reflective translation functioning as a targeted correction mechanism.\n\n\n\n\n6.3 Statistical Significance Testing\n\nTo test whether first-to-second pass improvements are statistically significant, paired nonparametric testing is performed between first-pass and second-pass scores at the sentence level. Because BLEU and COMET are not guaranteed to follow a normal distribution and are evaluated on matched sentence pairs, the Wilcoxon signed-rank test is used.\n\n\nReflective translation produces statistically significant improvements for both BLEU and COMET. For BLEU, the median paired improvement is +0.0788 over 324 sentence pairs, with the Wilcoxon test strongly rejecting the null hypothesis of zero median difference (p=1.45√ó10‚àí44p=1.45\\times 10^{-44}). For COMET, the median improvement is +0.1753 over 457 sentence pairs, with p=1.10√ó10‚àí65p=1.10\\times 10^{-65}.\n\n\nTo quantify practical significance, the rank-biserial correlation associated with the Wilcoxon test is reported. Effect sizes are large for both metrics (BLEU: r=0.95r=0.95; COMET: r=0.96r=0.96), indicating that reflective translation yields not only statistically reliable but also practically meaningful gains.\n\n\nTable 1: Paired Wilcoxon signed-rank test results comparing first- and second-pass translations.\n\n\n\nMetric\nN\nMedian Gain\np-value\nEffect Size (rr)\n\n\n\n\nBLEU\n324\n+0.0788\n1.45√ó10‚àí441.45\\times 10^{-44}\n0.95\n\n\nCOMET\n457\n+0.1753\n1.10√ó10‚àí651.10\\times 10^{-65}\n0.96\n\n\n\n\n\n\n\n\n7 Discussion\n\nThe results suggest that structured self-reflection can reliably improve translation quality in low-resource settings without fine-tuning. Improvements are stronger and more consistent in COMET than BLEU, which aligns with reflection corr"
  },
  {
    "title": "Bandits in Flux: Adversarial Constraints in Dynamic Environments",
    "url": "https://arxiv.org/abs/2601.19867v1",
    "source": "arxiv",
    "summary": "We investigate the challenging problem of adversarial multi-armed bandits operating under time-varying constraints, a scenario motivated by numerous real-world applications. To address this complex setting, we propose a novel primal-dual algorithm that extends online mirror descent through the incorporation of suitable gradient estimators and effective constraint handling. We provide theoretical g",
    "full_text": null
  },
  {
    "title": "Calibration without Ground Truth",
    "url": "https://arxiv.org/abs/2601.19862v1",
    "source": "arxiv",
    "summary": "Villalobos et al. [2024] predict that publicly available human text will be exhausted within the next decade. Thus, improving models without access to ground-truth labels becomes increasingly important. We propose a label-free post-processing framework that improves a strong but miscalibrated model using a weaker yet better-calibrated reference. Our framework guarantees a strict performance improv",
    "full_text": null
  },
  {
    "title": "Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living",
    "url": "https://arxiv.org/abs/2601.19853v1",
    "source": "arxiv",
    "summary": "In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range",
    "full_text": null
  },
  {
    "title": "Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering",
    "url": "https://arxiv.org/abs/2601.19847v1",
    "source": "arxiv",
    "summary": "Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this o",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Task Formulation\n\n2.2 LLM Reasoning Signatures\n\nFinding 1. LLM reasoning traces leading to correct versus incorrect answers exhibit distinct activation patterns.\nFinding 2. Token-level neuron activations are predictive of the final correctness of LLM reasoning.\n\n\n\n\n\n3 Methodology\n\n3.1 Reasoning Neuron Identification\n3.2 Critical Activation Selection\n3.3 Adaptive Intervention Strategy\n3.4 Contrastive Data Construction\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\nDataset.\nBaseline.\nEvaluation.\n\n\n\n4.2 Main Results\n\nAdaRAS consistently improves reasoning correctness across all datasets, even surpassing post-trained LLMs.\nProbing-based activation steering is inherently unstable.\n\n\n\n4.3 Generalizability\n\nRCNs exhibit strong cross-dataset and cross-task transferability.\nAdaRAS scales effectively to stronger reasoning models.\n\n\n4.4 Ablation Studies\n\n\n\n5 Analysis\n\n\n5.1 Effect of Hyperparameter\n\nEffect of intervention strength.\nEffect of top-KK RCNs selection.\n\n\n\n5.2 Visualization of Steering\n\nAdaRAS mainly intervenes on later layer neurons.\nAdaRAS stabilizes latent reasoning trajectories without altering semantic modeling.\n\n\n\n\n\n6 Related Works\n\nReasoning Reliability.\nMechanistic Interpretability.\n\n\n7 Conclusion\n\nA Preliminary Study of Probing\n\nA.1 Data Construction\nA.2 Preprocessing\nA.3 Probing Classifier\n\n\n\nB Detailed Data Statistics\n\nB.1 Data for Adaptive Intervention Module\nB.2 Data for Evaluation\n\n\n\nC Details of Implementation\n\nC.1 Adaptive Intervention Module\n\nC.2 Evaluation\n\nMathematic Benchmarks.\nCoding Benchmarks.\n\n\n\n\n\nD Details of Reasoning Features\n\nSequence-Averaged Magnitude (‚Ñ≥¬Ø\\bar{\\mathcal{M}}).\nSequence-Averaged Angle (ùíú¬Ø\\bar{\\mathcal{A}}).\n\n\n\nE Additional Analysis Experiments\n\nE.1 Separability of Reasoning Trajectories in Latent Space\nE.2 Empirical Result of Polarity of Neuron Activations\n\n\n\nF Case Study\n\nProblem:\nAnalysis:\nProblem:\nAnalysis:\nProblem:\nAnalysis:\n\n\n\n\n\n\n\nIdentifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering\n\n\n\nFangan Dong1, Zuming Yan1, Xuri Ge1, Zhiwei Xu1,\nMengqi Zhang1, Xuanang Chen2, \nBen He3, Xin Xin1, Zhumin Chen1, Ying Zhou1\n1Shandong University\n2Institute of Software, Chinese Academy of Sciences\n3University of Chinese Academy of Sciences\nfangan.dong@mail.sdu.edu.cn, yingzhou@sdu.edu.cn\nCorresponding author.\n\n\nAbstract\nDespite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency.\nIn this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness.\nBased on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations.\nAdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases.\nExperiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25.\nMoreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost111The code and data are available at https://github.com/cat-sk/AdaRAS.\n\n\n\nIdentifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering\n\n\n\n\nFangan Dong1, Zuming Yan1, Xuri Ge1, Zhiwei Xu1,\nMengqi Zhang1, Xuanang Chen2,\n\nBen He3, Xin Xin1, Zhumin Chen1, Ying Zhou1‚Ä†‚Ä†thanks: Corresponding author.\n\n1Shandong University\n\n2Institute of Software, Chinese Academy of Sciences\n\n3University of Chinese Academy of Sciences\n\nfangan.dong@mail.sdu.edu.cn, yingzhou@sdu.edu.cn\n\n\n\n\n\n1 Introduction\n\nRecent large language models (LLMs)¬†(Jaech et al., 2024; Yang et al., 2025a; DeepSeek-AI, 2025) have demonstrated strong capability across a wide range of natural language processing tasks.\nIn particular, test-time scaling¬†(Wei et al., 2022; Brown et al., 2024) has improved LLM performance by allocating additional computation at inference, enabling applications in mathematical problem solving¬†(Guan et al., 2025; Muennighoff et al., 2025), code generation¬†(Shi et al., 2024; Jain et al., 2025), and spatial reasoning¬†(Fu et al., 2024).\nDespite these advances, reliable reasoning remains challenging even for state-of-the-art models, and most improvements rely on post-training methods¬†(Shao et al., 2024; Cui et al., 2025; Yu et al., 2025) or costly test-time strategies such as prompt engineering¬†(Tian et al., 2024; Saha et al., 2025), self-consistency¬†(Wang et al., 2023; Qiu et al., 2024), or multi-step calibration¬†(Cobbe et al., 2021; Li et al., 2025; Snell et al., 2024).\nWhile effective, these approaches treat LLMs as black boxes, incur substantial inference overhead, and offer limited insight into the sources of reasoning errors.\n\n\nFigure 1: An example of activation steering correcting an erroneous reasoning trajectory.\n\n\n\nRecent research works in activation engineering¬†(Turner et al., 2023) provides an internal alternative, showing that selectively manipulating activations in attention heads¬†(Li et al., 2023b) or MLP neurons¬†(Rimsky et al., 2024) can control model behaviors, such as truthfulness¬†(Marks and Tegmark, 2023), refusal¬†(Lee et al., 2025), or reducing bias¬†(Lu and Rimsky, 2024), without modifying model parameters or increasing inference cost.\nThis suggests that internal activations encode functionally specialized signals that can be directly leveraged for controllable behavior.\nHowever, reasoning reliability is a fundamentally different problem: it is a trajectory-level property requiring temporally coherent, multi-step inference with long-range dependencies, rather than adjustment of a single output attribute.\nIt therefore remains unclear whether existing activation engineering techniques can identify and enhance the internal mechanisms underlying reasoning correctness.\nThis raises a central question: Can activation-level interventions improve LLM reasoning reliability?\n\n\nTo address this question, we first examine whether reasoning correctness can be inferred via internal activations.\nInspired by probing studies of LLMs¬†(Gurnee et al., 2023), a pilot analysis reveals that a small subset of MLP neurons exhibits polarized activations between correct and incorrect reasoning trajectories, and that these activations are predictive of reasoning outcomes.\nBuilding by this observation, we propose AdaRAS, an adaptive activation steering framework for test-time reasoning enhancement.\nAdaRAS identifies RCNs by contrasting activations from correct and incorrect samples and applies polarity-based filtering to obtain a sparse, functionally consistent neuron set.\nDuring inference, AdaRAS selectively intervenes on these RCNs only when a trajectory is likely incorrect, enabling targeted correction without degrading already-correct one.\nExperiments on ten mathematic and coding benchmarks show that AdaRAS consistently improves accuracy across models, generalizes across tasks and datasets, and requires no additional training or sampling.\n\n\nThe contributions of this paper are threefold:\n1) To our knowledge, we present the first systematic evidence that reasoning correctness can be predicted and improved through neuron interventions, establishing activation steering as a viable tool for enhancing LLM reasoning.\n2) We introduce AdaRAS, a parameter-free, test-time activation steering framework that consistently enhances reasoning performance and transfers across tasks and datasets.\n3) We provide mechanistic insights showing that AdaRAS stabilizes latent reasoning trajectories while preserving semantic representations, enabling plug-and-play deployment.\n\n\n\n\n\n\n2 Preliminaries\n\nIn this section, we present preliminary probing results showing that specific neuron activations are predictive of reasoning correctness, which motivates the design of our proposed AdaRAS.\n\n\n\n2.1 Task Formulation\n\nWe first define Reasoning-Critical Neurons (RCNs) as neurons who positively contribute to correct reasoning outcomes.\nGiven a dataset D={(xi,yi)}D=\\{(x_{i},y_{i})\\} and a reasoning language model ‚Ñ≥\\mathcal{M}, we denote by rir_{i} the model-generated reasoning trace for input xix_{i}, and by aia_{i} the final answer extracted from rir_{i}. We define a binary correctness label ci=ùïÄ‚Äã[ai=yi]c_{i}=\\mathbb{I}[a_{i}=y_{i}], which serves as the target signal throughout this work.\nArchitecturally, the model ‚Ñ≥\\mathcal{M} consists of LL Transformer decoder layers.\nFollowing prior work¬†(Geva et al., 2021; Meng et al., 2022), we focus on the MLP blocks, which are widely believed to encode high-level semantic patterns.\nSpecifically, we take the intermediate activation for each MLP block as the internal representation.\n\n\n\nFigure 2: Comparison of activations of key neurons under successful and failed reasoning on AIME.\n\n\n\n\n2.2 LLM Reasoning Signatures\n\nMotivated by recent advances in LLM interpretability¬†(Belinkov, 2022; Gurnee et al., 2023), we ask whether reasoning correctness can be inferred directly from intermediate neuron activations at test time.\nAs a preliminary study, we probe last-token activations of Qwen3 series models¬†(Yang et al., 2025a) on mathematic datasets (i.e., AIME and AMC-12) to assess their predictive power for reasoning reliability.\nIn contrast to prior work on stylistic control or knowledge editing, we focus on identifying neurons that are specifically predictive of reasoning correctness.\nDetailed experimental setups are provided in the Appendix¬†A.\n\n\nTable 1: AUROC of probing classifiers trained on last-token activations for "
  },
  {
    "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
    "url": "https://arxiv.org/abs/2601.19839v1",
    "source": "arxiv",
    "summary": "Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrate",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Personalization in Dialogue Systems\n2.2 Multi-User Human-Robot Interactions\n2.3 Multimodal Memory Management\n2.4 Ethical Challenges in SAR\n\n\n\n3 Methods\n\n3.1 Scenario\n3.2 Problem Formulation\n\n3.3 System Design\n\n3.3.1 Perception Module\n3.3.2 User Modeling Module\n3.3.3 World Modeling Module\n3.3.4 Generation Module\n\n\n3.4 User Interface Implementation\n\n\n\n4 Experiments\n\n4.1 Dataset\n4.2 Metrics\n\n4.3 Configuration\n\n\n5 Results\n\n5.1 Comparative Study\n5.2 Ablation Study\n\n5.3 Qualitative Evaluation\n\n\n6 Implementation\n\n7 Conclusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs\n\n\nJeanne Mal√©cot‚àó,1,2{}^{*,~1,~2}, Hamed Rahimi‚àó,2{}^{*,~2}, Jeanne Cattoni3, Marie Samson2, Mouad Abrini2, Mahdi Khoramshahi2, Maribel Pino3, Mohamed Chetouani2\n\n1Institut Curie, Universit√© Paris-Saclay\n2Institute of Intelligent Systems and Robotics (ISIR), Sorbonne University\n3Assistance Publique ‚Äì H√¥pitaux de Paris (AP-HP), Universit√© Paris Cit√©ParisFrance\n\n\n\nAbstract.\nExisting human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.\n\n\n\n\nCode and Data\n\n\n\n1. Introduction\n\nFigure 1. Overview of HARMONI. The proposed framework consists of four modules:(i) a multimodal perception module that identifies the active speaker and extracts the query from visual and auditory inputs; (ii) a world modeling module that maintains representations of all users and the ongoing conversation within a session; (iii) a user modeling module that retrieves and updates speaker-specific profiles for long-term personalization; and (iv) a generation module that produces contextually grounded and personalized responses conditioned on the speaker, their profile, surrounding users, and the conversational context.\n\n\nSocially Assistive Robotics (SAR)¬†(Feil-Seifer and Matariƒá, 2011b) has emerged as a promising field aimed at supporting individuals facing diverse challenges- including older adults, patients, and people with disabilities- by addressing their physical, cognitive, and social needs. While much of the research in this area has focused on one-on-one Human-Robot Interactions (HRI)¬†(Rahimi et al., 2025c, b), there is a growing demand for robots that can function effectively in multi-user environments¬†(Louie et al., 2014; Bettencourt et al., 2025), such as nursing homes, therapy centers, and group rehabilitation programs.\n\n\nFigure 2. Overview of a challenging multi-turn interaction scenario. The robot dynamically switches context to a new user (Resident B) upon interruption and accurately resolves anaphora (e.g., ‚Äúthe appointment‚Äù) in follow-up queries by maintaining the updated conversational state, clearly distinguishing it from the initial interaction with Resident A.\n\n\nExisting HRI systems, however, often lack robust mechanisms for personalization across multiple turns and multiple users¬†(Li et al., 2021; Tatarian et al., 2021). A robot must coordinate dynamically, adapt to evolving environments, and adjust behaviors in response to multiple interacting users¬†(Umbrico et al., 2022). Without effective personalization, these systems risk reduced engagement, diminished trust, and limited long-term effectiveness¬†(Lee et al., 2012). Multi-user adaptation over time including memory of prior interactions, recognition of user states, and flexible reuse of past experiences is critical to maintaining user interest, building rapport, and ensuring the acceptability of SARs (Finkel and Kr√§mer, 2023). To this day, however, such robots remain limited in their ability to provide deep, long-term personalization across multiple users and interaction sessions, highlighting a critical gap in current HRI systems.\n\n\nTo address these limitations, we propose a multimodal personalization framework for multi-user HRI that enables robots to dynamically adapt to both individual and group needs, while ensuring compliance with ethical and social constraints. As illustrated in Figure¬†1, the framework first receives a video-based query from the user and processes it through modular components operating in parallel to generate personalized dialogue responses. These responses leverage both short-term conversational context and long-term adaptation, enabling robots to recall past interactions and dynamically generate future behaviors through user modeling. This work advances the development of SARs by introducing a framework that integrates personalization with privacy- and ethics-preserving mechanisms, enabling deployment in sensitive contexts such as elder care. The key contributions of this paper are: (1) a multimodal personalization strategy that adapts to diverse user preferences and behaviors; (2) an online user modeling approach for adaptive and continuous personalization; (3) a real-time multi-user memory management system encompassing both long-term memory, which supports sustained adaptation across sessions, and short-term memory, which captures context within the present interaction; (4) an ethical design methodology that explicitly balances personalization with privacy and safety considerations; and (5) a scenario-driven evaluation conducted in realistic nursing home environments, Figure¬†2, demonstrating the effectiveness and challenges of SARs in multi-user, socially assistive settings.\n\n\n\n\n2. Related Work\n\n\n2.1. Personalization in Dialogue Systems\n\nPersonalization in dialogue systems has been studied extensively to enhance user engagement and response quality. Prior work spans several dimensions: (i) surveys of datasets and problem categories¬†(Chen et al., 2024), (ii) fine-tuning frameworks such as In-Dialogue Learning (IDL) that infer personas from dialogue history without predefined profiles¬†(Cheng et al., 2024), (iii) latent-variable models like MIRACLE that decompose complex traits into multi-faceted attributes¬†(Lu et al., 2023), (iv) attention-based mechanisms such as Persona-Adaptive Attention (PAA) that jointly model persona and context while reducing redundancy¬†(Huang et al., 2023), and (v) prompt-tuning approaches that enable efficient, multilingual personalization in large-scale pre-trained models¬†(Kasahara et al., 2022). While these advances substantially improve single-user, text-based personalization, challenges remain in multi-user, socially assistive settings where privacy, ethical safety, and long-term memory are critical.\n\n\n\n\n2.2. Multi-User Human-Robot Interactions\n\nMulti-user HRI research addresses challenges such as managing interruptions, coordinating overlapping tasks, and supporting collaborative decision-making. Approaches include intent-based interruption detection with LLMs and context-aware strategies¬†(Cao et al., 2025), multi-user dialogue modeling via contextual query rewriting on datasets like Multi-User MultiWOZ¬†(Jo et al., 2023), and analyses of interruption effects showing that extrinsic disruptions increase perceived workload despite stable task performance¬†(Dahiya and others, 2023). Collaborative task execution frameworks, such as activation spreading architectures, further enable dynamic task allocation across overlapping sub-tasks¬†(Anima et al., 2019). While these works advance interruption management, decision-making, and collaboration, they emphasize coordination over personalization. In contrast, our framework introduces multimodal personalization and real-time multi-user memory management, allowing robots to adapt to individual user profiles and conversational histories in addition to coordinating across users.\n\n\n\n\n2.3. Multimodal Memory Management\n\nMemory architectures and multimodal feature extraction are critical for maintaining coherence in adaptive dialogue systems. Recent work includes dynamic frameworks such as User-VLM R1¬†(Rahimi et al., 2025d) and Mem0, which consolidate and retrieve salient information with low latency¬†(Chhikara et al., 2024), and graph-based approaches that improve relational reasoning through entity‚Äìrelation modeling. In multimodal dialogue, position-aware fusion models like PMATE integrate textual and visual signals for improved response grounding¬†(He et al., 2024), while embodied agents benefit from context- and environment-aware planning frameworks such as CAPEAM¬†(Kim et al., 2023). Complementary studies further show that incorporating visual cues (e.g., facial expressions, gaze)¬†(Rahimi et al., 2025a) with textual inputs enhances intent disambiguation and emotion recognition¬†(Hasani et al., 2023). Although these advances underscore the value of memory and multimodal integration, they are largely restricted to single-user or task-specific settings. Our framework extends this direction by addressing multi-user socially assistive HRI, combining multimodal personalization, dynamic user modeling across temporal scales, and ethically aligned memory management for sensitive domains such as elder care.\n\n\n\n\n2.4. Ethica"
  },
  {
    "title": "Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models",
    "url": "https://arxiv.org/abs/2601.19834v1",
    "source": "arxiv",
    "summary": "Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 A World Model Perspective on Multimodal Reasoning\n\n3.1 Formulation: Multiple Observations of the World\n3.2 Atomic Capabilities of World Models\n3.3 Deliberate Reasoning with World Modeling Across Modalities\n3.4 The Visual Superiority Hypothesis\n\n\n\n4 Experiment Settings\n\n4.1 VisWorld-Eval: Task Suite for Reasoning with Visual World Modeling\n4.2 Unified Multimodal Model Training and Evaluation\n\n\n\n5 Experimental Results\n\n5.1 Visual World Simulation Boosts Multimodal Reasoning\n5.2 Visual World Reconstruction Boosts Multimodal Reasoning\n5.3 Visual World Modeling is Unhelpful for Certain Tasks\n5.4 Comparison with VLMs: Do UMMs Compromise Verbal Reasoning Capabilities?\n5.5 RL Enhances Various CoTs, Yet Does Not Close the Gap\n\n\n6 Discussions\n\n7 Theorectical Analysis\n\n7.1 Informativeness\n\n7.2 Prior Knowledge\n\n7.2.1 General Transfer Learning Analysis\n7.2.2 Remarks on Multimodal Reasoning\n\n\n\n\n\n8 Experiment Details\n\n8.1 VisWorld-Eval and Training Data\n8.2 Model Training\n8.3 Analytic Experiments\n\n\n\n9 Extended Experimental Results\n\n9.1 Full Results on MMSI-Bench\n9.2 Additional Qualitative Evaluation\n\n\n\n\n\n\n\n\n1]Tsinghua University\n2]ByteDance Seed\n\\contribution[*]Work done at ByteDance Seed\n\\contribution[‚Ä†]Corresponding authors\n\nVisual Generation Unlocks Human-Like Reasoning through Multimodal World Models\n\n\nJialong Wu\n\n‚ÄÉ‚ÄÉ\nXiaoying Zhang\n\n‚ÄÉ‚ÄÉ\nHongyi Yuan\n\n‚ÄÉ‚ÄÉ\nXiangcheng Zhang\n\n‚ÄÉ‚ÄÉ\nTianhao Huang\n\n‚ÄÉ‚ÄÉ\nChangjing He\n\n‚ÄÉ‚ÄÉ\nChaoyi Deng\n\n‚ÄÉ‚ÄÉ\nRenrui Zhang\n\n‚ÄÉ‚ÄÉ\nYoubin Wu\n\n‚ÄÉ‚ÄÉ\nMingsheng Long\n\n[\n\n[\n\nwujialong0229@gmail.com\n\nmingsheng@tsinghua.edu.cn\n\nzhangxiaoying.xy@bytedance.com\n\n\n(January 27, 2026)\n\nAbstract\nHumans construct internal models of the world and reason by manipulating the concepts within these models. Recent advances in artificial intelligence (AI), particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems, which rely predominantly on verbal reasoning as their primary information-processing pathway. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though a clear consensus on their benefits has not yet been reached. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks‚Äìparticularly those grounded in the physical world‚Äìvisual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of deliberate CoT reasoning and analyze distinctions among different forms of world models from both informativeness and knowledge aspects. Empirically, we identify and design tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Through controlled experiments on a state-of-the-art UMM, we show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling. Conversely, it offers no clear advantage for tasks that do not require explicit visual modeling. Together, these insights and findings clarify the applicability and potential of multimodal world modeling and reasoning for more powerful, human-like multimodal AI. We publicly release our evaluation suite to facilitate further research.\n\n\n\\checkdata\n[Project Lead]Jialong Wu at \n\\correspondenceMingsheng Long at , \n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÇ‚ÄÑ Xiaoying Zhang at \n\\checkdata[Project Page]https://thuml.github.io/Reasoning-Visual-World\n\n\n\n\n1 Introduction\n\nFigure 1: Overview of a world-model perspective on multimodal reasoning. (a) Humans construct mental models of the world, representing information and knowledge through two complementary channels‚Äìverbal and visual‚Äìto support reasoning, planning, and decision-making. (b) Recent advances in large language models (LLMs) and vision language models (VLMs) largely rely on verbal chain-of-thought reasoning, leveraging primarily verbal and symbolic world knowledge. (c) Unified multimodal models (UMMs) open a new paradigm by using visual generation for visual world modeling, advancing more human-like reasoning on tasks grounded in the physical world. Examples of reasoning with verbal world modeling are adapted from Guo et al. [18], Du et al. [14], Chen et al. [9], Zhang et al. [72].\n\n\nHumans construct internal mental models of the external world that represent objects and concepts, along with their relationships, structures, and operational mechanisms [11, 16]. These models support reasoning and decision-making by enabling mental simulation, allowing individuals to anticipate the outcome of actions without actually taking them [40]. For example, if a glass of water is spilled on the table, people can rapidly mentally simulate the ensuing events: the water falling downward, spreading across the surface, and potentially dripping onto the floor. Such predictions lead them to quickly move valuable items away or reach for a towel. Beyond physical systems, mental models also extend to any domain where relational structures can be simulated, such as mathematics and logic [31, 32], making them fundamental to how humans understand and interact with all aspects of the world.\n\n\nCross-disciplinary researchers in philosophy, psychology, cognitive science, and related fields have a long history of developing computational models of human mental models [44]. Among them, artificial intelligence (AI) shares a core ambition of building machines that reason like people. Although debates remain, recent breakthroughs, especially in large language models (LLMs) and chain-of-thought (CoT) reasoning, have made a substantial step towards approximating human reasoning grounded in mental models of the world, often referred to as world models [24, 34] in the AI literature. During chain-of-thought reasoning, LLMs explore, reflect, and backtrack within the structured solution space, guided by world knowledge acquired through large-scale pre-training. These capabilities have already driven progress in diverse domains, including programming [18], mathematics [57, 18], scientific discovery [53], clinical medicine [58], and robotics [42].\n\n\nSuch reasoning capabilities have also been extended to multimodal AI systems, particularly vision language models (VLMs) [28, 6, 19, 70]. These systems typically incorporate visual inputs by aligning visual representations with the embedding space of LLMs, resulting in reasoning that remains primarily constrained to a linguistic space. In contrast, human mental models operate over multiple forms of mental representations. Dual-coding theory [45] suggests that the mind processes information through two complementary codes: verbal and imagery (particularly visual) representations. These pathways can function independently but often collaborate to support reasoning. Indeed, visual imagery has been shown to have advantages over words in memory encoding and retrieval [33]; and individuals with aphantasia, who lack the ability to visualize mental imagery, exhibit worse performance on tasks such as visual search [43]. These evidence from psychology and cognitive science therefore suggest that the absence of a dedicated visual information pathway may explain why current multimodal AI systems excel in formal and abstract domains dominated by verbal world knowledge, yet continue to fall far short of human performance on tasks involving physical and spatial reasoning [49, 8], which fundamentally depend on visual world modeling.\n\n\nNext-generation multimodal AI systems are evolving to be built upon unified multimodal models (UMMs) [54, 63, 62, 13], which seamlessly integrate both verbal and visual generation capabilities. The newly introduced visual generation component offers the potential to explicitly realize visual world modeling, a critical element of multimodal world models in human-like reasoning that current systems largely lack. This naturally makes us ponder: Can current UMMs truly leverage their visual generation capability to enhance reasoning and thereby narrow the performance gap between multimodal AI and humans? A growing body of preliminary research [36, 77, 38, 76, 17] has begun exploring this question from different perspectives. However, the findings so far remain inconclusive. Reported empirical results are mixed, showing no consistent trends that visual generation reliably improves reasoning performance. Moreover, the evaluation tasks used in current studies are designed heuristically, lacking a principled basis for understanding when and how visual generation can meaningfully contribute to multimodal reasoning.\n\n\nIn this paper, we present the first principled study of when and how visual generation benefits reasoning from a world-model perspective (see Figure 1), making both theoretical and empirical contributions.\n\n\nTheoretically, we rigorously bridge the concepts of world models and reasoning. (1) World model formulations: We formulate multimodal world models to approximate the underlying multi-observable Markov decision processes (MOMDP) of tasks, and define two fundamental capabilities of world models, namely world reconstruction and world simulation. (2) World model-based reasoning: To realize world models for reasoning, we introduce three reasoning formulations. Two rely solely on verbal CoTs through implicit or verbal world modeling, while the third inter"
  },
  {
    "title": "A Multi-directional Meta-Learning Framework for Class-Generalizable Anomaly Detection",
    "url": "https://arxiv.org/abs/2601.19833v1",
    "source": "arxiv",
    "summary": "In this paper, we address the problem of class-generalizable anomaly detection, where the objective is to develop a unified model by focusing our learning on the available normal data and a small amount of anomaly data in order to detect the completely unseen anomalies, also referred to as the out-of-distribution (OOD) classes. Adding to this challenge is the fact that the anomaly data is rare and",
    "full_text": null
  },
  {
    "title": "Neural Neural Scaling Laws",
    "url": "https://arxiv.org/abs/2601.19831v1",
    "source": "arxiv",
    "summary": "Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers fro",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Neural Neural Scaling Laws\n\n2.1 Validation Loss Representations\n\n2.2 Architecture\n\nLoss Encoder.\nTransformer.\nPrediction Head.\n\n\n2.3 Training Data\n2.4 Evaluation\n\n\n\n3 Results\n\nZero-shot generalization.\n3.1 Case Study: ARC-Easy across scales\n3.2 Error Over Time and Calibration\n3.3 Ranking Accuracy\n\n\n4 Related Work\n\n5 Discussion\n\n5.1 Toward Foundation Models of Training Dynamics\n5.2 Limitations and Future Work\n\n\nA DiffHist\n\n\n\n\n\nNeural Neural Scaling Laws\n\n\nMichael Y. Hu\n\n‚ÄÉ‚ÄÉ\nJane Pan\n\n‚ÄÉ‚ÄÉ\nAyush Rajesh Jhaveri\n\n‚ÄÉ‚ÄÉ\nNicholas Lourie\n\n‚ÄÉ‚ÄÉ\nKyunghyun Cho\n\n\n\nAbstract\nNeural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors.\nTo address this, we propose Neural Neural Scaling Laws (NeuNeu), a neural network that frames scaling law prediction as time-series extrapolation. NeuNeu¬†combines temporal context from observed accuracy trajectories with token-level validation losses, learning to predict future performance without assuming any bottleneck or functional form. Trained entirely on open-source model checkpoints from HuggingFace, NeuNeu¬†achieves 2.04% mean absolute error in predicting model accuracy on 66 downstream tasks‚Äîa 38% reduction compared to logistic scaling laws (3.29% MAE). Furthermore, NeuNeu¬†generalizes zero-shot to unseen model families, parameter counts, and downstream tasks. Our work suggests that predicting downstream scaling laws directly from data outperforms parametric alternatives.\n\nscaling laws, meta-learning\n\n\n\n1 Introduction\n\nNeural scaling laws characterize how language model performance improves with increased compute, data, and parameters (Kaplan et al., 2020; Hoffmann et al., 2022). These laws typically take the form of power-law relationships such as L‚Äã(C)=Œ±‚ÄãC‚àíŒ≤L(C)=\\alpha C^{-\\beta}, where LL is training loss and CC is the input to training, such as compute. Such simple functional forms have proven remarkably useful for predicting training dynamics and choosing resource allocation.\n\n\nHowever, the translation from training loss to downstream task performance is far more complex. While aggregate metrics like training or validation loss follow smooth scaling curves, individual task accuracies exhibit diverse behaviors with scale: some improve monotonically, others plateau, and some even degrade with scale‚Äîa phenomenon known as inverse scaling (McKenzie et al., 2023). Taken together, it seems that no single parametric family can capture the full spectrum of scaling behaviors (Lourie et al., 2025).\n\n\nWe hypothesize that predicting future performance from validation loss suffers from two flaws, both of which limit the usefulness of downstream scaling laws: first, validation loss is an arbitrary bottleneck, and averaging the myriad token losses obscures signal; and second, no simple hypothesis class exists to fit all behaviors of downstream tasks. To fix these issues, we propose to use a neural network that predicts downstream task performance while incorporating token-level loss information.\n\n\n\n\n‚ÄÉ\n \n‚ÄÉ\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: \nRicher signal from token-level losses (center) enables NeuNeu¬†to better forecast accuracies for downstream tasks (right). Average validation loss, used in logistic scaling laws, averages away token-level loss changes.\n\n\n\nOur ‚Äúneural‚Äù neural scaling law, or NeuNeu, frames scaling law prediction as a time-series extrapolation problem.\nUnlike parametric approaches that rely on aggregate metrics, NeuNeu¬†predicts downstream performance by combining observed accuracy trajectories with token-level validation losses. This allows the model to leverage the rich signal within loss distributions that averaging typically obscures.\nTo ensure generalization across unseen model families and parameter counts, we design inputs to be invariant across model scales. We achieve this by abstracting training steps into relative compute intervals and converting unbounded losses into token probabilities, enabling the network to learn patterns in training dynamics decoupled from the specific training configuration.\n\n\nWe train NeuNeu¬†on open-source language model training trajectories (Magnusson et al., 2025) on HuggingFace (Wolf et al., 2020), meaning that anyone can fit their own neural neural scaling laws without first performing large numbers of training runs. Our results show that NeuNeu¬†achieves 2.04% mean absolute error (MAE) on 66 downstream tasks, a 38% reduction compared to the widely used logistic scaling laws (Magnusson et al., 2025; Gadre et al., 2025). Furthermore, NeuNeu¬†is a more robust decision-making tool: it generalizes zero-shot to unseen tasks with lower error than logistic scaling laws achieve on tasks they were explicitly fit to, and correctly ranks the final performance of competing model configurations with 75.6% accuracy, a 12.3% improvement over baselines. Our contributions:\n\n\n1.\n\nWe propose NeuNeu, the first model that predicts downstream scaling performance without parametric or prior assumptions. NeuNeu¬†outperforms baseline logistic scaling laws by an average of 39% at predicting downstream accuracy across 66 downstream tasks (¬ß3).\n\n\n\n2.\n\nWe train NeuNeu¬†with quantile regression (Koenker and Hallock, 2001), allowing the model to predict uncertainty. Over our test set, we find that NeuNeu‚Äôs 10%-90% interquantile range captures 74.9% of the true data out of an expected 80%, suggesting that the model is well-calibrated (¬ß3.2).\n\n\n\n3.\n\nNeuNeu¬†also outperforms neural baselines that use average or no validation loss. This demonstrates that predicting downstream performance from average validation loss discards information usable by a more power model. As the corpus of open-source models and training trajectories grows, we advocate for more expressive scaling laws that scale with data (¬ß5).\n\n\n\n\n\nCode: https://github.com/michahu/neuneu\n\n\nFigure 2: NeuNeu¬†encodes and processes token-level validation probabilities alongside a sequence of historical downstream accuracies and compute gaps, which are projected into context tokens. The BERT-style Transformer (Devlin et al., 2019) backbone uses this information to predict a distribution over the downstream accuracy via quantile regression on the [CLS] token.\n\n\n\n\n2 Neural Neural Scaling Laws\n\nThis work focuses on improving downstream performance prediction from validation losses, the main metric during language model pretraining. Periodically, we evaluate a language model on a validation set of NN tokens and DD downstream tasks. Suppose we are at time tt and want to predict a language model‚Äôs task performance at time t+Kt+K. We have the following information at our disposal:\n\n\n‚Ä¢\n\nToken-level loss vectors ‚Ñì1:t‚àà‚Ñùt√óN\\bm{\\ell}_{1:t}\\in\\mathbb{R}^{t\\times N}\n\n\n\n‚Ä¢\n\nDownstream accuracies ùê≤ùüè:ùê≠‚àà[0,1](t‚àí1)√óD\\mathbf{y_{1:t}}\\in[0,1]^{(t-1)\\times D}\n\n\n\n\n\nLogistic scaling laws solve this prediction problem by assuming that 1) the average validation loss ‚Ñì¬Øt\\bar{\\ell}_{t} is sufficient to predict all downstream task performances ùê≤t\\mathbf{y}_{t} and 2) the relationship between validation loss and downstream task performance is well-described by a logistic function, which is natural for predicting values that transition between chance and a saturating threshold. One then fits the parameters of the logistic function:\n\n\n\nyt(i)\\displaystyle y_{t}^{(i)}\n=f‚Äã(‚Ñì¬Øt;a,k,L0,b)\\displaystyle=f(\\bar{\\ell}_{t};a,k,L_{0},b)\n\n\n\n\n\n=a1+e‚àík‚Äã(‚Ñì¬Øt‚àíL0)+b\\displaystyle=\\frac{a}{1+e^{-k(\\bar{\\ell}_{t}-L_{0})}}+b\n\n\n\n\n\nThese scaling laws have high bias and low expressivity. In moving to a neural network, which is more expressive, we must choose input and output representations that allow the neural network to extrapolate. In ¬ß2.1, we discuss our representation choices, present the architecture in ¬ß2.2, and finish with training and evaluation details in ¬ß2.3 and ¬ß2.4.\n\n\n\n2.1 Validation Loss Representations\n\nOne drawback of logistic scaling laws is that the average loss ‚Ñì¬Øt\\bar{\\ell}_{t} does not retain distributional information, which we hypothesize is beneficial for predicting downstream performance. Two models can achieve the same validation loss with loss distributions of different skews or variances, which could be indicative of different underlying capabilities.\n\n\nFirst, to fix the issue with cross-entropy loss being unbounded, we convert token-wise losses into token-wise probabilities:\n\n\n\npi=e‚àí‚Ñìifor¬†‚Äãi=1,‚Ä¶,Np_{i}=e^{-\\ell_{i}}\\quad\\text{for }i=1,\\ldots,N\n\n\n\nIn general, we found that training on probabilities leads to better neural models than training on losses; see Figure 8 for discussion and ablation.\n\n\nTo test our hypothesis about distributional information, we consider three input representations:\n\n\n‚Ä¢\n\nNeuNeu: For the first and primary representation, we directly provide all token probabilities ùíët\\bm{p}_{t} to the model.\n\n\n\n‚Ä¢\n\nAverage: As a baseline, we use the previous sequence of average validation probabilities p¬Ø‚â§t\\bar{p}_{\\leq t}, similar to logistic scaling laws.\n\n\n\n‚Ä¢\n\nDiffHist: Finally, to provide intuition, we bin probabilities into a histogram of BB bins. We then provide the model the difference Œî‚Äãh\\Delta h in the histograms of ptp_{t} and pt+Kp_{t+K}. See Appendix A for precise equations. Unlike the previous two, this representation sees the future; we include it to make the point that distribution shape is helpful for predicting future accuracy.\n\n\n\n\n\nIn summary, we train NeuNeu¬†on token probabilities. As a baseline, we test using average validation probabilities. Last, we include a binned histogram representation to illustrate the kinds of distributional signal los"
  },
  {
    "title": "When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering",
    "url": "https://arxiv.org/abs/2601.19827v1",
    "source": "arxiv",
    "summary": "Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronize",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Datasets for Multi-hop Reasoning\n2.2 Iterative Retrieval-Augmented Generation\n\n\n\n3 Methodology\n\n\n3.1 Evaluation Framework and Metrics\n\nDifficulty Stratification.\n\n3.1.1 Diagnostic Suite\n\nI. Evidence Acquisition Diagnostics (Retrieval Quality)\nII. Strategic Control Diagnostics (Planning &amp; Adherence)\nIII. Information Synthesis Diagnostics\n\n\n\n\n\n3.2 Experimental Setup\n\n3.2.1 Dataset and Indexing\n\n3.2.2 Synergized Reasoning and Retrieval Implementation\n\nStep Definition and Loop.\nPartial Answer State.\nContext Management.\n\n\n3.2.3 Correctness and Difficulty\n\n\n\n\n\n4 Results\n\n4.1 Decomposing Performance: The \"Synchronized\" Advantage\n4.2 Stability Analysis: Recoveries vs. Regressions\n4.3 The Cost of Retrieval: Parametric Memory Suppression\n4.4 Portion of Unanswered Questions\n\n\n\n5 Analysis\n\n\n5.1 Dynamics of Iterative Utilization\n\n5.1.1 Mechanism of Control: Self-Correction via Anchor Propagation\n5.1.2 Step-Count Distribution and Model Strategy\n\n\n\n5.2 Failure Modes in Iterative RAG\n\n5.2.1 Retrieval Coverage Gaps: The Prerequisite for Reasoning\n5.2.2 Evidence Sufficiency vs. Retrieval Coverage\n5.2.3 Confidence Miscalibration: Stopping Too Early or Too Late\n5.2.4 Composition Failure: The Synthesis Bottleneck\n5.2.5 Distractor Latch: The Retrieval Trap\n\n\n\n5.3 Efficiency Analysis\n\n5.3.1 The Trade-off: Adaptivity vs. Predictability\n\n\n\n5.4 Adherence and Controllability\n\nSuccess Rate of Compliance Attempts\n\n\n\n\n6 Discussion\n7 Conclusion\n\nS1 Appendix\n\nS1.1 Extended Literature Review\nS1.2 Performance and Token Utilization\n\nS1.3 Detailed Failure Mode Analysis\n\nS1.3.1 Prevalence and Impact\nS1.3.2 Damage Index\n\n\nS1.4 Query Characteristics\nS1.5 Case Studies and System Prompts\n\n\n\n\n\n\n\nWhen Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering\n\n\nMahdi Astaraki  astarakm@mcmaster.ca \nDepartment of Computational Science and Engineering, McMaster University, Canada \nBASF Canada Inc., Canada\nMohammad Arshi Saloot  mohammad.arshi-saloot@basf.com \nBASF Canada Inc., Canada\nAli Shiraee Kasmaee  shiraeea@mcmaster.ca \nBASF Canada Inc., Canada\nHamidreza Mahyar  mahyarh@mcmaster.ca \nDepartment of Computational Science and Engineering, McMaster University, Canada\nSoheila Samiee  soheila.samiee@basf.com  \nBASF Canada Inc., Canada\nCorresponding author.\n\n\nAbstract\nRetrieval Augmented Generation (RAG) is widely used to extend large language models (LLMs) beyond their parametric knowledge, yet it remains unclear when iterative retrieval-reasoning loops meaningfully outperform traditional static RAG, particularly in scientific domains where multi hop reasoning, sparse domain knowledge, and heterogeneous evidence impose substantial complexity. This study provides the first controlled, mechanism level diagnostic evaluation of whether synchronized iterative retrieval and reasoning can surpass even an idealized static upper bound (Gold-Context) RAG.\nWe benchmark eleven State-of-the-Art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training free controller that alternates retrieval, hypothesis refinement, and evidence aware stopping. Using the chemistry focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze model behavior through a comprehensive diagnostic suite covering retrieval coverage gaps, anchor carry drop, query quality, composition fidelity, and control calibration.\nAcross models, iterative RAG consistently outperforms Gold Context, yielding gains up to 25.6 percentage points, particularly for non-reasoning fine-tuned models. Our analysis shows that synchronized retrieval and reasoning reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, benefits that static evidence cannot provide. However, we also identify limiting failure modes, including incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval.\nOverall, our results demonstrate that the process of staged retrieval is often more influential than the mere presence of ideal evidence. We provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and establish a foundation for developing more reliable, controllable iterative retrieval‚Äìreasoning frameworks.\nThe code and evaluation results are available here.\n\n\n\n1 Introduction\n\nMulti-hop question answering (QA) requires composing evidence across multiple steps and sources to arrive at a correct final answer. In scientific domains, this poses a particularly hard challenge: relevant knowledge is sparse, evidence must be chained across heterogeneous resources, and intermediate conclusions must be synthesized into final claims. As a result, multi-hop QA is a direct stress test of a model‚Äôs ability to manage cognitive load, control deliberation, and integrate retrieval with reasoning (Adapala, 2025). Retrieval-Augmented Generation (RAG) has emerged as a central strategy to reduce reliance on parametric memory by grounding generation in external evidence (Lewis et al., 2020). Yet most evaluations treat retrieval as a static preprocessing step, followed by one-shot generation over a fixed context. Recent work argues that advanced RAG algorithms, such as iterative or dynamic RAG, can outperform static pipelines by progressively focusing the evidence set and correcting course mid-chain (Gao et al., 2025).\n\n\nThis strategy supports multi-hop QA in two complementary ways: (i) reasoning-augmented retrieval and (ii) retrieval-augmented reasoning. Prior work on reasoning-augmented retrieval typically assumes that the ‚Äúideal evidence‚Äù, Gold Context supplied by dataset annotators, defines an upper bound, and thus evaluates how far improved retrieval can approach that bound without surpassing it (Nahid and Rafiei, 2025). Meanwhile, another group of studies (Xu et al., 2024; Li et al., 2025b; Wu et al., 2025) focus on enhancing retrieval-augmented reasoning, showing superior performance over direct reasoning without retrieval and over standard RAG pipelines.\nHowever, most existing comparisons use one step retrieval as the only baseline. This makes results highly sensitive to parsing, chunking, embedding, and re-ranking design choices, and obscures whether improvements stem from the algorithm or simply from retrieval configuration variance. Moreover, the Gold Context baseline (commonly included in reasoning-augmented retrieval studies) is often absent in evaluations of retrieval-augmented reasoning, making it difficult to form a complete picture. Although Gold Context is not guaranteed to be an operational upper bound, since it may be distracting for long chains with many internal hops, misaligned with a model‚Äôs reasoning trajectory, or insufficient for compositional synthesis (Nahid and Rafiei, 2025; Chen et al., 2025), its inclusion remains essential for understanding the limits of static RAG.\n\n\nTaken together, prior work tends to evaluate only one side of the retrieval‚Äìreasoning interaction‚Äîeither retrieval‚Äëenhanced reasoning or reasoning‚Äëenhanced retrieval‚Äîand rarely examines how both critical baselines (No Context and Gold Context) jointly shape conclusions. Moreover, most studies evaluate only a small set of language models (typically fewer than five), which limits any systematic assessment of how model architecture influences observed performance. Existing survey papers primarily summarize reported results without offering deeper diagnostic insights, and direct, mechanism-level evaluation of retrieval‚Äëaugmented reasoning in scientific multi-hop QA remains largely unexplored.\n\n\nThis paper offers a diagnostic re-examination of when and why synchronized retrieval and reasoning can beat ideal evidence in scientific multi-hop QA. Our goal is to jointly evaluate both aspects of potential enhancements within a single controlled framework, and to determine whether iterative retrieval can support reasoning strongly enough to surpass an idealized static evidence condition. To achieve this, we evaluate three regimes: (i) No Context (parametric memory only), (ii) Gold Context (Oracle evidence is supplied to the generator as a paragraph for each hop.), and (iii) Iterative RAG (a controlled retrieval - i.e., reasoning loop with explicit step allocation and stopping). Our study focuses on chemistry QA, a domain where general-purpose training provides limited coverage and where retrieval is genuinely required to bridge knowledge gaps. We begin with a No Context screen to remove questions answerable from internal memory and concentrate the analysis on retrieval-dependent cases.\n\n\nWe structure the investigation around four questions:\n(1) Accuracy: Under what conditions does iterative RAG outperform Gold Context, and how does this vary across model families and hop depths?\n(2) Utilization Dynamics: How do models use the retrieval loop to self-correct (e.g., anchor propagation), allocate steps across the chain, and calibrate stopping.\n(3) Failure Modes: What are the dominant sources of error in scientific multi-hop QA (e.g., coverage gaps in the final hop, composition failures despite sufficient evidence, distractor chains, and miscalibrated stopping)?\n(4) Efficiency and Compliance: What cost-accuracy archetypes emerge, and how strictly do models follow procedural constraints when parametric knowledge is tempting (Procedural Compliance Rate, PCR)?\nRather than proposing a new dynamic-RAG algorithm or producing a broad survey, we design a controlled evaluation that isolates the mechanisms by which synchronized retrieval and reasoning confer advantages ‚Äì or fail. We compare more than ten language models spanning non-reasoning and reasoning-oriented architectures, track iterative utilization signals, and quantify sufficiency and coverage at each ho"
  },
  {
    "title": "Routing End User Queries to Enterprise Databases",
    "url": "https://arxiv.org/abs/2601.19825v1",
    "source": "arxiv",
    "summary": "We address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly ",
    "full_text": "\n\n\n\nI Introduction\nII Problem Statement\nIII Dataset Construction\nIV Approach\nV Results and Discussion\nVI Conclusion\n\n\n\n\n\nRouting End User Queries to Enterprise Databases\n\n\n\nSaikrishna Sudarshan\n\n‚ÄÉ‚ÄÉ\nTanay Kulkarni\n\n‚ÄÉ‚ÄÉ\nManasi Patwardhan\n\n‚ÄÉ‚ÄÉ\nLovekesh Vig\n\n‚ÄÉ‚ÄÉ\nAshwin Srinivasan\n\n‚ÄÉ‚ÄÉ\nTanmay Tulsidas Verlekar\n\n\n\nAbstract\nWe address the task of routing natural language queries in multi-database enterprise environments. We construct realistic benchmarks by extending existing NL-to-SQL datasets. Our study shows that routing becomes increasingly challenging with larger, domain-overlapping DB repositories and ambiguous queries, motivating the need for more structured and robust reasoning-based solutions. By explicitly modelling schema coverage, structural connectivity, and fine-grained semantic alignment, the proposed modular, reasoning-driven re-ranking strategy consistently outperforms embedding-only and direct LLM-prompting baselines across all the metrics.\n\n\nFigure 1: Limitations of existing benchmarks and approaches addressed by our benchmark and approach\n\n\n\nI Introduction\n\n\nLarge enterprises operate across verticals, resulting in the enterprise data being distributed across a variety of heterogeneous sources, such as knowledge graphs, relational databases, document repositories, etc. In the context of enterprise-level search, when an end-user submits a query in natural language (NL), it becomes essential to accurately route the query to the most relevant data sources that are capable of providing a correct and comprehensive response. Motivated by this practical challenge, Mandal et al.¬†[1] introduce a novel task of routing NL queries to appropriate data sources, specifically focusing on enterprise databases (DBs) as data sources, which encapsulate information across diverse domains.\n\n\nMandal et al.¬†[1] extend existing datasets developed for cross-domain NL-to-SQL semantic parsing¬†[2, 3] to construct benchmarks for the novel task of query routing, and accordingly define only in-domain and cross-domain settings. However, by preserving the original DB splits, where the test sets contain DBs only from the test split of the underlying datasets, the resulting repository size is extremely small and uneven across settings. This leads to a skewed distribution of DBs and queries in the in-domain and cross-domain scenarios, causing an imbalance in their test sets. Consequently, the cross-domain setting exhibits disproportionately higher recall and mean average precision (mAP) compared to the in-domain setting, which is counter-intuitive and highlights limitations of the benchmark in fairly evaluating routing performance under realistic data distributions.\n\n\nIn enterprise environments, multiple DBs frequently overlap in domain coverage, leading to shared table names, column names, and values, which significantly increases the difficulty of accurate query routing. Motivated by this and to address the above limitations, we construct a more realistic benchmark (Section¬†III). We merge all DBs from both the train and test splits of the original datasets¬†[2, 3] to form a unified DB repository and perform a 50‚Äì50% random split of the queries within each DB into train and test queries. We provide both train and test splits to support future work that may require supervised training, although our proposed method itself is training-free, as DB schemas are inherently dynamic and frequent fine-tuning is impractical. In the spirit of prior work, we define two settings: (i) In-domain, where queries corresponding to every DB are observed during both training and testing, and the inference repository consists of all DBs, unlike prior work where the training repository contained only train DBs and the test repository contained only test DBs; and (ii) Cross-domain, where queries of a subset of DBs are completely unseen during training and are introduced only at inference time, while keeping the DB repository unchanged. Note that, in either setting, a query never appears in both the training and test sets, since a 50‚Äì50 partition of queries is performed prior to defining the experimental splits. The in-domain setting evaluates routing when query patterns of the target DBs are already observed, whereas the cross-domain setting models a realistic enterprise scenario in which DBs are changed and new queries must be routed to previously unseen DBs. This formulation preserves an identical repository across both settings and enables a fair and meaningful comparison.\n\n\nWe re-implement the techniques of¬†[1] as baselines on our benchmark. The inferior performance observed relative to their original results demonstrates the increased difficulty and realism of our setting. We further propose a novel training-free method that combines Schema Entity Recognition with Large Language Models (LLMs) to achieve state-of-the-art performance on this challenging benchmark. The main contributions of this work are as follows:\n\n\n\n\n\n‚Ä¢\n\nWe develop more realistic and robust benchmarks for the DB routing task defined in¬†[1] by extending two text-to-SQL datasets, viz, Spider¬†[2] and BirdSQL¬†[3].\n\n\n\n‚Ä¢\n\nWe define a new technique for the task that yields state-of-the-art results on the benchmark.\n\n\n\n‚Ä¢\n\nWe perform a detailed qualitative analysis of our results, highlighting the efficacy of our technique.\n\n\n\n\n\n\n\nII Problem Statement\n\n\nWe have an end-user question qq and a repository of a set of DBs DD, where each DB is indexed by a DB id dxd_{x} with schema SxS_{x} consisting of multiple tables and columns ({Tx}\\{T_{x}\\}, {Cx‚Äãy}\\{C_{xy}\\}), where {Tx}\\{T_{x}\\} is the set of tables that belong to the DB schema SxS_{x} and {Cx‚Äãy}\\{C_{xy}\\} is the set of columns of tables {Tx}\\{T_{x}\\}. The task is to rank the DBs in DD for the question qq based on their relevance in terms of answerability (whether the DB can provide the correct answer). We have training and test set queries {qxt‚Äãr}\\{q_{x_{tr}}\\} and {qxt‚Äãe}\\{q_{x_{te}}\\} for each DB dxd_{x} such that {qxt‚Äãr}‚à©{qxt‚Äãe}=œï\\{q_{x_{tr}}\\}\\cap\\{q_{x_{te}}\\}=\\phi.\n\n\n\n\nIII Dataset Construction\n\n\nTABLE I: Spider-Route and BIRD-Route Dataset Statistics\n\n\n\nMetric\nSpider route\nBIRD route\n\n\nTotal Number of DBs\n206\n80\n\n\n\n\nTotal Questions\n11,831\n10,962\n\n\nTrain Questions\n5,892\n5,461\n\n\nTest Questions\n5,939\n5,501\n\n\n\n\n\nWe extend two existing datasets constructed for cross-domain NL-to-SQL, viz., Spider¬†[2] and BIRD-SQL¬†[3]. We name the extended datasets as Spider-Route and Bird-Route, respectively. The dataset statistics are provided in Table¬†I.\n\nSpider-Route\nA sample in the original Spider dataset¬†[2] consists of a DB, a NL question posed on the DB and the corresponding SQL query. Each DB is associated with comprehensive schema information, including table and column names, data types, and primary-foreign relationships, which is defined through SQL Data Definition Language (DDL) scripts. The DBs in the train and test split of the original dataset are distinct. For Spider-Route, we combine the DBs to form a repository of 206 DBs. We formulate 50-50% split of the questions belonging to each DB to form train and test splits for Spider-Route, ensuring that there is no overlap between the questions. This leads us to a total number of 5,8925,892 training and 5,9395,939 test queries.\n\nBird-Route\nThe BirdSQL¬†[3] dataset is a collection of DBs spanning over a diverse set of domains such as medical, finance, education and sports. A question in the dataset is accompanied by domain knowledge, termed as ‚Äòevidence‚Äô, which can be used to resolve the query to SQL. A sample in the original Bird-SQL consists of a DB, a question in NL posed on the DB, question-specific evidence, and the corresponding SQL query. For each DB dxd_{x}, we take the union of all the question specific evidences for all the queries which belong to that DB to form DB level meta-data mxm_{x}. The meta-data varies from 2 to 48 sets of evidence sentences per table in the DB.\nWe formulate Bird-Route, by combining the DBs of the original train (69) and dev splits (11) of BirdSQL to form the DB corpus DD of 80 DBs and perform 50-50% split of questions of each DBs to form train and test splits of 5461 and 5501 queries, respectively.\n\n\n\n\nIV Approach\n\n\nIn this section we elaborate on our novel technique to perform the ranking task discussed in Section¬†II. For this technique, we use pre-trained models and do not perform any task specific fine-tuning. Thus, our method is invariant to in-domain and cross-domain settings. For each question we first compute the cosine similarity between the question embedding and the schema embeddings of the DBs in the repository. The DB schema is defined following the DDL script (Refer Table¬†II) Along with the schema definition for the BIRD-Route dataset we use the meta-data in the form of column and value descriptions (discussed in Section¬†III) for richer schema representation. The embeddings in this case are generated using the gte-Qwen2-7B-instruct¬†[4], yielding the best results. The DBs are ranked according to the similarity scores. The top-k DBs with the highest similarity score are then selected for the re-ranking step.\nFollowing¬†[1], we perform re-ranking by prompting a Large Language Model (LLM) (Gemini 2.0 Flash¬†[5] in our case) with the top-k DB schema, their meta-data, along with the query. The prompt is reported in Table¬†III.\n\n\nTABLE II: Example Schema Format (DDL)\n\n\n\n\n\nDatabase: authors_schema:\nTable: Author\n\n\n\n\n\n\nCREATE TABLE \"Author\" (Id INTEGER ... Name TEXT, Affiliation TEXT)\n\n\n\n\n\n\nTable Description: Author\n\n\n\n\n\n\nCOLUMN 1 column_name: Id ‚Ä¶\n\n\n\n\n\n\nCOLUMN 3 column_name: Affiliation; Column description: Organization name with which the author is affiliated.; Data format: text\n\n\n\n\n\n\n\nTABLE III: Prompt for LLM Re-Ranking (Top-5 DBs)\n\n\n\n\n\nYou are an expert DB Administrator tasked with evaluating and refining the relevance ranking of DBs for specific questions. You will be provided with a question and an initial ranking of the top 5 DBs deemed most relevant. Along wi"
  },
  {
    "title": "An Interpretable Recommendation Model for Psychometric Data, With an Application to Gerontological Primary Care",
    "url": "https://arxiv.org/abs/2601.19824v1",
    "source": "arxiv",
    "summary": "There are challenges that must be overcome to make recommender systems useful in healthcare settings. The reasons are varied: the lack of publicly available clinical data, the difficulty that users may have in understanding the reasons why a recommendation was made, the risks that may be involved in following that recommendation, and the uncertainty about its effectiveness. In this work, we addres",
    "full_text": null
  },
  {
    "title": "Learn and Verify: A Framework for Rigorous Verification of Physics-Informed Neural Networks",
    "url": "https://arxiv.org/abs/2601.19818v1",
    "source": "arxiv",
    "summary": "The numerical solution of differential equations using neural networks has become a central topic in scientific computing, with Physics-Informed Neural Networks (PINNs) emerging as a powerful paradigm for both forward and inverse problems. However, unlike classical numerical methods that offer established convergence guarantees, neural network-based approximations typically lack rigorous error bou",
    "full_text": null
  },
  {
    "title": "Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts",
    "url": "https://arxiv.org/abs/2601.19811v1",
    "source": "arxiv",
    "summary": "Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In th",
    "full_text": null
  },
  {
    "title": "Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals",
    "url": "https://arxiv.org/abs/2601.19810v1",
    "source": "arxiv",
    "summary": "Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream t",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Method\n\n3.1 Pre-trained Policy\n\n3.2 Goal Curriculum\n\n3.2.1 Goal Proposal\n3.2.2 Goal Selection\n3.2.3 Difficulty Prediction\n\n\n\n\n\n4 Experiments\n\n4.1 Benchmarks and Tasks\n4.2 Baselines and Ablations\n\n4.3 Experimental Results\n\n4.3.1 Exploration\n4.3.2 Fast Adaptation\n4.3.3 Fine-tuning on Fixed Tasks\n4.3.4 Fine-tuning with Supervised Meta-Learning\n4.3.5 Generalization to New Environment Structures\n\n\n\n\n5 Conclusion\n\nA Appendix\n\nA.1 Pseudocode\n\nA.2 Experimental Details\n\nA.2.1 Environments and Goal Mapping\nA.2.2 Hardware and Seeds\nA.2.3 Algorithm Implementations and Hyperparameters\nA.2.4 Experimental Procedures\nA.2.5 Hyperparameter Selection and Stability\n\n\nA.3 Learning Progress\nA.4 Use of Large Language Models\n\n\n\n\n\n\n\nUnsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals\n\n\nOctavio Pappalardo  \nIndependent Researcher\noctaviopappalardo@gmail.com\nWork conducted as an independent researcher. Currently at University College London (UCL).\n\n\nAbstract\nUnsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent‚Äôs post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent‚Äôs capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.\n\n\n\n1 Introduction\n\nLarge-scale pre-training has underpinned many real-world successes of deep learning in computer vision (Krizhevsky et al., 2012; Chen et al., 2020; Grill et al., 2020; He et al., 2022) and language modeling (Devlin, 2018; Brown, 2020). These advancements have motivated analogous efforts in deep reinforcement learning (RL) (Liu and Abbeel, 2021; Schwarzer et al., 2021; Seo et al., 2022), where the prevailing paradigm remains to train agents from scratch for each new task. The ultimate objective is to develop foundation policies: agents equipped with transferable knowledge that address RL‚Äôs fundamental challenges of sample inefficiency and lack of generalization. We investigate unsupervised RL, where agents interact freely with their environments without access to extrinsic rewards, as a means to acquire such policies. This setting raises key questions regarding what data an agent should seek and what it should learn from it. These questions are tightly coupled because the collected data is a direct consequence of the agent‚Äôs incorporated behaviors. Ultimately, we seek a system that can continuously gather useful data at scale in an unsupervised, open-ended fashion and leverage it to acquire transferable capabilities.\n\n\nA promising direction is to design agents that autonomously generate their own goals and learn by attempting to solve them (Oudeyer et al., 2007; Nair et al., 2018; Forestier et al., 2022). Our focus is on how such goals should be generated, selected, and exploited for pre-training. This connects to automatic curriculum learning, yet existing methods have often assumed either a fixed set of goals or a goal space that is closely aligned with the evaluation tasks. Progress toward foundation policies requires assuming little or no information about downstream objectives and instead pre-training for a broad range of human-relevant tasks. Moreover, a general base policy must have the capacity to contend not only with novel objectives but also with limited information, whether in the form of partial observability, uncertain dynamics, or unspecified goals. Thus, it must be effective across a spectrum of difficulty levels, delivering quick performance on easier tasks and sample-efficient adaptation over long timescales on complex ones.\n\n\nWith these desiderata in mind, we introduce a metric that guides goal generation using the agent‚Äôs performance on goals after an adaptation budget. This contrasts with much prior work that evaluates goal desirability based on immediate performance without any task-specific adaptation (Sukhbaatar et al., 2017; Florensa et al., 2018). We pair this metric with an adversarially trained goal-generation system that proposes challenging yet achievable goals for the pre-trained policy to learn from. This yields an adaptive curriculum that maintains goals at intermediate difficulty and avoids uninformative extremes of too easy or unsolvable goals. Shifting toward a measure of difficulty that relies on post-adaptation performance aligns better with the intended evaluation setting, where the policy must face novel tasks that require adaptation. Moreover, it focuses training on more challenging tasks and leads to coverage of a broader distribution, with more tasks becoming easy and fewer remaining effectively unsolvable.\n\n\nAnticipating the need for test-time adaptation naturally points to meta-learning (Duan et al., 2016; Finn et al., 2017), which explicitly optimizes for efficient learning. Meta-learning has primarily been studied with pre-training carried out over hand-designed task distributions, and its integration with unsupervised settings remains largely underexplored (Gupta et al., 2018). We bridge these directions by introducing an unsupervised meta-learning algorithm that meta-learns a base policy using an automatic curriculum of self-generated goals guided by our post-adaptation difficulty metric.\n\n\nOur main contributions are as follows:\n\n\n\n\n‚Ä¢\n\nWe introduce a post-adaptation task-difficulty metric for guiding automatic goal generation and selection in unsupervised autotelic agents.\n\n\n\n‚Ä¢\n\nWe present ULEE, an unsupervised meta-learning method composed of (1) an in-context learner trained with self-imposed goals, (2) a difficulty-prediction network estimating post-adaptation performance, (3) an adversarial agent trained to find challenging candidate goals, and (4) a sampling strategy that selects goals within a desired difficulty range.\n\n\n\n‚Ä¢\n\nWe evaluate ULEE in complex yet computationally accessible environments across multiple timescales and types of generalization. It outperforms baselines and ablations in exploration, fast adaptation, fine-tuning to fixed environments, and fine-tuning in meta-learning settings. All code is open-sourced.\n\n\n\n\n\n\n\n2 Related Work\n\nSeveral strategies have emerged to make use of reward-free objectives in RL. These include learning a world model (Sekar et al., 2020; Seo et al., 2022; Rajeswar et al., 2023), utilizing auxiliary objectives that aid representation learning (Jaderberg et al., 2016; Oord et al., 2018; Zhang et al., 2020a; Stooke et al., 2021; Schwarzer et al., 2021), and training agents with intrinsic rewards (Chentanez et al., 2004; Oudeyer et al., 2007; Schmidhuber, 2010) that lead to useful behaviors. The latter has been a major line of research into both pre-training and online exploration, with methods in both areas holding a close relationship. Most work in this line of research can be grouped into methods that consider predictions over some aspect of the environment and are guided by error, uncertainty, or progress in these predictions (Schmidhuber, 1991; Pathak et al., 2017; Burda et al., 2018; Pathak et al., 2019); those whose rewards come from measures of diversity in collected data (Bellemare et al., 2016; Hazan et al., 2019; Lee et al., 2019; Liu and Abbeel, 2021; Yarats et al., 2021); and those who seek to maximize empowerment (Klyubin et al., 2005; Mohamed and Jimenez Rezende, 2015; Gregor et al., 2016; Eysenbach et al., 2018; Sharma et al., 2019) or are guided by self-imposed goals (Sukhbaatar et al., 2017; Nair et al., 2018; Florensa et al., 2018; Pong et al., 2019; Forestier et al., 2022). Our work focuses on the latter and incorporates ideas from automatic curriculum learning (Bengio et al., 2009; Andrychowicz et al., 2017; Florensa et al., 2017; Matiisen et al., 2019; Jiang et al., 2021), which has proved capable of significantly accelerating learning in RL.\n\n\nIntermediate difficulty.\nPrevious work on unsupervised RL and automatic curriculum generation has explored dictating the agent‚Äôs curriculum by continually selecting tasks of intermediate difficulty. In GoalGAN (Florensa et al., 2018), a GAN (Goodfellow et al., 2014) variant is trained to output goals that are neither too easy nor too hard for the agent. The goal space is fixed beforehand and remains the same for training and evaluation. The curriculum‚Äôs aim is to accelerate learning across all feasible goals. Racaniere et al. (2019) address the same multitask setting but generate goals uniformly across all difficulty levels. Their goal generator is trained with custom objectives that promote goal validity, coverage, and controllable difficulty. They introduce a ‚Äújudge‚Äù network trained on past goals to estimate success probability. Conditioning the judge and generator on environment observations enables them to handle procedurally generated and partially observable environments. Zhang et al. (2020b) propose a cur"
  },
  {
    "title": "Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation",
    "url": "https://arxiv.org/abs/2601.19802v1",
    "source": "arxiv",
    "summary": "Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to au",
    "full_text": null
  },
  {
    "title": "Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation",
    "url": "https://arxiv.org/abs/2601.19794v1",
    "source": "arxiv",
    "summary": "The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to ca",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Magnitude-Based Criteria\n2.2 Importance Estimation and Second-Order Methods\n2.3 Optimization Frameworks\n\n\n\n3 Methodology\n\n\n3.1 Gradient-Based Importance Metrics\n\n3.1.1 Gradient Magnitude Accumulation\n3.1.2 Fisher Information (Diagonal Approximation)\n3.1.3 Bayesian (Empirical)\n\n\n\n3.2 Training Regime\n\n3.2.1 Temporal Smoothing\n3.2.2 Coefficient Parameters\n3.2.3 Loss Function Formulation\n3.2.4 Scheduler for Regularization Coefficients\n\n\n\n\n\n4 Experimental Setup\n\n\n4.1 Use Cases\n\n4.1.1 Autoencoder on MNIST\n4.1.2 TD-MPC for Balancing an Inverted Pendulum\n\n\n4.2 Component-Aware Pruning Group Formation\n\n4.3 Estimating Pruning Group Importance\n\n4.3.1 Limitations of Conventional Approaches\n4.3.2 Component-Specific and Coupling Groups\n\n\n\n\n\n5 Results and Discussion\n\n5.1 Use Case 1: MNIST Autoencoder\n5.2 Use Case 2: TD-MPC for Balancing an Inverted Pendulum\n\n\n6 Conclusion and Future Work\n\n\n\n\n\nComponent-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation\n\n\nGanesh Sundaram\n\n‚ÄÉ‚ÄÉ\nJonas Ulmen\n\n‚ÄÉ‚ÄÉ\nDaniel G√∂rges\n\nDepartment of Electrical and Computer Engineering, \nRPTU University Kaiserslautern-Landau, Germany \n(e-mail: {ganesh.sundaram, jonas.ulmen, daniel.goerges}@rptu.de).\n\n\n\nAbstract\nThe transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to capture functional significance. This paper introduces a component-aware pruning framework that utilizes gradient information to compute three distinct importance metrics during training: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. Experimental results with an autoencoder and a TD-MPC agent demonstrate that the proposed framework reveals critical structural dependencies and dynamic shifts in importance that static heuristics often miss, supporting more informed compression decisions.\n\n\nkeywords: \nAI-driven modeling and control, Machine learning for modeling and prediction, Reinforcement learning and deep learning in control,\nNeural network Compression, Structured pruning, Neural network controllers\n\n\n‚Ä†‚Ä†thanks: Authors have contributed equally.\n\n\n1 Introduction\n\nThe adoption of Deep Neural Networks (DNNs) has transformed numerous domains, including perception and control, by facilitating advanced representation learning and generalization. Despite these advances, the increasing complexity of multi-component model architectures for environmental understanding has widened the gap between research-grade models and the strict resource limitations of real-world embedded systems¬†(Liu et¬†al., 2025). Closing this gap necessitates effective model compression. However, conventional approaches such as unstructured pruning or magnitude-based structured pruning often treat networks as indivisible entities. Traditional methods typically use static, rule-based heuristics that fail to account for cross-component dependencies. This oversight can result in the elimination of critical connections that are essential for maintaining the stability and performance of complex closed-loop controllers.\n\n\nThis work addresses these limitations by introducing a component-aware structured pruning framework. Unlike conventional methods, the proposed approach explicitly distinguishes between component-specific groups and inter-component coupling groups, enabling a more granular and adaptable pruning strategy. Three prevailing heuristics for assessing group importance are systematically evaluated using two representative scenarios: a canonical Multi-Component Neural Architecture (MCNA) autoencoder and a control-oriented Temporal Difference Learning for Model Predictive Control (TD-MPC) agent. The empirical results challenge the prevailing assumption that coupling groups or early layers are inherently critical. Instead, the findings indicate that group importance is conditional, architecture-dependent, and highly dynamic, with significant shifts occurring throughout training. To capture these dynamics, the framework incorporates a unified pruning pipeline that assesses importance using three distinct metrics: Gradient Accumulation, Fisher Information, and Bayesian Uncertainty. This allows for informed, data-driven pruning decisions without incurring the high computational cost of post-training sensitivity analysis. By revealing essential structural dependencies, the method preserves control performance under aggressive compression and offers interpretable insights into the contributions of different components to overall controller fidelity.\n\n\nThe remainder of this paper is organized as follows: Section¬†2 reviews prior pruning methods, Section¬†3 details the structured pruning formalism and importance measures, Section¬†4 describes the experimental setup, and Section¬†5 presents the results and analysis.\n\n\n\n\n2 Related Work\n\nStructured pruning methods are typically classified into magnitude-based heuristics, importance estimation techniques, and learning-based optimization approaches.\n\n\n\n2.1 Magnitude-Based Criteria\n\nEarly heuristic methods performed pruning based on activation patterns or magnitude ranking, sometimes employing an ‚Ñì1\\ell_{1} penalty to promote sparsity (Hu et¬†al., 2016; Li et¬†al., 2016; Liu et¬†al., 2017). Structured sparsity can be applied to channels, filters, or blocks and may be guided by reconstruction error or gating functions¬†(Luo et¬†al., 2017; He et¬†al., 2017a, b). Gradually increasing sparsity regularization has been shown to reduce redundancy¬†(Wang et¬†al., 2020). However, recent surveys caution that naive heuristics risk discarding essential parameters and emphasize that training the target architecture from scratch can achieve performance comparable to pruned models¬†(He and Xiao, 2024; Cheng et¬†al., 2024; Blalock et¬†al., 2020).\n\n\n\n\n2.2 Importance Estimation and Second-Order Methods\n\nSecond-order methods, including Optimal Brain Damage, Optimal Brain Surgeon, and Taylor-series pruning, approximate the Hessian matrix to evaluate parameter importance¬†(LeCun et¬†al., 1989; Frantar et¬†al., 2023; Molchanov et¬†al., 2017). More recent approaches use gradient accumulation and Bayesian uncertainty to estimate the importance of parameter groups¬†(Molchanov et¬†al., 2019a, b; Ke and Fan, 2022).\n\n\n\n\n2.3 Optimization Frameworks\n\nLearning-based approaches conceptualize pruning as a search or optimization problem. Reinforcement learning and automated machine learning (AutoML) methods are employed to explore optimal sparsity ratios¬†(Huang et¬†al., 2018). Single-shot saliency techniques, such as SNIP¬†(Lee et¬†al., 2019), assess parameter importance at initialization. Hybrid frameworks enable the simultaneous pruning of layers, attention heads, and hidden units in transformer architectures¬†(Michel et¬†al., 2019; Xia et¬†al., 2022). Collectively, these methods demonstrate that pruning can be integrated within a comprehensive optimization pipeline rather than relying solely on fixed heuristics.\n\n\n\n\n\n3 Methodology\n\nThis section presents our primary contributions. First, we propose three importance metrics for each pruning group, each derived from the gradients of the corresponding parameters. Second, we introduce a training regime that accumulates gradients using an exponential moving average and incorporates a scheduling mechanism to prevent vanishing gradients in the importance metric as the model converges on the task loss.\n\n\n\n3.1 Gradient-Based Importance Metrics\n\nAlthough frameworks such as Torch-Pruning offer essential tools for managing architectural dependencies during structured pruning, they do not specify methods for measuring the importance of each parameter group¬†(Fang et¬†al., 2023). Users are therefore required to define appropriate scoring metrics to guide pruning decisions. Traditional approaches often use static, heuristic-based metrics, such as weight norms. However, these can be inadequate for complex models in which a group‚Äôs functional importance does not necessarily align with its magnitude. To address this limitation, this work investigates dynamic, gradient-based metrics that quantify a group‚Äôs influence on task loss, thereby providing a more principled basis for pruning decisions.\n\n\nPrevious approaches, such as¬†(Sundaram et¬†al., 2025b), employ grid search or gradient-based optimization to determine pruning-group coefficients, which specify the number of parameters each group must remove to achieve the desired pruning objective. However, these coefficient weightings can only be determined post-training, limiting their utility for importance estimation during training. To address this limitation, a set of gradient-based importance metrics is introduced, computed online, and integrated into the training process. These metrics leverage group-wise gradients of the training loss and capture complementary aspects of importance, including instantaneous sensitivity, curvature, and accumulated learning activity. Combining these metrics provides a more robust estimate of importance than relying on a single metric.\n\n\n\n3.1.1 Gradient Magnitude Accumulation\n\nFor a pruning group gg with parameter set Œòg\\Theta_{g}, define the per-parameter average absolute gradient at one iteration tt of an epoch as\n\n\n\nIggrad‚Äã(t)=1Ng‚Äã‚àëŒ∏‚ààŒòg|‚àáŒ∏‚Ñí‚Äã(t)|,I_{g}^{\\mathrm{grad}}(t)\\;=\\;\\frac{1}{N_{g}}\\sum_{\\theta\\in\\Theta_{g}}\\left|\\nabla_{\\theta}\\mathcal{L}(t)\\right|,\n\n\n\nHere, ‚àáŒ∏‚Ñí\\nabla_{\\theta}\\mathcal{L} represents the gradient of the loss ‚Ñí\\mathcal{L} with respect to the parameters Œ∏\\theta, and Ng‚àà‚ÑïN_{g}\\in\\mathbb{N} denotes the number of parameters in the group gg. This metric quantifies the model‚Äôs immediate tendency to update a group, reflecting the steepness of the loss landscape for that group and th"
  },
  {
    "title": "CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing",
    "url": "https://arxiv.org/abs/2601.19793v1",
    "source": "arxiv",
    "summary": "Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semant",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methodology\n\n3.1 System Architecture\n\n3.2 CASTER Design\n\n3.2.1 CASTER Architecture: Dual-Branch Feature Fusion Network\n\n\n\n3.3 Training Strategy\n\n3.3.1 Pre-training and Cold Start Strategy\n3.3.2 Iterative Fine-tuning via Negative Feedback\n\n\n\n\n\n4 Results &amp; Analysis\n\n4.1 Experiments Setup\n4.2 Model Evaluation\n\n4.3 Cost Evaluation\n\n4.3.1 Cumulative Cost and Average Cost\n4.3.2 Cost Distribution\n\n\n\n4.4 Quality Evaluation\n\n4.4.1 Overall Evaluation\n4.4.2 Granular Analysis: Categories, Capabilities, and Multi-Model Evaluate\n\n\n4.5 Compare to FrugalGPT\n\n4.6 Comparative Analysis of LLMs\n\n4.6.1 Cost Analysis Across Providers\n4.6.2 Performance Evaluation (Score)\n\n\n\n\n5 Conclusion and Future Work\n\nA Experiment.\n\n\nA.1 Experimental Implementation and Training Protocol\n\nA.1.1 Dataset Preparation\nA.1.2 Training Protocol\nA.1.3 Implementation Details\n\n\nA.2 Basic Model Test\nA.3 Benchmark\n\n\nB Multi-agent Design.\n\nC Automated Dataset for Training Construction via Adversarial Generation.\n\nC.1 Stochastic Difficulty Stratification\nC.2 Domain-Specific Adversarial Constraints\nC.3 Self-Evolving Diversity Control\n\n\nD Algorithm.\n\nE Basic Model Test.\n\n\nE.1 Software Engineering Scenario Model Test\n\nE.1.1 Inference Capability Validation\n\n\n\nE.2 Data Analysis Scenario Model Test\n\nE.2.1 Inference Capability Validation\n\n\n\nE.3 Scientific Discovery Scenario Model Test\n\nE.3.1 Inference Capability Validation\n\n\n\nE.4 Cybersecurity Scenario Model Test\n\nE.4.1 Inference Capability Validation\n\n\n\n\n\nF Quality Benchmark in Experiments\n\n\nF.1 Software Engineering Scenario\n\nF.1.1 Benchmark Framework\nF.1.2 Code Quality Assessment Protocol\n\n\n\nF.2 Data Analysis Scenario\n\nF.2.1 Benchmark Framework\n\nF.2.2 Multi-Modal Artifact Quality Assessment\n\n1. Code Logic Evaluator (40% Weight):\n2. Data Quality Evaluator (30% Weight):\n3. Visual Judge (30% Weight):\n\n\n\n\n\nF.3 Scientific Discovery Scenario\n\nF.3.1 Benchmark Framework\nF.3.2 Rigor-Oriented Assessment Protocol\n\n\n\nF.4 Cybersecurity Scenario\n\nF.4.1 Benchmark Framework\nF.4.2 Safety-First Assessment Protocol\n\n\n\n\nG Experiments Results.\n\n\n\n\n\nCASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing\n\n\nShanyv Liu\n\n‚ÄÉ‚ÄÉ\nXuyang Yuan\n\n‚ÄÉ‚ÄÉ\nTao Chen\n\n‚ÄÉ‚ÄÉ\nZijun Zhan\n\n‚ÄÉ‚ÄÉ\nZhu Han\n\n‚ÄÉ‚ÄÉ\nDanyang Zheng\n\n‚ÄÉ‚ÄÉ\nWeishan Zhang\n\n‚ÄÉ‚ÄÉ\nShaohua Cao\n\n\n\nAbstract\nGraph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semantic embeddings with structural meta-features to estimate task difficulty. During training, the router self-optimizes through a Cold Start to Iterative Evolution paradigm, learning from its own routing failures via on-policy negative feedback. Experiments using LLM-as-a-Judge evaluation across Software Engineering, Data Analysis, Scientific Discovery, and Cybersecurity demonstrate that CASTER reduces inference cost by up to 72.4% compared to strong-model baselines while matching their success rates, and consistently outperforms both heuristic routing and FrugalGPT across all domains.\n\nMachine Learning, ICML, Multi-Agent Systems, Large Language Models, Dynamic Model Routing, Cost-Efficient Inference, LLM Serving Optimization, Dynamic Resource Allocation\n\n\n\n1 Introduction\n\nFrom Multi-Agent Collaboration to the Cost-Performance Paradox. The evolution of Large Language Models (LLMs) has shifted the AI frontier toward Multi-Agent Systems (MAS). By decomposing complex objectives into sub-tasks, MAS achieves emergent intelligence essential for long-horizon domains like software engineering (Hong et al., 2023) and scientific discovery (Zhou et al., 2024). However, this scalability is constrained by the Cost-Performance Paradox. MAS workflows generate exponential context accumulation (Packer et al., 2023), forcing a rigid binary choice: relying exclusively on Strong Models (e.g., GPT-4o) incurs prohibitive costs and latency (Chen et al., 2023), while switching to Weak Models introduces a ‚Äùfragility of logic,‚Äù where a single upstream error cascades into total task failure (Yao et al., 2022). Balancing this trade-off is a significant challenge for industrial MAS adoption.\n\n\nLimitations of Existing Routing. Current routing techniques are ill-suited for the dynamic nature of MAS. Heuristic approaches relying on static metrics like query length often fail to capture semantic complexity, as a concise, logic-heavy prompt frequently demands more reasoning power than a lengthy summarization task. Similarly, cascading strategies such as FrugalGPT (Chen et al., 2023) employ a ‚Äùtry-and-fail‚Äù mechanism that introduces unacceptable latency and risks polluting the shared context with erroneous intermediate steps, thereby confusing subsequent agents. Furthermore, preference-based methods like RouteLLM (Ong et al., 2024), which depend on human feedback (RLHF) or chatbot arena data, prove inadequate for MAS; while effective for aligning single-turn conversations with subjective user preferences, they lack the objective precision required for the rigorous, multi-step reasoning chains essential to agentic workflows.\n\n\nOur Approach: Context-Aware Neural Routing. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight neural module designed to break the rigid trade-off between performance and cost. Unlike static configurations, CASTER acts as a dynamic decision-maker, mapping task semantics, agent roles, and evolving context to the most cost-effective model. By predicting the necessity of expert-level reasoning, it dispatches simple sub-tasks to weak models while reserving strong models for critical reasoning bottlenecks (System overview in Figure¬†1).\n\n\nContributions.\n\n\n‚Ä¢\n\nFramework: We introduce CASTER for MAS (e.g., LangGraph), integrating semantic embeddings with role-specific features for granular, dynamic model allocation.\n\n\n\n‚Ä¢\n\nDataset: We construct a comprehensive benchmark across four domains (Software, Data, Science, Security) with stratified difficulty levels to evaluate routing generalization.\n\n\n\n‚Ä¢\n\nMethodology: We propose an On-Policy iterative training pipeline. We empirically demonstrate that naive random exploration leads to data pollution, validating our approach of labeling difficulty based on performance divergence.\n\n\n\n‚Ä¢\n\nResults: Experiments show CASTER reduces token costs by 72.4% while maintaining success rates comparable to an all-GPT-4o baseline, significantly outperforming cascading and random strategies.\n\n\n\n\n\nFigure 1: The overall architecture of the CASTER framework. The system begins with mock data and dynamic task generation via GPT-4o. The core Router integrates semantic and meta-features to dispatch tasks, evolving through cold start and on-policy negative feedback mechanisms. Tasks are executed by domain-specific agents (Software, Data, Science, Security) and evaluated against a comprehensive benchmark to ensure multi-model capability.\n\n\n\n\n2 Related Work\n\nLLMs-based Agents and Multi-Agent Systems. The emergence of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of solving complex tasks. Early works focused on single-agent capabilities, but recent research has shifted towards Multi-Agent Systems (MAS) to emulate human-like collaboration. MetaGPT (Hong et al., 2023) and ChatDev (Qian et al., 2024) introduce Standard Operating Procedures (SOPs) into agent workflows, assigning specific roles (e.g., product manager, engineer) to LLMs to simulate software development companies. Similarly, AutoGen (Wu et al., 2024) and Camel (Li et al., 2023) facilitate complex task solving through communicative agents and role-playing frameworks. Furthermore, LangGraph 111https://github.com/langchain-ai/langgraphadvances this field by modeling agent workflows as stateful graphs, enabling cyclic execution and fine-grained control over multi-agent interactions. In the domain of software engineering, TaskWeaver (Qiao et al., 2023) and SWE-agent (Yang et al., 2024) further specialize agents for code-first tasks and automated issue resolving. A comprehensive review of these LLM-based multi-agent systems and their applications in software engineering is provided by (He et al., 2025).\n\n\nOptimization of Multi-Agent Systems. While foundation models like GPT-4 (Achiam et al., 2023) and open-weight counterparts like Qwen (Bai et al., 2023) serve as robust cognitive engines, the complexity of Multi-Agent Systems (MAS) demands optimization beyond individual model inference. Early efficiency efforts focused on cost-centric routing, such as FrugalGPT (Chen et al., 2023) and RouteLLM (Ong et al., 2024), which dynamically allocate queries between strong and weak models. However, recent advancements target the holistic optimization of agentic workflows. To automate system design, Agentic Supernet(Zhang et al., 2025a) introduces an architecture search framework that identifies optimal agent topologies, reducing the reliance on manual engineering. Improving collaborative efficiency, OSC(Zhang et al., 2025b) proposes a cognitive orchestration mechanism that dynamically aligns knowledge across agents to mitigate communication overhead. Furthermore, Multi-Agent Consensus Alignment(Samanta et al., 2025) internalizes the self-consistency of agents by leveraging consensus data, effectively distilling the reasoning capabilities of a swarm into a single efficient model.\n\n\nBenchmarks and Evaluation. Evaluation paradigms have shifted from static response quality to dynamic agent behaviors. MT-Bench (Zheng et al., 2023) validated the ‚ÄùLLM-as-a-judge‚Äù approach for open-ended conversations. For autonomous agents, AgentBoard (Chang et al., 2024) introduced fine-grained progress metrics to mea"
  },
  {
    "title": "LVLMs and Humans Ground Differently in Referential Communication",
    "url": "https://arxiv.org/abs/2601.19792v1",
    "source": "arxiv",
    "summary": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that int",
    "full_text": "\n\n\n\n1 Introduction\n2 Cognitive Science Background\n3 Related Work in Human-AI Interaction\n\n4 Experimental Design and Method\n\n4.1 Task Description\n4.2 Human Participants\n4.3 AI Participants\n4.4 Resulting Corpus\n\n4.5 Evaluation Metrics\n\nCommunicative Success\nCommunication Effort\nLexical Entrainment\nReferring Expression Extraction\n\n\n\n\n\n5 Results\n\nHuman-Human\nAI-AI\nHuman-AI\nAI-Human\nParticipants‚Äô Perceptions and Experiences\nSummary\n\n\n\n6 Follow-Up AI‚ÄìAI Experiments\n\nA Simplified Prompt\nVarying Reasoning Efforts\nMixed AI Pairs\n\n\n7 Conclusion\nA Prolific Data Collection\nB Dialogue Examples\n\nC Evaluation Metrics\n\nC.1 Additional Communication Effort Metrics\nC.2 Additional Lexical Entrainment Metrics\n\nC.3 Automatic Referring Expression Extraction and Validation\n\nExtraction Prompt\nValidation\n\n\n\n\nD Follow-Up AI-AI Experiments\nE Prompts for AI Director and AI Matcher\n\n\n\n\n\nLVLMs and Humans Ground Differently in Referential Communication\n\n\n\nPeter Zeng1,4 ‚ÄÉWeiling Li2 ‚ÄÉAmie Paige2 ‚ÄÉZhengxiang Wang3,4\nPanagiotis Kaliosis1 ‚ÄÉDimitris Samaras1 ‚ÄÉGregory Zelinsky2 ‚ÄÉ\nSusan Brennan2 ‚ÄÉOwen Rambow3,4\n1Department of Computer Science, Stony Brook University \n2Department of Psychology, Stony Brook University \n3Department of Linguistics, Stony Brook University \n4Institute for Advanced Computational Science \n\nCorrespondence: pezeng@cs.stonybrook.edu\n\n\n\n\nAbstract\nFor generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release\nthe online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs‚Äô limitations in\ninteractively resolving referring expressions, a crucial skill that underlies human language use.\n\n\n\nLVLMs and Humans Ground Differently in Referential Communication\n\n\n\n\n\nPeter Zeng1,4 ‚ÄÉ‚ÄäWeiling Li2 ‚ÄÉ‚ÄäAmie Paige2 ‚ÄÉ‚ÄäZhengxiang Wang3,4\n\nPanagiotis Kaliosis1 ‚ÄÉDimitris Samaras1 ‚ÄÉGregory Zelinsky2\n\nSusan Brennan2 ‚ÄÉOwen Rambow3,4\n\n1Department of Computer Science, Stony Brook University\n\n2Department of Psychology, Stony Brook University\n\n3Department of Linguistics, Stony Brook University\n\n4Institute for Advanced Computational Science\n\n\nCorrespondence: pezeng@cs.stonybrook.edu\n\n\n\n\n\n\n1 Introduction\n\nHuman conversation relies on common ground, accrued and updated by interacting partners (Clark and Brennan, 1991; Clark and Wilkes-Gibbs, 1986; Clark and Marshall, 1981). During conversation, partners ground meanings with one another, adapting the referring expressions they use to pick out objects of interest, such that there is less variability within a conversation than between conversations by different partners discussing the same objects (Brennan and Clark, 1996).\n\n\nConversational partners cannot read one another‚Äôs minds, but they can signal that they believe they are talking about the same referent by converging on a referring expression for it (the process of lexical entrainment; ibid, see Figure¬†1). They cannot rely solely on conventional word meanings, as the objects of interest may not be uniquely associated with distinctive labels known to both. They construct meaning jointly by engaging in the process of grounding, or seeking and providing evidence about what they mutually believe to be in their common ground.\n\n\nFigure 1: Repeated referring to two baskets (non-lexicalized objects) by a human-human pair in Rounds 1-4 of our experiment, with lexical overlap highlighted in blue. Entrainment on more concise language (a conceptual pact) occurs by Round 3, after they consider multiple proposals in Rounds 1-2.\n\n\nRecently, the field has begun to address the question of whether large language models (LLMs) and large vision language models (LVLMs) engage in grounding, as human discourse partners do (Tang et al., 2024; Hua and Artzi, 2024; Imai et al., 2025; Shaikh et al., 2025; Wang et al., 2025). This question is important both in order to try to understand how models work, and for practical applications in which an AI agent assists a human in a task that requires using language to pick out elements from a visual environment. Studies to date tend to conclude that LVLMs lack the pragmatic competence needed to coordinate with a partner (Grice, 1975) and that they struggle in multi-turn conversation.\n\n\nThis paper adds further empirical evidence about the pragmatic competence missing from LVLMs. This is the first study, to our knowledge, to generate (in real time) multi-turn task-oriented dialogues between pairs of partners in asymmetrical roles (offering different levels of initiative), and that covers all four combinations of human or AI partners filling each role. We conducted an experiment to examine language use by director-matcher pairs collaborating to do a referential communication task, based closely on Clark and Wilkes-Gibbs (1986)‚Äôs study that measured how partners accumulate common ground as they match the same objects across multiple rounds. We tested all four combinations of director/matcher roles (human-human, human-AI, AI-human, and AI-AI). This allowed us to quantify not only where LVLMs fail in collaboration, but also to discover why they fail (and in which roles they do so). We will release the transcripts of the experiments to the research community.\n\n\nThis paper is organized as follows: Section¬†2 motivates the project within the cognitive science of human communication, and Section¬†3 surveys relevant work in human-AI interaction. The experimental design is described in Section¬†4, and Section¬†5 presents the results along with illustrative examples from the collected dialogues. Section¬†6 presents follow-up AI-AI experiments and Section¬†7 concludes.\n\n\n\n\n2 Cognitive Science Background\n\nPsycholinguists, as well, cannot read the minds of those they study, so they rely on experiments to provide observable evidence about how humans process language. Referential communication is typically studied using matching tasks with carefully chosen stimuli; two partners are provided with a (typically shared) goal such as to discuss and manipulate picture cards, maps, or other visuospatial elements, and their behavior (e.g., words, eye gaze, actions, etc.) is measured. Task roles, available information, and other partner characteristics can be balanced, or else manipulated in order to vary each partner‚Äôs initiative or other factors of interest.\n\n\nIn addition to the process of grounding described in the previous section, other key forces shape dialogues, including pragmatic factors, such as Grice (1975)‚Äôs Cooperative Principle that governs human expectations when they engage in dialogue, along with its Maxims of Quality (‚Äúbe truthful‚Äù), Quantity (‚Äúsay enough but not too much‚Äù), Manner (‚Äúbe straightforward‚Äù) and Relation (‚Äúbe relevant‚Äù). Dialogue is also shaped by the costs and affordances of the communication medium Brennan and Clark (1996). Remarkably, those who share a common purpose (even one given to them by an experimenter) tend to increase their effort as much as it takes for them to accomplish the task, including distributing their individual effort collectively (e.g., one partner may work harder to make up for the other). In the context of grounding in different sorts of dialogues, human partners work together to put in the joint effort needed to meet a grounding criterion sufficient for current purposes (Clark and Wilkes-Gibbs, 1986), which could be accuracy in an experimental task, safety in an air traffic control dialogue, or passing the time in a checkout line. Human partners tend to increase their joint efficiency while they work together, and their common ground can save them effort while they realize a shared purpose.\n\n\nLanguage use by LVLMs, however, is quite a different matter.\n\n\n\n\n3 Related Work in Human-AI Interaction\n\nRelated work on referential communication with LVLMs typically uses human-human corpora as a gold standard for human-AI interaction, finding consistently that humans perform better than models. Although such corpora include spontaneous turn-taking by humans, the tests with AI partners tend to not allow multi-turn interaction, so does not test the abilities of models to collaboratively ground meaning or repair misunderstandings.\n\n\nHaber et al. (2019) introduced PhotoBook, a widely used dataset for exploring common ground in visually grounded dialogues. Two human participants play a multi-round online image identification game in which each sees a grid of six visually similar scenes, with some images shared and others visible to only one partner. The partners chat via text and decide which images among a set of highlighted targets are common to both partners or private to only one. Imai et al. (2025) adapted PhotoBook to an AI‚ÄìAI setting using LVLMs and evaluated the AI dyads against the human ones using a suite of proposed grounding metrics. Although LVLM pairs achieved near-human task accuracy, their dialogue still differed from human pairs in the formation of common ground, both in efficiency and in lexical adaptation.\n\n\nHawkins et al. (2020) had human-human and human-AI director-matcher pairs identify targets within sets that consisted of four photos from the COCO data set, with the goal of being able to use more efficient expressions with repeated referring to the same photos. Such photos were far more distinctive than the basket targets that we used, making their task much easier (and possibly the labeled COCO data would have been in the models‚Äô training data). Hawkins et al. (2020)‚Äôs model adapted over the course of repeated rounds"
  },
  {
    "title": "To Grok Grokking: Provable Grokking in Ridge Regression",
    "url": "https://arxiv.org/abs/2601.19791v1",
    "source": "arxiv",
    "summary": "We study grokking, the onset of generalization long after overfitting, in a classical ridge regression setting. We prove end-to-end grokking results for learning over-parameterized linear regression models using gradient descent with weight decay. Specifically, we prove that the following stages occur: (i) the model overfits the training data early during training; (ii) poor generalization persist",
    "full_text": null
  },
  {
    "title": "Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers",
    "url": "https://arxiv.org/abs/2601.19788v1",
    "source": "arxiv",
    "summary": "Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task ",
    "full_text": null
  },
  {
    "title": "Rethinking Discrete Speech Representation Tokens for Accent Generation",
    "url": "https://arxiv.org/abs/2601.19786v1",
    "source": "arxiv",
    "summary": "Discrete Speech Representation Tokens (DSRTs) have become a foundational component in speech generation. While prior work has extensively studied phonetic and speaker information in DSRTs, how accent information is encoded in DSRTs remains largely unexplored. In this paper, we present the first systematic investigation of accent information in DSRTs. We propose a unified evaluation framework that ",
    "full_text": null
  },
  {
    "title": "Reimagining Peer Review Process Through Multi-Agent Mechanism Design",
    "url": "https://arxiv.org/abs/2601.19778v1",
    "source": "arxiv",
    "summary": "The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as \"broken.\" This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community",
    "full_text": "\n\n\n\nI Introduction\n\nII Taking Stock: The Mechanism Failure\n\nII-A The Tragedy of the Review Commons\nII-B The LLM Amplification Effect\nII-C Related Mechanisms\n\n\n\nIII A Multi-Agent Framework\n\nIII-A The Community as a Stochastic Game\nIII-B Credit-Based Submission Economy\nIII-C MARL-Optimized Reviewer Assignment\nIII-D Hybrid Verification\n\n\n\nIV Threat Model and Mitigations\n\nIV-A Equity and Privacy\n\n\n\nV Research Agenda\n\nV-A Phase 0: Simulation &amp; Foundations (2025‚Äì2026)\nV-B Phase 1: Pilot Credit System (2026‚Äì2027)\nV-C Phase 2: MARL Assignment (2027‚Äì2028)\nV-D Phase 3: Verification and Federation (2028‚Äì2029)\nV-E Governance and Sustainability\n\n\nVI Discussion\nVII Conclusion\n\n\n\n\n\nReimagining Peer Review Process Through Multi-Agent Mechanism Design\n\n\n\nAhmad Farooq\n\n‚ÄÉ‚ÄÉ\nKamran Iqbal\n\n\n\nAbstract\nThe software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as ‚Äúbroken.‚Äù This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community as a stochastic multi-agent system and applying multi-agent reinforcement learning to design incentive-compatible protocols. We outline three interventions: a credit-based submission economy, MARL-optimized reviewer assignment, and hybrid verification of review consistency. We present threat models, equity considerations, and phased pilot metrics. This vision charts a research agenda toward sustainable peer review.\n\n\n\nI Introduction\n\n\nThe ICSE 2026 Future of Software Engineering call opens with a sobering observation: beneath thriving conferences lies ‚Äúan undercurrent of grumbling‚Äù suggesting ‚Äúour community may not be doing so well.‚Äù Among the complaints, one dominates: ‚ÄúPeer review is broken.‚Äù ¬†111https://conf.researchr.org/track/icse-2026/icse-2026-future-of-software-engineering\n\n\nThis is not hyperbole. The NeurIPS 2014 duplicate-review consistency experiment found that 57% of papers accepted by one committee were rejected by the other, highlighting substantial outcome instability¬†[3, 6]. Studies document that 6.5‚Äì16.9% of AI conference reviews show substantial LLM involvement¬†[16], and that, in LLM-based agent simulations of peer review, up to 37.1% of variance in decisions is attributable to modeled reviewer biases¬†[12]. The ‚Äúpublish or perish‚Äù culture has created a tragedy of the commons: researchers maximize submissions while minimizing review effort¬†[11].\n\n\nThe conventional response treats these as social problems requiring cultural change. This position paper offers a different perspective: the peer review crisis is a mechanism design problem, and mechanism design problems yield to engineering¬†[21].\n\n\nWe propose reconceptualizing the research community as a stochastic multi-agent system where agents pursue individual objectives. Current dysfunction arises because reward structures incentivize individually rational but collectively misaligned behaviors. Multi-agent reinforcement learning (MARL) provides tools to redesign these incentives¬†[10, 8].\n\n\nThis paper outlines three interventions (Figure¬†1) and a phased research agenda. We do not claim these are proven solutions; rather, we argue they constitute a principled framework warranting systematic investigation.\n\nCreditEconomyMARLAssignmentHybridVerificationSubmissionsReviewersDecisionsPriceDynamicsConstrainedRLArgumentationAnalysisReward Signal &amp; Updates\nFigure 1: Three-pillar architecture. Note the feedback loop: review quality verification (Pillar 3) directly informs credit issuance and price dynamics (Pillar 1), creating a closed-loop adaptive system.\n\n\n\n\nII Taking Stock: The Mechanism Failure\n\n\n\nII-A The Tragedy of the Review Commons\n\n\nThe fundamental problem is economic: publishing yields rewards (tenure, promotion), while reviewing yields almost none¬†[22].\n\n\nThis creates a classic commons tragedy. The ‚Äúreviewer attention budget‚Äù is finite, but depletion costs are externalized. Overburdened reviewers produce rushed evaluations, leading to ‚Äúreviewer roulette‚Äù¬†[18].\n\n\nScientific publishing volume increased by about 47% between 2016 and 2022, intensifying strain on editorial and reviewer capacity¬†[9]. Reviewer invitation acceptance rates have declined, so editors may need multiple invitations to secure each completed review¬†[22].\n\n\n\n\n\nII-B The LLM Amplification Effect\n\n\nLarge language models are increasingly used in scientific writing¬†[17] and can reduce the effort required to draft and revise manuscripts. They also enable superficially coherent but substantively hollow reviews. There are growing concerns that LLM assistance may shift review tone and scoring, while detection remains unreliable¬†[20].\n\n\n\n\nII-C Related Mechanisms\n\n\nPrior work corroborates our approach. Peer prediction methods elicit truthful reports by rewarding agreement with reference raters¬†[19]; information-theoretic variants offer stronger incentive guarantees¬†[14]. Strategyproof mechanisms¬†[28] provide theoretical rationing guarantees. Isotonic mechanisms use multi-submission knowledge for calibration¬†[26].\n\n\nRecent market-based alternatives like Impact Market¬†[23] propose decoupling dissemination from credentialing entirely. While promising, our framework seeks to repair the existing workflow rather than replace it, minimizing disruption. Similarly, endogenous matching models have explored linking effort to future assignment probabilities¬†[27].\n\n\nUnlike auction-based rationing mechanisms (e.g., VCG combined with peer-prediction¬†[25]), which clear the market in single-shot rounds, our Credit Economy introduces a persistent, transferable asset. This allows researchers to smooth their labor across time (reviewing now to submit later), addressing the ‚Äúbursty‚Äù nature of conference deadlines. Our framework synthesizes these insights, prioritizing practical deployability.\n\n\n\n\n\nIII A Multi-Agent Framework\n\n\n\nIII-A The Community as a Stochastic Game\n\n\nWe model the research community as a stochastic multi-agent system ‚ü®ùíú,ùíÆ,ùíØ,‚Ñõ‚ü©\\langle\\mathcal{A},\\mathcal{S},\\mathcal{T},\\mathcal{R}\\rangle where agents pursue individual objectives. Currently, Rrev‚âà0R_{\\text{rev}}\\approx 0 while Rpub&gt;0R_{\\text{pub}}&gt;0. The solution reshapes rewards so quality reviewing becomes individually rational¬†[21].\n\n\n\n\nIII-B Credit-Based Submission Economy\n\n\nWe propose a ‚ÄúReview Credit‚Äù (RC) system where submissions cost RC and quality reviews earn RC.\n\n\nPrice Dynamics. Prices follow a closed-loop update rule: pt+1=pt+Œ≥‚Äã(Dt‚àíSt)p_{t+1}=p_{t}+\\gamma(D_{t}-S_{t}) clipped to [pmin,pmax][p_{\\min},p_{\\max}]. Here, DtD_{t} represents the rolling average of submission volume (demand for review slots), and StS_{t} represents the cleared review capacity (supply) over epoch tt. To prevent oscillations (e.g., credit runs), updates occur at fixed monthly epochs with adaptive damping Œ≥\\gamma¬†[15]. To ensure long-term budget balance and prevent deflationary spirals, the system employs an adaptive issuance policy: if the total credit supply drops below a safety threshold (velocity &lt;Vmin&lt;V_{\\min}), the protocol temporarily subsidizes review rewards from a central reserve.\n\n\nStability. Heterogeneous agent strategies (e.g., hoarding) pose stability risks. We propose monitoring the Credit Velocity V=‚àëTransactionsTotal SupplyV=\\frac{\\sum\\text{Transactions}}{\\text{Total Supply}} and employing Lyapunov-based control policies to adjust issuance rates if system volatility exceeds safety margins.\n\n\nSupply Policy. RC issuance occurs through review completion; sinks include submission fees and expiration. Caps prevent hoarding.\n\n\nQuality Measurement. Review quality combines: blinded author ratings, meta-reviewer consistency, and specificity metrics. Information-theoretic scoring¬†[14] offers stronger incentive properties and should be evaluated as a primary method.\n\n\nNewcomer Support. First-time submitters receive initial endowments; hardship waivers address career breaks; mentoring-linked credit earning provides alternative pathways.\n\n\nGovernance Model. Credits are centrally ledgered by the venue. Disputes are adjudicated by program chairs with escalation to a standing ethics committee; clawbacks require documented misconduct with appeal rights.\n\n\n\n\nIII-C MARL-Optimized Reviewer Assignment\n\n\nCurrent algorithms optimize primarily for topic match¬†[13]. Classical OR methods (min-cost flow, MIP with fairness constraints) excel at static allocation¬†[4] but fail to capture dynamic reviewer fatigue.\n\n\nWe propose a Constrained Multi-Agent Reinforcement Learning (CMARL) approach¬†[1].\n\n\n‚Ä¢\n\nState (sts_{t}): Reviewer load, historical lateness, recent decline patterns, and topic embedding distance.\n\n\n\n‚Ä¢\n\nAction (ata_{t}): The assignment matrix subject to hard constraints (COIs, max load).\n\n\n\n‚Ä¢\n\nReward (rtr_{t}): A multi-objective function combining timeliness, specificity score, and fairness penalties.\n\n\n\n\n\nCounterfactuals. Offline RL on historical logs suffers from confounding (we only observe outcomes for realized matches). To mitigate this, we propose using Doubly Robust Estimators or Causal Bandits to estimate the potential reward of counterfactual assignments during the training phase.\n\n\nLearning Setup. Learning adds value when reviewer behavior is non-stationary or when latent features (past timeliness, decline patterns) improve predictions beyond static matching. Minimum effect sizes (e.g., &gt;&gt;10% timeliness improvement) justify added complexity.\n\n\n\n\nIII-D Hybrid Verification\n\n\nCombining structured checklists¬†[24] with argumentation frameworks¬†[7]: reviewers complete rubrics; review text is parsed into claims.\n\n\nTo ensure scalability, we adopt an ‚ÄúAgent-as-a-Judge‚Äù paradigm¬†[29]. An LLM-based verifier extracts claims and checks for stance consistency against the paper content. We acknowledge that LLM verifiers may encode biases or succumb to Goodhart‚Äôs law (au"
  }
]