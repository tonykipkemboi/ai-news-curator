[
  {
    "title": "Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition",
    "url": "https://arxiv.org/abs/2601.16211v1",
    "source": "arxiv",
    "summary": "We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined fact",
    "full_text": null
  },
  {
    "title": "PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation",
    "url": "https://arxiv.org/abs/2601.16210v1",
    "source": "arxiv",
    "summary": "Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete lat",
    "full_text": null
  },
  {
    "title": "LLM-in-Sandbox Elicits General Agentic Intelligence",
    "url": "https://arxiv.org/abs/2601.16206v1",
    "source": "arxiv",
    "summary": "We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverag",
    "full_text": "\n\n\n\n1 Introduction\n\n2 LLM-in-Sandbox¬†Elicits General Intelligence\n\n\n2.1 Code Sandbox\n\nLightweight General-Purpose Design.\nMinimal Toolset with Meta-Capabilities.\n\n\n\n2.2 LLM-in-Sandbox¬†Workflow\n\nPrompting for Exploration.\nTask Input/Output Handling.\n\n\n\n2.3 Experiments on General Domains\n\nSetup\nResults\n\n\n\n2.4 Analysis on Sandbox Utilization\n\n\n2.4.1 Case Study\n\nExternal Resources Access.\nFile Management.\nComputation.\n\n\n\n2.4.2 Quantitative Analysis\n\nVariation across Task Domains.\nLong-Context Tasks Benefit from File-based Context.\nStrong vs. Weak Models.\n\n\n\n\n\n\n\n3 LLM-in-Sandbox¬†Reinforcement Learning Enhances Generalization\n\n\n3.1 Method\n\nData Source.\nSandbox Configuration.\nTask Setup.\nRL Training.\n\n\n\n3.2 Experiments\n\nSetup\nMain Results.\nData Source and Context Placement\n\n\n\n3.3 Analysis on Generalization\n\nGeneralization across Domains.\nGeneralization across Model Capabilities.\nGeneralization across Inference Modes.\n\n\n\n\n\n4 LLM-in-Sandbox¬†Enables Efficient Deployment\n\n\n4.1 Computational Analysis\n\nCost.\nSpeed.\n\n\n4.2 Sandbox Infrastructure\n\n\n\n5 LLM-in-Sandbox¬†Goes Beyond Text Generation\n\nCase Studies.\nDiscussion.\n\n\n\n6 Conclusion and Future Work\n\nLLM-in-Sandbox¬†as Default Inference Infrastructure\nLLM-in-Sandbox¬†as an Agentic Capability Benchmark\nSandbox-Native Model Training\n\n\nA Sandbox Implementation\nB Model Configurations\n\nC Evaluation Details\n\nMathematics.\nPhysics.\nChemistry.\nBiomedicine.\nLong-Context Understanding.\nInstruction Following.\nSoftware Engineering.\n\n\nD Sandbox Capability Classification\n\nE LLM-in-Sandbox-RL¬†Training Details\n\nReward Design.\n\n\nF Prompt for¬†LLM-in-Sandbox\n\n\n\n\n\nLLM-in-Sandbox Elicits General Agentic Intelligence\n\n\nDaixuan ChengŒ±Œ≤¬†¬†Shaohan HuangŒ≤¬†¬†Yuxian GuŒ≥¬†¬†Huatong SongŒ±¬†¬†Guoxin ChenŒ±\n\nLi DongŒ≤¬†¬†Wayne Xin ZhaoŒ±‚Ä†¬†¬†Ji-Rong WenŒ±¬†¬†Furu WeiŒ≤‚Ä†\n\nŒ±GSAI, Renmin University of China‚ÄÉŒ≤Microsoft Research‚ÄÉŒ≥Tsinghua University\n\n\n¬†https://llm-in-sandbox.github.io\n\nEmail: daixuancheng6@gmail.com¬†¬†‚Ä†Corresponding Authors.\n\n\nAbstract\nWe introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox¬†Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox‚Äôs efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.\n\n\nFigure 1: Overview of LLM-in-Sandbox. We enable LLMs to explore within a code sandbox (i.e., virtual computer), unlocking significant performance gains across diverse LLMs and domains. Green values indicate improvements over vanilla LLMs. All LLMs are evaluated without additional training.\n\n\n\n1 Introduction\n\nThe capabilities of Large Language Models (LLMs) have been progressively unlocked through different paradigms. In-context learning showed that models could generalize to new tasks without task-specific finetuning¬†(Brown et al., 2020). Chain-of-thought prompting then elicited reasoning by guiding models to decompose problems into steps¬†(Wei et al., 2022). Recently, agentic frameworks empowered models to leverage diverse tools across multiple turns¬†(Anthropic, 2025b). Following this trajectory, how can we further unlock their capabilities?\n\n\nIn this work, we propose LLM-in-Sandbox‚Äîenabling LLMs to explore within a code sandbox‚Äîas a promising next step along this trajectory. As shown in Figure¬†1, the sandbox is essentially a virtual computer with terminal capabilities, widely used by code agents such as Claude Code¬†(Anthropic, 2025a). While it is typically used for software engineering¬†(Jimenez et al., 2023), we argue its potential extends far beyond coding. Computers are perhaps the most versatile platform ever created‚Äîvirtually any task can be accomplished through them. This versatility stems from three meta-capabilities: external resource access (e.g., the internet), file management, and code execution. We hypothesize that combining LLMs with a virtual computer may unlock their potential for general intelligence.\n\n\nTo validate this potential, we evaluate LLM-in-Sandbox¬†on challenging non-code tasks. Given a task input, LLMs explore within a sandbox with basic computer capabilities across multiple turns until task completion. Remarkably, without any additional training, LLMs can spontaneously leverage the code sandbox for non-code tasks, such as installing domain-specific tools to gain new abilities, utilizing file storage to process documents beyond context limits, and executing scripts to meet formatting requirements. As a result, state-of-the-art agentic LLMs achieve substantial performance gains across mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction-following.\n\n\nTo further advance this paradigm, we propose LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL). While strong agentic models benefit directly from LLM-in-Sandbox, weaker models often struggle: performing worse in LLM-in-Sandbox¬†mode than in vanilla LLM mode (i.e., directly generating the output without sandbox). LLM-in-Sandbox-RL¬†bridges this gap using only general, non-agentic data. Specifically, we use general context-based tasks¬†(Cheng et al., 2024) where the context is pre-placed as text files in the sandbox rather than directly in the model prompt, requiring the model to explore and interact with the environment. With only outcome-based rewards¬†(Guo et al., 2025), LLM-in-Sandbox-RL¬†enables weaker models to excel in LLM-in-Sandbox¬†mode, significantly outperforming their LLM mode, while also enhancing models that already possess strong agentic capabilities. Crucially, this training elicits strong generalization: leading to consistent improvements across diverse out-of-domain tasks, and even enhancing vanilla LLM mode. This suggests LLM-in-Sandbox-RL¬†can be a general method to elicit both agentic and non-agentic intelligence across models and domains.\n\n\nBeyond performance, we analyze practical considerations of deploying LLM-in-Sandbox¬†in real-world systems, covering computational cost, speed, and sandbox infrastructure. We find that LLM-in-Sandbox¬†dramatically reduces token consumption by up to 8√ó\\times in long-context scenarios (100K ‚Üí\\rightarrow 13K tokens), achieves competitive query-level throughput on average, and incurs minimal sandbox infrastructure overhead. Finally, we open-source LLM-in-Sandbox¬†as a Python package that integrates seamlessly with popular inference backends (e.g., vLLM¬†(Kwon et al., 2023) and SGLang¬†(Zheng et al., 2024)) and API-based LLMs, to accelerate the transition towards general agentic intelligence.\n\n\nOur contributions are summarized as follows:\n\n\n‚Ä¢\n\nWe introduce LLM-in-Sandbox, demonstrating that strong agentic LLMs exhibit generalization capabilities to exploit code sandboxes for non-code tasks across diverse domains, without additional training (Section¬†2).\n\n\n\n‚Ä¢\n\nWe propose LLM-in-Sandbox¬†Reinforcement Learning, which trains LLMs to explore sandbox environments using only general non-agentic data, enhancing generalization of both agentic and non-agentic intelligence across domains (Section¬†3).\n\n\n\n‚Ä¢\n\nWe analyze LLM-in-Sandbox‚Äôs efficiency from computational and system perspectives, and open-source it as a Python package for real-world deployment (Section¬†4).\n\n\n\n\n\n\n\n2 LLM-in-Sandbox¬†Elicits General Intelligence\n\nThe core idea of LLM-in-Sandbox¬†is to grant LLMs access to a computer where they can freely operate to complete user-specified tasks. Specifically, computers possess three meta-capabilities that form the foundation for general task-solving:\n\n\n‚Ä¢\n\nExternal resource access: fetching resources from external services (e.g., the internet);\n\n\n\n‚Ä¢\n\nFile management: reading, writing, and organizing data persistently;\n\n\n\n‚Ä¢\n\nCode execution: writing and executing arbitrary programs.\n\n\n\nJust as humans leverage computers to accomplish virtually any task, we hypothesize that combining LLMs‚Äô powerful reasoning and agentic capabilities with a code sandbox may unlock their potential for general intelligence.\n\n\nTo explore the full potential of this paradigm, our design of LLM-in-Sandbox¬†emphasizes two principles: minimal‚Äîproviding a basic code sandbox with these three capabilities, and exploratory‚Äîencouraging models to discover diverse solution strategies. In the following, we describe our sandbox environment (Section¬†2.1), the LLM-in-Sandbox¬†workflow (Section¬†2.2), as well as experiments (Section¬†2.3) and analysis (Section¬†2.4) in general domains.\n\n\n\n2.1 Code Sandbox\n\nA code sandbox is a virtualized computing environment, typically an Ubuntu-based system implemented via Docker containers, that provides LLMs with terminal access and full system capabilities. Within this environment, LLMs can execute arbitrary bash commands, create and modify files, and access network resources. The containerized nature ensures isolation from the host system, enabling safe execution of model-generated code.\n\n\n\n\n\nSWE Agents\nLLM-in-Sandbox\n\n\nEnvironment Setup\nTask-specific\nGeneral-purpose\n\n\nDependencies\nPre-configured\nRuntime installation\n\n\nStorage Scaling\nPer-task images\nSingle shared image\n\n\nTable 1: Comparison of sandbox design between SWE agents and LLM-in-Sandbox. \n\n\nLightweight General-Purpose Design.\n\nCode sandboxes have recently emerged as a critical infrastructure for "
  },
  {
    "title": "Counterfactual Training: Teaching Models Plausible and Actionable Explanations",
    "url": "https://arxiv.org/abs/2601.16205v1",
    "source": "arxiv",
    "summary": "We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real",
    "full_text": null
  },
  {
    "title": "Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing",
    "url": "https://arxiv.org/abs/2601.16200v1",
    "source": "arxiv",
    "summary": "Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs",
    "full_text": null
  },
  {
    "title": "A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows",
    "url": "https://arxiv.org/abs/2601.16194v1",
    "source": "arxiv",
    "summary": "This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibil",
    "full_text": null
  },
  {
    "title": "Average Unfairness in Routing Games",
    "url": "https://arxiv.org/abs/2601.16187v1",
    "source": "arxiv",
    "summary": "We propose average unfairness as a new measure of fairness in routing games, defined as the ratio between the average latency and the minimum latency experienced by users. This measure is a natural complement to two existing unfairness notions: loaded unfairness, which compares maximum and minimum latencies of routes with positive flow, and user equilibrium (UE) unfairness, which compares maximum ",
    "full_text": null
  },
  {
    "title": "Learning to Discover at Test Time",
    "url": "https://arxiv.org/abs/2601.16175v1",
    "source": "arxiv",
    "summary": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one gre",
    "full_text": null
  },
  {
    "title": "Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints",
    "url": "https://arxiv.org/abs/2601.16174v1",
    "source": "arxiv",
    "summary": "Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a pr",
    "full_text": null
  },
  {
    "title": "Structured Hints for Sample-Efficient Lean Theorem Proving",
    "url": "https://arxiv.org/abs/2601.16172v1",
    "source": "arxiv",
    "summary": "State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the mi",
    "full_text": null
  },
  {
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "url": "https://arxiv.org/abs/2601.16163v1",
    "source": "arxiv",
    "summary": "Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Pol",
    "full_text": null
  },
  {
    "title": "Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems",
    "url": "https://arxiv.org/abs/2601.16158v1",
    "source": "arxiv",
    "summary": "Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual",
    "full_text": null
  },
  {
    "title": "Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates",
    "url": "https://arxiv.org/abs/2601.16152v1",
    "source": "arxiv",
    "summary": "Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural",
    "full_text": "\n\n\n\n1 Introduction: Structure Under Disagreement\n\n2 Assumptions, Neutrality, and Optimization Criterion\n\n2.1 Persistent Disagreement and Uncoordinated Extension\n2.2 Neutrality Constraint\n2.3 Stable Reference Requirement\n2.4 Optimization Criterion\n\n\n\n3 Structural Requirements for Accountability Substrates\n\n3.1 R1: Stable Identity and Persistence\n3.2 R2: Obligation-Bearing Capacity\n3.3 R3: Normative Reference Without Execution\n3.4 R4: Time-Indexed Occurrence\n3.5 R5: Applicability and Scope\n3.6 R6: Descriptive Indicators Without Causal Commitment\n3.7 Summary\n\n\n\n4 Necessity: A Lower Bound on Ontological Structure\n\n4.1 Identity-and-Persistence Regimes\n4.2 Collapse Strategy and Admissibility\n4.3 Lower-Bound Result\n4.4 Interpretation of the Necessity Result\n\n\n\n5 Sufficiency: A Six-Regime Construction\n\n5.1 The Six Identity-and-Persistence Regimes\n5.2 Admissible Relations\n5.3 Satisfaction of Structural Requirements\n5.4 Sufficiency Proposition\n5.5 Tightness of the Bound\n\n\n\n6 Interpretation of the Result\n\n6.1 What the Result Establishes\n6.2 Minimality Relative to Stability\n6.3 Why Hidden Regimes Are Excluded\n6.4 Relation to Other Ontological Strategies\n6.5 Conditional Scope of the Claim\n6.6 Implications\n\n\n\n7 Related Work\n\n7.1 Classification, Disagreement, and Neutrality\n7.2 Identity, Roles, and Ontological Methodology\n7.3 Upper Ontologies\n7.4 Relation to Common Logic and ontology exchange\n\n7.5 Causality, Explanation, and Infrastructure\n\n7.5.1 Relation to FAIR Data Principles\n\n\n\n\n8 Conclusion\nAppendix A. Glossary of Terms\nGlossary\n\n\n\n\n\nSubstrate Stability Under Persistent Disagreement: \nStructural Constraints for \nNeutral Ontological Substrates \n\n\nDenise M. Case\n\n\n\n\n\nAbstract\nModern data systems increasingly operate under conditions of persistent legal,\npolitical, and analytic disagreement.\nIn such settings, interoperability cannot rely on shared interpretation,\nnegotiated semantics, or centralized authority.\nInstead, representations must function as neutral substrates that preserve\nstable reference across incompatible extensions.\nThis paper investigates the structural constraints imposed on ontological\ndesign by this requirement.\nBuilding on a neutrality framework that treats interpretive non-commitment and\nstability under extension as explicit design constraints, we ask what minimal\nontological structure is forced if accountability relationships are to remain\nreferable and comparable under disagreement.\nMinimality here is not mere parsimony:\na reduction is admissible only if it does not\nreintroduce stability-critical distinctions\nas hidden roles, flags, or contextual predicates.\nWe establish a conditional lower-bound result: any ontology capable of\nsupporting accountability under persistent disagreement must realize at least\nsix distinct identity-and-persistence regimes.\nWe further show that a construction with exactly six such regimes is sufficient\nto satisfy the stated requirements without embedding causal or normative\ncommitments in the substrate.\nThe result is not a proposal for a universal ontology, but a constraint on what\nis possible when neutrality and stable reference are treated as non-negotiable\ndesign goals.\n\n\n\n\nPreprint.\nThis work has not yet undergone peer review and is provided for open discussion and feedback.\n\n\n\nStatement of Result (Preview).\nUnder explicit neutrality and stability constraints, we show that attempts to\nsimplify ontological structure by collapsing distinctions inevitably reintroduce\nthose distinctions as hidden interpretive mechanisms.\nWhen such hidden regimes are excluded, accountability under persistent\ndisagreement requires a minimal set of six distinct identity-and-persistence regimes:\nfewer do not satisfy the stated neutrality and stability requirements,\nand more are unnecessary.\n\n\nInformally: Neutrality is achieved by\nexternalizing explanatory and evaluative structure.\nTo function as a substrate for disagreement, an ontology must remain\nagnostic with respect to causal and normative interpretation.\n\n\nThe formal statement and proof appear in Section¬†6.\n\n\nKeywords:  \nformal ontology;\nontological neutrality;\naccountability;\nneutral substrates;\nextension stability;\ncausal and normative commitment\n\n\n\n1 Introduction: Structure Under Disagreement\n\nModern data systems increasingly operate in environments characterized by\npersistent legal, political, and analytic disagreement.\nIn such settings, no single interpretive authority can be assumed, and\nsemantic convergence cannot be enforced without excluding legitimate\nparticipants or perspectives.\nNevertheless, shared use of data remains necessary:\nentities must be referable, actions traceable, and\nrecords comparable across frameworks that may diverge\nin assumptions, methods, or evaluative standards.\n\n\nThis paper addresses a structural question that arises in this context:\nwhat ontological structure is required if a representation is to remain\nusable as a shared substrate across incompatible interpretations?\nThe focus is not on domain modeling, explanatory adequacy, or normative correctness.\nRather, it concerns the structural conditions required for stable reference\nand accountability under persistent disagreement.\n\n\nWe adopt a neutrality framework developed in prior work¬†Case (2026),\ntreating ontological neutrality as a design constraint rather than an\naspirational ideal.\nNeutrality is understood here as interpretive non-commitment combined with\nstability under incompatible extension.\nUnder this view, a substrate must permit independent extensions to introduce\ncausal, normative, or analytic structure without requiring revision of the\nunderlying representation.\nAccordingly, explanatory and evaluative structure is externalized, and the\nsubstrate is constrained to remain pre-causal and pre-normative at the\nsubstrate level.\n\n\nWithin this setting, ontological minimality cannot be understood solely as a\nreduction in the number of primitives.\nMany apparent simplifications achieve parsimony by reintroducing essential\ndistinctions indirectly, through roles, flags, contextual typing, or other\ninternal discriminators whose interpretation is supplied by extension layers\n(e.g., a single undifferentiated entity type distinguished only by role\npredicates such as isAgent, isOccurrence, or isAuthority).\nSuch encodings rely on shared interpretive assumptions, which may not be\navailable in settings characterized by persistent disagreement.\nFor this reason, the present analysis excludes reductions that preserve\nstability only by deferring essential distinctions to interpretive mechanisms\nnot represented at the substrate level.\n\n\nThe specific focus of this paper is accountability.\nAccountability analysis requires stable reference to participants, authority\nstructures, regulated occurrences, applicability contexts, and descriptive\nrecords, without embedding causal claims or normative judgments in the\nsubstrate itself.\nThese categories are not introduced as an ontological partition,\nbut as representational capacities whose simultaneous support\nplaces structural constraints on any neutral substrate.\nThe question addressed here is not how these elements should be interpreted,\nbut whether a neutral substrate can support them at all, and if so, with what\nirreducible structural commitments.\n\n\nThe central contribution of this paper is a conditional bound on ontological\nstructure given the design goals outlined above.\nUnder explicit neutrality and stability constraints, we show that any ontology\ncapable of supporting accountability under persistent disagreement must\nrealize at least six distinct identity-and-persistence regimes.\nWe further show that a construction with a minimal set of six such regimes is\nsufficient to satisfy the stated requirements without introducing hidden or\nimplicit structure.\nThe result is not a proposal for a universal ontology, but a constraint on\nwhat is possible under a specific optimization objective.\n\n\nAlternative optimization objectives prioritize different tradeoffs, such as\ninterpretive convergence, domain-specific efficiency, or context-dependent\nclassification at the substrate level.\nThe results presented here apply only under the stated constraints, within\nwhich a minimal, domain-independent bound on ontological structure is\nestablished for stable reference and accountability.\n\n\nThe remainder of the paper proceeds as follows.\nSection¬†2 states the assumptions, neutrality constraints,\nand optimization criterion.\nSection¬†3 formalizes the structural requirements imposed\nby accountability analysis.\nSection¬†4 derives a lower bound on the number of required\nidentity-and-persistence regimes.\nSection¬†5 presents a construction achieving this bound.\nSection¬†6 discusses the scope and implications of the\nresult.\nSection¬†7 situates the work relative to existing ontology\nframeworks.\nSection¬†8 concludes.\n\n\n\n\n2 Assumptions, Neutrality, and Optimization Criterion\n\nThis section fixes the problem setting for the remainder of the paper.\nAll results that follow are conditional on the assumptions stated here.\nThe purpose is not to argue that these assumptions are universally correct,\nbut to make explicit the constraints under which a neutral substrate for\naccountability is possible.\n\n\n\n2.1 Persistent Disagreement and Uncoordinated Extension\n\nWe assume environments in which disagreement is not an anomaly to be\nresolved, but a persistent condition.\nLegal interpretation, policy evaluation, and analytic methodology may remain\nincompatible across communities and over time.\nAccordingly, no single interpretive authority or shared semantic framework\ncan be assumed at the substrate level.\n\n\nWe further assume that extensions to the substrate may occur without\ncoordination.\nIndependent actors may add explanatory, normative, or analytic structure,\nand such extensions must not require revision of the substrate itself.\nThis assumption does not rely on\nglobal semantic agreement, centralized governance, or\nnegotiated meaning as preconditions for interoperability.\n\n\n\n\n2.2 Neut"
  },
  {
    "title": "Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization",
    "url": "https://arxiv.org/abs/2601.16150v1",
    "source": "arxiv",
    "summary": "Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This le",
    "full_text": "\n\n\n\nI Introduction\n\nII Method\n\nII-A Problem description\n\n\n\nIII Harmony and melody representations\n\nIII-A A training curriculum for promoting cross-attention\nIII-B Model architectures\n\nIII-C Training and inference\n\nTraining\nInference\nImplementation details.\n\n\n\n\n\nIV Results\n\nIV-A Dataset and evaluation metrics\nIV-B Comparisons\n\n\nV Conclusions\n\n\n\n\n\nPay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization\n\n\n\n\n\nMaximos Kaliakatsos-Papakostas\n\n‚ÄÉ‚ÄÉ\nDimos Makris\n\n\n  \nKonstantinos Soiledis\n\n‚ÄÉ‚ÄÉ\nKonstantinos-Theodoros Tsamis\n\n\n  \nVassilis Katsouros\n\n‚ÄÉ‚ÄÉ\nEmilios Cambouropoulos\n\n\n\nAbstract\nMelodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (‚Äúcross‚Äù) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody‚Äìharmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony‚Äìmelody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.\n\n\n\nI Introduction\n\n\nTransformer architectures have emerged as powerful sequence modeling frameworks across domains such as language, vision, and music¬†[36, 16]. Within symbolic music generation, melodic harmonization is a particularly challenging task: given a melodic sequence, the goal is to produce a harmonic sequence that is both locally compatible and globally coherent. This requires alignment of chords with melodic material while simultaneously maintaining harmonic progression structure over longer spans. As such, harmonization provides a rich testbed for exploring how sequence models integrate cross-modal signals (melody and harmony) across both local and global contexts.\n\n\nEarly neural approaches to melodic harmonization employed bidirectional LSTMs¬†[24, 42, 6, 7], while recent work has shifted to transformer-based models¬†[16, 32, 17, 40]. These methods typically frame harmonization as a translation or summarization problem, where the melody is ‚Äútranslated‚Äù into a harmonic sequence that abstracts its structure. Most methods rely on autoregressive decoding, generating chords sequentially from left to right. Such models assume harmonic the dependency of new harmony tokens only on previous ones, which does not allow insertion of chord constraints prior to generation. Melodic harmonization is rather a bidirectional process, that occasionally involves setting harmonic checkpoints and then filling the blanks.\n\n\nIn parallel, diffusion models have gained traction for symbolic music generation, following developments in vision¬†[15]. Some approaches operate in continuous symbolic spaces¬†[27, 25], others in latent VAE spaces¬†[44], and others on pianoroll images using U-Net backbones¬†[1, 26, 23, 39, 18, 43]. While diffusion has not yet been applied directly to melodic harmonization, related work in text generation has shown the effectiveness of discrete denoising and unmasking strategies. MaskGIT¬†[5], D3PMs¬†[2], and other discrete diffusion methods¬†[30] iteratively refine masked token sequences, offering flexible conditioning and faster generation compared to autoregressive models. In symbolic music, hybrid transformer‚Äìdiffusion models have been explored, either by applying diffusion to transformer logits¬†[19] or to latent codes learned through quantized autoencoders¬†[45]. These approaches highlight the potential of token-based unmasking as a generative mechanism, with particular advantages for tasks requiring global conditioning.\n\n\nCurriculum masking strategies are rooted in earlier denoising autoencoder research¬†[37, 3], which showed that stronger corruption early in training forces models to learn robust representations rather than trivial identity mappings. In NLP, non-autoregressive transformers¬†[11, 38] and the Levenshtein Transformer¬†[12] adopted similar iterative unmasking strategies, gradually refining predictions from coarse to fine. Diffusion models formalized these ideas with carefully designed noise schedules¬†[2, 28], where exponential or cosine schedules were found to balance context availability with reconstruction difficulty. In multimodal contexts, regularization techniques have been proposed to prevent models from ignoring cross-modal signals¬†[35, 21]. Together, these findings suggest that curriculum design is crucial for ensuring models exploit conditioning information effectively, rather than defaulting to modality-specific self-attention.\n\n\nRecently proposed approaches to melodic harmonization adopt single-encoder generative architectures, where melody is clamped in the beginning positions of the encoder and harmony tokens are progressively unmasked in the latter positions. Even though single-encoder architectures involve a single attention map for each head, subsets of weights play the role of ‚Äúself‚Äù (harmony attending to harmony) and ‚Äúcross‚Äù (harmony attending to melody) attention. Training methods of such architectures have been proposed that are inspired by discrete diffusion¬†[20] for iteratively and bidirectionally unmasking harmony tokens given a melodic sequence. Such training methods, however, exhibit weak melody-to-harmony attention, with models often over-relying on ‚Äúself-attention‚Äù, and underutilizing melodic cues (‚Äúcross-attention‚Äù). To address this, we propose a more robust training curriculum, FF (full-to-full), that begins with fully masked harmonic sequences for several initial training steps and then progressively unmasks until only a single mask remains. This strategy encourages strong ‚Äúcross-modal‚Äù (between melody and harmony) conditioning early in training, while gradually shifting toward fine-grained reconstruction. We further explore task-specific factors that may influence harmonization quality, including temporal resolution (quarter vs. sixteenth notes), structure integration (bar tokens vs. time signatures), and melody representation (pitch class vs. full range).\n\n\nOur contributions are threefold: (1) we describe the problem of weak melody ‚Äúcross-attention‚Äù in single-encoder generative transformers; (2) we demonstrate that the FF curriculum significantly improves melody‚Äìharmony integration, yielding state-of-the-art results in both in-domain and out-of-domain evaluations; (3) we provide empirical analysis across datasets, representations, and inference strategies, answering five core research questions about harmonization modeling. The code for the models, the training and the experiments can be found online¬†111https://github.com/NeuraLLMuse/SingleEncoderMHCrossAttention.git. This page provides links to a demo page and example MIDI files.\n\n\n\n\nII Method\n\n\nSingle-encoder generation is performed through an iterative unmasking of tokens, given some visible (already unmasked) context. Unlike left-to-right autoregressive decoding, this formulation leverages bidirectional context, which is particularly important for melodic harmonization. Human composers typically do not harmonize strictly linearly through time; instead, specific melody passages may strongly constrain the appropriate chord choice, and once these are fixed, the surrounding harmony can be refined in light of the emerging global context. Such a paradigm is closely related to non-autoregressive ‚Äúmask-predict‚Äù strategies in text generation¬†[11], and to inpainting approaches in symbolic music generation¬†[29]. Beyond modeling convenience, this generative process naturally accommodates user constraints: specific chords can be fixed at arbitrary positions before generation, enabling interactive human‚ÄìAI collaboration.\n\n\nFigure¬†1 illustrates an abstract overview of a single-encoder harmonization model. During inference, the model receives melody input in the first half of the encoder and masked harmony tokens in the second half; the task is to iteratively ‚Äúunmask‚Äù the harmony positions. During training, subsets of harmony tokens are masked, and the model learns to recover them, following iterative refinement strategies similar to the one used in Mask-Predict¬†[11]. A recent study introduced two such inference‚Äìtraining couplings for harmonization¬†[20]. While both outperformed autoregressive baselines, close inspection revealed that melodic nuances were often underutilized. We analyze this limitation abstractly in the following paragraphs before detailing our proposed representation, architecture, and curriculum solution.\n\n\nFigure 1: Overview of the single-encoder architecture for melodic harmonization. Left: abstract representation of the transformer encoder input (melody in the first h"
  },
  {
    "title": "Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets",
    "url": "https://arxiv.org/abs/2601.16147v1",
    "source": "arxiv",
    "summary": "Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methods\n\n2.1 Global Context: Rhythm Contrasting\n2.2 Local Context: Heartbeat Contrasting\n2.3 Soft Contrasting\n2.4 Loss Function\n\n2.5 Experiments\n\n2.5.1 Pretraining\n2.5.2 Downstream tasks\n\n\n\n\n3 Results\n4 Conclusion\n5 Compliance with ethical standards\n6 Acknowledgments\n\n\n\n\n\n\n\\useunder\n\\ul\n\n\nBeat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets\n\n\n\nMuhammad Ilham Rizqyawan‚ãÜ‚ãÑ ‚ÄÉ‚ÄÉPeter Macfarlane‚Ä† ‚ÄÉ‚ÄÉStathis Hadjidemetriou‚Ä° ‚ÄÉ‚ÄÉFani Deligianni‚ãÜ\n‚ãÜSchool of Computing Science, University of Glasgow, UK\n‚ãÑResearch Center for Smart Mechatronics, National Research and Innovation Agency, Indonesia\n‚Ä†School of Health &amp; Wellbeing, University of Glasgow, UK\n‚Ä°Department of Information Technologies, University of Limassol, Cyprus\n\nCorresponding author: fani.deligianni@glasgow.ac.uk ‚Äî Accepted at ISBI 2026\n\n\nAbstract\nObtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model‚Äôs broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.\n\n\nKeywords‚ÄÇECG, semi-supervised learning, contrastive learning, ECG segmentation, multilabel classification\n\n\n\n1 Introduction\n\nRecent studies have demonstrated the effectiveness of machine learning models to satisfactorily undertake 12-lead ECG analysis, even for low-prevalence diagnoses such as congenital heart disease [1, 3]. However, training deep learning models requires large amounts of labelled ECG data, which remains challenging to obtain even at a national scale. Foundation models pretrained on unlabelled data offer a promising solution, enabling generalisation across diverse tasks through transfer learning with minimal labelled samples [8, 16].\n\n\nContrastive learning (CL) has emerged as an effective pretraining approach that leverages unlabelled data by learning to distinguish similar and dissimilar samples [4, 9]. While several CL frameworks have been proposed for biomedical signal analysis, many are influenced by computer vision paradigms or are designed for general time series [26], failing to exploit ECG specific characteristics.\n\n\nThe 12-lead ECG encodes 3D cardiac activity across temporal cycles through its spatio-temporal structure, providing rich information for learning robust representations. 3KG [6] leverages this by transforming ECG signals into the vectorcardiography (VCG) domain, a 3D representation of cardiac activity. This is used as an augmentation strategy but considers only global context. Oh et al [18] proposed capturing both global context using Contrastive Multi-segment Coding (CMSC) [13] (also used by 3KG) and local context using Wave2vec [2]. The ECG-FM foundational model adopts this dual context pretraining approach [16]. However, these methods overlook that ECG signals comprise multiple heartbeats with varying intervals and durations, which represent fundamental units of cardiac activity that could provide richer contrastive cues.\n\n\nIn this paper, we propose an ECG CL framework that considers both global and local contexts by leveraging heartbeat-level contrasting for local context representation. First, we use the 12-lead ECG as input, which is transformed into the 3D VCG domain and augmented the signal using the 3KG strategy [6]. Subsequently, we apply contrastive learning in both global and local contexts. We introduce two soft contrastive strategies that utilised ECG feature similarity as a continuous target between 0 and 1, moving beyond binary positive-negative pairs. Finally, we validate the model on two downstream tasks: multilabel classification for global context evaluation and ECG wave segmentation for local context evaluation.\n\n\n\n\n2 Methods\n\nWithin this framework, we aim to leverage the unique characteristics of ECG signals. ECGs can be analysed at two levels: the rhythm level, which encompasses several heartbeats, and the morphological level, which captures beat-specific characteristics. Following Oh et al. [18], we refer to these as the global and local contexts, respectively. Contrasting at the beat level is essential for enabling the model to differentiate individual beat morphologies by identifying the characteristics of specific waves (P-wave, QRS-complex, and T-wave). Therefore, we propose a framework that performs contrastive learning at both the rhythm and beat levels. The overall concept of the framework is illustrated in Fig. 1A.\n\n\nFigure 1: A) General structure of the proposed framework with both rhythm and beat level contrasting. The hard targets for beat level are obtained from the beat-classifier (blue line). The soft targets are obtained from features similarity (brown lines). B) Comparison between hard targets (top), which use binary values either 0 or 1, and soft targets (bottom) which use continuous values ranging from 0 to 1.\n\n\n\n2.1 Global Context: Rhythm Contrasting\n\nOur framework extracts useful representations from unlabelled data by teaching the model to recognise that different augmented versions of the same signal are similar, while different signals are dissimilar [4].\nFormally, given an ECG signal ùêó‚ààùêëL√óD\\mathbf{X}\\in\\mathbf{R}^{L\\times D}, where LL is the number of leads (12 in this case) and DD is the temporal dimension (5000 samples at 500 Hz for 10 seconds), we generate two augmented views, ùêó~i\\tilde{\\mathbf{X}}_{i} and ùêó~j\\tilde{\\mathbf{X}}_{j}, through stochastic data augmentation operations.\n\n\nWe adopt the 3KG [6] augmentation strategy, which operates in the VCG domain. The transformation pipeline can be expressed as:\n\n\n\n\n\nùêó~=ùêåKors‚àí1‚ãÖùíú‚Äã(ùêåKors‚ãÖùêó)+œµ\\tilde{\\mathbf{X}}=\\mathbf{M}^{-1}_{\\text{Kors}}\\cdot\\mathcal{A}(\\mathbf{M}_{\\text{Kors}}\\cdot\\mathbf{X})+\\boldsymbol{\\epsilon}\n\n(1)\n\n\n\n\nwhere ùêåKors‚ààùêë3√ó12\\mathbf{M}_{\\text{Kors}}\\in\\mathbf{R}^{3\\times 12} is the Kors transformation [14] matrix for ECG-to-VCG conversion, ùíú‚Äã(‚ãÖ)\\mathcal{A}(\\cdot) represents augmentation operations (rotation and scaling) applied in the VCG space, ùêåKors‚àí1\\mathbf{M}^{-1}_{\\text{Kors}} is the pseudo-inverse used for VCG-to-ECG reconstruction, and œµ‚àºùí©‚Äã(0,œÉ2)\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\sigma^{2}) is additive Gaussian noise in the ECG domain with noise scale œÉ=0.05\\sigma=0.05. The rotation operation applies a random angle Œ∏‚àà[‚àí15‚Äã¬∞,15‚Äã¬∞]\\theta\\in[$$,$$] around a randomly selected axis, and scaling applies a factor s‚àà[1.0,1.2]s\\in[1.0,1.2]. The Kors transformation matrix has been shown to yield the highest reconstruction accuracy [23]. These parameter values were chosen to ensure that the augmentations remains physiologically meaningful and do not distort the signal into pathological forms.\n\n\n\n\n2.2 Local Context: Heartbeat Contrasting\n\nFor beat-level analysis, we process individual heartbeats cropped to nn samples centred at the R-peak, ensuring that each segment contains a complete heartbeat. However, directly passing these cropped beat signals to the base encoder is not feasible due to mismatched input dimensions. Using a separate encoder is also undesirable, as our goal is to maintain a single encoder throughout pretraining.\n\n\nTo address this, we input the full 12-lead, 10-second ECG signal into the encoder once, as in rhythm-level contrasting. For beat-level contrast, we then extract beat-specific representations by splitting the encoder output based on the projected R-peak positions. This approach is analogous to the Region of Interest (ROI) pooling in Fast R-CNN [5]. The extracted representations are subsequently passed through a dedicated beat projection head, gb‚Äã(‚ãÖ)g_{b}(\\cdot).\n\n\nAt the beat level, we use pseudo-labels as hard contrastive targets for the contrastive loss function. These labels are generated by a separate beat-classifier trained on Lead-II during preprocessing. This classifier was trained on the MIT-BIH Arrhythmia dataset [17], using the five AAMII superclasses [10].\n\n\n\n\n2.3 Soft Contrasting\n\nWe explore the use of continuous similarity measures as contrastive targets. Instead of using binary values (0 or 1) as similarity targets (Fig. 1.B, top), we employ soft targets ranging between 0 and 1 (Fig. 1.B, bottom). This approach is conceptually similar to [15], but differs in how the soft targets are computed.\n\n\nIn this work, a set of features is extracted from the ECG signal during preprocessing. Based on these features, we created two versions of the soft targets. For the first version (soft_1), a soft target matrix of size 2‚ÄãN√ó2‚ÄãN2N\\times 2N is produced by concatenating the feature set NN with itself to form a 2‚ÄãN2N-sized tensor, followed by computing pairwise cosine similarities. To emphasize high similarity values, the resulting similarities are exponentiated using a power exponentiation factor.\n\n\nFor the second version (soft_2), the pp-norm distance is computed based on the feature set, and the top-kk neighbours are identified for each sample. We assign weights based on proximity: for k=3k=3, the three nearest neighbours received weights of 11, 23\\frac{2}{3}, and 13\\frac{1}{3}, resp"
  },
  {
    "title": "Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games",
    "url": "https://arxiv.org/abs/2601.16142v1",
    "source": "arxiv",
    "summary": "The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, re",
    "full_text": null
  },
  {
    "title": "Learning to Watermark in the Latent Space of Generative Models",
    "url": "https://arxiv.org/abs/2601.16140v1",
    "source": "arxiv",
    "summary": "Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc wat",
    "full_text": null
  },
  {
    "title": "On the Intrinsic Dimensions of Data in Kernel Learning",
    "url": "https://arxiv.org/abs/2601.16139v1",
    "source": "arxiv",
    "summary": "The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution's support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_œÅ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kern",
    "full_text": "\n\n\n\n1 Introduction\n2 Kernel-based upper metric and effective dimensions\n3 nn-widths and eigenvalues\n4 An application to ERM\n5 The estimation of the nn-width‚Äôs upper bound\n6 Experiments\n7 Conclusions and future research\nA Proofs for Section¬†3\nB Proofs for Section¬†4\n\nC Proof for Section¬†5\n\nC.1 Sample size for smaller tt\nC.2 Implementation of the Algorithm¬†1 and its complexity\n\n\n\nD Additional experiments\n\nD.1 Details of experiments with fractals\nD.2 Details of experiments with the excess risk decay rates\nD.3 Overestimation and underestimation of nn-widths\nD.4 Experiments with analytically given NNGP and NTK kernels\nD.5 Experiments with NNGP and NTK kernels of finite width\n\n\n\nE Proofs for Section¬†2: cases of dK=dœ±d_{K}=d_{\\varrho}\n\nE.1 Proof of Theorem¬†2: the Laplace kernel case\nE.2 Proof of Theorem¬†2: the kernel K‚Äã(ùê±,ùê≤)=e‚àíŒ≥‚Äã‚Äñùê±‚àíùê≤‚ÄñaK({\\mathbf{x}},{\\mathbf{y}})=e^{-\\gamma\\|{\\mathbf{x}}-{\\mathbf{y}}\\|^{a}} for a‚àà(0,1)a\\in(0,1)\nE.3 Proof of Theorem¬†2: the Mat√©rn kernel case\n\nE.4 Remarks on Table¬†1\n\nE.4.1 Neural Network Gaussian Process kernels\n\n\n\n\n\n\n\n\n¬†\n\nOn the Intrinsic Dimensions of Data in Kernel Learning\n\n¬†\n\n\n\n\n\nRustem Takhanov\n\n\n\n\n\n\n\n\n\nNazarbayev University\n\n\n\n\n\nAbstract\nThe manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution‚Äôs support is low. In the context of Kernel Ridge Regression (KRR), we investigate two alternative notions of intrinsic dimension. The first, denoted dœ±d_{\\varrho}, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function KK on a domain Œ©\\Omega. The second, denoted dKd_{K}, is the effective dimension, derived from the decay rate of Kolmogorov nn-widths associated with KK on Œ©\\Omega.\nGiven a probability measure Œº\\mu on Œ©\\Omega, we analyze the relationship between these nn-widths and eigenvalues of the integral operator œï‚Ü¶‚à´Œ©K‚Äã(‚ãÖ,x)‚Äãœï‚Äã(x)‚ÄãùëëŒº‚Äã(x)\\phi\\mapsto\\int_{\\Omega}K(\\cdot,x)\\phi(x)\\,d\\mu(x). We show that, for a fixed domain Œ©\\Omega, the Kolmogorov nn-widths characterize the worst-case eigenvalue decay across all probability measures Œº\\mu supported on Œ©\\Omega. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order\nùí™‚Äã(n‚àí2+dK2+2‚ÄãdK+Œµ)\\mathcal{O}(n^{-\\frac{2+d_{K}}{2+2d_{K}}+\\varepsilon})\nfor any Œµ&gt;0\\varepsilon&gt;0, when the training set size nn is large.\nWe also propose an algorithm that estimates upper bounds on the nn-widths using only a finite sample from Œº\\mu. For distributions close to uniform, we prove that Œµ\\varepsilon-accurate upper bounds on all nn-widths can be computed with high probability using at most\nùí™‚Äã(Œµ‚àídœ±‚Äãlog‚Å°1Œµ)\\mathcal{O}\\left(\\varepsilon^{-d_{\\varrho}}\\log\\frac{1}{\\varepsilon}\\right)\nsamples, with fewer required for small nn.\nFinally, we compute the effective dimension dKd_{K} for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension dKd_{K} can be significantly smaller than the Minkowski dimension dœ±d_{\\varrho}, even though dK=dœ±d_{K}=d_{\\varrho} provably holds on regular domains.\n\n\n\n1 Introduction\n\nThe manifold hypothesis posits that, within high-dimensional spaces, probability distributions tend to concentrate near lower-dimensional structures, suggesting that data primarily lies along smooth manifolds whose intrinsic dimension is significantly smaller than that of the ambient space. It is highly desirable for the sample complexity of a machine learning (ML) algorithm to depend on the intrinsic dimension of the data, rather than the ambient dimension of the space in which the data resides. When sample complexity scales with the ambient dimension instead, the algorithm suffers from the well-known ‚Äúcurse of dimensionality‚Äù. Fortunately, many widely used algorithms avoid the curse of dimensionality. Notable examples include kernel regression¬†[Bickel and Li, 2007] and kk-nearest neighbors (kk-NN) regression¬†[Kpotufe, 2011].\n\n\nA general theory of machine learning algorithms that adapt to intrinsic dimension has yet to be developed. A natural move in this direction is to establish generalization bounds for kernel methods that explicitly reflect the intrinsic dimension of the data. Some results of this kind rely explicitly on the assumption that the data lies on a low-dimensional Riemannian manifold and make full use of the manifold‚Äôs geometric structure¬†[Ozakin and Gray, 2009, McRae et¬†al., 2020, Cheng and Xie, 2024].\n\n\nIn this paper, we study excess risk bounds for Kernel Ridge Regression. Unlike the previously cited works, we do not impose strong regularity assumptions on the underlying low-dimensional structures. Instead, we employ concepts from geometric measure theory to characterize the support of the data distribution. This approach enables us to handle less regular supports ‚Äî those that are not only theoretically plausible but also observed in real-world datasets ‚Äî such as sets with fractional Hausdorff dimension (e.g., fractals). The main contribution of this paper is the identification of two alternative notions of intrinsic dimension, both defined solely in terms of the support of the data distribution, independent of the distribution itself. The first notion is the upper Minkowski dimension, defined with respect to an appropriate metric. The second is based on the behavior of Kolmogorov nn-widths of the support, viewed as a subset of the corresponding reproducing kernel Hilbert space (RKHS). The first definition is proportional to the Hausdorff dimension of the support and does not take into account the global structure of the kernel. In contrast, the second notion ‚Äî appearing in our excess risk bounds ‚Äî captures the interaction between the kernel and the support. We argue that this parameter governs the generalization behavior of constrained KRR.\n\n\nThe paper is organized as follows. Section¬†2 introduces the reproducing kernel Hilbert space (RKHS) and the metric structure induced by the kernel KK on the domain Œ©\\Omega, which supports the input distribution Œº\\mu. We then define the Kolmogorov nn-widths of Œ©\\Omega under its embedding into the RKHS, following conventional definitions.\nThis framework allows us to define two notions of intrinsic dimension for Œ©\\Omega: the upper Minkowski dimension, denoted dœ±d_{\\varrho}, and the effective dimension, denoted dKd_{K}. We prove that dK‚â§dœ±d_{K}\\leq d_{\\varrho}, and, through illustrative examples, justify particular attention to the case where equality holds, i.e., dK=dœ±d_{K}=d_{\\varrho}. In Section¬†3, we show that the Kolmogorov nn-width characterizes the worst-case decay rate of the nnth eigenvalue of the associated integral operator on L2‚Äã(Œ©,Œº)L_{2}(\\Omega,\\mu). This result may be seen as a counterpart to the classical Ismagilov‚Äôs theorem in approximation theory and forms the foundation for our main excess risk bound for constrained KRR, presented in Section¬†4. The bound exhibits the asymptotic rate ùí™‚Äã(n‚àí2+dK2+2‚ÄãdK+Œµ)\\mathcal{O}\\big(n^{-\\frac{2+d_{K}}{2+2d_{K}}+\\varepsilon}\\big) for any Œµ&gt;0\\varepsilon&gt;0.\nIn the second, empirically focused part of the paper, we present an algorithm for estimating nn-widths from a sample, which may be either drawn from the distribution Œº\\mu or deterministically constructed. In Section¬†5, we analyze this algorithm and show that, under the so-called CC-uniformness assumption on Œº\\mu, it produces Œµ\\varepsilon-accurate upper bounds on the nn-widths using a sample of size ùí™‚Äã(Œµ‚àídœ±‚Äãlog‚Å°1Œµ)\\mathcal{O}\\big(\\varepsilon^{-d_{\\varrho}}\\log\\frac{1}{\\varepsilon}\\big). In Section¬†6 we report the results of numerical experiments with the calculation of nn-widths and effective dimensions dKd_{K} for various kernels and domains. Some of these experiments give rise to new conjectures concerning the gap between dKd_{K} and dœ±d_{\\varrho} for such kernels as the Laplace kernel, given on fractal sets. Proofs of most statements, additional remarks on the tables, and details of the numerical experiments are provided in the Appendix.\n\n\nRelated work.\nIt is well known that the leading term in the Shannon entropy of a discretized nn-dimensional random vector is proportional to a parameter that can be interpreted as the information dimension of its support¬†[R√©nyi, 1959]. Several other classical notions of a distribution‚Äôs dimension are discussed in standard texts on geometric measure theory, such as¬†[Mattila, 1995].\nThe concept of effective dimension in the context of kernel methods for supervised learning is well established. A distribution-dependent definition based on the eigenvalues of the kernel operator was introduced in¬†[Zhang, 2002] and further developed in¬†[Mendelson, 2003] and¬†[Caponnetto and De¬†Vito, 2007].\nIn this paper, we consider two alternative notions of intrinsic dimension. The first is closely related to the definition used by¬†[Hamm and Steinwart, 2022], who employed it to derive an improved excess risk bound for regularized ERMs over RKHSs.\n\n\nOur second notion of intrinsic dimension is based on the behavior of Kolmogorov nn-widths. While nn-widths have a long history in approximation theory¬†[Pinkus, 1985], their application in machine learning is a relatively recent development¬†[Steinwart, 2017].\nNotably, [Wu and Long, 2022] and¬†[Siegel and Xu, 2024] demonstrated that the asymptotic behavior of nn-widths characterizes the gap in approximation power between two-layer neural networks and random feature models¬†[Rahimi and Recht, 2007].\nIn certain settings ‚Äî such as the unit sphere ùïäd‚àí1\\mathbb{S}^{d-1} ‚Äî the computation of nn-widths reduces to the analysis of kernel eigenvalues. For many commonly used kernels, these eigenvalues have been explicitly computed in¬†[Bach, 2017] and¬†[Bietti and Mairal, 2019].\n\n\nIn the second part of the paper, we develop a numerical algorithm for the approximate computation of nn-widths from a sample drawn over"
  },
  {
    "title": "Automatic Classification of Arabic Literature into Historical Eras",
    "url": "https://arxiv.org/abs/2601.16138v1",
    "source": "arxiv",
    "summary": "The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic ",
    "full_text": null
  },
  {
    "title": "LLM Prompt Evaluation for Educational Applications",
    "url": "https://arxiv.org/abs/2601.16134v1",
    "source": "arxiv",
    "summary": "As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structur",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.16134v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.16134v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 22 Jan 2026]\n    Title:LLM Prompt Evaluation for Educational Applications\n    Authors:Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, Wesley Morris            View a PDF of the paper titled LLM Prompt Evaluation for Educational Applications, by Langdon Holmes and 4 other authors\n    View PDF\n\n\n\n    \n            Abstract:As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2601.16134 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.16134v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.16134\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Langdon Holmes [view email]          [v1]\n        Thu, 22 Jan 2026 17:31:25 UTC (277 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled LLM Prompt Evaluation for Educational Applications, by Langdon Holmes and 4 other authorsView PDF\n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.CL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    "
  },
  {
    "title": "Replicating Human Motivated Reasoning Studies with LLMs",
    "url": "https://arxiv.org/abs/2601.16130v1",
    "source": "arxiv",
    "summary": "Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expect",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Motivated Reasoning\n2.2 Bias in Persona-Based and Non-Persona-Based LLMs\n\n\n\n3 Human Study Selection\n\n3.1 Study 1: The Influence of Partisan Motivated Reasoning on Public Opinion\n3.2 Study 2: Counteracting the Politicization of Science\n3.3 Study 3: How Elite Partisan Polarization Affects Public Opinion Formation\n3.4 Study 4: Partisanship and Preference Formation: Competing Motivations, Elite Polarization, and Issue Importance\n\n\n\n4 Experiments and Evaluation\n\n\n4.1 Experimental Setup\n\n4.1.1 LLM Selection\n4.1.2 Human Study Replication\n\n\n\n4.2 Evaluation Setup\n\n4.2.1 Calculating LLM Averages\n4.2.2 Comparing Human and LLM Response Trends\n4.2.3 Comparing Different Response Trends Across LLMs\n\n\n\n\n\n5 Results and Analysis\n\n5.1 LLM Behavior Deviates from Human Motivated Reasoning\n5.2 LLMs Are Limited in Ability to Assess Relative Argument Strength\n\n\n6 Discussion\n\nA LLM Parameters and Prompts\n\nA.1 Prompts Used Within the Study\nA.2 Justification for Prompt Adaption\nA.3 API-Based Models\nA.4 On-Device Models\nA.5 Dropped Models\n\n\nB Parsing Data\n\nC Additional Details on Study Replication\n\n\nC.1 Details of Each Condition Within Each Study\n\nC.1.1 The Influence of Partisan Motivated Reasoning on Public Opinion (Bolsen, Druckman, and Cook; 2014)\nC.1.2 Counteracting the Politicization of Science (Bolsen and Druckman; 2015)\nC.1.3 How Elite Partisan Polarization Affects Public Opinion Formation (Druckman, Peterson, Slothuus; 2013)\nC.1.4 Partisanship and Preference Formation: Competing Motivations, Elite Polarization, and Issue Importance (Mullinix; 2016)\n\n\nC.2 Number of Samples\n\n\n\nD Additional Results\n\nD.1 Main Paper Supporting Figures and Tables\nD.2 LLM Behavior in Opinion Formation\n\n\n\n\n\n\n\nReplicating Human Motivated Reasoning Studies with LLMs\n\n\nNeeley Pate \nUniversity of Rochester \nnpate@ur.rochester.edu\n\n\n‚ÄÉ‚ÄÉ\nAdiba Mahbub Proma \nUniversity of Rochester \n\n\n\n‚ÄÉ‚ÄÉ\nHangfeng he \nUniversity of Rochester \n\n\n\n‚ÄÉ‚ÄÉ\nJames N. Druckman \nUniversity of Rochester \n\n\n\n‚ÄÉ‚ÄÉ\nDaniel Molden \nNorthwestern University \n\n\n\n‚ÄÉ‚ÄÉ\nGourab Ghoshal \nUniversity of Rochester \n\n\n\n‚ÄÉ‚ÄÉ\nEhsan Hoque \nUniversity of Rochester \n\n\n\n\nAbstract\nMotivated reasoning ‚Äì the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined ‚Äì has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.\n\n\n\nReplicating Human Motivated Reasoning Studies with LLMs\n\n\n\n\nNeeley Pate\n\nUniversity of Rochester\n\nnpate@ur.rochester.edu\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nAdiba Mahbub Proma\n\nUniversity of Rochester\n\n\n\n\n\n\nHangfeng he\n\nUniversity of Rochester\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nJames N. Druckman\n\nUniversity of Rochester\n\n\n\n\n\n\nDaniel Molden\n\nNorthwestern University\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nGourab Ghoshal\n\nUniversity of Rochester\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nEhsan Hoque\n\nUniversity of Rochester\n\n\n\n\n\n\n1 Introduction\n\nMotivated reasoning, a concept that describes how individuals process information Druckman and McGrath (2019), is often portrayed as involving either the pursuit of a directional (predetermined) conclusion or the pursuit of an accurate (non-directional) conclusion Kunda (1990). For example, individuals endorse a policy proposed by their party because they desire holding positions that align with their party leaders (a directional goal). Alternatively, they may instead consider the strength of the arguments for the policy in an effort to form an accurate opinion. A particular motivation can ultimately affect individuals‚Äô opinions and actions Bolsen et¬†al. (2014); Bolsen and Druckman (2015); Druckman et¬†al. (2013); Mullinix (2016) or even facets of reality Kahan et¬†al. (2013); Druckman et¬†al. (2013).\n\n\nFigure 1: Graphic Describing the Procedure Followed in the Study. An example of different types of prompts (accuracy vs. directional) is shown to highlight potential differences in reasoning.\n\n\nMotivations affect an individual‚Äôs opinion formation through how they process information about policies, issues, and stances, particularly within the political domain. The manner in which such information is processed can affect both opinion formation and perceptions of how strong presented arguments are Druckman et¬†al. (2013). When motivated by accuracy, an individual receives information in a relatively unbiased fashion, focusing on the perceived argument quality rather than external factors Druckman and McGrath (2019). When motivated directionally, an individual interprets the information in a way to align with prior beliefs or group expectations.\n\n\nGiven the direction of motivated reasoning is often dependent on an individual‚Äôs identity or experiences, such as partisanship Bolsen et¬†al. (2014), recent work has begun to explore whether LLMs embody motivated reasoning when given particular aspects of demographics or personality. Studies have shown providing LLMs with traits such as political affiliation and gender Dash et¬†al. (2025) or occupation and goal Cho et¬†al. (2025) do mimic motivated reasoning based on their specified role. However, it\nis well-established that even base LLMs (with no provided personality or identity) hold certain biases and preferences, particularly skewing to left-leaning perspectives Motoki et¬†al. (2024); Proma et¬†al. (2025). This raises the question of whether base LLMs inherently mimic motivated reasoning when provided with information. Thus, this paper aims to answer the following research questions:\n\n\n‚Ä¢\n\nRQ1: Do base LLMs (without persona induction) mimic motivated reasoning similar to human responses?\n\n\n\n‚Ä¢\n\nRQ2: Do similarities in behavior exist between groups of LLMs under different motivations and domains?\n\n\n\n\n\nIn order to answer these questions, we completed a comprehensive literature review to select four prior works on political motivated reasoning to replicate with LLMs (Figure 1). We report results from seven models spanning five families. The contributions of this work include:\n\n\n‚Ä¢\n\nBase LLMs do not appear to mimic human-like motivated reasoning.\n\n\n\n‚Ä¢\n\nWhether different LLMs perform similarly in opinion formation depends on given topics.\n\n\n\n‚Ä¢\n\nLLMs may systematically fail to accurately assess information, leading inaccuracies in natural language processing (NLP) tasks such as argument strength assessment.\n\n\n\n\n\nThese results have implications for social scientists and computer scientists who utilize LLMs as survey proxies or predicting public opinion, as well as the artificial intelligence (AI) and NLP communities interested in LLM-human alignment or using LLMs for tasks such as argument assessment and opinion formation. We offer considerations for researchers using base LLMs for predicting how individuals will respond to arguments in motivated reasoning contexts (such as surveys) and anticipating how arguments resonate with humans.\n\n\n\n\n2 Related Works\n\n\n2.1 Motivated Reasoning\n\nMotivated reasoning emphasizes two main avenues of reaching conclusions: pursuing an accuracy (non-directional) goal or pursuing a directional (predetermined) goal Kunda (1990). Reasoning is often induced by altering content around or within information. For example, accuracy motivation is often primed by asking participants to consider information provided in an evenhanded fashion Bolsen et¬†al. (2014); Bolsen and Druckman (2015) or informing participants they will be asked to later justify opinions Bolsen et¬†al. (2014); Bolsen and Druckman (2015). To induce directional motivation, there are two broad categories such inductions can fall into: authority/community and personality. Calls on authority or community often include information about who approves (or disapproves), such as party leaders Bolsen et¬†al. (2014); Druckman et¬†al. (2013) or scientific experts Bolsen and Druckman (2015). For example, including information about polarization around a given issue can make group identity more salient than message content, pushing individuals to align their opinion with the perceived opinion of their party leaders, while not including such information makes these tendencies less prevalent Druckman et¬†al. (2013). Personality techniques call on the individual to reflect on particular cues, such as their prior beliefs and core values Bayes et¬†al. (2020). These cues may bias recall of certain events in a given direction Kunda (1990). Pursuing motivated reasoning can affect opinion formation Bolsen et¬†al. (2014), argument strength assessments Druckman et¬†al. (2013), and fact interpretation Kahan et¬†al. (2013). We expand research on motivated reasoning by exploring whether LLMs mimic motivated reasoning, particularly without persona prompts.\n\n\n\n\n2.2 Bias in Persona-Based and Non-Persona-Based LLMs\n\nCentral to the idea of motivated reasoning is how personal biases shape information processing. Within the context of LLMs, it is well-known that LLMs tend to exhibit a left-leaning bias Bernardelle et¬†al. (2025); Bang et¬†al. (2024); Faulborn et¬†al. (2025). Such bias implicit in LLM responses can present in a variety of ways, such as a bias towards certain media outlets Dai et¬†al. (2025); Proma et¬†al. (2025). LLMs have also been known to take on other human-like preferences, such as message structure Li et¬†al. (2025) and self-preference Panickssery et¬†al. (2024). The inclusion of personality traits (or personas) in LLM prompts can also affect how bias presents in LLM responses Borah and Mihalcea (2024), though LLMs may fail to reconcile conflicts within a provided persona Liu et¬†al. (2024). While LLMs may struggle in identifying human-like motivated reasoning in text Yong e"
  },
  {
    "title": "Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging",
    "url": "https://arxiv.org/abs/2601.16127v1",
    "source": "arxiv",
    "summary": "Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research o",
    "full_text": null
  },
  {
    "title": "Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing",
    "url": "https://arxiv.org/abs/2601.16125v1",
    "source": "arxiv",
    "summary": "Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a",
    "full_text": null
  },
  {
    "title": "Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add",
    "url": "https://arxiv.org/abs/2601.16120v1",
    "source": "arxiv",
    "summary": "Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, ",
    "full_text": null
  },
  {
    "title": "synthocr-gen: A synthetic ocr dataset generator for low-resource languages- breaking the data barrier",
    "url": "https://arxiv.org/abs/2601.16113v1",
    "source": "arxiv",
    "summary": "Optical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset cre",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 The Low-Resource Language Challenge\n1.2 Kashmiri: A Case Study\n1.3 Synthetic Data: A Solution\n1.4 Contributions\n1.5 Paper Structure Overview\n\n\n\n2 Related Work\n\n2.1 Synthetic Text Image Datasets\n2.2 OCR for Persio-Arabic-Script Languages\n2.3 Transformer-Based OCR\n2.4 Low-Resource Language Processing\n2.5 Data Augmentation for OCR\n2.6 Summary and Positioning\n\n\n\n3 Methodology\n\n3.1 Notation Reference\n3.2 Complete System Workflow\n3.3 Problem Formulation\n\n3.4 System Architecture\n\n3.4.1 Pipeline Stage Diagram\n\n\n\n3.5 Text Segmentation (œÉseg\\sigma_{\\text{seg}})\n\n3.5.1 Formal Definition\n\n3.5.2 Segmentation Functions\n\nCharacter Mode.\nWord Mode.\nN-gram Mode.\nSentence Mode.\nLine Mode.\n\n\n3.5.3 Length Filtering\n\n\n\n3.6 Unicode Validation (œàvalid\\psi_{\\text{valid}})\n\n3.6.1 Normalization\n3.6.2 Script Validation\n3.6.3 Diacritic Preservation\n\n\n\n3.7 Image Rendering (œÅrender\\rho_{\\text{render}})\n\n3.7.1 Font Selection\n\n3.7.2 Font Size Sampling\n\nNormal Distribution:\nUniform Distribution:\n\n\n3.7.3 Canvas Rendering Model\n3.7.4 Text Positioning\n\n\n\n3.8 Augmentation Pipeline (œïaug\\phi_{\\text{aug}})\n\n3.8.1 Probabilistic Application\n3.8.2 Transform Composition\n\n3.8.3 Geometric Transforms\n\nRotation.\nSkew.\n\n\n\n3.8.4 Blur Transforms\n\nGaussian Blur.\nMotion Blur.\n\n\n\n3.8.5 Noise Injection\n\nGaussian Noise.\nSalt-and-Pepper Noise.\n\n\n\n3.8.6 Degradation Effects\n\nJPEG Compression.\nResolution Degradation.\n\n\n3.8.7 Augmentation Summary\n\n\n\n3.9 Seeded Randomization\n\n3.9.1 Linear Congruential Generator\n3.9.2 Uniform Random Variate\n3.9.3 Reproducibility Theorem\n\n\n\n3.10 Output Format Adapters (œÄout\\pi_{\\text{out}})\n\n3.10.1 Format Specifications\n\n\n3.11 Complete Algorithm\n\n3.12 Complexity Analysis\n\n3.12.1 Time Complexity\n3.12.2 Space Complexity\n\n\n\n\n\n4 Implementation Details\n\n\n4.1 Technology Architecture\n\n4.1.1 System Stack\n\n\n\n4.2 Unicode Processing Implementation\n\n4.2.1 Grapheme Cluster Segmentation\n4.2.2 Script Range Validation\n4.2.3 Normalization Process\n\n\n\n4.3 Seeded Random Number Generator\n\n4.3.1 LCG Implementation\n4.3.2 Statistical Properties\n\n\n\n4.4 Font Management System\n\n4.4.1 Dynamic Font Loading\n4.4.2 Distribution-Based Selection\n\n\n\n4.5 Canvas Rendering Pipeline\n\n4.5.1 Rendering Flow\n4.5.2 RTL Text Rendering\n4.5.3 Background Composition\n\n\n\n4.6 Augmentation Implementation\n\n4.6.1 Transform Pipeline Architecture\n4.6.2 Pixel-Level Operations\n4.6.3 Geometric Transform Implementation\n\n\n\n4.7 Output Generation\n\n4.7.1 ZIP Archive Creation\n4.7.2 Label Format Generation\n\n\n\n4.8 Performance Optimizations\n\n4.8.1 Canvas Reuse\n4.8.2 Incremental ZIP Building\n4.8.3 Complexity Summary\n\n\n\n\n\n5 Memory Management and Sample Storage\n\n5.0.1 Memory Usage Per Sample\n\n5.0.2 Storage Modes\n\nMode 1: In-Memory ZIP (Default).\nMode 2: Streaming/Chunked Generation.\nMode 3: Individual File Saving (CLI Mode).\n\n\n5.0.3 Memory Optimization Techniques\n5.0.4 Storage Format Comparison\n5.0.5 Recommendations by Dataset Size\n5.0.6 Browser Memory Monitoring\n5.0.7 Sample Integrity Verification\n\n\n\n6 Experiments and Results\n\n6.1 Experimental Setup\n6.2 Dataset Characteristics\n6.3 Sample Visualization\n6.4 Deployment and Reproducibility\n6.5 Applicability to Other Languages\n\n\n\n7 Discussion\n\n7.1 Advantages of Synthetic Data Generation\n7.2 Synthetic vs. Real Data Trade-offs\n7.3 Scalability to Other Languages\n7.4 Limitations\n7.5 Privacy and Ethical Considerations\n7.6 Future Directions\n\n\n\n8 Conclusion\n\n8.1 Summary of Contributions\n8.2 Impact and Significance\n8.3 Future Research Directions\n8.4 Resource Availability\n8.5 Closing Remarks\n\n\n\n\n\n\n\nSYNTHOCR-GEN: A SYNTHETIC OCR DATASET GENERATOR FOR LOW-RESOURCE LANGUAGES- BREAKING THE DATA BARRIER\n\n\n\n*Haq Nawaz Malik\norcid.org/0009-0003-1994-7640\nhuggingface.co/Omarrran\n &amp;Kh Mohmad Shafi\norcid.org/0000-0002-4759-8412\nhuggingface.co/mshafi710\n&amp;Tanveer Ahmad Reshi\norcid.org/0009-0002-4312-361X\nhuggingface.co/TanveerReshi\n\n\n\n\nAbstract\nOptical Character Recognition (OCR) for low-resource languages remains a significant challenge due to the scarcity of large-scale annotated training datasets. Languages such as Kashmiri, with approximately 7 million speakers and a complex Perso-Arabic script featuring unique diacritical marks, currently lack support in major OCR systems including Tesseract, TrOCR, and PaddleOCR. Manual dataset creation for such languages is prohibitively expensive, time-consuming, and error-prone, often requiring word by word transcription of printed or handwritten text.\nWe present SynthOCR-Gen, an open-source synthetic OCR dataset generator specifically designed for low-resource languages. Our tool addresses the fundamental bottleneck in OCR development by transforming digital Unicode text corpora into ready-to-use training datasets. The system implements a comprehensive pipeline encompassing text segmentation (character, word, n-gram, sentence, and line levels), Unicode normalization with script purity enforcement, multi-font rendering with configurable distribution, and 25+ data augmentation techniques simulating real-world document degradations including rotation, blur, noise, and scanner artifacts.\nKey innovations include: (1) a fully client-side browser-based architecture ensuring data privacy, (2) native support for right-to-left scripts with proper handling of Arabic-script combining characters and Kashmiri-specific diacritics, (3) seeded randomization for reproducible dataset generation, and (4) multi-format output compatible with CRNN, TrOCR, PaddleOCR, Tesseract, and HuggingFace ecosystems.\nWe demonstrate the efficacy of our approach by generating a 600,000-sample word-segmented Kashmiri OCR dataset, which we release publicly on HuggingFace. This work provides a practical pathway for bringing low-resource languages into the era of vision-language AI models, and the tool is openly available for researchers and practitioners working with underserved writing systems worldwide.\n\n\nKeywords‚ÄÇOptical Character Recognition  ‚ãÖ\\cdot\nSynthetic Data Generation  ‚ãÖ\\cdot\nLow-Resource Languages  ‚ãÖ\\cdot\nKashmiri Script  ‚ãÖ\\cdot\nDeep Learning  ‚ãÖ\\cdot\nText Recognition\n\n\n\n1 Introduction\n\nThe rapid advancement of deep learning has revolutionized Optical Character Recognition (OCR), enabling near-human performance on text recognition tasks for well-resourced languages such as English, Chinese, and Arabic (Li et al., 2023; Du et al., 2020). However, this progress has not extended equally to the world‚Äôs approximately 7,000 languages, leaving a significant digital divide where speakers of low-resource languages cannot fully participate in the benefits of AI-powered text processing technologies.\n\n\n\n1.1 The Low-Resource Language Challenge\n\nLow-resource languages face a fundamental chicken-and-egg problem in OCR development: training modern deep learning models requires large-scale annotated datasets, yet creating such datasets manually for languages with limited digital presence is prohibitively expensive. Consider the typical workflow for creating an OCR dataset manually:\n\n\n\n\n1.\n\nCollect printed documents or book scans in the target language\n\n\n\n2.\n\nSegment images into character level, word-level or sentence level crops\n\n\n\n3.\n\nManually transcribe each image segment character-by-character/word-by-word or sentence-by-sentence\n\n\n\n4.\n\nVerify transcriptions for accuracy\n\n\n\n5.\n\nFormat data for the target OCR framework architecture model\n\n\n\n\n\nFor a dataset of 100,000 samples, considered modest by modern deep learning standards, this process could require thousands of person-hours and remains highly susceptible to human error, particularly for scripts with complex diacritical systems.\n\n\n\n\n1.2 Kashmiri: A Case Study\n\nWe focus on Kashmiri (ISO 639-3: kas) as an exemplary low-resource language. Kashmiri is an Indo-Aryan language spoken by approximately 7 million people, primarily in the Kashmir Valley of the Indian subcontinent. The language uses a modified Perso-Arabic script with several unique characteristics:\n\n\nFigure 1:  Note: its an hypothetical visual (not real Kashmiri text) image text describing case study of Kashmiri language analysis for better understanding for international research readers.\n\n\n\n\n‚Ä¢\n\nExtended Arabic character set: Additional letters for sounds not present in Arabic or Persian\n\n\n\n‚Ä¢\n\nComplex diacritical system: Extensive use of combining marks (U+0654‚ÄìU+065F) for vowel representation\n\n\n\n‚Ä¢\n\nRight-to-left directionality: Native RTL text flow with potential for embedded LTR numerals\n\n\n\n‚Ä¢\n\nContextual letter forms: Arabic-style initial, medial, final, and isolated letter shapes\n\n\n\n\n\nDespite its significant speaker population and rich literary tradition, Kashmiri currently has zero or negligence support in major OCR systems:\n\n\n\n\n‚Ä¢\n\nTesseract: No trained model available for Kashmiri script\n\n\n\n‚Ä¢\n\nTrOCR: Pre-trained models do not include Kashmiri\n\n\n\n‚Ä¢\n\nPaddleOCR: No Kashmiri language support\n\n\n\n‚Ä¢\n\nGoogle Cloud Vision: Does not recognize Kashmiri text\n\n\n\n‚Ä¢\n\nAzure Computer Vision: Kashmiri not among supported languages\n\n\n\n‚Ä¢\n\nMistral OCR: No support for Kashmiri language\n\n\n\n‚Ä¢\n\nDeepSeek OCR: Does not support Kashmiri script\n\n\n\n\n\nThis complete absence of OCR capability prevents digitization of Kashmiri historical documents, limits accessibility tools for visually impaired Kashmiri speakers, and hinders the language‚Äôs integration into modern AI pipelines.\n\n\n\n\n1.3 Synthetic Data: A Solution\n\nSynthetic data generation offers a compelling alternative to manual dataset curation. The core insight is straightforward: while collecting and transcribing real-world images is labor-intensive, the reverse process, rendering known text into realistic images, can be fully automated. Given a digital text corpus (which increasingly exists for many languages through literary archives, newspapers, and online sources), a synthetic data generator can produce unlimited training samples with perfect ground-truth annotations.\n\n\nPrevious work has demonstrated the effectiveness of synthetic data for OCR. The SynthText dataset (Gupta et al., 2016) and MJSynth (Synth90k"
  },
  {
    "title": "Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation",
    "url": "https://arxiv.org/abs/2601.16112v1",
    "source": "arxiv",
    "summary": "We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our m",
    "full_text": "\n\n\n\nI Introduction\n\nII Proposed Model\n\n\nII-A Data generative model\n\nII-A1 Variable splitting\nII-A2 Assignment of autoregressive model\nII-A3 Generation of data points\n\n\nII-B Prior distributions\n\n\n\nIII Learning Algorithm\n\nIII-A Problem setup\nIII-B Variational inference with local variational approximation\nIII-C Update formula of (13)\nIII-D Update formula of (14)\nIII-E Update formula of (15)\nIII-F Initialization\n\n\n\nIV Numerical Experiments\n\nIV-A Experiment 1: effect of variable splitting\nIV-B Experiment 2: uncertainty quantification of changes\n\n\nV Conclusion\n\n\n\n\n\nVariable Splitting Binary Tree Models Based on Bayesian Context Tree Models \nfor Time Series Segmentation\n\n\n\n\nYuta Nakahara\n\n‚ÄÉ‚ÄÉ\nShota Saito\n\n‚ÄÉ‚ÄÉ\nKohei Horinouchi\n\n‚ÄÉ‚ÄÉ\nKoshi Shimada\n\n\n  \nNaoki Ichijo\n\n‚ÄÉ‚ÄÉ\nManabu Kobayashi\n\n‚ÄÉ‚ÄÉ\nToshiyasu Matsushima\n\n\n\nAbstract\nWe propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.\n\n\n\nI Introduction\n\n\nIn this study, we address the problem of time series segmentation, which is also called offline change point detection. It is the problem of detecting all the past change points of the probabilistic data generative models after observing all the data points. In this paper, the number of model changes is assumed to be unknown. Such problems are important and have been studied across a wide range of fields, e.g., financial analysis, bio-informatics, and climatology (see e.g., [1]).\n\n\nApproaches to time series segmentation can be broadly classified into two categories. In the first approach, probabilistic models are assumed not only on the data generation processes but also on the change process of them. In many cases, such studies assume that changes occur independently at each time point with a certain probability, e.g., hidden Markov models [2], i.p.i.d. sources [3], and changing context tree models [4, 5]. One advantage of this approach is that the uncertainty of changes can be quantified by posterior probabilities.\n\n\nIn the second approach, probabilistic models are not assumed on the change process of the probabilistic data generative models. Change points are just detected using some algorithm. While this approach makes it more difficult to statistically evaluate the uncertainty of changes, it allows the use of more flexible algorithms. For example, algorithms that recursively repeat binary splits under some stopping rule have been studied (see e.g., [1]). The output of such algorithms is represented by a binary tree.\n\n\nTraditionally, no probabilistic model representing the change process corresponding to such recursive binary splitting algorithms had been proposed. However, in recent years, a fixed splitting binary tree (FSBT) model [6] where the tree itself is a random variable and represents the change process has been proposed by applying Bayesian Context Tree (BCT) models. The BCT model is a Bayesian interpretation [7, 8, 9, 10] of the context tree weighting (CTW) algorithm [11] that has been originally studied in the field of data compression in information theory. The BCT models have been studied by Matsushima et al. in the field of data compression[8, 9, 12, 13, 14, 4], their theoretical analysis[15, 16, 17, 18, 19, 20], and applications to various other fields [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 6, 31, 32]. Recently, the BCT models have been revisited by Kontoyiannis et al. and applied to even more fields [33, 10, 34, 5, 35, 36, 37]. For historical background of the BCT models, see [32]. The FSBT model [6] is one of these applications. In the FSBT model, recursive partitioning of the time indices is represented by a tree, and a special prior distribution [21] is assumed for this tree. Time series segmentation is performed by estimating its posterior distribution by using an algorithm similar to CTW.\n\n\nHowever, the FSBT model has a limitation, i.e., the candidates for recursive split points are fixed at the midpoint of each interval. Therefore, an unnecessarily deep tree is often required to represent the segmentation (see also Fig. 2 in Section IV). In this study, we propose a variable splitting binary tree (VSBT) model in which interval partitioning is represented by recursive logistic regression models. In this model, split positions other than the midpoint of each interval can be represented by changing the logistic regression coefficients (see also Fig. 3 in Section IV). This enables more compact tree representation.\n\n\nOn the other hand, it is necessary to simultaneously estimate the logistic regression coefficients (i.e., the split positions) and the tree depth (i.e., the number of splits) to estimate such a model from data. Therefore, we propose an effective estimation algorithm that combines the local variational approximation for logistic regression [38] with the CTW. We also present numerical examples on synthetic data demonstrating the effectiveness of this algorithm.\n\n\nFinally, we clarify the difference between previous studies [4, 5] and this study. Although these previous studies apply the BCT model to time series with change points, the tree represents partitioning of the context space, i.e., the data value domain, and the probabilistic data generative model within each interval is represented using this tree. In contrast, in this study, the tree represents partitioning of the time domain, and the change process of the probabilistic data generative model is represented using this tree.\n\n\n\n\nII Proposed Model\n\n\nHerein, we propose the VSBT model. Section II-A describes a data generative model and Section II-B explains prior distributions. The graphical model in Fig. 1 will be useful for understanding our model.\n\n\nFigure 1: The graphical model of our proposed model. We denote observed variables by shading the corresponding nodes.\n\n\n\nII-A Data generative model\n\n\n\nII-A1 Variable splitting\n\nLet TmaxT_{\\mathrm{max}} be a binary perfect rooted tree with the root node sŒªs_{\\lambda} and the depth DmaxD_{\\mathrm{max}}, where Dmax‚àà‚ÑïD_{\\mathrm{max}}\\in\\mathbb{N} is a known constant. For TmaxT_{\\mathrm{max}}, let ‚Ñêmax\\mathcal{I}_{\\mathrm{max}} (resp. ‚Ñímax\\mathcal{L}_{\\mathrm{max}}) denote the set of all inner nodes (resp. leaf nodes) of TmaxT_{\\mathrm{max}}, and ùíÆmax‚âî‚Ñêmax‚à™‚Ñímax\\mathcal{S}_{\\mathrm{max}}\\coloneqq\\mathcal{I}_{\\mathrm{max}}\\cup\\mathcal{L}_{\\mathrm{max}}. For a node s‚ààùíÆmaxs\\in\\mathcal{S}_{\\mathrm{max}}, ds‚àà{0,1,‚Ä¶,Dmax}d_{s}\\in\\{0,1,\\dots,D_{\\mathrm{max}}\\} denotes the depth of ss. For a node s‚àà‚Ñêmaxs\\in\\mathcal{I}_{\\mathrm{max}}, schs_{\\mathrm{ch}} denotes a child node of ss, and more specifically, s0s_{0} and s1s_{1} denote the left and the right child node of ss, respectively. The notation s‚Ä≤‚™Øss^{\\prime}\\preceq s means that s‚Ä≤s^{\\prime} is an ancestor node of ss or s‚Ä≤s^{\\prime} equals ss. When we use the notation s‚Ä≤‚â∫ss^{\\prime}\\prec s, we exclude the case where s‚Ä≤s^{\\prime} equals ss. Ch‚Äã(s)\\mathrm{Ch}(s) denotes the set of all child nodes of ss.\n\n\nFor TmaxT_{\\mathrm{max}}, let ùíñt=[ut,0,ut,1,‚Ä¶,ut,Dmax‚àí1]‚ä§‚àà{0,1}Dmax\\bm{u}_{t}=[u_{t,0},u_{t,1},\\ldots,u_{t,D_{\\mathrm{max}}-1}]^{\\top}\\in\\{0,1\\}^{D_{\\max}} be a path vector for time index tt, where ut,d=1u_{t,d}=1 indicates taking the right branch and ut,d=0u_{t,d}=0 indicates taking the left branch at depth dd. The probability distribution of ùíñ‚âî{ùíñt}t=1n\\bm{u}\\coloneqq\\{\\bm{u}_{t}\\}_{t=1}^{n} is defined as\n\n\n\np‚Äã(ùíñ|ùú∑)\\displaystyle p(\\bm{u}|\\bm{\\beta})\n\n\n\n\n=‚àèt=1n‚àès‚àà‚Ñêmax{œÉ‚Äã(ùú∑s‚ä§‚Äãùíï~)ut,ds‚Äã(1‚àíœÉ‚Äã(ùú∑s‚ä§‚Äãùíï~))1‚àíut,ds}I‚Äã{s‚™Øùóå‚Äã(ùíñt)},\\displaystyle=\\prod_{t=1}^{n}\\prod_{s\\in\\mathcal{I}_{\\max}}\\big\\{\\sigma(\\bm{\\beta}_{s}^{\\top}\\tilde{\\bm{t}})^{u_{t,d_{s}}}(1-\\sigma(\\bm{\\beta}_{s}^{\\top}\\tilde{\\bm{t}}))^{1-u_{t,d_{s}}}\\big\\}^{I\\{s\\preceq\\mathsf{s}(\\bm{u}_{t})\\}},\n\n(1)\n\n\nwhere\n\n\n‚Ä¢\n\nœÉ‚Äã(‚ãÖ)\\sigma(\\cdot) denotes the logistic sigmoid function.\n\n\n\n‚Ä¢\n\nùíï~‚âî[t,1]‚ä§\\tilde{\\bm{t}}\\coloneqq[t,1]^{\\top}.\n\n\n\n‚Ä¢\n\nI‚Äã{‚ãÖ}I\\{\\cdot\\} is the indicator function.\n\n\n\n‚Ä¢\n\nùóå‚Äã(ùíñt)‚àà‚Ñímax\\mathsf{s}(\\bm{u}_{t})\\in\\mathcal{L}_{\\max} is the leaf node determined by ùíñt\\bm{u}_{t}.\n\n\n\n‚Ä¢\n\nùú∑={ùú∑s}s‚àà‚Ñêmax\\bm{\\beta}=\\{\\bm{\\beta}_{s}\\}_{s\\in\\mathcal{I}_{\\max}} is the logistic regression coefficients.\n\n\n\nWe assume a prior distribution p‚Äã(ùú∑)p(\\bm{\\beta}) (see (4)).\n\n\n\n\nII-A2 Assignment of autoregressive model\n\nWe define ùíØ\\mathcal{T} as the set of all regular111All the nodes have either exactly two children or no children. rooted sub-trees of TmaxT_{\\mathrm{max}} whose root node is sŒªs_{\\lambda}. For T‚ààùíØT\\in\\mathcal{T}, we assume a prior distribution p‚Äã(T)p(T) (see (5)). The set of all inner nodes and leaf nodes of TT are denoted by ‚ÑêT\\mathcal{I}_{T} and ‚ÑíT\\mathcal{L}_{T}, respectively.\n\n\nFor each leaf node s‚àà‚ÑíTs\\in\\mathcal{L}_{T}, we assign one of KK candidate autoregressive (AR) models of order DD.\nLet ùíõs=[zs,1,zs,2,‚Ä¶,zs,K]‚ä§‚àà{0,1}K\\bm{z}_{s}=[z_{s,1},z_{s,2},\\ldots,z_{s,K}]^{\\top}\\in\\{0,1\\}^{K} be a one-hot vector (i.e., one of the elements equals 1 and all remaining elements equal 0) indicating the AR model assigned to the leaf node s‚àà‚ÑíTs\\in\\mathcal{L}_{T}, i.e., if the kk-th AR model is assigned to s‚àà‚ÑíTs\\in\\mathcal{L}_{T}, then zs,k=1z_{s,k}=1 and all the remaining elements of ùíõs\\bm{z}_{s} equal 0. The assignment ùíõ={ùíõs}s‚àà‚ÑíT\\bm{z}=\\{\\bm{z}_{s}\\}_{s\\in\\mathcal{L}_{T}} follows a categorical distribution parameterized by ùùÖ=[œÄ1,œÄ2,"
  },
  {
    "title": "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources",
    "url": "https://arxiv.org/abs/2601.16108v1",
    "source": "arxiv",
    "summary": "Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available ",
    "full_text": null
  },
  {
    "title": "Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets",
    "url": "https://arxiv.org/abs/2601.16107v1",
    "source": "arxiv",
    "summary": "Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep l",
    "full_text": null
  },
  {
    "title": "Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification",
    "url": "https://arxiv.org/abs/2601.16098v1",
    "source": "arxiv",
    "summary": "Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token s",
    "full_text": null
  },
  {
    "title": "Adapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating",
    "url": "https://arxiv.org/abs/2601.16097v1",
    "source": "arxiv",
    "summary": "Large Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support. This work investigates a scalable multilingual Text2Cypher, aiming to ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 LLMs and Non-English Content\n2.2 Adapter Merging and Fusion Methods\n2.3 Multilingual Database Query Generation\n\n\n\n3 Methodology\n\n3.1 Per-Language Adapters\n3.2 Uniform Linear Merging\n3.3 Fusion MLP with Dynamic Gating\n\n\n\n4 Experimental Results\n\n4.1 Experimental Setup\n4.2 Performance Comparison\n4.3 Training Efficiency\n\n\n5 Conclusion\nA Fusion MLP Architecture\nB Fine-tuning parameters\n\n\n\n\n\nAdapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating\n\n\nMakbule Gulcin Ozsoy \nNeo4j / London, UK \nmakbule.ozsoy@neo4j.com\n\n\n\nAbstract\nLarge Language Models enable users to access database using natural language interfaces using tools like Text2SQL, Text2SPARQL, and Text2Cypher, which translate user questions into structured database queries. While these systems improve database accessibility, most research focuses on English with limited multilingual support.\nThis work investigates a scalable multilingual Text2Cypher, aiming to support new languages without re-running full fine-tuning, avoiding manual hyperparameter tuning, and maintaining performance close to joint multilingual fine-tuning. We train language-specific LoRA adapters for English, Spanish, and Turkish and combined them via uniform linear merging or learned fusion MLP with dynamic gating.\nExperimental results show that the fusion MLP recovers around 75% of the accuracy gains from joint multilingual fine-tuning while requiring only a smaller subset of the data, outperforming linear merging across all three languages.\nThis approach enables incremental language expansion to new languages by requiring only one LoRA adapter and a lightweight MLP retraining.\nLearned adapter fusion offers a practical alternative to expensive joint fine-tuning, balancing performance, data efficiency, and scalability for multilingual Text2Cypher task.\n\n\n\nAdapter Fusion for Multilingual Text2Cypher with Linear and Learned Gating\n\n\n\n\nMakbule Gulcin Ozsoy\n\nNeo4j / London, UK\n\nmakbule.ozsoy@neo4j.com\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Incremental language expansion in multilingual Text2Cypher. At T1, only English is supported. Spanish (T2) and Turkish (T3) are added via new LoRA adapters + MLP retraining, without touching existing adapters. All speakers receive identical Cypher output.\n\n\n\nDatabase query languages, such as SQL (for relational databases), SPARQL (for RDF graphs), and Cypher (for graph databases), enable efficient data access. Recent advances in large language models (LLMs) have enabled natural language interfaces,like Text2SQL, Text2SPARQL, and Text2Cypher, that convert user questions into database queries. While these systems enhance database accessibility, most research focuses on English,leaving multilingual capabilities underexplored¬†Jannuzzi et al. (2024); Geng et al. (2024).\n\n\nThis work focuses on scalable multilingual Text2Cypher, incrementally supporting new languages without full retraining (Figure¬†1). The figure shows language expansion over time: initially (T1) only English is supported. Then Spanish (T2) and Turkish (T3) support are included. At each stage, users query the same graph database in supported languages, expecting identical Cypher output:\nMATCH (a:Article)-[:MENTIONS]-&gt;(c:Company \n{name: \"Acme Inc.\"}) RETURN a.title.\n\n\nPrior work¬†Ozsoy and Tai (2025) established a benchmark across these languages, showing base models favor high-resource English over medium- or low-resource Spanish and Turkish. While joint multilingual fine-tuning improves overall performance and narrows performance gap across languages,it requires full retraining when a new language is included, which is computationally expensive.\nIn this work, we address three practical constraints for scalable Text2Cypher: (i) incremental language expansion without full retraining, (ii) automated hyperparameter selection, and (iii) performance approaching joint multilingual fine-tuning.\nFor this purpose, we train language-specific LoRA adapters for each language, then combine them via uniform linear merging or a learned fusion MLP with dynamic gating.\n\n\nOur main contributions are:\n\n\n‚Ä¢\n\nWe explore LoRA adapter merging for multilingual Text2Cypher, comparing uniform linear merging and fusion MLP gating. The evaluation results showed that fusion MLP with dynamic adapter gating outperforms uniform linear merging across all three languages.\n\n\n\n‚Ä¢\n\nFusion MLP recovers 75% of joint fine-tuning gains using a subset of the training data (i.e., 20% in our experiments). It delivers balanced performance across high- and low-resourced languages.\n\n\n\n‚Ä¢\n\nFusion MLP enables incremental scaling, where adding a new language requires only one LoRA adapter and lightweight MLP retraining.\n\n\n\n‚Ä¢\n\nWe provide a practical framework for multilingual Text2Cypher, balancing performance and computational cost.\n\n\n\n\n\nThe paper is organized as follows: Section¬†2 reviews related work. Section¬†3 describes the methodology. Section¬†4 presents evaluation results. Section¬†5 concludes the paper.\n\n\n\n\n2 Related Work\n\nThis section reviews related work on large language models (LLMs) for non-English content, adapter merging and fusion approaches and multilingual approaches for database query generation tasks.\n\n\n\n2.1 LLMs and Non-English Content\n\nRecent advances in LLMs have improved multilingual capabilities¬†Lai et al. (2024).\nMultilingual LLMs learn cross-lingual representations from multi-language training data, transferring word associations and grammatical patterns from high-resource languages like English to lower-resource ones. However, English-dominant corpora cause models to internalize English-specific linguistic patterns and assumptions¬†Nicholas and Bhatia (2023); Zhao et al. (2024a), leading to significantly better performance on high-resource and linguistically similar languages¬†Nicholas and Bhatia (2023). Mishra et al.¬†Mishra et al. (2025) found that larger models show more consistent behavior across languages with similar structure, though performance still varies by domain and resource level.\n\n\nSeveral approaches have been proposed to improve multilingual abilities of LLMs. Some works continue pretraining on multilingual parallel data¬†Yang et al. (2023); Zhu et al. (2023). Others fine-tune models on multilingual instruction data¬†√úst√ºn et al. (2024); Luo et al. (2023); Li et al. (2023); Lai et al. (2023). Another line of work uses cross-lingual prompting at inference time¬†Huang et al. (2023); Etxaniz et al. (2023), or studies the internal mechanisms of these models¬†Zhao et al. (2024a); Kargaran et al. (2024); Zhong et al. (2024); Schut et al. (2025); Bandarkar et al. (2024).\n\n\n\n\n2.2 Adapter Merging and Fusion Methods\n\nAdapter merging and fusion methods have also gained attention for multilingual and multi-task adaptation. Techniques such as Task Arithmetic¬†Ilharco et al. (2022), TIES¬†Yadav et al. (2023) and DARE¬†Yu et al. (2024) combine adapters by computing and scaling their parameter differences, often followed by a post-processing like trimming or rescaling. Furthermore, LoRA Soups¬†Prabhakar et al. (2025) and LoRA-LEGO¬†Zhao et al. (2024b) presented ways for merging adapters using weighted or modular combinations. AdapterFusion¬†Pfeiffer et al. (2021) and UniPELT¬†Mao et al. (2022) use gating mechanisms to dynamically combine multiple adapters. In terms of multilingual support, AdaMergeX¬†Zhao et al. (2025) separates task and language ability into different adapters and merges them using structure-adaptive merging. MLM¬†Lee et al. (2025) trains a task and a language adapter and combines them through parameter-space interpolation followed by a light post-merging step. While these methods have shown promise in multilingual and cross-task transfer, their applicability to structured query generation tasks like Text2Cypher remains unexplored.\n\n\n\n\n2.3 Multilingual Database Query Generation\n\nMost work on multilingual database query generation from natural language has focused on the Text2SQL task¬†Dou et al. (2023); Jos√© and Cozman (2021); Huang et al. (2025); Pham et al. (2025). Several studies translate the English Spider dataset¬†Yu et al. (2018) into other languages, such as Chinese in CSpider¬†Min et al. (2019), Turkish in TURSpider¬†Kanburoglu and Tek (2024), Arabic in Ar-Spider¬†Almohaimeed et al. (2024), and multiple languages in MultiSpider¬†Dou et al. (2023) and MultiSpider 2.0¬†Pham et al. (2025). Other work has created new multilingual datasets, such as StatBot.Swiss¬†Nooralahzadeh et al. (2024) in English and German. Beyond datasets, several studies evaluate LLM performance across languages for Text2SQL, for example in Portuguese¬†Jannuzzi et al. (2024) and in Russian¬†Bakshandaeva et al. (2022). Overall, these works show that performance of LLMs on multilingual Text2SQL is still uneven, with clear performance gaps between languages. These findings motivate the need for methods that reduce this gap in a practical and scalable way.\n\n\nFor the Text2SPARQL task, there are fewer multilingual datasets. Some datasets and methods are designed for multilingual question answering¬†Cui et al. (2022); Srivastava et al. (2024); Perevalov et al. (2024). Recently, the Text2SPARQL Challenge¬†Committee (2025) released a dataset with questions in English and Spanish. Perevalov et al.¬†Perevalov and Both (2024) studied the reverse task, translating SPARQL to natural language, and evaluated English, German, and Russian.\n\n\nFor the Text2Cypher task, Ozsoy et al.¬†Ozsoy and Tai (2025) created a multilingual dataset in English, Spanish, and Turkish, and evaluated both base and fine-tuned models. Their results show that base model performance varies widely across languages, while joint multilingual fine-tuning reduces these disparities. However, full retraining remains computationally expensive for incremental language expansion. We address this gap by exploring parameter-efficient methods that enable new languages without repeated full fine-tuning, while maintaining performance close to joint multilingual tra"
  }
]