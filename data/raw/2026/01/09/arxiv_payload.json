[
  {
    "title": "Optimal Lower Bounds for Online Multicalibration",
    "url": "https://arxiv.org/abs/2601.05245v1",
    "source": "arxiv",
    "summary": "We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.\n  In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Œ©(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2",
    "full_text": "\n\n\n\n\n1 Introduction\n\nOnline calibration\nOnline multicalibration\nWhat are the optimal multicalibration rates?\n1.1 Our Results\n\n1.2 Proof Overviews\n\n1.2.1 Lower Bound for the General Case\n1.2.2 Lower Bound for Prediction Independent Groups\n\n\n\n\n2 Model and Definitions\n\n3 Optimal Lower Bound for the General Case\n\nProof overview.\n\n3.1 The Hard Instance\n\n3.1.1 Defining ùíüT,m\\mathcal{D}_{T,m}\n3.1.2 Defining GG\n\n\n3.2 Probabilistic Tools\n\n3.3 Proof of Theorem¬†1\n\n3.3.1 Big Deviations are Punished by g1g_{1} and g2g_{2}\n3.3.2 Many Œ∑\\eta-Honest Rounds are Punished by g3g_{3}\n\n\n\n3.4 Context-wise tradeoff between big deviations and noise\n\n3.4.1 Lower Bound for the ‚ÄúHonest‚Äù Group\n\n3.4.2 Putting it All Together\n\nCase 1: ùîº‚Äã[BT]‚â•T4\\mathbb{E}[B_{T}]\\geq\\frac{T}{4}.\nCase 2: ùîº‚Äã[BT]&lt;T4\\mathbb{E}[B_{T}]&lt;\\frac{T}{4}.\n\n\n\n\n\n\n\n4 Optimal Lower Bound for Prediction-Independent Groups\n\nProof roadmap.\n4.1 The Hard Distribution: Time-Augmented Contexts.\n\n4.2 The Hard Group Family.\n\n4.2.1 Hadamard and Walsh systems.\n\n4.2.2 The Complete Group Family\n\nConstant groups\nGlobal Walsh groups.\nBlock Hadamard groups.\nThe full group family.\nSimulating signed weights by differences.\n\n\n\n\n\n4.3 Global Walsh Groups Enforce ‚Ñì1\\ell_{1}-Truthfulness\n\n4.3.1 Walsh Expansion of Discrete Threshold Signs on the Grid\n4.3.2 Global Walsh Groups Enforce ‚Ñì1\\ell_{1}-Truthfulness\n\n\n\n4.4 Multicalibration Requires Diverse Predictions\n\n4.4.1 ‚Ñì1\\ell_{1} Time-Quantization and Diverse Predictions\n\n\n4.5 Controlling Bias Using Hadamard Groups\n\n4.6 Controlling the Noise Contribution under Adaptive Bucketing\n\nAdaptive bucketing\n\n4.6.1 Proof Roadmap\n\nStep 1: Reduce to bounding the expected number of returns.\nStep 2: Excursions imply LŒµL_{\\varepsilon} must be large if ‚àëvnv\\sum_{v}\\sqrt{n_{v}} is large.\n\n\n4.6.2 Proof of Theorem¬†3: Step 1\n4.6.3 Proof of Theorem¬†3: Step 2\n4.6.4 Finishing the proof of Theorem¬†3\n4.6.5 Applying Theorem¬†3 to the Noise Contribution\n\n\n\n4.7 Proof of Theorem¬†2\n\nAveraging over block Hadamard functions and blocks.\nBounding the bias penalty.\nFinish: lower bound multicalibration error.\n\n\n\n\n\n5 Discussion\n\nIntermediate group family sizes.\n\n\nA Reducing Constant Sized Families of Binary Prediction Independent Groups to Marginal Calibration\n\nB An Oracle Lower Bound for Better Black-Box Reductions\n\nB.1 Context-blind oracles and proper reductions\n\nB.2 Hard instance and a logarithmic-size group family\n\nDistribution over contexts and labels.\nGroup family.\n\n\nB.3 Main theorem\n\nB.4 Proof of Theorem¬†5\n\nInterpretation\n\n\n\n\n\n\n\n\n\nOptimal Lower Bounds for Online Multicalibration\n\n\nNatalie Collina ‚ÄÉ‚ÄÉJiuyao Lu ‚ÄÉ‚ÄÉGeorgy Noarov ‚ÄÉ‚ÄÉAaron Roth\nUniversity of Pennsylvania\n\n\n\nAbstract\nWe prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.\nIn the general setting where group functions can depend on both context and the learner‚Äôs predictions, we prove an Œ©‚Äã(T2/3)\\Omega(T^{2/3}) lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the O‚Äã(T2/3‚àíŒµ)O(T^{2/3-\\varepsilon}) upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.\nWe then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner‚Äôs predictions. In this case, we establish an Œ©~‚Äã(T2/3)\\widetilde{\\Omega}(T^{2/3}) lower bound for online multicalibration via a Œò‚Äã(T)\\Theta(T)-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.\n\n\n\n\nContents\n\n\n1 Introduction\n\n1.1 Our Results\n\n1.2 Proof Overviews\n\n1.2.1 Lower Bound for the General Case\n1.2.2 Lower Bound for Prediction Independent Groups\n\n\n\n\n2 Model and Definitions\n\n3 Optimal Lower Bound for the General Case\n\n\n3.1 The Hard Instance\n\n3.1.1 Defining ùíüT,m\\mathcal{D}_{T,m}\n3.1.2 Defining GG\n\n\n3.2 Probabilistic Tools\n\n3.3 Proof of Theorem¬†1\n\n3.3.1 Big Deviations are Punished by g1g_{1} and g2g_{2}\n3.3.2 Many Œ∑\\eta-Honest Rounds are Punished by g3g_{3}\n\n\n\n3.4 Context-wise tradeoff between big deviations and noise\n\n3.4.1 Lower Bound for the ‚ÄúHonest‚Äù Group\n3.4.2 Putting it All Together\n\n\n\n\n\n4 Optimal Lower Bound for Prediction-Independent Groups\n\n4.1 The Hard Distribution: Time-Augmented Contexts.\n\n4.2 The Hard Group Family.\n\n4.2.1 Hadamard and Walsh systems.\n4.2.2 The Complete Group Family\n\n\n\n4.3 Global Walsh Groups Enforce ‚Ñì1\\ell_{1}-Truthfulness\n\n4.3.1 Walsh Expansion of Discrete Threshold Signs on the Grid\n4.3.2 Global Walsh Groups Enforce ‚Ñì1\\ell_{1}-Truthfulness\n\n\n\n4.4 Multicalibration Requires Diverse Predictions\n\n4.4.1 ‚Ñì1\\ell_{1} Time-Quantization and Diverse Predictions\n\n\n4.5 Controlling Bias Using Hadamard Groups\n\n4.6 Controlling the Noise Contribution under Adaptive Bucketing\n\n4.6.1 Proof Roadmap\n4.6.2 Proof of Theorem¬†3: Step 1\n4.6.3 Proof of Theorem¬†3: Step 2\n4.6.4 Finishing the proof of Theorem¬†3\n4.6.5 Applying Theorem¬†3 to the Noise Contribution\n\n\n4.7 Proof of Theorem¬†2\n\n\n5 Discussion\nA Reducing Constant Sized Families of Binary Prediction Independent Groups to Marginal Calibration\n\nB An Oracle Lower Bound for Better Black-Box Reductions\n\nB.1 Context-blind oracles and proper reductions\nB.2 Hard instance and a logarithmic-size group family\nB.3 Main theorem\nB.4 Proof of Theorem¬†5\n\n\n\n\n\n\n\n1 Introduction\n\nOnline calibration\n\nA sequence of predictions p1,‚Ä¶,pTp^{1},\\ldots,p^{T} is calibrated to a sequence of outcomes y1,‚Ä¶,yTy^{1},\\ldots,y^{T} if, informally, the average of the predictions equals the average of the outcomes, even conditional on the value of the prediction (Dawid, 1982). To measure the deviation from perfect calibration, one can define the cumulative empirical bias conditional on a prediction v‚àà‚Ñùv\\in\\mathbb{R} as BT‚Äã(v)=‚àët:pt=v(pt‚àíyt)B_{T}(v)=\\sum_{t:p^{t}=v}(p^{t}-y^{t}). The classical mis-calibration measure known as expected calibration error (ECE) sums the magnitude of the empirical bias conditional on each prediction:\n\n\n\nErrT=‚àëv‚àà{p1,‚Ä¶,pT}|BT‚Äã(v)|.\\textrm{Err}_{T}=\\sum_{v\\in\\{p^{1},\\ldots,p^{T}\\}}|B_{T}(v)|.\n\n\n\n\n\nIn a seminal result, Foster and Vohra (1998) showed that there exists a randomized algorithm able to generate predictions that are guaranteed to have expected calibration error scaling as o‚Äã(T)o(T) for arbitrary/adversarially selected sequences y1,‚Ä¶,yTy^{1},\\ldots,y^{T}. The optimal rate at which calibration error can be guaranteed has been a long-standing open question, which has seen recent partial progress. A long-standing upper bound established that it was possible to obtain calibration error scaling as O‚Äã(T2/3)O(T^{2/3}) (Foster and Vohra, 1998; Hart, 2025; Abernethy et al., 2011). For many years no lower bound better than Œ©‚Äã(T0.5)\\Omega(T^{0.5}) was known, until Qiao and Valiant (2021) proved a lower bound of Œ©‚Äã(T0.528)\\Omega(T^{0.528}). The current state of the art, due to Dagan et al. (2025), establishes that the optimal rate for calibration is between Œ©‚Äã(T0.54389)\\Omega(T^{0.54389}) and O‚Äã(T2/3‚àíŒµ)O(T^{2/3-\\varepsilon}) for some (extremely small) constant Œµ&gt;0\\varepsilon&gt;0. Dagan et al. (2025)‚Äôs result was a breakthrough for giving the first upper bound improvement showing that the long-standing T2/3T^{2/3} rate was not optimal for marginal calibration.\n\n\n\nOnline multicalibration\n\nCalibration is on its own a weak guarantee in that it marginalizes over the entire sequence, which substantially limits its applicability in contextual prediction settings. But it is possible to give stronger guarantees, asking for calibration not just marginally, but simultaneously on many different subsequences or weightings of the data that can be defined both by external context and the predictions themselves (Dawid, 1985; Lehrer, 2003; Sandroni et al., 2003).\n\n\nA modern CS formulation of this idea is called multicalibration, introduced by H√©bert-Johnson et al. (2018). Multicalibration reweights the residuals of the predictions by ‚Äúgroup functions‚Äù, which are simply mappings g:X√ó‚Ñù‚Üí[0,1]g:X\\times\\mathbb{R}\\rightarrow[0,1] from any pair (x,v)=(x,v)= (context, learner‚Äôs prediction) to a bounded weight g‚Äã(x,v)g(x,v). When g‚Äã(x,v)g(x,v) is independent of vv, we will refer to such a group as prediction-independent. The group- and prediction-conditional cumulative empirical bias is defined as BT‚Äã(v,g)=‚àët:pt=vg‚Äã(xt,pt)‚Äã(pt‚àíyt)B_{T}(v,g)=\\sum_{t:p^{t}=v}g(x^{t},p^{t})(p^{t}-y^{t}), and the group-conditional calibration error is then given by ErrT‚Äã(g)=‚àëv‚àà{p1,‚Ä¶,pT}|BT‚Äã(v,g)|\\textrm{Err}_{T}(g)=\\sum_{v\\in\\{p^{1},\\ldots,p^{T}\\}}|B_{T}(v,g)|. The multicalibration error with respect to a collection of group functions GG is defined as\n\n\n\nMCerrT‚Äã(G)=maxg‚ààG‚Å°ErrT‚Äã(g).\\textrm{MCerr}_{T}(G)=\\max_{g\\in G}\\textrm{Err}_{T}(g).\n\n\n\n\n\nMulticalibration and related guarantees have found many applications in recent years, from learning in a loss-function agnostic manner (Gopalan et al., 2022a, 2023a) to strengthening complexity theoretic constructions (Casacuberta et al., 2024; Dwork and Tankala, 2025) to low complexity algorithms for distributed information aggregation (Collina et al., 2025, 2026). Moreover, similar techniques to those that have been used to derive algorithms guaranteeing marginal calibration in sequential adversarial settings have also been adapted to multicalibration, including methods based on multi-objective optimization and Blackwell approachability (Gupta et al., 2022; Lee et al., 2022; Noarov et al., 2025; Haghtalab et al., 2023), swap regret minimization (Globus-Harris et al., 2023; Gopalan et al., 2023b; Garg et al., 2024), and defensive forecasting (Perdomo and Recht, 2025).\n\n\n\nWhat are the optimal multicalibration rates?\n\nJust as for marginal calibration, the minimax online multicalibration rate has remained a difficult open challenge. Recently, Noarov et al. (2025) (and later Ghuge et al. (2025)) established that online multicalibration can be obtained at the rate O~‚Äã(T2/3‚Äãlog‚Å°|G|)\\widetilde{O}(T^{2/3}\\sqr"
  },
  {
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "url": "https://arxiv.org/abs/2601.05242v1",
    "source": "arxiv",
    "summary": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work h",
    "full_text": "\n\n\n\n1 Introduction\n2 GRPO‚Äôs propensity for reward signal collapse in multi-reward RL\n\n3 Method\n\n3.1 Group reward-Decoupled normalization Policy Optimization\n3.2 Effective incorporation of priority variation\n\n\n\n4 Experiments\n\n\n4.1 Tool calling\n\n4.1.1 Does removing the standard deviation normalization term in GRPO provide any benefit?\n\n\n\n4.2 Mathematical reasoning\n\n4.2.1 Impact analysis of different reward priority variation configurations\n\n\n4.3 Coding reasoning\n\n\n\n5 Related Work\n\nGRPO Variants\nMulti-Reward Reinforcement Learning\n\n\n6 Conclusion\nA Training stability issue of GDPO without batch-wise advantage normalization\nB ToolRL Training Prompt Format\n\nC Tool Calling Reward Functions\n\nFormat Reward.\nCorrectness Reward.\n\n\nD ToolRL Hyperparameters Setting\nE Math/Coding Reasoning Hyperparameters Setting\nF Training curves of GRPO and GDPO when training DeepSeek-R1-7B and Qwen3-4B-Instruct with ‚Ñõlength\\mathcal{R}_{\\text{length}} and ‚Ñõcorrect\\mathcal{R}_{\\text{correct}} on math reasoning data.\nG Comparison of GRPO/GDPO finetuned DeepSeek-R1-7B models under varying length reward weights {1.0,0.75,0.5,0.25}\\{1.0,0.75,0.5,0.25\\} with and without the conditioned length reward ‚Ñõ~length\\tilde{\\mathcal{R}}_{\\text{length}} on math reasoning tasks\n\n\n\n\n\n\n\\correspondingauthor\nX\n\nGDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization\n\n\nShih-Yang Liu\n\n\n\n\n Xin Dong*\n\n\n\n\n Ximing Lu\n\n\n\n\n Shizhe Diao\n\n\n\n\n Peter Belcak\n\n\n\n\n Mingjie Liu\n\n\n\n\n Min-Hung Chen\n\n\n\n\n Hongxu Yin\n\n\n\n\n Yu-Chiang Frank Wang\n\n\n\n\n Kwang-Ting Cheng\n\n\n\n\n Yejin Choi\n\n\n\n\n Jan Kautz\n\n\n\n\n Pavlo Molchanov \n\nNVIDIA\n\n\n\n\n\n\nAbstract\nAs language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability.\nIn this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.\n\n\nImplementations:\nHF-TRL,\nverl,\nNemo-RL\n\n|\nLinks:\nProject,\nLab\n\n\n\n\n\n\n(a) An overview of our proposed GDPO\n\n\n\n\n(b) Reward trends: GDPO vs. GRPO\n\n\n\nFigure 1: (a): An overview of GDPO, which performs group-wise normalization per reward and then applies batch-wise advantage normalization to preserve a stable numerical range independent of reward count and improve update stability. (b): Median and IQR reward curves over five runs of Qwen2.5-Instruct-1.5B tool-calling RL, demonstrating that GDPO consistently converges to higher correctness and format reward score than GRPO.\n\n\n\n1 Introduction\n\nAs language models continue to advance in capability, expectations for their behavior have grown accordingly. Demand for models to not only provide accurate responses but also exhibit behaviors aligned with a wide range of human preferences across diverse scenarios has continued to increase. These preferences span efficiency [1, 2, 3], safety [4], response coherence and logic [5, 6], gender biases [7] and many other objectives. Meeting such heterogeneous requirements within a single model is a challenging task.\n\n\nReinforcement learning (RL) has emerged as the de facto training pipeline for aligning large language models to fulfill such diverse human preferences. In particular, recent RL-based approaches have begun to incorporate multiple rewards into training, with each reward designed to capture different human preferences and collectively guide models toward human-favored behaviors. Despite this growing interest in multi-reward RL, recent work [1, 3, 5] has largely focused on the reward design itself and often directly relied on applying Group Relative Policy Optimization (GRPO) directly for multi-reward RL optimization, often without examining whether GRPO is well-suited for optimizing combinations of heterogeneous rewards.\n\n\nIn this paper, we revisit the applicability of GRPO in multi-reward settings and show that directly applying GRPO to normalize different combinations of rollout rewards can cause them to collapse into identical advantage values, which effectively limits the precision of the training signal, as illustrated in Fig. 2. This collapse removes important distinctions across reward dimensions and leads to inaccurate policy updates, suboptimal reward convergence, and, in many cases, early training failure.\n\n\nTo overcome these challenges, we propose Group reward-Decoupled Normalization Policy\nOptimization (GDPO) which decouples the group-wise normalization of each individual reward as illustrated in Fig. 1(a), to ensure that distinctions across different reward combinations are better preserved and more accurately reflect the relative differences in model responses. This leads to more precise multi-reward optimization and substantially improved training convergence. After this decoupled group-wise normalization, we apply batch-wise advantage normalization to ensure that the magnitude of advantage does not increase as the number of individual rewards increases.\n\n\nWe compare GDPO and GRPO across three tasks: tool calling, math reasoning, and code reasoning. These tasks cover a wide range of objectives, including tool-calling accuracy and format correctness, mathematical reasoning accuracy and adherence to reasoning-length constraints, and code pass rate and bug ratio. Across all tasks, GDPO converges better.\nFor example, in Fig. 1(b), training Qwen2.5-1.5B-Instruct with GDPO attains both higher correctness and format compliance than GRPO on the tool-calling task. On challenging math tasks, GDPO consistently outperforms GRPO. For instance, training DeepSeek-R1-1.5B and Qwen3-4B-Instruct with GDPO yields up to 6.3% and 2.3% higher accuracy on AIME compared to GRPO, while keeping more responses short simultaneously.\n\n\nTaken together, these results demonstrate the effectiveness and generalizability of GDPO, showing it to be a better alternative to GRPO for multi-reward RL optimization.\n\n\nOur contributions are as follows:\n\n\n‚Ä¢\n\nAnalysis of GRPO reward collapse. We demonstrate that applying GRPO naively for multi-reward RL optimization can collapse distinct rollout reward combinations into identical advantage values, thereby diminishing the resolution of the learning signal.\n\n\n\n‚Ä¢\n\nRemediation of GRPO reward collapse. We propose GDPO, which performs group-wise decoupled normalization of each reward separately to better preserve cross-reward distinctions and enable more accurate multi-reward optimization.\n\n\n\n‚Ä¢\n\nIn addition to GDPO, we provide a systematic overview of how to modify reward functions and adjust reward weights to more faithfully align with preferences of varying priority.\n\n\n\n‚Ä¢\n\nWe carry out extensive experiments on three tasks: tool calling, math reasoning, and code reasoning, and compare the effectiveness of GDPO on optimizing a wide range of rewards corresponding to accuracy, format correctness, length constraints, and code quality.\nIn all settings, GDPO consistently outperforms GRPO, showing improved training convergence and stronger downstream performance that align more closely with a diverse set of preferences.\n\n\n\n\n\n\n\n2 GRPO‚Äôs propensity for reward signal collapse in multi-reward RL\n\nRecent advancements such as Group Relative Policy Optimization (GRPO) [8] and its variants, including DAPO [9] and Reinforce++-Baseline [10], have emerged as widely adopted reinforcement learning algorithms due to their efficiency and simplicity.\nIn contrast to Proximal Policy Optimization (PPO) [11], GRPO eliminates the need for a value model by leveraging group-relative advantage estimation for policy updates.\n\n\nCurrently, GRPO has been primarily employed for optimizing a single-objective reward, typically focusing on accuracy.\nHowever, as model capability continues to grow, recent works have increasingly sought to optimize multiple rewards, such as response length constraint and formatting quality, in addition to accuracy [1, 12, 3], to better align with human preferences. Existing approaches for multi-reward RL generally adopt a straightforward strategy: summing all reward components and applying GRPO directly.\n\n\nFormally, for a given question‚Äìanswer pair (qi,oj)(q_{i},o_{j}), where the behavior policy œÄŒ∏old\\pi_{\\theta_{\\mathrm{old}}} samples a group of GG responses {oj}j=1G\\{o_{j}\\}_{j=1}^{G}, and assuming nn objectives, the aggregated reward for the jj-th response is computed as the sum of each objective‚Äôs reward:\n\n\n\nrsum(i,j)=r1(i,j)+‚ãØ+rn(i,j)r^{(i,j)}_{\\text{sum}}=r_{1}^{(i,j)}+\\cdots+r_{n}^{(i,j)}\n\n(1)\n\n\nThe group-relative advantage for the jj-th response is then obtained by normalizing the group-level aggregated rewards:\n\n\n\nAsum(i,j)=rsum(i,j)‚àímean‚Äã{rsum(i,1),‚Ä¶,rsum(i,G)}std‚Äã{rsum(i,1),‚Ä¶,rsum(i,G)}A^{(i,j)}_{\\text{sum}}=\\frac{r_{\\text{sum}}^{(i,j)}-\\mathrm{mean}\\{r_{\\text{sum}}^{(i,1)},\\ldot"
  },
  {
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "url": "https://arxiv.org/abs/2601.05241v1",
    "source": "arxiv",
    "summary": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and ",
    "full_text": null
  },
  {
    "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
    "url": "https://arxiv.org/abs/2601.05240v1",
    "source": "arxiv",
    "summary": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braid",
    "full_text": "\n\n\n\n\n\nRobust Reasoning as a Symmetry-Protected Topological Phase\n\n\n\nIlmo Sung\nScience and Technology Directorate, Department of Homeland Security, Washington, DC 20032, USA\nilmo.sung@hq.dhs.gov\n\nThe opinions expressed in this article are the author‚Äôs own and do not reflect the view of the Science and Technology Directorate¬†(S&amp;T), the Department of Homeland Security, or the United States government.\n\n\nAbstract\nLarge language models suffer from ‚Äúhallucinations‚Äù‚Äîlogical inconsistencies induced by semantic noise. We propose that current architectures operate in a ‚ÄúMetric Phase,‚Äù where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic ‚Äúmass gap,‚Äù maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on S10S_{10} (3.6√ó1063.6\\times 10^{6} states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating 100√ó100\\times beyond training (L=50‚Üí5000L=50\\to 5000), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.\n\n\nIntroduction\n\nThe derivation of macroscopic reasoning from microscopic statistical correlations remains the fundamental challenge of modern Artificial Intelligence. While Transformer-based architectures¬†[1] excel at pattern matching, they suffer from a fundamental fragility: the inability to distinguish between a probabilistically likely continuation and a logically necessary conclusion¬†[2, 3]. In the context of safety and alignment, this manifests as ‚Äúhallucination‚Äù¬†[4], where models drift smoothly from factual truth to plausible fabrication. From the perspective of statistical physics, this drift suggests that the ‚Äúground state‚Äù of truth is not energetically separated from the excited states of error. The system lacks a spectral gap.\n\n\nCurrent approaches to this problem rely on the Scaling Hypothesis¬†[5] or reinforcement learning from human feedback (RLHF)¬†[6]. However, these methods optimize the energy landscape without altering the underlying symmetry of the state space. We argue that standard neural networks operate in a Metric Phase, analogous to a spin glass¬†[7, 8, 9]. In this phase, information is encoded in the local geometry of a continuous vector space. Because the embedding space possesses continuous global symmetries¬†[10, 11], the selection of a specific logical path constitutes spontaneous symmetry breaking (SSB)¬†[12, 13]. In the language of statistical mechanics, the continuous symmetries of the embedding space imply that the loss landscape possesses flat directions. By analogy to Goldstone‚Äôs theorem, these flat directions allow the system to drift along ‚Äúgapless modes‚Äù with near-zero energy cost. In a large language model (LLM), these gapless modes correspond to hallucinations‚Äîa form of semantic drift where the system generates perturbations that are syntactically valid but semantically untethered.\n\n\nTo resolve this fragility, we propose that reasoning must emerge from a different phase of matter: a topological phase. Intuitively, this distinction is analogous to the difference between measuring distance on a flat plane and tying a knot in a rope. In a standard ‚Äúmetric‚Äù network, errors accumulate like drift in a random walk; without a barrier, the system eventually wanders away from the truth. In a Topological Phase, the logical state is encoded like a knot. One can shake the rope (inject semantic noise) or stretch it (extend the sequence length), but the knot structure‚Äîthe logical information‚Äîremains invariant unless the rope is cut. This topological stability provides a robust substrate for reasoning that geometric precision alone cannot achieve.\n\n\nThis metric fragility imposes a severe penalty on long-horizon reasoning. Because standard architectures encode causal history as geometric distances (via attention or fading memory), the signal-to-noise ratio decays with sequence length. We hypothesize that this leads to a logical context horizon‚Äîa critical length beyond which logical coherence is lost. Overcoming this requires an architecture where the memory state is not a decaying signal, but a conserved topological charge, allowing for indefinite extrapolation beyond the training window.\n\n\nIf reasoning is to be robust against semantic noise (such as adversarial prompts or accumulation of floating-point errors), it cannot be a broken-symmetry phase. This fragility in deductive reasoning stems from a fundamental structural mismatch: logical inference is inherently non-commutative (A‚ãÖB‚â†B‚ãÖAA\\cdot B\\neq B\\cdot A), whereas the information aggregation in standard architectures is commutative (A+B=B+AA+B=B+A). Standard networks‚Äîwhether via attention mechanisms (‚àëŒ±i‚ÄãVi\\sum\\alpha_{i}V_{i}) or residual streams (h+f‚Äã(x)h+f(x))‚Äîrely on additive accumulation (hn‚Äãe‚Äãw=ho‚Äãl‚Äãd+updateh_{new}=h_{old}+\\text{update}), treating tokens as features to be summed rather than operators to be composed. To resolve this, the system must belong to a universality class where local perturbations are forbidden by a global invariant. In condensed matter physics, such robustness is found in the Quantum Hall Effect¬†[14, 15], where the Hall conductivity is quantized and immune to local impurities due to the topology of the wavefunction.\n\n\nIn this Article, we propose that logical inference is a Symmetry-Protected Topological (SPT) phase¬†[16, 17]. We formalize the semantic space not as a flat vector space, but as a principal fiber bundle with a non-Abelian structure group GG. In the experiments reported here, we instantiate G=S‚ÄãO‚Äã(N)G=SO(N) using real orthogonal holonomies to obtain a minimal, numerically stable gauge-constrained model. This yields the Holonomic Network‚Äîan architecture that converts the abstract gauge constraint into a practical, drop-in recurrent layer capable of theoretically indefinite generalization. While quantum mechanical topological phases are typically described by unitary groups over complex fields¬†[18], the core mechanism of our proposal‚Äîthe protection of information via non-Abelian holonomy‚Äîrequires only a non-commutative gauge group. S‚ÄãO‚Äã(N)SO(N) represents the minimal real-valued realization of this symmetry, avoiding the computational overhead of complex-valued neural networks while preserving the distinct topological sectors (discrete invariants) required for the phase transition.\n\n\nTo test this hypothesis, we avoid the confounding complexity of natural language benchmarks, where syntax, semantics, and rote memorization are inextricably entangled. Instead, we seek the minimal universality class of logical inference. We focus on two tasks that isolate the physics of reasoning: the symmetric group S3S_{3} (the mathematical atom of non-Abelian logic) and high-complexity variable binding on S10S_{10} (a proxy for the symbolic state-tracking required for coherent reasoning in LLMs)¬†[19, 20]. We argue that the failure modes observed in these minimal systems represent the lower bound of fragility for large-scale models. If an architecture cannot maintain topological protection in the simplest non-commutative group (S3S_{3}), it cannot be expected to maintain causal consistency in the complex non-Abelian structure of natural language. Thus, just as the 2D Ising model isolates the physics of phase transitions without modeling the complexity of real alloys, our tasks isolate the topology of reasoning without the noise of syntax, a methodological approach shared with recent studies on algorithmic generalization¬†[21].\n\n\nWe identify this topological framework with an effective topological quantum field theory (TQFT). We emphasize that this is an effective description of the macroscopic information dynamics¬†[22], not a claim that the neural network operates on quantum mechanical principles. Just as the statistical mechanics of traffic flow can be described by fluid dynamics without cars being liquid molecules, the logical inference in deep networks is governed by the non-Abelian statistics of the gauge group. This places the architecture in the same universality class as anyonic braiding, where information is stored non-locally in the topological winding of the state rather than in local metric magnitudes.\n\n\nWe support this hypothesis with three distinct contributions. First, we present a theoretical framework showing how the causal, arrow-of-time structure of inference acts as a symmetry-breaking mechanism. This induces a topological term in the effective action, causing the system to flow under the renormalization group (RG)‚Äîthe transition from microscopic metric fluctuations (local noise) to macroscopic logical invariants (global truth)‚Äîto a (2+1)-dimensional Chern-Simons topological quantum field theory¬†[23, 24, 25]. In this phase, tokens behave analogously to anyons (quasiparticles that encode information in the braiding of their paths)¬†[26, 27, 28], and their interactions resemble the braiding of world-lines. Standard attention mechanisms rely on additive vector aggregation (‚àëŒ±i‚ÄãVi\\sum\\alpha_{i}V_{i}); order information is injected via positional encodings rather than represented as an explicit non-commutative composition of operators. While positional encodings‚Äîincluding modern relative schemes such as RoPE¬†[29] and ALiBi¬†[30]‚Äîare introduced to break this permutation symmetry"
  },
  {
    "title": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
    "url": "https://arxiv.org/abs/2601.05232v1",
    "source": "arxiv",
    "summary": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showe",
    "full_text": "\n\n\n\nI Introduction\n\nII Methods: News Data\n\nII-A Strategy\n\nII-B Neural Network Classification of Peace\n\nII-B1 Data Preparation\nII-B2 Model Architectures\nII-B3 Training Procedure\n\n\n\n\n\nIII Results: News Data\n\n\nIII-A Evaluation of Neural Networks for Peace Classification\n\nIII-A1 Model Performance on Test Data\nIII-A2 Cross-Dataset Generalization\nIII-A3 Comparison Between Network Architectures\nIII-A4 Cross-Domain Generalization to YouTube Transcripts\n\n\n\n\n\nIV Methods: YouTube Data\n\nIV-A Strategy\nIV-B Analytical Models for YouTube Transcripts\nIV-C Google GoEmotions\n\nIV-D Large Language Models (LLMs)\n\nInitial Prompting (News Media Framework)\nIterative Refinement (Simplistic-Generalizable Prompt)\n\n\nIV-E Human Feedback\n\n\nV Results: YouTube Data\nVI Discussion\nVII Conclusions\n\n\n\n\n\nMeasuring and Fostering Peace through Machine Learning and Artificial Intelligence ‚Ä†‚Ä†thanks: Funded by grant from the Toyota Research Institute.\n\n\n\nPranav Gilda\n\n‚ÄÉ‚ÄÉ\nPrakhar Dungarwal\n\n‚ÄÉ‚ÄÉ\nAriel Thongkham\n\n‚ÄÉ‚ÄÉ\nElena T. Ajayi\n\n‚ÄÉ‚ÄÉ\nShaifali Choudhary\n\n‚ÄÉ‚ÄÉ\nTeresa Mondria Terol\n\n‚ÄÉ‚ÄÉ\nChristine Lam\n\n‚ÄÉ‚ÄÉ\nJessica Pardim Araujo\n\n‚ÄÉ‚ÄÉ\nMegan McFadyen-Mungall\n\n‚ÄÉ‚ÄÉ\nLarry S. Liebovitch\n\n‚ÄÉ‚ÄÉ\nPeter T. Coleman\n\n‚ÄÉ‚ÄÉ\nHarry West\n\n‚ÄÉ‚ÄÉ\nKate Sieck\n\n‚ÄÉ‚ÄÉ\nScott Carter\n\n\n\nAbstract\nWe used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. \nFor news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset.\nFor social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods.\nTo promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.\n\n\nKeywords: peace; NLP; machine learning; AI; LLMs.\n\n\n\nI Introduction\n\n\nThe content and tone of the language that people use plays an essential role in starting or ending conflicts. ‚ÄúHate Speech‚Äù leads to conflict and violence. Recently, our group and others have been seeking to identify the characteristics of ‚ÄúPeace Speech‚Äù that reinforce peaceful behavior [1]. Understanding the importance of the role of language leads to: 1) new ways to measure the levels of peace in countries, regions, or cities; 2) illuminate the social processes that underlie those language differences; and 3) suggest interventions that can promote peace. We show here how modern methods from machine learning and artificial intelligence (AI) can measure levels of peace and design interventions to increase it.\n\n\nOver the last 25 years our interdisciplinary team and collaborators have used multiple social science methods to study intractable conflicts and sustainable peace [2]. More recently we have used mathematical modeling and data science methods to study and measure peace including:\n\n\n‚Ä¢\n\ntransforming knowledge graphs (also known as causal loop diagrams and concept maps) into sets of ordinary differential equations analyzed as dynamical systems to identify attractors of peace and conflict [3]\n\n\n\n‚Ä¢\n\nsupervised learning using logistic regression, random forest, support vector machines, decision trees, and embeddings to find the words of highest feature importance that classify lower vs. higher peace countries [1] [4] [5]\n\n\n\n‚Ä¢\n\nthose words of highest feature importance were clustered into topics using k-means, principal component analysis, large language models, and feedback from workshops with journalists, anthropologists, social psychologists, and poets [6]\n\n\n\n‚Ä¢\n\nlarge language models with retrieval augmented prompt generation (RAG) were used to measure levels of the social processes, positive and negative intergroup reciprocity, in lower and higher peace countries [7]\n\n\n\n\n\nIn this paper, we now report how we extended that previous work to analyze quantitative levels of peace and social dimensions important in peace from on-line news sources and social media such as text transcripts of YouTube videos, using more sophisticated machine learning and AI methods:\n\n\n‚Ä¢\n\nneural networks using text embeddings\n\n\n\n‚Ä¢\n\nGoogle‚Äôs GoEmotions (with the RoBERTa transformer) [8] to measure the emotional tone at the word level\n\n\n\n‚Ä¢\n\nlarge language models (LLMs) such as ChatGPT-3, GPT-4mini [9] and latest others from Google Gemini to measure meanings at the context level\n\n\n\n\n\nWe also report that we developed a real-time tool to foster peace. ‚ÄúWe become what we behold‚Äù [10]. 71% of people ages 16 to 40 get their news daily from social media, especially Facebook and YouTube [11]. Content creators on those sites create videos to increase emotional activation, especially anger, to increase engagement and clicks. We sought to apply our research findings to see how we can go beyond simple engagement metrics to encourage more respectful, nuanced, and informative communication.\n\n\nWe developed a Chrome extension, MirrorMirror, that provides real-time feedback to YouTube viewers on the peacefulness of the videos they are watching. This was done through Human Centered Design (HCD) using iterative feedback from our team members, students in a design class, and a broader set of people to design the User Experience / User Interface (UX/UI). Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption.\n\n\nWe will make our data/code available upon acceptance on our GitHub repository. \n\n\n\n\n\nII Methods: News Data\n\n\n\nII-A Strategy\n\n\nThere are many surveys that measure levels of peace in a country conducted by international organizations, national governmental agencies, non-profit groups, and for-profit consulting companies [1]. Thus, we can use that data to tag samples of text from different countries with their level of peace to form training and testing sets for supervised learning. We also tested the accuracy of models trained on one dataset analyzing another dataset of different countries.\n\n\n\n\nII-B Neural Network Classification of Peace\n\n\n\nII-B1 Data Preparation\n\nA large-scale training dataset was constructed from the News on the Web (NOW) corpus, comprising approximately 700,000 news articles from 18 countries. Each article included metadata specifying its country of origin and the associated peace level. High- and low-peace labels were assigned following the method described by Liebovitch et al. [1], which maps linguistic features of national media to established peace indices.\n\n\nTo prepare the data, n-gram preprocessing was applied to capture multi-word contextual patterns and reduce sparsity. The processed text was then embedded using the OpenAI text-embedding-3-small model, which converts each article into a 1,536-dimensional vector representation encoding its semantic meaning. Of the 18 countries represented, 10 were identified as high-peace, resulting in a balanced distribution for unbiased model training and evaluation.\n\n\n\n\n\nII-B2 Model Architectures\n\nThree neural architectures were developed to perform binary classification of articles as high-peace or low-peace:\n\n\n\n\n‚Ä¢\n\nCNN Model: Two convolutional layers (64 and 32 filters; kernel size = 3, ReLU activation) were followed by a flattening layer and two dense layers (128 and 64 units) with a 0.3 dropout rate for regularization and a sigmoid output. This configuration was designed to extract localized semantic patterns within the embeddings, capturing phrases or n-grams indicative of peaceful discourse.\n\n\n\n‚Ä¢\n\nFeed-Forward Model: A fully connected network of four dense layers (512, 256, 128, and 64 units), each followed by 0.3 dropout, concluded with a sigmoid output layer. This deeper architecture enabled the model to learn global semantic relationships across articles, representing broader conceptual differences in peace-related language.\n\n\n\n‚Ä¢\n\nRevised CNN Model: The revised design incorporated a max-pooling layer (pool size = 2) between convolutional layers, enabling dimensionality reduction and improved generalization. It retained the two fully connected layers (128 and 64 units) from the original CNN, followed by a sigmoid classifier.\n\n\n\n\n\nThese architectures were selected to compare the performance of spatially sensitive versus fully connected learning strategies on high-dimensional text embeddings.\n\n\n\n\n\nII-B3 Training Procedure\n\nThe dataset was divided into 80% training and 20% testing subsets using a fixed random seed to ensure reproducibility. For the convolutional models, each input sample was represented as a sequence of 1,536 elements with a single feature channel, while the feed-forward network received the same 1,536-dimensional vectors in flattened form. All models were trained for 10 epochs with a batch size of 32, using the Adam optimizer (learning rate = 0.001) and binary cross-entropy loss. Accuracy served as the primary evaluation metric.\n\n\nTraining histories were recorded to monitor loss and accuracy over epochs, assessing both convergence and generalization on the test set. Dropout regulari"
  },
  {
    "title": "Learning Latent Action World Models In The Wild",
    "url": "https://arxiv.org/abs/2601.05230v1",
    "source": "arxiv",
    "summary": "Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning lat",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.05230v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.05230v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 8 Jan 2026]\n    Title:Learning Latent Action World Models In The Wild\n    Authors:Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat            View a PDF of the paper titled Learning Latent Action World Models In The Wild, by Quentin Garrido and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.\n    \n\n    \n    \n              \n          Comments:\n          37 pages, 25 figures\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)\n        \n          Cite as:\n          arXiv:2601.05230 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05230v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05230\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Quentin Garrido [view email]          [v1]\n        Thu, 8 Jan 2026 18:55:39 UTC (38,656 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Learning Latent Action World Models In The Wild, by Quentin Garrido and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.CV\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced a"
  },
  {
    "title": "Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data",
    "url": "https://arxiv.org/abs/2601.05227v1",
    "source": "arxiv",
    "summary": "I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It√¥ SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-tim",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Literature Review\n\n2.1 Probabilistic Machine Learning\n2.2 Stochastic Differential Equations and Machine Learning\n2.3 Latent Variable Models with Temporal Stochasticity\n2.4 Contribution\n\n\n\n3 Methodology\n\n3.1 Latent Stochastic Dynamics\n3.2 Encoder and Variational Approximation\n3.3 Decoder and Emission Likelihood\n3.4 Objective Function\n3.5 Adjoint-based Backpropagation\n3.6 Model Architecture\n\n3.7 Theoretical Guarantees: Pathwise Variational Equivalence\n\nTheorem: Pathwise Variational Equivalence:\nProof:\n ‚Ä£ 3.7 Theoretical Guarantees: Pathwise Variational Equivalence\n\n\n\n\n4 Discussion\n5 Summary and Conclusion\n\n\n\n\n\n\nStochastic Deep Learning:\nA Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data\n\n\nJames K Rice\nPhD Candidate, University of Essex; james.k.rice@essex.ac.uk\n\n(January 8, 2026)\n\nAbstract\nI propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an It√¥ SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.\nA central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.\n\n\nKeywords: Stochastic Differential Equations; Variational Inference; Deep Generative Models; Continuous-Time Models; Adjoint Sensitivity; Uncertainty Quantification.\n\n\n\n1 Introduction\n\nUncertainty is an intrinsic characteristic of real-world systems, from financial markets and climate dynamics to neural activity and disease progression. Traditional machine learning approaches, though highly flexible and performant, often provide point estimates with little or no quantification of uncertainty. This limitation can lead to overconfidence in predictions and reduced reliability in critical applications. To address this, probabilistic machine learning methods have emerged as a compelling alternative, aiming to integrate principled statistical reasoning with the representational power of deep learning.\n\n\nRecent advances have seen the synthesis of probabilistic and deep learning paradigms, including Bayesian neural networks ([9]), Gaussian processes ([36]), and variational inference frameworks ([23]). However, a persistent challenge remains in modeling data that is both temporal and structured ‚Äî where uncertainty evolves over time and across latent spatial or relational manifolds. Standard methods may either ignore the dynamics of noise (by assuming static distributions) or struggle with scalability when integrated into deep architectures.\n\n\nIn this work, I propose a novel probabilistic framework that explicitly incorporates time-evolving uncertainty into deep generative modeling. My approach is grounded in the mathematics of stochastic differential equations (SDEs), particularly those of the It√¥ type, which naturally model continuous-time processes under randomness. By embedding an SDE within a variational autoencoder (VAE) structure, I enable learning of flexible, interpretable, and temporally coherent latent dynamics. This ‚ÄúStochastic Deep Learning‚Äù framework leverages both stochastic calculus and deep learning, providing a bridge between classical stochastic modeling and modern data-driven inference.\n\n\nI evaluate the method on several real-world and synthetic datasets, demonstrating superior performance in both predictive accuracy and uncertainty calibration compared to deterministic models and standard variational approaches. Moreover, I show that the learned latent SDEs are interpretable and offer insights into the temporal evolution of uncertainty.\n\n\n\n\n2 Background and Literature Review\n\n\n2.1 Probabilistic Machine Learning\n\nProbabilistic approaches to machine learning aim to capture epistemic and aleatoric uncertainty in predictive modeling. Unlike deterministic neural networks, probabilistic models assign a distribution over parameters or latent variables, enabling a richer understanding of model confidence.\n\n\nA foundational model in this space is the Bayesian neural network (BNN), in which posterior distributions over weights are learned, typically using approximate inference techniques such as variational inference or Markov Chain Monte Carlo ([31]). While theoretically appealing, BNNs often suffer from computational overhead and convergence instability, motivating the use of alternatives like dropout-based approximations ([14]) or deep ensembles ([25]).\n\n\nVariational autoencoders (VAEs) represent another major branch of probabilistic deep learning. VAEs use an encoder-decoder architecture to learn a low-dimensional latent space, where uncertainty is captured via a variational posterior ([23]). However, standard VAEs often model time using discrete latent states or ignore temporal coherence, which limits their utility in dynamic environments.\n\n\n\n\n2.2 Stochastic Differential Equations and Machine Learning\n\nSDEs extend ordinary differential equations (ODEs) by adding stochastic noise components, typically modeled as Wiener processes (Brownian motion). A standard form of an It√¥ SDE is:\n\n\n\n\n\nd‚ÄãXt=Œº‚Äã(Xt,t)‚Äãd‚Äãt+œÉ‚Äã(Xt,t)‚Äãd‚ÄãWt,dX_{t}=\\mu(X_{t},t)\\,dt+\\sigma(X_{t},t)\\,dW_{t},\n\n\n\nwhere Xt‚àà‚ÑùdX_{t}\\in\\mathbb{R}^{d} is the state at time tt, Œº:‚Ñùd√ó‚Ñù‚Üí‚Ñùd\\mu:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}^{d} is the drift function describing the deterministic trend, œÉ:‚Ñùd√ó‚Ñù‚Üí‚Ñùd√óm\\sigma:\\mathbb{R}^{d}\\times\\mathbb{R}\\to\\mathbb{R}^{d\\times m} is the diffusion coefficient, and Wt‚àà‚ÑùmW_{t}\\in\\mathbb{R}^{m} denotes a standard Brownian motion. These equations are fundamental to modeling dynamical systems where noise plays a critical role, with classic applications ranging from the Black-Scholes model for option pricing ([7]) to the modeling of infectious disease spread using stochastic epidemic models ([2]).\n\n\nThe mathematical theory of SDEs builds upon the It√¥ integral, which is well-defined for stochastic processes and forms the basis for It√¥‚Äôs lemma‚Äîa cornerstone of stochastic calculus. Unlike ODEs, the solution XtX_{t} of an SDE is not a deterministic function but rather a stochastic process with its own probability distribution over time. For instance, if œÉ\\sigma is constant and Œº=0\\mu=0, the solution reduces to standard Brownian motion, which satisfies the scaling property Wc‚Äãt‚àºc‚ÄãWtW_{ct}\\sim\\sqrt{c}W_{t} for any c&gt;0c&gt;0. This implies the trajectories are nowhere differentiable with probability one, a feature fundamentally different from the smooth solutions of ODEs.\n\n\nThe numerical approximation of SDEs requires careful treatment to ensure convergence in distribution. The Euler‚ÄìMaruyama method is the most common numerical scheme and serves as a stochastic analog to the classical Euler method for ODEs. It updates the state as follows:\n\n\n\n\n\nXt+Œî‚Äãt=Xt+Œº‚Äã(Xt,t)‚ÄãŒî‚Äãt+œÉ‚Äã(Xt,t)‚ÄãŒî‚Äãt‚ãÖœµt,œµt‚àºùí©‚Äã(0,I).X_{t+\\Delta t}=X_{t}+\\mu(X_{t},t)\\Delta t+\\sigma(X_{t},t)\\sqrt{\\Delta t}\\cdot\\epsilon_{t},\\quad\\epsilon_{t}\\sim\\mathcal{N}(0,I).\n\n\n\n\n\nWhile this method is simple and widely used, its convergence rate is only strong order ùí™‚Äã(Œî‚Äãt)\\mathcal{O}(\\sqrt{\\Delta t}), and more sophisticated methods like Milstein‚Äôs scheme ([24]) may be required for higher accuracy, especially when simulating financial models or systems with stiff dynamics.\n\n\nIncorporating SDEs into machine learning has gained traction with the advent of Neural SDEs ([22]), where the drift and diffusion terms are parameterized by neural networks. This formulation allows for modeling flexible, nonparametric continuous-time dynamics, making it suitable for tasks such as irregular time-series modeling and generative modeling over paths. These approaches generalize neural ordinary differential equations (Neural ODEs) by allowing randomness in the system dynamics, thereby enabling the quantification of uncertainty in prediction trajectories.\n\n\nHowever, standard Neural SDEs primarily focus on mapping initial conditions to distributions over future states, akin to black-box stochastic simulators. While effective in predictive tasks, they often lack the interpretability and structure required for principled inference in latent variable models. This limitation motivates the incorporation of SDEs into the variational autoencoder (VAE) framework, where latent variables evolve via stochastic dynamics rather than deterministic transitions. The resulting models, such as Latent SDEs ([26]), enable a richer representation of uncertainty in learned temporal structures.\n\n\nMoreover, by embedding the SDE within a variational framework, one can jointly infer the posterior over latent trajectories and optimize model parameters via stochastic gradient descent. The reparameterization trick, extended to SDEs, plays a key role in allowing low-variance gradient estimates of the evidence lower bound (ELBO). This yields a flexible and scalable approach to learning latent dynamics under uncertainty, combining the interpretability of probabilistic modeling with the adaptability of deep learning architectures.\n\n\n\n\n2.3 Latent Variable Models w"
  },
  {
    "title": "CAOS: Conformal Aggregation of One-Shot Predictors",
    "url": "https://arxiv.org/abs/2601.05219v1",
    "source": "arxiv",
    "summary": "One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggreg",
    "full_text": null
  },
  {
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "url": "https://arxiv.org/abs/2601.05215v1",
    "source": "arxiv",
    "summary": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machi",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Benchmark Setting and MineNPC-Task\n4 Evaluation Framework\n\n5 Scenario Walkthroughs of ¬†Thor\n\n5.1 S1: Bookmark a landmark, then fetch a tool\n5.2 S2: Collect 20 oak logs\n\n\n\n6 Evaluating the Framework with GPT-4o and Expert Players\n\nSetup.\nDenominator.\n6.1 Aggregate Engagement\n6.2 Perception and Interaction Quality\n6.3 Breakdown Patterns and Recoverability\n6.4 Transparency and Language Alignment\n6.5 Expectations of Companionship and Shared Perception\n6.6 Design Implications\n\n\n7 Conclusion\n8 Limitations\n9 Future Work\n\nA User Interface and Runtime Architecture\n\n\nA.1 Player-Facing Interface Design\n\nDesign Philosophy.\n\n\nA.2 Core Runtime Data Structures\nA.3 Pydantic Schema Definitions\n\n\n\nB Task Planning and Replanning Prompts\n\n\nB.1 Initial Task Planning\n\nB.1.1 System Prompt: Initial Task Planner\n\n\n\n\n\nC Clarification and Feedback Management\n\nC.1 Subtask Clarification System\n\n\n\nD Validation, Code Generation, and System Updates\n\nD.1 Subtask Validation System\nD.2 Player Landmark Tracking System\nD.3 Code Generation and Review Examples\n\n\n\nE MineNPC-Task and Subtask Breakdown\n\n\nE.1 High-Level Task Categories in MineNPC-Task\n\nE.1.1 Resource Collection and Mining\nE.1.2 Tool and Equipment Management\nE.1.3 Agriculture and Food\nE.1.4 Construction and Building\nE.1.5 Crafting and Processing\nE.1.6 Storage and Inventory\n\n\nE.2 Full Subtask Decomposition for MineNPC-Task\n\nE.3 Complexity Analysis for MineNPC-Task\n\nSuite Statistics.\nDomain Distribution (within the suite).\nSystem Requirements Demonstrated by MineNPC-Task.\n\n\n\n\nF User Personalization and Context Management\n\n\n\n\n\nMineNPC-Task: Task Suite for Memory-Aware Minecraft Agents\n\n\nTamil Sudaravan Mohan Doss\n\nMicrosoft\n\ntsudaravanm@microsoft.com\n\n, \nMichael Xu\n\nMicrosoft ResearchUnited States\n\nmichaelxu@microsoft.com\n\n, \nSudha Rao\n\nMicrosoft ResearchUnited States\n\nSudha.Rao@microsoft.com\n\n, \nAndrew D. Wilson\n\nMicrosoft ResearchUnited States\n\nawilson@microsoft.com\n\n and \nBalasaravanan Thoravi Kumaravel\n\nMicrosoft ResearchUnited States\n\nbala.kumaravel@microsoft.com\n\n\n\nAbstract.\nWe present MineNPC-Task, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world Minecraft. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.\nAs an initial snapshot, we instantiate the framework with GPT-4o and evaluate 216 subtasks across 8 experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.\n\n‚Ä†‚Ä†copyright: none‚Ä†‚Ä†conference: ; ; \n\nFigure 1. \n(a) MineNPC-Task: Each row depicts a naturalistic, player-authored request in Minecraft, grounding evaluation in distinct phases of co-play building, farming, and mining.\n(b) Bounded, reproducible evaluation: a model-agnostic harness that routes intents, plans with single-turn clarification, executes via public Mineflayer APIs under a bounded-knowledge policy, and judges only from in-world evidence with lightweight memory.\n(c) Questions this benchmark enables: Where do AI NPCs fail on real player tasks? Which failure modes does GPT-4o exhibit under this bounded setup? Which design choices make evaluations fair?\n\nLeft: A vertical stack of Minecraft scenes, each showing one natural player request‚Äîbuild house, harvest wheat, mine coal. Center: block diagram of the evaluation framework with intent, plan, execute, and memory modules. Right: a callout listing three benchmark questions.\n\n\n\n\n1. Introduction\n\nBuilding truly capable AI companions for open-world games requires more than one-shot instruction following, it requires agents that can plan, clarify, and remember in order to operate effectively in dynamic, long-horizon environments. However, existing evaluation practices fall short: many rely on overly prescriptive prompts or grant agents privileged access to hidden environment state, artificially inflating their apparent competence and undermining fair comparison across models. This highlights the need for a transparent, reproducible benchmark that captures the pressures of mixed-initiative interaction¬†(Lai et al., 2022), without granting unfair advantages. Recent efforts, such as the Minecraft Universe (MCU) benchmark¬†(Chen et al., 2023a, b), move in this direction by introducing scalable, composable tasks in open-world environments and emphasizing repeatable, human-aligned evaluation.\n\n\nWe introduce MineNPC-Task, a practical benchmark for evaluating memory-aware, mixed-initiative LLM agents in Minecraft. Rather than synthetic prompts, tasks are elicited from expert co-play: during our formative and evaluation sessions, experienced players issued real requests, which we normalized into compact templates with explicit preconditions and a small set of slot parameters, and paired with simple machine-checkable validators. The benchmark runs inside a Mineflayer envelope so perception and action are limited to public, in-game APIs; a bounded-knowledge policy forbids admin commands, global map introspection, and scans beyond loaded chunks. The aim is straightforward: a clean, reusable setup that others can run, swap in different models, and obtain comparable numbers, while avoiding hidden shortcuts that have complicated evaluation in prior Minecraft-based agents (Johnson et al., 2016; Fan et al., 2022; Wang et al., 2023).\n\n\nTo execute tasks reproducibly we use a model-agnostic evaluation framework (Section¬†4). The agent presents a brief plan preview that breaks the request into a handful of subtasks, asks a targeted clarifying question only when a slot is unbound (for example, a tool variant or a search radius), acts through Mineflayer skills, and is judged solely on in-world evidence drawn from inventory and equipment deltas, position changes, nearby entities and blocks within loaded chunks, and recent chat. This mixed-initiative pattern aligns with recent agent designs that combine planning with lightweight memory and reflection while keeping behavior legible to human partners (Park et al., 2023; Sarch et al., 2024; Hou et al., 2024). Short scenario walkthroughs (Section¬†5) illustrate how planning, clarification, and reuse of prior context work together.\n\n\nAs an initial snapshot, we instantiate the framework with GPT-4o and run live co-play with 8 experienced players (Section¬†6). Across 44 user-authored tasks and 216 subtasks, we observe 71 subtask failures (approximately 33%) and report where things went wrong, including code execution, inventory handling, referencing, and navigation, alongside common recoveries such as clarifying a slot, simplifying a goal, or constraining location. Participants rated interaction quality and interface usability positively, with mixed views on personalization and completion, consistent with the need for stronger memory scaffolding. We intentionally omit ablations or model comparisons. The intent is to provide a compact, user-authored benchmark that others can extend. For context, our focus complements broader surveys on AI in games and adaptive interaction (Yannakakis and Togelius, 2018; Riedl and Bulitko, 2021) and ongoing efforts to evaluate reasoning in live games (Hu et al., 2024).\n\n\nThe following are the contribution of this work:\n\n\n‚Ä¢\n\nMineNPC-Task suite: a user-authored benchmark for Minecraft, normalized into templates with explicit preconditions and paired with simple, machine-checkable validators under a bounded-knowledge policy. See Appendix¬†E and Section¬†3. Compared with prior Minecraft agents and datasets, we emphasize human-elicited goals, public-API constraints, and validator-backed judging (Johnson et al., 2016; Fan et al., 2022; Wang et al., 2023).\n\n\n\n‚Ä¢\n\nEvaluation framework: a model-agnostic procedure that enforces plan previews and single-turn clarifications when required, constrains perception and action to Mineflayer APIs, and produces validator-backed outcomes suitable for controlled comparisons across LLMs. See Section¬†4. The design follows calls for transparent, reproducible evaluation in interactive systems (Jennings and others, 2024; Hu et al., 2024).\n\n\n\n‚Ä¢\n\nEmpirical snapshot with GPT-4o: co-play results from 8 experts over 44 tasks, comprising 216 subtasks with 71 failures (approximately 33%), along with observations about where mixed-initiative interaction and lightweight memory help‚Äîand where brittleness remains. See Section¬†6.\n\n\n\n\n\nFigure 2. Plan-Clarify-Act-Judge: our model-agnostic evaluation framework.\n(a) Intent routing parses chat into {intent, slots, confidence}.\n(b) Planning and clarification compiles a short plan (3‚Äì5 steps); if a required slot is missing, the agent issues a single, contextual question.\n(c) Code generation and review synthesizes a small JavaScript snippet against Mineflayer APIs and a skill library; a lightweight reviewer caps retries (K‚â§3K{\\leq}3).\n(d) Execution dispatches approved code and streams concise progress updates.\n(e) Evaluation and bounded repair reads recent chat and state deltas to emit TaskFeedback; on success the harness advances to the next subtask, and on failure it offers a bounded r"
  },
  {
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "url": "https://arxiv.org/abs/2601.05214v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production ",
    "full_text": "\n\n\n\nIntroduction\n\nRelated Work\n\nHallucination Detection in Large Language Models\nTool-Augmented Language Models\nInternal Representation Analysis\nAgent Systems and Reliability\nEvaluation Methodologies for tool calling\n\n\nProblem Formulation\n\nMethod\n\nSetup and Notation\nData Collection and Label Generation\nFinal-Layer Feature Extraction\nClassifier Architecture and Training Objective\nInference Protocol\n\n\n\nExperiments\n\n\nExperimental Setup\n\nTarget Models\nBaseline Methods\nDataset Construction\nTraining Configuration\n\n\nBaseline Comparison\nMain Results\n\nAblation Study on Feature Extraction Methods\n\nFeature Extraction Methods\n\n\n\n\nConclusion and Limitations\n\n\n\n\n\nInternal Representations as Indicators of Hallucinations in Agent Tool Selection\n\n\n\nKait Healy\\equalcontrib,\nBharathi Srinivasan \\equalcontrib,\nVisakh Madathil\\equalcontrib,\nJing Wu \\equalcontrib\n\n\n\nAbstract\nLarge Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit ‚Äôtool bypass‚Äô behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs‚Äô internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.\n\n\nIntroduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities as autonomous agents, enabling them to interact with external APIs, execute code, and orchestrate complex workflows¬†(Brown et al. 2020; Schick et al. 2023; Qin et al. 2024). However, these models exhibit a critical vulnerability: tool-calling hallucinations, where they generate structurally plausible but functionally incorrect tool calls. Unlike textual hallucinations, tool-calling hallucinations manifest as inappropriate tool selection, malformed parameters, incorrect tool chaining, tool bypass behavior, or semantically incorrect function invocations that can lead to system failures or data corruption.\n\n\nThe detection of tool-calling hallucinations presents unique challenges. Tool calls possess rigid structural and semantic constraints‚Äîparameters must conform to specific types and must be accurately interpreted from the query to the agent, required arguments cannot be omitted, and function names must exist in the available repertoire. Moreover, the consequences of undetected hallucinations can be severe in computational domains where precision is critical. Incorrect tool usage can also exacerbate security vulnerabilities of agents due to incorrect data access\n\n\nTo address these challenges, we propose a novel approach that leverages the internal representations of LLMs during tool call generation to detect hallucinations in real-time. Our method builds upon recent advances in understanding LLM internal states¬†(Su et al. 2024; Azaria and Mitchell 2023) and extends them to structured tool calling. We introduce an unsupervised training framework that automatically generates labeled data by masking ground-truth tool calls, prompting LLMs to predict appropriate functions, and training lightweight classifiers on the resulting contextualized embeddings.\n\n\nOur key contributions are:\n\n\n‚Ä¢\n\nWe demonstrate that internal representations of LLMs contain discriminative information for detecting tool-calling hallucinations in reasoning tasks.\n\n\n\n‚Ä¢\n\nWe evaluate our approach on tool calling scenarios, showing effective real-time detection capabilities with minimal computational overhead.\n\n\n\n\n\n\nRelated Work\n\nHallucination Detection in Large Language Models\n\nHallucination detection has emerged as a critical challenge in deploying LLMs for real-world applications. Early approaches focused on post-processing methods that analyze generated text for factual inconsistencies¬†(Maynez et al. 2020; Zhou et al. 2020; Ji et al. 2023; Wu et al. 2025b, a, 2023). These methods typically require external knowledge sources or multiple model generations to assess consistency.\nRecent work has explored uncertainty-based approaches that leverage model confidence signals. Kadavath et al. (2022) demonstrated that LLMs can express uncertainty about their knowledge, while Zhang et al. (2023) proposed enhanced uncertainty estimation methods for hallucination detection. However, these approaches struggle with the discrete nature of tool calling, where traditional uncertainty measures may not capture semantic correctness of API usage.\nConsistency-based methods represent another major direction. SelfCheckGPT¬†(Manakul et al. 2023) generates multiple responses and checks for consistency, assuming that hallucinated content will be inconsistent across samples. Non Contradiction Probability (NCP)¬†(Hou et al. 2025) and Semantic Similarity¬†(Kuhn et al. 2023) are two techniques using this principle of consistency to detect hallucinations. After sampling, NCP probabilistically scores whether a response contradicts entries in a curated belief set using information-theoretic scoring or belief tree propagation, while semantic similarity measures the degree of meaning alignment between a model‚Äôs outputs from multiple samples using cosine similarity or nearest-neighbor score between them. Li et al. (2023a) introduced evaluation frameworks for assessing hallucination detection methods. While effective for free-form text, these approaches face limitations in tool calling scenarios where there may be unique correct solutions, making consistency-based detection less reliable.\n\n\n\nTool-Augmented Language Models\n\nThe integration of external tools with language models has gained significant attention as a means to extend LLM capabilities beyond the parametric knowledge acquired from their training data. Toolformer¬†(Schick et al. 2023) pioneered self-supervised learning for tool use, demonstrating that LLMs can learn when and how to call APIs through minimal demonstrations. This work established the foundation for automated tool learning but did not address the detection of incorrect tool usage.\nToolLLM¬†(Qin et al. 2024) scaled tool learning to thousands of real-world APIs, introducing comprehensive training frameworks and evaluation benchmarks. Gorilla¬†(Patil et al. 2024) focused specifically on API calling accuracy, achieving strong performance through retrieval-augmented training. ReAct¬†(Yao et al. 2023) proposed reasoning and acting paradigms that interleave tool use with reasoning, while Lu et al. (2023) developed plug-and-play compositional reasoning systems.\nDespite these advances, existing work primarily focuses on improving tool-use accuracy during training rather than detecting errors during inference. Our work addresses this gap by providing real-time detection capabilities that can identify when models generate inappropriate tool calls.\n\n\n\nInternal Representation Analysis\n\nUnderstanding the internal mechanisms of neural networks has become increasingly important for building reliable AI systems. Azaria and Mitchell (2023) demonstrated that internal states of LLMs indicate the veracity of responses, showing that classifiers trained on hidden representations can detect when models produce false statements. This work provided crucial evidence that internal representations encode semantic properties beyond surface-level text generation.\nBuilding on this foundation, Su et al. (2024) developed unsupervised methods for real-time hallucination detection using contextualized embeddings from transformer layers. Their approach eliminated the need for manual annotation by automatically generating training data through entity masking in Wikipedia articles. Our work extends these insights to the structured domain of tool calling, where the challenges and patterns differ significantly from free-form text generation.\nRecent work has also explored mechanistic interpretability of transformers¬†(Elhage et al. 2021; Wang et al. 2022), providing insights into how these models process and represent information. Meng et al. (2022) investigated knowledge storage and editing in transformers, while Li et al. (2023b) analyzed attention patterns in reasoning tasks. These insights inform our understanding of where tool-calling information might be encoded within transformer representations.\n\n\n\nAgent Systems and Reliability\n\nThe deployment of LLM-based agents in real-world scenarios has highlighted the need for robust error detection and mitigation strategies. Xi et al. (2023) surveyed the landscape of autonomous agents, identifying reliability as a key challenge for practical deployment. Wang et al. (2023) demonstrated lifelong learning agents but noted the challenges of ensuring consistent performance across diverse environments.\nRecent work has explored agent evaluation frameworks. Liu et al. (2023) introduced comprehensive benchmarks for agent capabilities, while Xu et al. (2023) proposed evaluation metrics for tool-using agents. However, these evaluation frameworks primarily focus on task completion rather than real-time error detection during agent execution.\nSafety and alignment in agent systems have also received attention. Hendrycks et al. (2023) discussed risks associated with autonomous AI systems, while Kenton et al. (2021) exp"
  },
  {
    "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI",
    "url": "https://arxiv.org/abs/2601.05205v1",
    "source": "arxiv",
    "summary": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of trad",
    "full_text": null
  },
  {
    "title": "Stock Market Price Prediction using Neural Prophet with Deep Neural Network",
    "url": "https://arxiv.org/abs/2601.05202v1",
    "source": "arxiv",
    "summary": "Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock pric",
    "full_text": null
  },
  {
    "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models",
    "url": "https://arxiv.org/abs/2601.05201v1",
    "source": "arxiv",
    "summary": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimatio",
    "full_text": null
  },
  {
    "title": "An interpretable data-driven approach to optimizing clinical fall risk assessment",
    "url": "https://arxiv.org/abs/2601.05194v1",
    "source": "arxiv",
    "summary": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were in",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.05194v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2601.05194v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 8 Jan 2026]\n    Title:An interpretable data-driven approach to optimizing clinical fall risk assessment\n    Authors:Fardin Ganjkhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Holley Farley, Kimia Ghobadi            View a PDF of the paper titled An interpretable data-driven approach to optimizing clinical fall risk assessment, by Fardin Ganjkhanloo and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study&#39;s risk labels, and without changing the tool&#39;s form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.\n    \n\n    \n    \n              \n          Comments:\n          arXiv admin note: substantial text overlap with arXiv:2510.20714\n        \n\n          Subjects:\n          \n            Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.05194 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05194v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05194\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Emmett Springer [view email]          [v1]\n        Thu, 8 Jan 2026 18:17:31 UTC (1,042 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled An interpretable data-driven approach to optimizing clinical fall risk assessment, by Fardin Ganjkhanloo and 5 other authorsView PDF\n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs"
  },
  {
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "url": "https://arxiv.org/abs/2601.05192v1",
    "source": "arxiv",
    "summary": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any",
    "full_text": null
  },
  {
    "title": "Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable",
    "url": "https://arxiv.org/abs/2601.05191v1",
    "source": "arxiv",
    "summary": "When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a si",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.05191v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2601.05191v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 8 Jan 2026]\n    Title:Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable\n    Authors:Zuhair Ahmed Khan Taha, Mohammed Mudassir Uddin, Shahnawaz Alam            View a PDF of the paper titled Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable, by Zuhair Ahmed Khan Taha and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.05191 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05191v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05191\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Shahnawaz Alam [view email]          [v1]\n        Thu, 8 Jan 2026 18:13:46 UTC (19 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable, by Zuhair Ahmed Khan Taha and 2 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to co"
  },
  {
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "url": "https://arxiv.org/abs/2601.05187v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretabilit",
    "full_text": "\n\n\n\n\n1 Introduction\n\nOur Contributions.\n\n\n\n2 SimuAgent Architecture\n\nOverall architecture.\nPython-based model representation and validation environment.\nSimplification and hierarchical structuring of complex systems.\nLightweight Python testing environment.\nIntegration with external tools.\n\n\n\n3 SimuAgent Training Framework\n\n\n3.1 Staged Training Strategy\n\nStage¬†1: Execution focus.\nStage¬†2: Planning integration.\n\n\n3.2 Abstract‚ÄìReconstruct Data Augmentation\n\n3.3 Reflection-GRPO (ReGRPO)\n\nGroup Relative Policy Optimization (GRPO).\nReflection-GRPO (ReGRPO).\nGRPO / ReGRPO with Tool Calls.\n\n\n\n\n\n4 Experimental Setup and Findings\n\n4.1 SimuBench Dataset\n\n4.2 Experimental Results\n\nTwo-stage curriculum.\nBenchmark comparison.\nAblation studies.\n\n\n\n4.3 Generalization and Cross-Platform Transfer\n\nReGRPO‚Äôs consistent advantages.\nCross-platform modeling transfer.\n\n\n\n\n5 Conclusion\n\nA Appendix\n\nA.1 LLM Usage\nA.2 SimuBench Dataset\n\nA.3 Experimental Setup and Hyperparameters\n\nHardware and Optimizer Settings\nReward Design\n\n\nA.4 Hyperparameter Ablation Study\nA.5 Failure Analysis on SimuBench\nA.6 Extended Results and Case Studies\n\n\n\n\n\n\n\nSimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning\n\n\nYanchang¬†Liang &amp; Xiaowei¬†Zhao \nIntelligent Control and Smart Energy (ICSE) Research Group \nSchool of Engineering, University of Warwick \nCoventry CV4 7AL, United Kingdom \n{Yanchang.Liang,Xiaowei.Zhao}@warwick.ac.uk\nCorresponding author. Email: Xiaowei.Zhao@warwick.ac.uk. This work has received funding from the UK Engineering and Physical Sciences Research Council under Grant EP/Z533130/1.\n\n\nAbstract\nLarge language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan‚Äìexecute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness.\nExperiments on SimuBench, our newly released benchmark comprising 53005300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark.\nAblations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.\n\n\n\n1 Introduction\n\nSimulink has become the de facto standard for model-based design in safety-critical industries, with over five million engineers relying on it for developing automotive, aerospace, and energy systems (MathWorks, 2023). Major industry players from Tesla to Boeing have deeply integrated Simulink into their workflows, while stringent safety standards like ISO 26262 and DO-178C explicitly recommend or mandate its use for compliance (ISO, 2018; RTCA, 2011). This entrenched position means that even modest improvements in modeling efficiency can yield substantial economic benefits and accelerate certification processes‚Äîaddressing the urgent need to reduce the high costs of developing complex models.\n\n\nBeyond its industrial dominance, Simulink presents unique technical challenges for large language models (LLMs). Unlike text-based programming, Simulink employs a hierarchical, graphical paradigm with complex block diagrams, signal routing, and strict topological constraints. This makes it an ideal testbed for evaluating and advancing LLM reasoning capabilities in highly structured, non-textual domains. Moreover, Simulink serves as a crucial bridge to address a fundamental limitation of current LLMs: their disconnection from the physical world. Trained almost exclusively on text, LLMs lack grounded understanding of physical laws, causality, and dynamic processes (Wang et al., 2023b). Simulink provides a ‚Äúphysics sandbox‚Äù where LLMs can model, simulate, and interact with systems spanning control logic, mechanics, electronics, and thermodynamics‚Äîenabling them to move beyond abstract textual knowledge toward embodied understanding of how physical principles govern real-world systems.\n\n\nDeveloping LLMs for graphical modeling environments faces three core challenges. First, data scarcity: publicly available Simulink model datasets are extremely limited (Zhang et al., 2025), making direct training of large models challenging. Second, syntactic and semantic constraints: graphical models must adhere to strict structural rules (e.g., connection validity, parameter consistency), and LLMs can easily violate these constraints or hallucinate non-existent blocks (Shrestha and Csallner, 2021). Third, context window limitations: the descriptions of complex models (e.g., industrial systems with hundreds of blocks) can far exceed the LLM‚Äôs context window, necessitating strategies like modular generation or hierarchical refinement.\n\n\nRecent exploratory work has begun to investigate LLM applications in Simulink environments. SLGPT fine-tunes GPT-2 to translate text into Simulink‚Äôs XML format, inadvertently uncovering toolchain defects (Shrestha and Csallner, 2021). For model analysis, a requirements-driven slicing technique was proposed to textualize Simulink models for LLM processing (Luitel et al., 2024). In mutation testing, BERTiMuS adapts CodeBERT to generate model variants for requirements-aware testing (Zhang et al., 2025). While these pioneering efforts demonstrate the feasibility of applying LLMs to Simulink-related tasks, they represent only isolated solutions and remain at a prototypical stage rather than a comprehensive modeling assistant. Developing an LLM-driven agent capable of interacting with engineers, understanding context, and providing proactive support throughout the entire modeling workflow remains a significant gap in current research.\n\n\nAlthough LLMs excel in reasoning and coding (Guo et al., 2024; 2025), they often suffer from hallucinations (Zhang et al., 2023) in scenarios requiring specialized knowledge, such as the domain-knowledge-intensive field of Simulink. To mitigate this, a common approach is to enable LLMs to retrieve external information via tool invocation (Schick et al., 2023), typically through prompting strategies (e.g., IRCoT (Trivedi et al., 2022), ReAct (Yao et al., 2023)) or fine-tuning (e.g., Toolformer (Schick et al., 2023)). However, these methods often depend on high-quality, manually annotated trajectories that are difficult to obtain at scale. Reinforcement Learning (RL) (Kaelbling et al., 1996; Sutton et al., 1999) offers another viable path for enhancing LLM tool use and reasoning capabilities; recent studies show RL can enable LLMs to learn complex reasoning skills solely from environmental rewards (Guo et al., 2025). To simplify RL tuning, direct optimization methods (Rafailov et al., 2023; Meng et al., 2024) and alternatives like Group Relative Policy Optimization (GRPO) (Shao et al., 2024) have emerged, the latter estimating baselines from group scores, obviating the need for a critic model. However, RL still faces sparse rewards in tool interaction‚Äîmodels receive feedback only upon final success despite requiring multiple reasoning steps. Inspired by Reflexion (Shinn et al., 2023), which achieves improvements through linguistic feedback but lacks any parameter update mechanism, we integrate reflection directly into GRPO training: our approach generates reflection traces from environmental feedback and tool results, enabling the model to learn from richer signals during parameter optimization, thus accelerating convergence and improving reasoning accuracy.\n\n\nOur Contributions.\n\nTo advance LLM-driven graphical modeling, we propose SimuAgent‚Äîa plan-execute agent framework operable on laptop-grade GPUs, designed to drive the full Simulink modeling, analysis, and fine-tuning loop end-to-end. Its key contributions are:\n\n\n1.\n\nSimuAgent Framework and Lightweight Representation: SimuAgent transforms Simulink models into a compact Python dictionary (JSON) format that LLMs can process more efficiently. This lightweight representation compresses verbose XML while preserving hierarchical model structure, significantly reducing token consumption and improving interpretability. Integrated with an in-process Python test harness, SimuAgent enables instant structural validation and parameter tuning. For complex designs, it automatically encapsulates related modules into subsystems to simplify modeling.\n\n\n\n2.\n\nReflection-GRPO (ReGRPO): We introduce ReGRPO, an enhanced version of GRPO, which incorporates reflection traces‚Äîautomatically generated feedback derived from discrepancies with reference models and tool invocation results. This provides rich training signals beyond binary success, accelerating convergence and improving robustness in sparse-reward settings.\n\n\n\n3.\n\nAbstract‚ÄìReconstruct Data Augmentation: To strengthen model abstraction and generalization, we propose a self-supervised Abstract‚ÄìReconstruct augmentation strategy. The agent first generates a structured summary from a Simulink model, then attempts to reconstruct the original model based solely on that summary. This "
  },
  {
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "url": "https://arxiv.org/abs/2601.05184v1",
    "source": "arxiv",
    "summary": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dyna",
    "full_text": null
  },
  {
    "title": "FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts",
    "url": "https://arxiv.org/abs/2601.05174v1",
    "source": "arxiv",
    "summary": "Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on he",
    "full_text": "\n\n\n\n1 Introduction\n2 Problem Formulation\n\n3 Methodology\n\n3.1 Network Backbone\n3.2 MoE-based Temporal Compression Input\n3.3 Heterogeneity-Aware MoE (HA-MoE)\n3.4 Adaptive Graph Agent Attention (AGA-Att)\n3.5 Prediction Layer\n3.6 Loss Function\n3.7 Complexity Analysis\n\n\n\n4 Experiments\n\n\n4.1 Experimental Settings\n\nDatasets.\nEvaluation Metrics.\nBaselines.\nImplementation Details.\n\n\n4.2 Performance Comparison\n4.3 Ablation Study\n4.4 Hyperparameter Sensitivity\n4.5 Model Efficiency Comparison\n4.6 Visualization Analysis\n\n\n5 Related Work\n6 Conclusion\n\nA Theoretical Analysis of AGA-Att Fidelity\n\nA.1 Spatial Reconstruction Error\n\nA.2 Fidelity Bounds: Lower and Upper Bounds\n\nLower bound.\nUpper bound.\n\n\nA.3 Empirical Reconstruction Errors\n\n\nB Short-Horizon Forecasting Performance\nC Generalization to Other Domains\n\n\n\n\n\nFaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts\n\n\nYiji Zhao\n\nYunnan UniversitySchool of Information Science and EngineeringKunmingChina\n\nyjzhao@ynu.edu.cn\n\n, \nZihao Zhong\n\nYunnan UniversitySchool of Information Science and EngineeringKunmingChina\n\nzhongzihao@stu.ynu.edu.cn\n\n, \nAo Wang\n\nYunnan UniversitySchool of Information Science and EngineeringKunmingChina\n\nwangao@stu.ynu.edu.cn\n\n, \nHaomin Wen\n\nCarnegie Mellon UniversityPittsburghUnited States\n\nhaominwe@andrew.cmu.edu\n\n, \nMing Jin\n\nGriffith UniversityBrisbaneAustralia\n\nmingjinedu@gmail.com\n\n, \nYuxuan Liang\n\nThe Hong Kong University of Science and Technology (Guangzhou)GuangzhouChina\n\nyuxliang@outlook.com\n\n, \nHuaiyu Wan\n\nBeijing Jiaotong UniversityBeijingChina\n\nhywan@bjtu.edu.cn\n\n and \nHao Wu\n\nYunnan UniversitySchool of Information Science and EngineeringKunmingChina\n\nhaowu@ynu.edu.cn\n\n\n(2026)\n\nAbstract.\nSpatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines.\nSource code is publicly available at: https://github.com/yijizhao/FaST.\n\nLong-Horizon Forecasting, Large-Scale Spatial-Temporal Graph, Mixture of Experts\n\n‚Ä†‚Ä†copyright: acmlicensed‚Ä†‚Ä†journalyear: 2026‚Ä†‚Ä†conference: Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1; August 09‚Äì13, 2026; Jeju Island, Republic of Korea‚Ä†‚Ä†ccs: Information systems¬†Spatial-temporal systems\n\n\n1. Introduction\n\nSpatial-Temporal Graph (STG) serves as a powerful data schema for modeling complex dependencies in urban sensing systems, where nodes represent sensors (e.g., traffic detectors, energy meters) and edges capture spatial relationships¬†(Rahmani et al., 2023; Jin et al., 2024). Accurate long-horizon STG forecasting (e.g., days ahead) can shift urban management from reactive responses to proactive planning. For example, a seven-day electricity demand outlook allows renewable generation and storage to be scheduled ahead of peak loads. Such scenarios are typically coupled with two features: 1) a long forecasting horizon (e.g., 672 steps at 15-minute granularity covers one week) and 2) a large-scale graph (e.g., over thousands of nodes).\n\n\nExtensive efforts have been made in STG forecasting, where most works focus on short-term forecasting (i.e., 12-step) in graphs with fewer than hundreds of nodes. Representative models build a Spatial-Temporal Graph Neural Networks (STGNNs), such as DCRNN¬†(Li et al., 2018) and STGCN (Yu et al., 2018), coupling Graph Neural Networks (GNNs) with sequence models to capture both the spatial and temporal dependencies. Later improvements incorporate graph attention¬†(Guo et al., 2019; Zheng et al., 2020; Guo et al., 2022), temporal transformers¬†(Guo et al., 2022; Liang et al., 2023), and neural graph ODEs¬†(Fang et al., 2021) to boost short-term accuracy. However, those STGNNs suffer from quadratic complexity hidden in both spatial and temporal modules: GNNs and spatial/temporal attention incur O‚Äã(N2)O(N^{2}) and O‚Äã(T2)O(T^{2}) pairwise node/time-step interactions over NN nodes and sequence length TT, respectively. When NN and TT scale jointly, resource demands increase exponentially. To give a concrete example, we report the training cost vs. performance on a large-scale STG benchmark (Liu et al., 2023b) in Figure¬†1, when confronted with 8,600 nodes and a 672-step prediction horizon (a practical setting in CA dataset), most representative STGNNs exhaust 48 GB of GPU memory, making them impractical to train and deploy in most GPU devices.\n\n\nTo elevate the STGNNs for large-scale and long-horizon forecasting, a critical challenge is to achieve linear computational complexity while retaining the rich spatial and temporal semantics required for long-horizon forecasting. Recent studies have explored two complementary directions to reduce pairwise node/time-step interactions, though promising, they still face major limitations in both performance and efficiency.\n\n\nSpatially, existing efficient methods can be broadly categorized into structure-aware and structure-free approaches. Structure-aware methods leverage the explicit graph topology to reduce spatial interactions through techniques such as sparse aggregation (e.g., SGP¬†(Cini et al., 2023)), neighbor sampling (e.g., SAGDFN¬†(Jiang et al., 2024)), and graph partitioning (e.g., PatchSTG¬†(Fang et al., 2025)). However, these approaches often discard long-range dependencies and heavily rely on an accurate graph structure (which may not always be available in real-world scenarios). To circumvent this, structure-free methods avoid using graph structure and instead rely on node embeddings and feature-mixing mechanisms. For example, BigST¬†(Han et al., 2024b) adopts linear attention to bypass pairwise node computation; RPMixer¬†(Yeh et al., 2024) employs random projection and MLPs to facilitate efficient spatial fusion; and STID¬†(Shao et al., 2022) eliminates spatial interactions by encoding spatial identity with positional embeddings. While these techniques reduce the complexity from O‚Äã(N2)O(N^{2}) to O‚Äã(N‚ãÖœï)O(N\\cdot\\phi), where œï&lt;N\\phi\\textless N is a tunable factor depending on the implementation and hyperparameters (e.g., sampled neighbors, feature dimension, or partition granularity), they often oversimplify the interaction operation, which can dilute meaningful relationships, introduce irrelevant noise, and therefore lead to significant loss of spatial semantics.\n\n\nFigure 1. Efficiency‚Äìeffectiveness comparison on large-scale STG benchmark (CA dataset; 8,600 nodes; 672-step prediction horizon). Smaller bubble means faster inference speed. The proposed FaST achieves the best performance and speed (both in training and inference).\n\n\nTemporally, two main strategies have emerged: sparse/linear attention mechanisms and temporal pattern compression methods. The former, adopted in models like Airformer¬†(Liang et al., 2023), SAGDFN¬†(Jiang et al., 2024), and BigST¬†(Han et al., 2024b), aims to alleviate the quadratic cost of temporal attention by approximating it with lower-complexity variants. The latter strategy, employed in methods such as CycleNet¬†(Lin et al., 2024) and STID¬†(Shao et al., 2022), compresses long historical sequences into a compact low-dimensional embedding per node, effectively removing explicit pairwise interactions across time steps. For example, STID directly compresses the TT-step history into a dense embedding via a simple linear projection, while CycleNet encodes periodicity by learning a fixed-length recurrent cycle template, both avoiding explicit token-wise temporal interactions. However, due to sharing a single compression module across both spatial units and temporal segments, these methods effectively impose a one-size-fits-all scheme, often homogenizing temporal representations and ignoring heterogeneous temporal patterns across nodes and time periods, risking the loss of temporal diversity and fine-grained dynamics and thus compromising expressiveness and generalization, especially for long-range forecasting tasks.\n\n\nTo address the above limitations, we propose FaST, a Fast long-horizon forecasting framework for large-scale Spatial-Temporal graphs. FaST is a heterogeneity-aware parallelized Mixture-of-Experts (MoE) networks. Firstly, an MoE-based temporal compression input module projects historical sequences into a low-dimensional dense embedding, improving computational efficiency and preventing memory explosion concerning the historical input length. To mitigate the information loss and representation homogenization induced by compression, we introduce a heterogeneity-aware router that dynamically selects expert-specific compression pathways across nodes and time periods, thereby extracting diversified temporal features. Furthermore, an expert-concurrent mechanism based on Gated Linear Units (GLUs) is proposed to achieve efficient feature extraction. Secondly, an adaptive graph agent attention module is introduced, where aa agent tokens (a‚â™Na\\ll N) are learned "
  },
  {
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "url": "https://arxiv.org/abs/2601.05172v1",
    "source": "arxiv",
    "summary": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nTest-time reasoning.\nScene understanding.\n3D VLMs.\n\n\n\n3 Method\n\n3.1 Problem Setting\n3.2 Method Overview\n3.3 Coarse-Grained View Selection\n3.4 Fine-Grained View Adjustment\n\n\n\n4 Experiments\n\n4.1 Benchmarks and Metrics\n4.2 Implementation Details\n4.3 Main Results\n4.4 Test-Time Scaling\n4.5 Ablation Studies\n4.6 Qualitative Result\n\n\n5 Conclusion\nA Prompt Templates\nB Result Visualization\n\n\n\n\n\nCoV: Chain-of-View Prompting for Spatial Reasoning\n\n\nFirst Author \nAffiliation / Address line 1 \nAffiliation / Address line 2 \nAffiliation / Address line 3 \nemail@domain\n&amp;Second Author \nAffiliation / Address line 1 \nAffiliation / Address line 2 \nAffiliation / Address line 3 \nemail@domain\n\n\n‚ÄÉ‚ÄÉ\nHaoyu Zhao1‚àó‚ÄÉAkide Liu2‚àó‚ÄÉZeyu Zhang2‚àó‚ÄÉWeijie Wang1‚àó\nFeng Chen3‚ÄÉRuihan Zhu1‚ÄÉGholamreza Haffari2‚ÄÉBohan Zhuang1‚Ä†\n\n1ZIP Lab, Zhejiang University‚ÄÉ2Monash University‚ÄÉ3AIML, Adelaide University\n‚àóEqual contribution. ‚Ä†Corresponding author: bohan.zhuang@gmail.com\n\n\n\nAbstract\nEmbodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision‚Äìlanguage models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.\nWe evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56% improvement in LLM-Match, with a maximum gain of +13.62% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51% average improvement, peaking at +3.73% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.\nCode: https://github.com/ziplab/CoV.\n\n\n\nCoV: Chain-of-View Prompting for Spatial Reasoning\n\n\n\n\nHaoyu Zhao1‚àó‚ÄÉ‚ÄäAkide Liu2‚àó‚ÄÉ‚ÄäZeyu Zhang2‚àó‚ÄÉ‚ÄäWeijie Wang1‚àó\n\nFeng Chen3‚ÄÉRuihan Zhu1‚ÄÉGholamreza Haffari2‚ÄÉBohan Zhuang1‚Ä†\n\n\n\n\n1ZIP Lab, Zhejiang University‚ÄÉ2Monash University‚ÄÉ3AIML, Adelaide University\n\n‚àóEqual contribution. ‚Ä†Corresponding author: bohan.zhuang@gmail.com.\n\n\n\n\n\n\n1 Introduction\n\nAs artificial intelligence transitions from digital domains to physical reality, Embodied Question Answering (EQA) has emerged as a critical capability for enabling intuitive, human-centric interaction with the environment. It holds significant potential across domains like robotics, autonomous navigation, and human‚Äìcomputer interaction.\nIn EQA settings, the agent processes a textual question based on a sequence of egocentric images (optionally with a 3D scene representation such as point clouds or 3D meshes).\nThe agent must perceive and reason within the real environment to derive the correct answer.\n\n\nHowever, existing methods¬†Mo and Liu (2024); Fu et¬†al. (2024); Zhu et¬†al. (2024); Li et¬†al. (2024c) encounter a substantial limitation:\nconventional methods use a limited and fixed set of viewpoints as input (see¬†Fig.Àú2), making it difficult for VLMs to acquire sufficient question-relevant views.\nIn complex embodied QA tasks, answers are not immediately apparent, and a question often requires multi-step reasoning to solve.\nFor example, for the question ‚ÄúWhere can I get some pop drinks?‚Äù, the scene does not directly show soda. The model must invoke world knowledge and navigate autonomously to locate objects like a refrigerator. Answering such complex real-world questions requires sufficient question-relevant context and cannot be accomplished through one-step answer generation.\n\n\nIn this paper, we propose the chain-of-view (CoV) prompting framework (see LABEL:fig:teaser), a two-stage agent system designed to shift from passive observation to active exploration and iterative reasoning.\nSpecifically, the framework operates in a coarse-to-fine manner:\nin the coarse-grained view selection stage, the View Selection Agent selects the most question-relevant view from the available viewpoints as the starting point for exploration.\nMeanwhile, we provide the agent with a bird-eye view of the entire scene to facilitate a global understanding of the environment.\nIn the fine-grained view adjustment stage, the CoV Agent executes an action‚Äìreasoning loop.\nAt each step, the VLM generates an action instruction based on the current observation and the question (e.g., forward-movement or right-rotation).\nThe action is mapped to a rigid-body camera transformation, producing the next observation, which is fed back to the VLM for the next reasoning step.\nThe process terminates when the CoV agent determines that sufficient information has been acquired or when a predefined limit on action steps is reached, at which point the final answer is produced.\n\n\nUnlike prior 2D VLMs that rely on predetermined views (as illustrated in¬†Fig.Àú2), our chain-of-view framework addresses complex embodied QA problems through multi-step reasoning.\nMoreover, we improve the alignment between visual content and the question via explicit view selection and fine-grained adjustment.\n\n\nFigure 2: Video VLM vs. CoV. Unlike prior approaches (top) that rely on fixed-frame video inputs and answer from a limited temporal window, our chain-of-view framework (bottom) explores an open-ended view space constructed from a 3D scene. CoV dynamically selects informative viewpoints and performs step-by-step reasoning during inference, enabling more complete and grounded answers without additional training.\n\n\nTo evaluate the efficacy of our methodology, we conduct comprehensive experiments on the latest EQA benchmark OpenEQA¬†(Majumdar et¬†al., 2024), which serve as standard metrics for assessing 3D scene understanding and question answering capabilities.\nWe evaluate both mainstream open-source and proprietary VLMs, and applying the CoV framework yields an average improvement of 10.82%10.82\\%, with a maximum gain of 13.62%13.62\\% on Qwen3-VL-Flash.\nWe further empirically verify the test-time scaling capability of CoV: as the number of action steps increases, the agent‚Äôs score improves by an average of 2.51%2.51\\%, with a maximum gain of 3.73%3.73\\% on Gemini-2.5-Flash.\n\n\nQualitative analyses further validate that our CoV framework produces more coherent and interpretable reasoning chains, particularly in complex or cluttered environments.\nThese results demonstrate the potential of test-time scaling strategies to enhance scene understanding without requiring additional model training or dataset-specific tuning, making our framework robust and adaptable across diverse 3D tasks and domains.\n\n\nThe main contributions of our work can be summarized as follows:\n\n\n‚Ä¢\n\nWe propose Chain-of-View Prompting, a test-time reasoning framework that enhances VLMs‚Äô ability to handle complex spatial reasoning in embodied question answering. By leveraging coarse-to-fine view selection and camera adjustment, the agent acquires sufficient question-relevant views to answer spatially complex questions, thereby improving performance on EQA tasks.\n\n\n\n‚Ä¢\n\nCoV prompting enables test-time scaling. As the number of exploration steps increases, the agent‚Äôs performance improves gradually.\n\n\n\n‚Ä¢\n\nExperimental results on the latest embodied QA benchmarks demonstrate significant improvements through our systematic view exploration approach. Our method achieves up to a 13.62%13.62\\% improvement on the OpenEQA benchmark.\n\n\n\n\n\n\n\n2 Related Work\n\nRecent advances in 3D scene understanding have unified perception and language.\nMethods like Vote2Cap-DETR¬†Chen et¬†al. (2023a), D3Net¬†Chen et¬†al. (2022), and SpaCap3D¬†Wang et¬†al. (2022) integrate object localization and description generation, enabling more grounded scene understanding for robotics, AR/VR, and embodied AI.\n3D vision-language models such as LLaVA-3D¬†Zhu et¬†al. (2024) and LL3DA¬†Chen et¬†al. (2024) further advance scene understanding by synthesizing 2D multimodal perception with 3D spatial context.\nThese architectures leverage multi-view images augmented with 3D positional embeddings, facilitating more context-aware reasoning without dependency on external object proposals or segmentation mechanisms.\n\n\nTest-time reasoning.\n\nLarge models such as Qwen, ChatGPT and Gemini Bai et¬†al. (2025); OpenAI (2023); Georgiev et¬†al. (2024)\nshow strong performance in multi-modal reasoning tasks.\nDue to high fine-tuning costs, recent work explores efficient adaptation methods that keep pretrained weights.\nIn-context learning¬†Brown et¬†al. (2020); Sahoo et¬†al. (2024), prompt engineering, and chain-of-thought prompting¬†Wei et¬†al. (2022) guide model behavior at inference time.\nRecent works like Simple Scaling¬†Muennighoff et¬†al. (2025a), adaptive compute¬†Snell et¬†al. (2024), and calibration¬†McKenna and Carse (2024) offer practical, training-free improvements.\n\n\n\nScene understanding.\n\n3D scene understanding primarily encompasses tasks such as 3D question answering Ma et¬†al. (2023); Azuma et¬†al. (2022) and 3D dense captioning Chen et¬†al. (2020); Achlioptas et¬†al. (2020).\nEarly 3D dense captioning used a detect-then-describe pipeline¬†Chen et¬†al. (2021); Wang et¬†al. (2022).\nNewer methods adopt end-to-end transformers Chen et¬†al. (2023a, 2022); Huang et¬†al. (2025a, b) to predicts object-cap"
  },
  {
    "title": "Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems",
    "url": "https://arxiv.org/abs/2601.05171v1",
    "source": "arxiv",
    "summary": "Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By c",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Personalization and Memory\n2.2 LLM Agents with External Memory\n\n\n\n3 The Inside Out Framework\n\n\n3.1 Overview Architecture\n\nFramework Pipeline.\nProblem Formulation.\n\n\n\n3.2 Dynamic PersonaTree Evolution\n\nPersonaTree Initialization.\nIterative PersonaTree Updating.\n\n\n\n3.3 MemListener Training\n\nTraining Data Synthesis.\nWarm-up via SFT.\nAlignment via Process-Reward RL.\n\n\n\n3.4 Adaptive Response Generation\n\nPersonaTree-Augmented Generation.\nAgentic Recall and Fusion.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\nDatasets and Metrics.\nBaselines.\nImplementation Details.\n\n\n4.2 Main Results\n\n4.3 Ablation Study\n\nEffectiveness of Components.\nEnhancement through Training.\nSelection of Generation Strategies.\n\n\n4.4 Hyperparameter Analysis\n\n\n5 Conclusion\nA Rethinking Personalization\nB MemListener Training\n\nC Extra Experiments\n\n\nC.1 Baselines\n\nOnly LLM.\nFull Context.\nLangMem.\nMem0 [chhikara2025mem0].\nA-Mem (Agentic Memory) [xu2025mem].\nMemoryOS [kang2025memory].\n\n\n\nC.2 Extra Experiments Results\n\nAblation Study.\n\n\nC.3 Visualization\n\n\nD Initial PersonaTree Instance\n\n\n\n\n\n\n1]School of Information, Renmin University of China\n2]MemTensor (Shanghai) Technology Co., Ltd.\n3]Institute for Advanced Algorithms Research, Shanghai\n4]Beijing University of Aeronautics and Astronautics\n5]Nankai University\n\n\n\n\n\n\n\n\n\n\nInside Out: \nEvolving User-Centric Core Memory Trees\nfor Long-Term Personalized Dialogue Systems\n\n\n\n\n\nJihao Zhao\n\n‚ÄÉ‚ÄÉ\nDing Chen\n\n‚ÄÉ‚ÄÉ\nZhaoxin Fan\n\n‚ÄÉ‚ÄÉ\nKerun Xu\n\n‚ÄÉ‚ÄÉ\nMengting Hu\n\n‚ÄÉ‚ÄÉ\nBo Tang\n\n‚ÄÉ‚ÄÉ\nFeiyu Xiong\n\n‚ÄÉ‚ÄÉ\nZhiyu Li\n\n[\n\n[\n\n[\n\n[\n\n[\n\nlizy@memtensor.cn\n\n\n\nAbstract\nExisting long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD,UPDATE,DELETE,NO_OP}\\{\\texttt{ADD},\\texttt{UPDATE},\\texttt{DELETE},\\texttt{NO\\_OP}\\} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.\n\n\n\\correspondence\nTeam Leader at \n\\checkdata[Author Legend]‚Ä†Corresponding author\n\n\n\n1 Introduction\n\n\nCore memories shape Riley‚Äôs personality islands, with each island serving as a unique emblem of her identity.\n\n ‚Äî‚Äî \"Inside Out\"\n\n\n\nWith the rapid advancement of large language models (LLM), dialogue-based agents have demonstrated substantial potential in applications such as personal assistants, affective companionship, and long-term question answering [chhikara2025mem0, rasmussen2025zep, li2025memos_long]. However, within personalized dialogue systems aimed at fostering long-term human-machine trust and emotional connection, a fundamental contradiction exists between the finite context window and the unbounded growth of interaction history [xiao2024efficient, liu2024lost]. As conversational turns continue to accumulate, the traditional single-context paradigm encounters a severe form of context saturation: indiscriminate aggregation of massive historical information not only drives computational costs sharply upward, but also introduces substantial irrelevant noise, markedly degrading the signal-to-noise ratio. More critically, this unstructured accumulation makes it difficult for the model to accurately extract and sustain a user‚Äôs personal characteristics from lengthy histories, leading to personalization inconsistency over long-term interactions and thereby seriously undermining user experience and the system‚Äôs long-term usability [zhong2024memorybank, salemi2024lamp].\n\n\nTo address these challenges, existing studies have primarily explored routes such as explicit profile augmentation and vector-based retrieval, yet neither directly confronts the central bottleneck of personalized memory evolution. Profile-based approaches rely on predefined, static attributes; they are not only slow to update but also struggle to capture implicit cues that users reveal over prolonged interactions, including linguistic style, deeper value orientations, and affective preferences, resulting in superficial personalization modeling [tan2023usermodeling]. In contrast, memory-augmented agents based on vector retrieval, while introducing external storage, still essentially treat memory as text fragments or simple lists of facts. Such systems lack an intrinsic, trained decision mechanism for determining which information merits long-term retention, and instead often depend on rigid heuristics or elaborate prompt engineering [liu2023thinkinmemory]. This accumulation of memories without value-based judgment causes the memory repository either to become bloated and uninterpretable due to noise accretion, or to lose the long-range logical thread through fragmentation of key context, ultimately failing to sustain a vivid and coherent persona [yoran2024making].\n\n\nThis discrepancy between memory accumulation and core persona formation‚Äù motivates us to return to the foundations of human cognition for an answer. As illustrated by the film \"Inside Out\", individual identity does not stem from a simplistic stacking of all experiences, but rather is constructed upon core memories that shape distinct \"Islands of Personality\". This aligns with theoretical findings in cognitive psychology, such as Self-Schema theory [Markus1977SelfschemataAP, tikka2019tailoring], which emphasizes that humans maintain a stable self-concept by filtering and hierarchically organizing key memories.\n\n\nInspired by these insights, we propose the Inside Out framework, which aims to grow an evolvable user core memory tree \"from the inside out\" through unbounded interactions. Firstly, to delineate the theoretical boundaries of the memory tree, we construct a hierarchical Schema based on the Biopsychosocial model, scientifically decomposing user characteristics into three core dimensions. This interdisciplinary Schema design establishes the initial structure of the user PersonaTree. Secondly, to endow the system with dynamic evolution, we propose an iterative tree-update mechanism and introduce a reinforcement learning (RL) strategy based on process rewards to train a lightweight model, MemListener. This model learns to compress a continuous stream of unstructured dialogue in real time into standardized tree-structured operations, encoding user core features within the branch and leaf nodes. Finally, addressing the trade-off between efficiency and effectiveness during the inference stage, this paper designs an adaptive response generation mechanism: In latency-sensitive scenarios, a fast mode is enabled to perform reasoning directly based on the PersonaTree. When facing long-tail detail requirements, the system switches to the agentic recall mode, utilizing the PersonaTree to guide deep retrieval. The primary contributions of our work are summarized as:\n\n\n\n\n‚Ä¢\n\nWe propose PersonaTree, grounded in the biopsychosocial schema. By transforming unstructured dialogue streams into standardized atomic tree operations in real-time, PersonaTree achieves the dynamic compression, explicit management, and high signal-to-noise ratio maintenance of implicit user profiles.\n\n\n\n‚Ä¢\n\nWe design a training strategy utilizing RL with process rewards. Leveraging the constructed dataset of 28k instructions, we train a lightweight model, MemListener, to execute precise memory editing.\n\n\n\n‚Ä¢\n\nOur experiments reveal the potential of a collaborative paradigm where \"small models maintain memory while LLMs handle generation\". Results show that MemListener achieves memory-decision performance comparable to strong reasoning models, and that PersonaTree offers a new pathway toward low-cost, highly reliable deployment of long-term personalized dialogue systems.\n\n\n\n\n\nFigure 1: Overview of the entire process of our Inside Out framework.\n\n\n\n\n2 Related Works\n\n\n2.1 Personalization and Memory\n\nPersonalization aims to adapt a dialogue system‚Äôs linguistic style and interaction policy to a specific user‚Äôs stable traits and evolving state. In interactive settings, personalization is inherently coupled with memory: models must distill reusable user representations from past interactions and fuse them during generation. li2016persona proposed persona-based dialogue generation to mitigate inconsistency and lack of personality in open-domain dialogue, and zhang2018personalizing formalized the PersonaChat task. Subsequent studies emphasized multi-dimensional user attributes. For example, zheng2019personalized introduced the large-scale multi-turn dataset PersonalDialog. In parallel, madotto2019personalizing framed personalization as a meta-learning problem to enable few-shot adaptation. In the LLM era, chen2024recent systematically reviewed major directions in personalized dialogue generation, while tan-etal-2024-democratizing assigned parameter-efficient personalization modules to users to improve multi-task personalization.\n\n\n\n\n2.2 LLM Agents with External Memory\n\nTo"
  },
  {
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "url": "https://arxiv.org/abs/2601.05170v1",
    "source": "arxiv",
    "summary": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and ",
    "full_text": null
  },
  {
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "url": "https://arxiv.org/abs/2601.05167v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when th",
    "full_text": "\n\n\n\n1 Introduction\n\n2 RelayLLM¬†Inference\n\n2.1 Small Model Generation\n2.2 Large Model Intervention\n2.3 Iterative Relay of LLM and SLM\n\n\n\n3 RelayLLM¬†Training\n\n3.1 Cold Start via Supervised Warm-up\n\n3.2 Policy Refinement with Reinforcement Learning\n\n3.2.1 GRPO Training with RLVR\n3.2.2 Data Filtering\n\n3.2.3 Reward Design\n\nSimple Reward.\nDifficulty-Aware Reward.\nScenario 1: Student-Solvable (Encouraging Independence).\nScenario 2: Teacher-Dependent (Penalizing Stubbornness).\nScenario 3: Teacher-Unsolvable (Incentivizing Exploration).\n\n\n\n\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\n4.1.1 Models\n\n\n4.2 Evaluation Setup\n4.3 Training Details\n4.4 Main Results\n\n\n\n5 Analysis\n\n5.1 RelayLLM Generalizes to Unseen Reasoning Domains\n\n5.2 Ablation Study\n\nData filtering prevents wasteful calls where teacher models fail.\nEncouraging independence reduces reliance on teacher model and improves efficiency.\nExploration reward effectively increases accuracy.\n\n\n5.3 Intrinsic Reasoning Capability\n5.4 Dynamic Token-Length Calling Minimizes Computational Cost\n5.5 Distributional Alignment\n\n\n\n6 Related Work\n\n6.1 Model Collaboration\n6.2 RL for LLM Reasoning\n\n\n7 Conclusion\nA Case Study\n\nB Detailed Analysis of Delegation Length Strategies\n\nEfficiency on Standard Benchmarks.\nAdaptability on Complex Benchmarks.\n\n\n\nC Prompt Templates\n\nC.1 Inference Prompt\nC.2 GPT-4o-mini Judge Prompt\n\n\nD Other Related Work\nE Hyperparameter\n\n\n\n\n\n\nRelayLLM: Efficient Reasoning via Collaborative Decoding\n\n\nChengsong Huang1 ‚ÄÉTong Zheng2 ‚ÄÉLanglin Huang1\nJinyuan Li1 ‚ÄÉ‚ÄäHaolin Liu3‚ÄÉ‚ÄäJiaxin Huang1\n 1Washington University in St. Louis ‚ÄÉ2University of Maryland ‚ÄÉ3University of Virginia ‚ÄÉ\n{chengsong, h.langlin, ljinyuan, jiaxinh}@wustl.edu ‚ÄÉtzheng24@umd.edu ‚ÄÉsrs8rh@virginia.edu\n\n\n\nAbstract\nDeploying Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM¬†empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM¬†improves the average accuracy from 42.5% to 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers. Our code is available at https://github.com/Chengsong-Huang/RelayLLM.\n\n\n\nRelayLLM: Efficient Reasoning via Collaborative Decoding\n\n\n\n\nChengsong Huang1 ‚ÄÉ‚ÄäTong Zheng2 ‚ÄÉ‚ÄäLanglin Huang1\n\nJinyuan Li1 ‚ÄÉ‚ÄäHaolin Liu3‚ÄÉ‚ÄäJiaxin Huang1\n\n1Washington University in St. Louis ‚ÄÉ2University of Maryland ‚ÄÉ3University of Virginia\n\n{chengsong, h.langlin, ljinyuan, jiaxinh}@wustl.edu ‚ÄÉtzheng24@umd.edu ‚ÄÉsrs8rh@virginia.edu\n\n\n\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning and problem-solving¬†(Comanici et al., 2025; Yang et al., 2025a; Achiam et al., 2023). However, their deployment is often constrained by high computational costs and latency. In contrast, Small Language Models (SLM) are resource-efficient options but typically struggle with hard reasoning tasks due to their limited capacity¬†(Kaplan et al., 2020). This trade-off has motivated the development of collaborative systems that combine the reasoning capabilities of LLMs with the efficiency of smaller models¬†(Hakimov et al., 2025).\n\n\nExisting approaches to different-sized model collaboration often rely on ‚Äúcascading‚Äù or ‚Äúrouting‚Äù mechanisms, where a router determines the difficulty of a query and directs it to either a small or large model¬†(Ding et al., 2024; Hu et al., 2024; Ong et al., 2024). While effective to some extent, these methods typically operate at a coarse granularity by offloading the entire generation task to the large model once a query seems difficult. This ‚Äúall-or-nothing‚Äù strategy leads to significant computational waste, as the small model often possesses the competence to handle the majority of the reasoning steps, requiring expert intervention only at specific critical positions¬†(Lin et al., 2024; Ruan et al., 2025).\n\n\nFigure 1: Results are averaged across six mathematical benchmarks.\nThe ‚ÄúRandom Router‚Äù baseline randomly directs questions to either the small or large model.\nThe ‚ÄúPerfect Router‚Äù baseline directs only questions SLM cannot solve to large model.\nThe x-axis represents the Call Ratio (percentage of tokens generated by the teacher model), and the y-axis denotes the average accuracy.\n\n\nTo address these problems, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding¬†(Shen et al., 2024) without an additional controller. Unlike static routers, RelayLLM empowers the small model to act as both a problem solver and an active controller that dynamically requests assistance only when necessary.\n\n\nInspired by tool-use agents¬†(W√∂lflein et al., 2025; Zheng et al., 2025a), we introduce an interleaved generation process where the small model generates a special command token (&lt;call&gt;) to pause its own generation and invoke the large model for a specified number of tokens. The small model then receives the expert‚Äôs guidance and resumes reasoning, effectively ‚Äúrelaying‚Äù the output process between models.\nWe propose a two-stage framework to equip the small model with this strategic delegation capability to train the RelayLLM. We first employ a supervised warm-up phase to teach the model the syntactic structure of calling commands. This is followed by a reinforcement learning stage using Group Relative Policy Optimization (GRPO)¬†(Shao et al., 2024) training, where we design a context-aware reward that guides the model to balance independence with necessary help-seeking, penalizing both wasted costs and avoidable errors.\n\n\nEmpirical results on six benchmarks demonstrate the effectiveness of our approach. As illustrated in Figure¬†1, RelayLLM¬†achieves an average accuracy of 49.52% on six benchmarks, largely recovering the performance gap between the small model and the large one. Remarkably, this gain is achieved with minimal cost, as RelayLLM¬†invokes the large model for only 1.07% of the total generated tokens.\nFurthermore, in comparison to a resource-equivalent Random Router, RelayLLM¬†yields a substantial 6.9% accuracy improvement. These results confirm that RelayLLM¬†effectively identifies critical reasoning steps for expert intervention, reducing token costs by 98.2% compared to a performance-matched router. Surprisingly, evaluations in a teacher-free setting reveal that the model internalizes effective reasoning patterns during collaboration, enabling it to surpass baselines on easier benchmarks even without expert assistance.\n\n\n\n\n2 RelayLLM¬†Inference\n\nFigure 2: Overview of the RelayLLM¬† framework.(Left) Collaborative Inference: The Small Language Model acts as a central controller. During generation, it can actively trigger an ‚Äúintervention‚Äù by generating a &lt;call&gt; command. The Large Language Model then generates the specified number of tokens, after which control returns to the SLM to complete the reasoning. (Right) Difficulty-Aware Reward Design: During GRPO training, we sample a group of rollouts (both with and without LLM intervention) and classify the query difficulty into three distinct scenarios: Solvable, Teacher-Dependent, and Unsolvable, to guide the policy optimization.Distinct reward designs are applied to each scenario to align the model‚Äôs behavior with the optimal strategy.\n\n\nAs illustrated in Fig.¬†2, we consider a hybrid inference setting involving a primary, resource-efficient Small Language Model (SLM), denoted as ‚Ñ≥S\\mathcal{M}_{S}, and a powerful but computationally expensive Large Language Model (LLM), denoted as ‚Ñ≥L\\mathcal{M}_{L}. Given an input query ùê±\\mathbf{x}, the system aims to generate a high-quality response ùê≤\\mathbf{y} through dynamic collaboration.\n\n\nDifferent from standard cascading methods that simply offload the entire remaining task to a larger model, we define an interleaved generation process where the small model acts as both reasoner and central controller. The process operates as follows:\n\n\n\n2.1 Small Model Generation\n\nBy default, ‚Ñ≥S\\mathcal{M}_{S} generates tokens autoregressively based on the current context history as a normal language model. ‚Ñ≥S\\mathcal{M}_{S} is augmented with a special control capability: it can actively request assistance by generating a specific command pattern:\n\n\n\nùíûcmd‚Äã(n)=&lt;call&gt;‚äïn‚äï&lt;/call&gt;\\mathcal{C}_{\\text{cmd}}(n)=\\texttt{&lt;call&gt;}\\oplus n\\oplus\\texttt{&lt;/call&gt;}\n\n\n\nwhere ‚äï\\oplus denotes string concatenation, and n‚àà‚Ñ§+n\\in\\mathbb{Z}^{+} represents the number of tokens required from the larger language model.\n\n\n\n\n2.2 Large Model Intervention\n\nWhen this trigger pattern is detected, the generation by ‚Ñ≥S\\mathcal{M}_{S} pauses. The current context (including query and tokens generated by SLM) is forwarded to ‚Ñ≥L\\mathcal{M}_{L}.\nCrucially, to maintain compatibility with the large model‚Äôs standard input distribution, we strip the special command tokens (ùíûcmd\\mathcal{C}_{\\text{cmd}}) from the context provided to ‚Ñ≥L\\mathcal{M}_{L}. The large model then generates the next nn tokens (or stops early if an end-of-sequence ‚Äú[EOS]‚Äù token is reached), p"
  },
  {
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "url": "https://arxiv.org/abs/2601.05163v1",
    "source": "arxiv",
    "summary": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models doc",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methods\n\n3.1 Agent Setup\n3.2 Data Synthesis\n3.3 Agent Training\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Overall Performance (RQ1)\n4.3 Effectiveness of Synthetic Data (RQ2)\n4.4 Influence of Agentic Tools (RQ3)\n4.5 Qualitative Analysis (RQ4)\n\n\n5 Conclusion\nA Case Study of Synthetic Data\n\nB Implementation Details\n\nB.1 Details on Prompts\nB.2 Tool Schema\nB.3 Training Details\nB.4 Inference Details\nB.5 Hyperparameter\nB.6 Details on Prompts for Data Synthesis\n\n\nC Baselines\n\n\n\n\n\nDocDancer: Towards Agentic Document-Grounded Information Seeking\n\n\n\nQintong Zhang‚ô°\\heartsuit,\nXinjie Lv‚ô°\\heartsuit‚àó,\nJialong Wu‚ô°\\heartsuit‚àó\n\n,\nBaixuan Li‚àó,\nZhengwei Tao‚ô°\\heartsuit, \n\nGuochen Yan‚ô°\\heartsuit,\nHuanyao Zhang‚ô°\\heartsuit,\nBin Wang‚ô¢\\diamondsuit,\nJiahao Xu‚ô£\\clubsuit,\nHaitao Mi‚ô£\\clubsuit,\nWentao Zhang‚ô°\\heartsuit\n\n‚ô°\\heartsuitPeking University,\n‚ô¢\\diamondsuitShanghai AI Lab,\n‚ô£\\clubsuitTencent AI Lab\n\nwujialongml@gmail.com, wentao.zhang@pku.edu.cn\n\nEqual Contributions. \n\nJialong Wu is the project leader. Corresponding Author.\n\n\nAbstract\nDocument Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models.\nIn this work, we introduce DocDancer, an end-to-end trained open-source Doc agent.\nWe formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension.\nTo enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA.\nTraining on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness.\nFurther analysis provides valuable insights for the agentic tool design and synthetic data.\n\n\n\nDocDancer: Towards Agentic Document-Grounded Information Seeking\n\n\n\n\nQintong Zhang‚ô°\\heartsuit‚Ä†‚Ä†thanks: Equal Contributions. \n\nJialong Wu is the project leader. ,\nXinjie Lv‚ô°\\heartsuit‚àó,\nJialong Wu‚ô°\\heartsuit‚àó\n\n,\nBaixuan Li‚àó,\nZhengwei Tao‚ô°\\heartsuit,\n\n\nGuochen Yan‚ô°\\heartsuit,\nHuanyao Zhang‚ô°\\heartsuit,\nBin Wang‚ô¢\\diamondsuit,\nJiahao Xu‚ô£\\clubsuit,\nHaitao Mi‚ô£\\clubsuit,\nWentao Zhang‚ô°\\heartsuit‚Ä†‚Ä†thanks: Corresponding Author.\n\n‚ô°\\heartsuitPeking University,\n‚ô¢\\diamondsuitShanghai AI Lab,\n‚ô£\\clubsuitTencent AI Lab\n\nwujialongml@gmail.com, wentao.zhang@pku.edu.cn\n\n\n\n\n\n1 Introduction\n\nUnderstanding and answering questions over long, multi-modal documents is a critical capability for real-world intelligent systems¬†Tkaczyk et al. (2015); Liu et al. (2025b).\nDocument Question Answering (DocQA) lies at the core of document-centric intelligence, enabling models to access, reason over, and synthesize information from complex and heterogeneous document sources.\n\n\nFigure 1: The overall of DocDancer for document-grounded information seeking, where search and read tools for effective document retrieval and comprehension over processed documents.\n\n\nExisting DocQA methods can be broadly categorized into three paradigms.\nThe first paradigm relies on optical character recognition (OCR) to convert documents into plain text, which is then processed by downstream language models¬†Xu et al. (2020).\nThe second paradigm adopts embedding-based retrieval mechanisms, most commonly instantiated through retrieval-augmented generation (RAG), to identify and incorporate relevant document segments during inference¬†Saad-Falcon et al. (2024).\nMore recently, agent-based paradigms have gained increasing attention, as they better support complex scenarios that require iterative exploration, tool invocation, and multi-step reasoning over long and structured documents¬†Sun et al. (2025a); Zhu et al. (2025).\nRecent advances in large language models (LLMs) Team (2025); Liu et al. (2025a) enable such agents to dynamically decompose queries, interact with documents, and adapt to intermediate observations, alleviating the limitations of OCR- and RAG-based approaches.\nDespite their promise, existing DocQA agents are typically implemented as prompt-based pipelines, with limited learning of autonomous agentic behaviors.\n\n\nIn contrast, we aim to train the first end-to-end DocQA agent model that is explicitly grounded in information-seeking principles, moving beyond prompt-based agent designs.\nWe first formulate DocQA as an agentic information-seeking problem and design a tool-centric agent framework that decomposes document understanding into two complementary capabilities.\nSpecifically, we introduce efficient search tools for global information acquisition and fine-grained read tools for localized comprehension.\nThis design enables the agent to actively explore long documents, iteratively refine its hypotheses, and dynamically adapt its strategy based on intermediate observations.\nNotably, when instantiated with a proprietary LLM, our framework achieves state-of-the-art performance and exceeds reported human-level performance.\n\n\nFurthermore, a key bottleneck in training such agent models is the scarcity of high-quality DocQA pairs¬†Huang et al. (2025), as most publicly available datasets provide only test splits and lack sufficiently annotated training data.\nTo address this challenge, we propose an Exploration-then-Synthesis DocQA generation pipeline that progressively enhances QA pairs from easy to hard.\nSpecifically, we first explore a source document through intent-guided, tool-augmented interactions to collect grounded evidence (the Exploration stage), and then synthesizes high-quality document-grounded QA pairs via multi-observation reasoning (the Synthesis stage).\nWe then train our DocQA agent, DocDancer, on the synthesized dataset, instantiating it with two open-source backbones, Qwen3-4B-Thinking-2507 and Qwen3-30B-A3B-Thinking-2507¬†Team (2025).\nDespite being trained with only 5,000 instances, both variants achieve competitive performance, with the 30B-A3B model attaining state-of-the-art results in several settings.\n\n\nExtensive experiments are conducted on two long-context document understanding benchmarks, MMLongBench-Doc¬†Ma et al. (2024) and DocBench¬†Zou et al. (2025).\nThe results demonstrate the effectiveness of the proposed DocDancer.\nFurther analyses provide insights into document parsing strategies, tool design, and the role of synthetic data in agent learning. In summary, our contributions are three-fold:\n\n\n‚Ä¢\n\nEffective Agentic DocQA Framework:\nWe propose a tool-driven DocQA agent framework grounded in information-seeking principles, which achieves SOTA performance when paired with a proprietary LLM.\n\n\n\n‚Ä¢\n\nAutonomous Data Synthesis Pipeline:\nWe introduce an Exploration-then-Refine data synthesis pipeline that generates high-quality training data for learning agentic behaviors.\n\n\n\n‚Ä¢\n\nEmpirical Performance: Our method achieves state-of-the-art results and provides practical insights into effective and efficient agentic system design.\n\n\n\n\n\n\n\n2 Related Work\n\nDocument Question Answering Methods.\nTraditional DocQA methods rely on OCR-based pipelines¬†Ding et al. (2022) or end-to-end vision‚Äìlanguage models¬†Sukh (2025); Hu et al. (2025), but both are constrained by limited input length and struggle with long documents¬†Ma et al. (2024); Zou et al. (2025); Dong et al. (2025a).\nRetrieval-augmented generation¬†Zhang et al. (2024); Dong et al. (2025a, b) improves scalability, yet most approaches decouple retrieval and reasoning in a single-shot manner, making them brittle to retrieval errors and ineffective for complex, multi-step queries¬†Zhang et al. (2025).\nRecent agent-based DocQA systems¬†Wu et al. (2025c); Sun et al. (2025a); Dong et al. (2025c) address these issues through iterative document navigation and reading, but they predominantly depend on prompt-engineered, closed-source LLMs.\nIn this work, we aim to train an open-source document agent with learnable behaviors for robust and scalable DocQA.\n\n\nSynthetic Data for Agent Training.\nHigh-quality training data is critical for training agents.\nDue to its scalability, rapid iteration, and inherent trainability, synthetic data offers significant advantages over manually annotated data, serving as a highly effective alternative to human-labeled datasets for agent learning¬†Liu et al. (2025a); Team et al. (2025b).\nPrior work has demonstrated that large-scale agent-synthesized data can be effectively generated for search agents¬†Wu et al. (2025a); Li et al. (2025b); Tao et al. (2025), code agents¬†Yang et al. (2025), GUI agents¬†Sun et al. (2025b); Guo et al. (2025a) and general-purpose agents¬†Fang et al. (2025); Prabhakar et al. (2025).\nIn contrast, this work focuses on the DocQA agent setting.\nExisting DocQA datasets are primarily constructed through semi-automated¬†Van Landeghem et al. (2023); Dong et al. (2025b) or expert-annotated¬†Hendrycks et al. (2021); Deng et al. (2025) processes, both of which require substantial human involvement or result in questions that lack sufficient depth.\nInspired by advances in search agents, we formulate DocQA as an agentic information-seeking problem, with the goal of synthesizing high-quality training data tailored for DocQA agents.\n\n\n\n\n3 Methods\n\n\n3.1 Agent Setup\n\nFramework.\nWe adopt the vanilla ReAct¬†(Yao et al., 2022) as the agent‚Äôs framework, which synergizes reasoning and acting.\nIn this paradigm, the agent generates both a reasoning trace (thought), œÑ\\tau, and a subsequent action, aa, in an interleaved manner.\nThis process forms a trajectory, ‚ÑãT\\mathcal{H}_{T}, which is a sequence of thought-action-observation triplets:\n\n\n\n‚ÑãT=(œÑ0,a0,o0,‚Ä¶,œÑi,ai,oi,‚Ä¶,œÑT,aT),\\mathcal{H}_{T}=(\\tau_{0},a_{0},o_{0},\\dots,\\tau_{i},a_{i},o_{i},\\dots,\\tau_{T},a_{T}),\n\n(1)\n\n\nwhere aTa_{T} represents the final answer to the given task.\nAt any given step t‚â§Tt\\leq T, the agent‚Äôs policy, œÄ\\pi, generates the current thought œÑt\\tau_{t} and action ata_{t} based on the history of all previous interactions, ‚Ñãt"
  },
  {
    "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering",
    "url": "https://arxiv.org/abs/2601.05159v1",
    "source": "arxiv",
    "summary": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current l",
    "full_text": null
  },
  {
    "title": "Learning Mixture Models via Efficient High-dimensional Sparse Fourier Transforms",
    "url": "https://arxiv.org/abs/2601.05157v1",
    "source": "arxiv",
    "summary": "In this work, we give a ${\\rm poly}(d,k)$ time and sample algorithm for efficiently learning the parameters of a mixture of $k$ spherical distributions in $d$ dimensions. Unlike all previous methods, our techniques apply to heavy-tailed distributions and include examples that do not even have finite covariances. Our method succeeds whenever the cluster distributions have a characteristic function ",
    "full_text": null
  },
  {
    "title": "Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art",
    "url": "https://arxiv.org/abs/2601.05152v1",
    "source": "arxiv",
    "summary": "This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adapta",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Our contribution\n1.2 Related Studies\n1.3 Overview\n1.4 Scope of Review\n\n\n\n2 Preliminaries\n\n2.1 Notations\n2.2 Markov Decision Process (MDP)\n2.3 Nonstationarity\n\n2.4 Nonstationary MDPs\n\n2.4.1 POMDP\n2.4.2 HM-MDP\n2.4.3 NSMDP\n2.4.4 DP-MDP\n\n\n\n\n\n3 Problem Formulation\n\n3.1 Learning in a nonstationary environment\n3.2 Adaptation to nonstationarity\n\n3.3 Safety consideration\n\n3.3.1 Formulation safety constraints\n3.3.2 Safety during the adjustment to DS\n\n\n3.4 General problem formulation\n\n\n4 Challenges of continual safe online reinforcement learning\n\n5 Current Solutions\n\n5.1 Passive safety adaptation\n5.2 Reactive safety adaptation\n5.3 Quick safety adaptation\n5.4 Proactive safety adaptation\n\n\n\n6 Details of the algorithms\n\n6.1 Learning\n6.2 Adaptation mechanism\n6.3 Optimization\n6.4 Safety Consideration\n6.5 Environment and non-stationarity type\n\n\n7 Taxonomy of constraints\n8 Discussion\n9 Conclusion\n\n\n\n\n11institutetext: McMaster Centre for Software Certification, Department of Computing and Software, McMaster University, Canada\n11email:  tomashet@mcmaster.ca \n\nSafe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art\n‚Ä†‚Ä†thanks: if necessary\n\n\n\nTimofey Tomashevskiy\n\n\n\nAbstract\nSafety is a vital component for the future development and acceptance of reinforcement\nlearning (RL) methods. Although over the last decades, online reinforcement learning and learning in nonstationary environments have made huge progress, safe continual online reinforcement learning remains one of the most challenging topics of research. Despite numerous recent works in the safe RL field and in continual online RL learning, a systematic understanding of how to establish safety during continual online learning under nonstationary conditions remains limited. This happened because of the complexity of problem that requires a combination of knowledge from different research areas, including safe learning, adaptation to the distribution shift, and safe optimization. Combination of knowledge from these domains is not a straightforward task because of numerous challenges including the consecutive nature of data flow, potential delay of the reward, unpredictable nature of the nonstationarity (NS) dynamics, and difficulties in precise constraint formulation. In this article, we provide a review of existing\ncontinual online safe reinforcement learning (COSRL) methods.\nWe provide the taxonomy of COSRL based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorise safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss challenges and prospects for creating reliable, safe online learning methods.\n\n\n\n\n1 Introduction\n\nReinforcement learning is a method for solving sequential tasks when the agent perceives the states and acts to maximize the long-term return, which is based on a real-valued reward Altman (1999). In real life, aside from the return, safety comes into play. The potential users of the algorithms should be confident not just in performance but also in the safety of the method they apply. While there are many definitions of safety, most of them take uncertainty and risk consideration into account Garcƒ±a and Fern√°ndez (2015).\nTherefore, to avoid or limit the risk and guarantee safety for traditional RL methods, the paradigm of RL was extended, and different types of safety constraints were introduced. Safe reinforcement learning has made great progress during the last decades and is now a relatively mature discipline able to provide safety constraints satisfaction guarantees in many settings.\nHowever, despite the progress achieved, acceptance of RL applications in the real world remains limited. One of the reasons for that is the nonstationary stochastic nature of the real world. Different forms of non-stationarity can\naffect the performance and compromise the safety of the learning agent.\nDistribution shift is one of the most common forms of non-stationarity faced by RL. It appears when training and testing distributions are different. Most existing safe RL (SRL) methods are based on the assumption of stationarity of the environment and underlying Markov process, where the state, reward, action space distribution, and transition dynamics of the environment remain stationary and don‚Äôt change over time. In practice, all these parameters can show some time dependence. Nonstationarity makes it difficult to directly apply traditional stationary reinforcement learning methods in potentially nonstationary environments without some adjustment to nonstationarity conditions. While existing safe online reinforcement methods based on the assumption of stationarityBerkenkamp and Schoellig (2015),Daulton et al. (2022) can guarantee some robustness to non-stationarity, it is not always clear to what extent initial safety constraints can be respected during and after the distribution shift. Successful adjustment of safe reinforcement learning to nonstationary environments requires adaptation of both: the performance of the agent, i.e, adaptation of the reinforcement learning mechanism, and adaptation of the safety constraints, as initial constraints can become inefficient or unsafe in a new, changed environment. \nAdaptation of unconstrained RL methods to nonstationarity has been a subject of intensive research over the last few years, including continual learning Brunskill and Li (2014); Chandak et al. (2020b); Parisi et al. (2019); Abel et al. (2018) and meta-learning Al-Shedivat et al. (2017); Xie et al. (2020); Vanschoren (2019) directions.\nAt the same time, the number of studies specifically focused on safe continual reinforcement learning under non-stationarity remains limited because of the complicated nature of the subject that lies at the intersection of sequential learning, adaptation to non-stationarity, and safety constraints satisfaction.\n\n\nThe research studies related to adaptation to nonstationarity, while satisfying constraints, are scattered across different domains of RL, including RL meta-learning Wang et al. (2023); Finn et al. (2017); Rakelly et al. (2019), methods of predictive control, constrained Markov decision processes (CMDP) Altman (1999, 2021); Borkar and Jain (2014); Wachi and Sui (2020), and Bayesian exploration/optimization Berkenkamp et al. (2023); Ghavamzadeh et al. (2015); Mockus (2002, 2005). Existing works study adaptation to different forms of NS, such as active, passive, and mixed NS Chandak (2022). Most of them are primarily focused on the adaptation and high performance of the learning agent, rather than on satisfying safety constraints. Some of them respect constraint satisfaction but don‚Äôt pay much attention to performance or to speed of NS-adaption. We aim to bring knowledge of learning safety and adaptation together to shed light on COSRL that can facilitate future research.\n\n\n\n1.1 Our contribution\n\nIn this paper, we review and evaluate current state-of-the-art approaches to the safety of continual online RL under non-stationarity.\nWe discuss existing adaptation mechanisms, optimization methods, and types of safety constraints. We provide a taxonomy of existing safe online learning methods based on the adaptation mechanism. We also provide a taxonomy and detailed review of the state-of-the-art safety constraints formulation and discuss possible directions of constraints formulation for continual online safe RL methods.\n\n\n\n\n1.2 Related Studies\n\nSafe reinforcement learning is a general and flexible concept that covers optimization and safety and exists in many different settings, including online and offline learning, model-based, model-free, stationary, or nonstationary, to name just a few. Several previous papers covered different aspects of safe RL. The comprehensive survey of Garcia and Fernandez Garcƒ±a and Fern√°ndez (2015) covers safety approaches and provides a taxonomy of safe RL algorithms. Shangding et al. Gu et al. (2022) covered five fundamental problems of safe RL and problems related to the practical application of reinforcement learning. Embedding soft constraints in policy optimization was covered by Altman (2021); Wachi and Sui (2020); Kim et al. (2020) surveys. Liu et al. (2021b) reviews the state of research in constrained policy optimization for model-free algorithms. Several research works covered particular methods in RL, classified by the type of learning or by the type of applications. Hospedales et al. (2021) reviewed safe meta-learning methods. Brunke et al. (2022) provided a survey of safe RL methods in Robotics. All these studies focus on safety for reinforcement learning methods based on the stationarity assumption of the environment, and don‚Äôt cover safe continual online learning under nonstationarity. Padakandla (2021); Khetarpal et al. (2022) outline approaches for RL adaptation for non-stationarity, but don‚Äôt tackle safety constraints during or after the adaptation. Concise but very well focused on safety constraints Wachi et al. (2024) provides a great review of constraints formulation; however, this work does not specifically cover constraints formulation for nonstationary environments or online learning algorithms. Our survey aims to fulfill these gaps and provide a review of the state-of-the-art methods and theoretical aspects of building safe continual reinforcement learning algorithms able to work in nonstationary environments.\n\n\n\n\n1.3 Overview\n\nThe rest of this review is organized as follows. In Section 2, we provide preliminaries for safe continual reinforcement learning. We provide definitions, notations, and explain the main concepts required for understanding COSR.\nSection 3 provides problem formulation. In this section, we explain three key components of COSRL.\nSection 4 describes challenges of building safe online RL algorithms.Section 5 categorises current solutions based on safety adaptation techniques.\nSection "
  },
  {
    "title": "ROOFS: RObust biOmarker Feature Selection",
    "url": "https://arxiv.org/abs/2601.05151v1",
    "source": "arxiv",
    "summary": "Feature selection (FS) is essential for biomarker discovery and in the analysis of biomedical datasets. However, challenges such as high-dimensional feature space, low sample size, multicollinearity, and missing values make FS non-trivial. Moreover, FS performances vary across datasets and predictive tasks. We propose roofs, a Python package available at https://gitlab.inria.fr/compo/roofs, design",
    "full_text": null
  },
  {
    "title": "Atlas 2 -- Foundation models for clinical deployment",
    "url": "https://arxiv.org/abs/2601.05148v1",
    "source": "arxiv",
    "summary": "Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art perfo",
    "full_text": null
  },
  {
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "url": "https://arxiv.org/abs/2601.05144v1",
    "source": "arxiv",
    "summary": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.05144v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.05144v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 8 Jan 2026]\n    Title:Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models\n    Authors:Shuliang Liu, Xingyu Li, Hongyi Liu, Yibo Yan, Bingchen Duan, Qi Zheng, Dong Fang, Lingfeng Su, Xuming Hu            View a PDF of the paper titled Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models, by Shuliang Liu and 8 other authors\n    View PDF\n\n\n\n    \n            Abstract:Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.05144 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05144v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05144\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Shuliang Liu [view email]          [v1]\n        Thu, 8 Jan 2026 17:32:22 UTC (6,736 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models, by Shuliang Liu and 8 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n      "
  }
]