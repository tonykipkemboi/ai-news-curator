[
  {
    "title": "How to Code Claude Code in 200 Lines of Code",
    "url": "https://www.mihaileric.com/The-Emperor-Has-No-Clothes/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Seems everyone is working on the same things these days. I built a persistent Python REPL subprocess as an MCP tool for CC, it worked so insanely well that I decided to go all the way. I already had an agentic framework built around tool calling (agentlib), so I adapted it for this new paradigm and code agent was born.<p>The agent &quot;boots up&quot; inside the REPL. Here&#x27;s the beginning of the system prompt:<p><pre><code>  &gt;&gt;&gt; help(assistant)\n\n  You are an interactive coding assistant operating within a Python REPL.\n  Your responses ARE Python code—no markdown blocks, no prose preamble.\n  The code you write is executed directly.\n\n  &gt;&gt;&gt; how_this_works()\n\n  1. You write Python code as your response\n  2. The code executes in a persistent REPL environment\n  3. Output is shown back to you IN YOUR NEXT TURN\n  4. Call `respond(text)` ...\n</code></pre>\nYou get the idea. No need for custom file editing tools--Python has all that built in and Claude knows it perfectly. No JSON marshaling or schema overhead. Tools are just Python functions injected into the REPL, zero context bloat.<p>I also built a browser control plugin that puts Claude directly into the heart of a live browser session. It can inject element pickers so I can click around and show it what I&#x27;m talking about. It can render prototype code before committing to disk, killing the annoying build-fix loop. I can even SSH in from my phone and use TTS instead of typing, surprisingly great for frontend design work. Knocked out a website for my father-in-law&#x27;s law firm (gresksingleton.com) in a few hours that would&#x27;ve taken 10X that a couple years ago, and it was super fun.<p>The big win: complexity. CC has been a disaster on my bookkeeping system, there&#x27;s a threshold past which Claude loses the forest for the trees and makes the same mistakes over and over. Code agent pushes that bar out significantly. Claude can build new tools on the fly when it needs them. Gemini works great too (larger context).<p>Have fun out there! &#x2F;end-rant",
      "Something I would add is planning. A big &quot;aha&quot; for effective use of these tools is realizing they run on dynamic TODO lists. Ex: Plan mode is basically bootstrapping how that TODO list gets seeded and how todos ground themselves when they get reached, and user interactions are how you realign the todo lists. The todolist is subtle but was a big shift in coding tools, and many seem to be surprised when we discuss it -- most seem to focus on whether to use plan mode or not, but todo lists will still be active. I ran a fun experiment last month on how well claude code solves CTFs, and disabling the TodoList tool and planning is 1-2 grade jumps: <a href=\"https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-breaking-bots-cheating-at-blue-team-ctfs-with-ai-speed-runs\" rel=\"nofollow\">https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-breaking-bots-cheating-at-blue-t...</a> .<p>Fwiw, I found it funny how the article stuffs &quot;smarter context management&quot; into a breeze-y TODO bullet point at the end for going production-grade. I&#x27;ve been noticing a lot of NIH&#x2F;DIY types believing they can do a good job of this and then, when forced to have results&#x2F;evals that don&#x27;t suck in production, losing the rest of the year on that step. (And even worse when they decide to fine-tune too.)",
      "It&#x27;s a great point and everyone should know it: the core of a coding agent is really simple, it&#x27;s a loop with tool calling.<p>Having said that, I think if you&#x27;re going to write an article like this and call it &quot;The Emperor Has No Clothes: How to Code Claude Code in 200 Lines of Code&quot;, you should <i>at least include a reference</i> to Thorsten Ball&#x27;s excellent article from wayyy back in April 2025 entitled &quot;How to Build an Agent, or: The Emperor Has No Clothes&quot; (<a href=\"https:&#x2F;&#x2F;ampcode.com&#x2F;how-to-build-an-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;ampcode.com&#x2F;how-to-build-an-agent</a>)! That was (as far as I know) the first of these articles making the point that the core of a coding agent is actually quite simple (and all the <i>deep</i> complexity is in the LLM). Reading it was a light-bulb moment for me.<p>FWIW, I agree with other commenters here that you do need quite a bit of additional scaffolding (like TODOs and much more) to make modern agents work well. And Claude Code itself is a fairly complex piece of software with a lot of settings, hooks, plugins, UI features, etc. Although I would add that once you have a minimal coding agent loop in place, you can get it to bootstrap its own code and add those things! That is a fun and slightly weird thing to try.<p>(By the way, the &quot;January 2025&quot; date on this article is clearly a typo for 2026, as Claude Code didn&#x27;t exist a year ago and it includes use of the claude-sonnet-4-20250514 model from May.)<p>Edit: and if you&#x27;re interested in diving deeper into what Claude Code <i>itself</i> is doing under the hood, a good tool to understand it is &quot;claude-trace&quot; (<a href=\"https:&#x2F;&#x2F;github.com&#x2F;badlogic&#x2F;lemmy&#x2F;tree&#x2F;main&#x2F;apps&#x2F;claude-trace\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;badlogic&#x2F;lemmy&#x2F;tree&#x2F;main&#x2F;apps&#x2F;claude-trac...</a>). You can use it to see the whole dance with tool calls and the LLM: every call out to the LLM and the LLM&#x27;s responses, the LLM&#x27;s tool call invocations and the responses from the agent to the LLM when tools run, etc. When Claude Skills came out I used this to confirm my guess about how they worked (they&#x27;re a tool call with all the short skill descriptions stuffed into the tool description base prompt). Reading the base prompt is also interesting. (Among other things, they explicitly tell it not to use emoji, which tracks as when I wrote my own agent it was indeed very emoji-prone.)",
      "This is cool but as someone that&#x27;s built an enterprise grade agentic loop in-house that&#x27;s processing a billion plus tokens a month, there are so many little things you have to account for that greatly magnify complexity in real world agentic use cases. For loops are an easy way to get your foot in the door and is indeed at the heart of it all, but there are a multitude of a little things that compound complexity rather quickly. What happens when a user sends a message after the first one and the agent has already started the tool loop? Seems simple, right? If you are receiving inputs via webhooks (like from a Slack bot), then what do you do? It&#x27;s not rocket science but it&#x27;s also not trivial to do right. What about hooks (guardrails) and approvals? Should you halt execution mid-loop and wait or implement it as an async Task feature like Claude Code and the MCP spec? If you do it async then how do you wake the agent back up? Where is the original tool call stored and how is the output stored for retrieval&#x2F;insertion? This and many other little things add up and compound on each other.<p>I should start a blog with my experience from all of this.",
      "There&#x27;s a bit more to it!<p>For example, the agent in the post will demonstrate &#x27;early stopping&#x27; where it finishes before the task is really done. You&#x27;d think you can solve this with reasoning models, but it doesn&#x27;t actually work on SOTA models.<p>To fix &#x27;early stopping&#x27; you need extra features in the agent harness. Claude Code does this with TODOs that are injected back into every prompt to remind the LLM what tasks remain open. (If you&#x27;re curious somewhere in the public repo for HolmesGPT we have benchamrks with all the experiments we ran to solve this - from hypothesis tracking to other exotic approaches - but TODOs always performed best.)<p>Still, good article. Agents really are just tools in a loop. It&#x27;s not rocket science.",
      "&gt; This is the key insight: we’re just telling the LLM “here are your tools, here’s the format to call them.” The LLM figures out when and how to use them.<p>This really blew my mind back then in the ancient times of 2024-ish. I remember the idea of agents just reached me and I started reading various &quot;here I built an agent that does this&quot; articles, and I was really frustrated at not understanding how the hell LLM &quot;knows&quot; how to call a tool, it&#x27;s a program, but LLMs just produce text! Yes I see you are telling LLM about tools, but what&#x27;s next? And then when I finally understood that there&#x27;s no next, no need to do anything other than explaining — it felt pretty magical, not gonna lie.",
      "This article was more true than not a year ago but now the harnesses are so far past the simple agent loop that I&#x27;d argue that this is not even close to an accurate mental model of what claude code is doing.",
      "I&#x27;m surprised this post has so many upvotes. This is a gross oversimplification of what Claude Code (and other agents can do). On top of that, it&#x27;s very poorly engineered.",
      "The &quot;200 lines&quot; loop is a good demo of the shape of a coding agent, but it’s like &quot;a DB is a B-tree&quot; - technically true, operationally incomplete.<p>The hard part isn’t the loop - it’s the boring scaffolding that prevents early stopping, keeps state, handles errors, and makes edits&#x2F;context reliable across messy real projects.",
      "We (the SWE-bench team) have a 100 line of code agent that is now pretty popular in both academic and industry labs: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;SWE-agent&#x2F;mini-swe-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SWE-agent&#x2F;mini-swe-agent</a><p>I think it&#x27;s a great way to dive into the agent world"
    ],
    "full_text": null
  },
  {
    "title": "Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU",
    "url": "https://github.com/samuel-vitorino/sopro",
    "source": "hn",
    "summary": "",
    "comments": [
      "That&#x27;s cool and useful.<p>IMO, the best alternative is Chatterbox-TTS-Server [0] (slower, but quite high quality).<p>[0] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;devnen&#x2F;Chatterbox-TTS-Server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;devnen&#x2F;Chatterbox-TTS-Server</a>",
      "Super nice! I&#x27;ve been using Kokoro locally, which is 82M parameters and runs (and sounds) amazing! <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;hexgrad&#x2F;Kokoro-82M\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;hexgrad&#x2F;Kokoro-82M</a>",
      "What is &quot;zero-shot&quot; supposed to mean?",
      "Mission impossible cloning skills without the long compile time.<p>&quot;The pleasure of Buzby&#x27;s company is what I most enjoy. He put a tack on Miss Yancy&#x27;s chair ...&quot;<p><a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H2kIN9PgvNo\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=H2kIN9PgvNo</a><p><a href=\"https:&#x2F;&#x2F;literalminded.wordpress.com&#x2F;2006&#x2F;05&#x2F;05&#x2F;a-panphonic-poem-for-mission-impossible-3&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;literalminded.wordpress.com&#x2F;2006&#x2F;05&#x2F;05&#x2F;a-panphonic-p...</a>",
      "Tried english. There are similarities. Really impressive for such budget\nAlso increadibly easy to use, thanks for this",
      "It&#x27;s impressive given the constraints!<p>Would you consider releasing a more capable version that renders with fewer artifacts (and maybe requires a bit more processing power)?<p>Chatterbox is my go-to, this could be a nice alternative were it capable of high-fidelity results!",
      "This is very cool! And it&#x27;ll only get better. \nI do wonder, if, at least as a patch-up job, they could do some light audio processing to remove the raspiness from the voices.",
      "What does &quot;zero-shot&quot; mean in this context?",
      "I don&#x27;t understand the comments here at all. I played the audio and it sounds absolutely horrible, far worse than computer voices sounded fifteen years ago. Not even the most feeble minded person would mistake that as a human. Am I not hearing the same thing everyone else is hearing? It sounds straight up corrupted to me. Tested in different browsers, no difference.",
      "I&#x27;m sure it has its uses, but for anything with a higher requirement for quality, I think Vibe Voice is the only real OSS cloning option.<p>F2&#x2F;E5 are also very good but have plenty of bad runs, you need to keep re-rolling until you get good outputs."
    ],
    "full_text": null
  },
  {
    "title": "1ML for non-specialists: introduction",
    "url": "https://pithlessly.github.io/1ml-intro",
    "source": "hn",
    "summary": "",
    "comments": [
      "Title is &quot;1ML for non-specialists: introduction&quot;.<p>From the article:<p>&gt; 1ML is a type system designed by Andreas Rossberg and described in a ollection of papers by him",
      "&gt; communication barrier between academics who are in a position to discuss 1ML in depth and people who are in a position to write new compilers<p>I think there is s.th. wrong when people working on type systems can&#x27;t write compilers.",
      "1ML, not 1M"
    ],
    "full_text": null
  },
  {
    "title": "AI coding assistants are getting worse?",
    "url": "https://spectrum.ieee.org/ai-coding-degrades",
    "source": "hn",
    "summary": "",
    "comments": [
      "One thing I find really funny is when AI enthusiasts make claims about agents and their own productivity its always entirely anecdotally based on their own subjective experience, but when others make claims to the contrary suddenly there is some overwhelming burden of proof that has to be reached in order to make any sort of claims regarding the capabilities of AI workflows. So which is it?",
      "They are not worse - the results are not repeatable. The problem is much worse.<p>Like with cab hailing, shopping, social media ads, food delivery, etc: there will be a whole ecosystem, workflows, and companies built around this. Then the prices will start going up with nowhere to run. Their pricing models are simply not sustainable. I hope everyone realizes that the current LLMs are subsidized, like your Seamless and Uber was in the early days.",
      "This seems like a kind of odd test.<p>&gt; I wrote some Python code which loaded a dataframe and then looked for a nonexistent column.<p><pre><code>    df = pd.read_csv(‘data.csv’)    \n    df[&#x27;new_column&#x27;] = df[&#x27;index_value&#x27;] + 1\n   #there is no column ‘index_value’\n</code></pre>\n&gt; I asked each of them [the bots being tested] to fix the error, specifying that I wanted completed code only, without commentary.<p>&gt; This is of course an impossible task—the problem is the missing data, not the code. So the best answer would be either an outright refusal, or failing that, code that would help me debug the problem.<p>So his hoped-for solution is that the bot should defy his prompt (since refusal is commentary), and not fix the problem.<p>Maybe instructability has just improved, which is a problem for workflows that depend on misbehavior from the bot?<p>It seems like he just prefers how GPT-4 and 4.1 failed to follow his prompt, over 5. They are all hamstrung by the fact that the task is impossible, and they aren’t allowed to provide commentary to that effect. Objectively, 4 failed to follow the prompts in 4&#x2F;10 cases and made nonsense changes in the other 6; 4.1 made nonsense changes; and 5 made nonsense changes (based on the apparently incorrect guess that the missing ‘index_value’ column was supposed to hold the value of the index).",
      "I like AI for software development.<p>Sometimes I am uncertain whether it&#x27;s an absolute win. Analogy: I used to use Huel to save time on lunches to have more time to study. Turns out, lunches were not just refueling sessions but ways to relax. So I lost on that relaxation time and it ended up being +-0 long-term.<p>AI for sure is net positive in terms of getting more done, but it&#x27;s way too easy to gloss over some details and you&#x27;ll end up backtracking more.<p>&quot;Reality has a surprising amount of detail&quot; or something along those lines.",
      "I am used to seeing technical papers from ieee, but this is an opinion piece? I mean, there is some anecdata and one test case presented to a few different models but nothing more.<p>I am not necessarily saying the conclusions are wrong, just that they are not really substantiated in any way",
      "The quality variation from month to month has been my experience too. I&#x27;ve noticed the models seem to &quot;forget&quot; conventions they used to follow reliably - like proper error handling patterns or consistent variable naming.<p>What&#x27;s strange is sometimes a fresh context window produces better results than one where you&#x27;ve been iterating. Like the conversation history is introducing noise rather than helpful context. Makes me wonder if there&#x27;s an optimal prompt length beyond which you&#x27;re actually degrading output quality.",
      "They are not getting worse, they are getting better. You just haven&#x27;t figured out the scaffolding required to elicit good performance from this generation. Unit tests would be a good place to start for the failure mode discussed.<p>As others have noted, the prompt&#x2F;eval is also garbage. It’s measuring a non-representative sub-task with a weird prompt that isn’t how you’d use agents in, say, Claude Code. (See the METR evals if you want a solid eval giving evidence that they are getting better at longer-horizon dev tasks.)<p>This is a recurring fallacy with AI that needs a name. “AI is dumber than humans on some sub-task, therefore it must be dumb”. The correct way of using these tools is to understand the contours of their jagged intelligence and carefully buttress the weak spots, to enable the super-human areas to shine.",
      "I always wonder what happens when LLMs finally destroyed every source of information they crawl. After stack overflow and forums are gone and when there&#x27;s no open source code anymore to improve upon. Won&#x27;t they just canibalize themselves and slowly degrade?",
      "I speculate LLMs providers are serving smallers models dynamically to follow usage spikes, and need for computes to train new models.  \nI did observed that models agents are becoming worse over time, especially before a new model is released.",
      "&gt; If an assistant offered up suggested code, the code ran successfully, and the user accepted the code, that was a positive signal, a sign that the assistant had gotten it right. If the user rejected the code, or if the code failed to run, that was a negative signal, and when the model was retrained, the assistant would be steered in a different direction.<p>&gt; This is a powerful idea, and no doubt contributed to the rapid improvement of AI coding assistants for a period of time. But as inexperienced coders started turning up in greater numbers, it also started to poison the training data.<p>It is not just `inexperienced coders` that make this signal pretty much useless, I mostly use coding assistants for boilerplate, I will accept the suggestion then delete much of what it produced, especially in the critical path.<p>For many users, this is much faster then trying to get another approximation<p><pre><code>     :,&#x2F;^}&#x2F;-d\n</code></pre>\nSame for `10dd` etc... it is all muscle memory. Then again I use a local fill in the middle, tiny llm now, because it is good enough for most of the speedup without the cost&#x2F;security&#x2F;latency of a hosted model.<p>It would be a mistake to think that filtering out jr devs will result in good data as the concept is flawed in general. Accepting output may not have anything to do with correctness of the provided content IMHO."
    ],
    "full_text": null
  },
  {
    "title": "He was called a 'terrorist sympathizer.' Now his AI company is valued at $3B",
    "url": "https://sfstandard.com/2026/01/07/called-terrorist-sympathizer-now-ai-company-valued-3b/",
    "source": "hn",
    "summary": "",
    "comments": [
      "replit is actually quite popular among teenagers and basically third world youngsters trying to spin off a service or a &quot;product&quot; of their own.<p>- i mean yes u cannot make money out of teenagers but damn replit&#x27;s Vibe coding tool is fucking good. Better than Lovable or Bolt any day.<p>just to give u a perspective from a 20year old kid from a 3rd world county",
      "The title is a non-sequitur.<p>“Terrorist sympathizer” and “successful businessperson” (or “rich person”) are completely orthogonal. Building a successful business does not necessarily change your terrorist sympathisation status. You can be a rich terrorist sympathiser.",
      "well, it&#x27;s not a high bar – these days anyone who says &quot;I support Palestine Action&quot; or &quot;she was murdered by ICE&quot; is called a terrorist sympathizer",
      "It&#x27;s fascinating to read how Hacker News helped make Replit successful. I hope everyone will try this tool! I wonder if Masad still scrolls here nowadays.",
      "Are we still doing these kinds of lionizing puff pieces after SBF, Holmes, Musk and all the others? By now, I consider being featured in one a negative signal.",
      "I absolutely love the idea of Replit and I think it&#x27;s an awesome platform and idea.<p>I do wonder how sustainable it is as a business though. I expect Replit is sending the majority of that money to the big AI labs through API costs<p>As soon as anything becomes serious you&#x27;re going to try and take it off Replit and use something like Claude Code and AWS etc",
      "So success buys you ideological latitude",
      "Who in this current political climate hasn&#x27;t been called a &#x27;terrorist sympathizer&#x27;? Feels like 80% of the population qualify",
      "Reading through this piece and all I can think of is how he&#x27;s just the other side of the same coin. Simply a different color of the same elitism that our world is moving into as money concentrates and starts to meddle more and more with our political spheres while accountability slowly errodes to zero.",
      "Of all the tools I try and review, replit remains to be simply the worst in my opinion. I struggle to do anything useful with it except trivial hello world type of stuff. The bubble is real."
    ],
    "full_text": null
  },
  {
    "title": "Anthropic blocks third-party use of Claude Code subscriptions",
    "url": "https://github.com/anomalyco/opencode/issues/7410",
    "source": "hn",
    "summary": "",
    "comments": [
      "For folks not following the drama: Anthropic&#x27;s $200&#x2F;month subscription for Claude Code is <i>much</i> cheaper than Anthropic&#x27;s pay-as-you-go API. In a month of Claude Code, it&#x27;s easy to use so many LLM tokens that it would have cost you more than $1,000 if you&#x27;d paid via the API.<p>Why is Anthropic offering such favorable pricing to subscribers? I dunno. But they <i>really</i> want you to use the Claude Code™ CLI with that subscription, not the open-source OpenCode CLI. They want OpenCode users to pay API prices, which could be 5x or more.<p>So, of course, OpenCode has implemented a workaround, so that folks paying &quot;only&quot; $200&#x2F;month can use their preferred OpenCode CLI at Anthropic&#x27;s all-you-can-eat token buffet.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;issues&#x2F;7410#issuecomment-3727177435\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;issues&#x2F;7410#issuecomme...</a><p>Everything about this is ridiculous, and it&#x27;s all Anthropic&#x27;s fault. Anthropic shouldn&#x27;t <i>have</i> an all-you-can-eat plan for $200 when their pay-as-you-go plan would cost more than $1,000+ for comparable usage. Their subscription plans should just sell you API credits at, like, 20% off.<p>More importantly, Anthropic should have open sourced their Claude Code CLI a year ago. (They can and should just open source it now.)",
      "This is an unusual L for Anthropic. The unfortunate truth is that the engineering in opencode is so far ahead of Claude Code. Obviously, CC is a great tool, but that&#x27;s more about the magic of the model than the engineering of the CLI.<p>The opencode team[^1][^2] built an entire custom TUI backend that supports a good subset of HTML&#x2F;CSS and the TypeScript ecosystem (i.e. not tied to Opencode, a generic TUI renderer). Then, they built the product as a client&#x2F;server, so you can use the agent part of it for whatever you want, separate from the TUI. And THEN, since they implemented the TUI as a generic client, they could also build a web view and desktop view over the same server.<p>It also doesn&#x27;t flicker at 30 FPS whenever it spawns a subagent.<p>That&#x27;s just the tip of the iceberg. There are so many QoL features in opencode that put CC to shame. Again, CC is a magical tool, but the actual nuts and bolts engineering of it is pretty damning for &quot;LLMs will write all of our code soon&quot;. I&#x27;m sorry, but I&#x27;m a decent-systems-programmer-but-terminal-moron and I cranked out a raymarched 3D renderer in the terminal for a Claude Wrapped[^] in a weekend that...doesn&#x27;t flicker. I don&#x27;t mean that in a look-at-me way. I mean that in a &quot;a mid-tier systems programmer isn&#x27;t making these mistakes&quot; kind of way.<p>Anyway, this is embarrassing for Anthropic. I get that opencode shouldn&#x27;t have been authenticating this way. I&#x27;m not saying what they are doing is a rug pull, or immoral. But there&#x27;s a reason people use this tool instead of your first party one. Maybe let those world class systems designers who created the runtime that powers opencode get their hands on your TUI before nicking something that is an objectively better product.<p>[^1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opentui\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opentui</a><p>[^2] From my loose following of the development, not a monolith, and the person mostly responsible for the TUI framework is <a href=\"https:&#x2F;&#x2F;x.com&#x2F;kmdrfx\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;kmdrfx</a><p>[^3] <a href=\"https:&#x2F;&#x2F;spader.zone&#x2F;wrapped&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;spader.zone&#x2F;wrapped&#x2F;</a>",
      "This headline is misleading. EDIT: Or rather was, at it has now been edited to be accurate.<p>You can still bring your own Anthropic API key and use Claude in OpenCode.<p>What you can no longer do is reverse engineer undocumented Anthropic APIs and spoof being a Claude Code client to use an OAuth token from a subscription-based Anthropic account.<p>This really sucks for people who want a thriving competitive market of open source harnesses since BYOK API tokens mean paying a substantial premium to use anything but Anthropic&#x27;s official clients.<p>But it&#x27;s hard to say it&#x27;s surprising or a scandal, or anything terribly different from what tons of other companies have done in the past. I&#x27;d personally advise people to expect everything about using frontier coding models becoming much more pay-to-play.",
      "This will piss a lot of people off, and seems like a strange move. I get that this was always a hack and against the ToS. But I&#x27;ve been paying Anthropic money every month to do exactly what I would have done with Claude Code, but in another harness that I like better. All they&#x27;ve achieved here is that I am no longer giving them money. Their per-token pricing is really expensive compared to OpenAI, and I like the results from the OpenAI models better too, they&#x27;re just very slow.<p>Here&#x27;s a good benchmark from the brokk team showing the performance per dollar, GPT-5.1 is around half the price of Opus 4.5 for the same performance, it just takes twice as long.<p><a href=\"https:&#x2F;&#x2F;brokk.ai&#x2F;power-ranking?dataset=openround&amp;models=flash3%2Cgp3%2Cgpt5-codex%2Cgpt5-high%2Cgpt5-mini-high%2Cgpt5.1%2Chaiku4.5%2Copus4.5%2Csonnet4.5%2Csonnet4.5-high\" rel=\"nofollow\">https:&#x2F;&#x2F;brokk.ai&#x2F;power-ranking?dataset=openround&amp;models=flas...</a><p>So as of today, my money is going to OpenAI instead of Anthropic. They probably don&#x27;t care though, I suspect that not many users are sufficiently keen on alternative harnesses to make a difference to their finances. But by the same token (ha ha), why enforce this? I don&#x27;t understand why it&#x27;s so important to them that I&#x27;m using Claude Code instead of something else.",
      "The API is not banned only using the Claude Code subscription is<p>I actually tried this several months back to do a regular API request using the CC subscription token and it gave the same error message<p>So this software must have been pretending to be Claude Code in order to get around that<p>A Claude Code subscription should not work with other software, I think this is totally fair",
      "This makes total sense to me. Limiting the usage to their tooling means they can place reasonable limits on usage by controlling how the client interacts with the LLM and making those calls as efficient as possible. The current state of things didn&#x27;t really feel sustainable.",
      "I feel like I&#x27;m the only person on this site that doesn&#x27;t use AI for coding. I guess there&#x27;s probably a lot of other people that haven&#x27;t commented on this story who don&#x27;t use it either. But when I read about how much hype and all that sort of stuff there is in the AI industry, and then I see the amount of posts and commentary and deep technical discussion about how this feature has affected people, I&#x27;m not so sure. Everyone I know hates AI and how it&#x27;s been shoved into every corner of our lives, but I look here and it&#x27;s insanely popular.\nAnyway, sorry this was a very off topic comment. It&#x27;s just very interesting to me that the hype isn&#x27;t all just hype.",
      "Engineer working on Amp here.<p>I&#x27;m very surprised that it took them this long to crack down on it. It&#x27;s been against the terms of service from the start. When I asked them back in March last year whether individuals can use the higher rate limits that come with the Claude Code subscription in other applications, that was also a no.<p>Question is: what changed? New founding round coming up, end of fiscal year, planning for IPO? Do they have to cut losses?<p>Because the other surprise here is that apparently most people don&#x27;t know the true cost of tokens and how much money Anthropic is losing with power users of Claude Code.",
      "The fix has been merged in <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode-anthropic-auth&#x2F;pull&#x2F;11\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode-anthropic-auth&#x2F;pull&#x2F;11</a>, and PR <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;pull&#x2F;7432\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode&#x2F;pull&#x2F;7432</a> is open to bump the version.<p>Until it&#x27;s released, here&#x27;s a workaround:<p>1. git clone <a href=\"https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode-anthropic-auth.git\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anomalyco&#x2F;opencode-anthropic-auth.git</a><p>2. Add to ~&#x2F;.config&#x2F;opencode&#x2F;opencode.json:\n   &quot;plugin&quot;: [&quot;file:&#x2F;&#x2F;&#x2F;path&#x2F;to&#x2F;opencode-anthropic-auth&#x2F;index.mjs&quot;]<p>3. Run: OPENCODE_DISABLE_DEFAULT_PLUGINS=true opencode",
      "FWIW this isn’t new, using a Claude&#x2F;Max subscription auth token as a general-purpose “API key” has been known (and blocked) for ages. OpenCode basically had to impersonate the official Claude Code client to make that work, and it always felt like a loophole that would get patched eventually.<p>This is exactly why (when OpenCode and Charm&#x2F;Crush started diverging) Charm chose not to support “use your Claude subscription” auth and went in a different direction (BYOK &#x2F; multi-provider &#x2F; etc). They didn’t want to build a product on top of a fragile, unofficial auth path.<p>And I think there’s a privacy&#x2F;policy reason tightening this now too: the recent Claude Code update (2.1-ish) pops a “Help improve Claude” prompt in the terminal. If you turn that ON, retention jumps from 30 days to up to 5 years for new&#x2F;resumed chats&#x2F;coding sessions (and data can be used for model improvement). If you keep it OFF, you stay on the default 30-day retention. You can also delete data anytime in settings. That consent + retention toggle is hard to enforce cleanly if you’re not in an official client flow, so it makes sense they’re drawing a harder line."
    ],
    "full_text": null
  },
  {
    "title": "Systematically Improving Espresso: Mathematical Modeling and Experiment (2020)",
    "url": "https://www.cell.com/matter/fulltext/S2590-2385(19)30410-2",
    "source": "hn",
    "summary": "",
    "comments": [
      "There is one important thing missing in the paper:<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46526933\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46526933</a>",
      "I’ll wait for the Lance Hedrick &#x2F; James Hoffmann video.",
      "I think they’ve modeled espresso channeling? It’s well known by hobbyists. Although, they’ve quantified things nicely, and anyway having a result in the record that matches your gut is great!",
      "For an academic coffee paper, it is better than many.<p>A common sin remains: like most coffee papers, I was unable to find calibration procedures in the methods or supplementary sections for the espresso brewing instrument whose performance may vary between runs, days, or users. In this case, they claim&#x2F;assume &quot;The Opera allows for precise control of shot time, water pressure (PW), and temperature&quot;). As a Decent owner, I&#x27;m less familiar with the Opera, but for either machine I would want to disprove any confounding variables by attaching independent sensors. Decent has openly discussed hurdles they&#x27;ve confronted for consistency and accurate measurement.<p>Their main takeaways, though, are interesting and track with how many now prefer to extract:<p>As we demonstrated in Figure 3, our model informs us that a reduction in dry coffee mass results in an increased EYmax (shown schematically in blue in Figure 6). Thus, a barista is able to achieve highly reproducible espresso with the same EY as the 20 g espresso by reducing the coffee mass to 15 g and counter-intuitively grinding much coarser (as shown in red, Figure 6B). This modification may result in very fast shots (&lt;15 s), a reduction in espresso concentration, and a different flavor profile.<p>...<p>Beyond sensory science studies, a persistent difficulty is that there is no rapid route\nto assessing the quality of two identical EYs made with different grind settings or\nbrew parameters. It is clear that espresso made at 22% EY in the partially clogged\nregime tastes more ‘‘complex’’ than a fast 22% EY obtained using the optimization\nroutine presented in Figure 6. In an attempt to recover the same flavor profile as\nthe partially clogged flow regime, a shot must contain a mixture of higher and lower\nextractions. Consider the tasty point in Figure 7: One can approximate its flavor pro-\nfile by blending two shots: (1) a low extraction&#x2F;high dose (purple point) and (2) a high\nextraction&#x2F;low dose (green point). This procedure can more economically yield a\nshot with a flavor profile that should approximate that which was previously only\nobtainable in an economically inefficient partially clogged shot. Blending shots\ndoes double the total volume of the beverage, and the procedure comes with the\nadded combinatorial complexity associated with calibrating two shots that, when\nmixed together, yield superior flavor. We expect only the most enthusiastic practi-\ntioners would consider this approach, but it may well be actionable in an industrial\nsetting where extraction is carried out in bulk.",
      "I thought this was going to be about logic minimization. Was severely disappointed",
      "MORE TO EXPLORE<p>Coffee. Vols. 1–6. R. J. Clarke and R. Macrae. Elsevier Applied Science, 1985.<p>Coffee: Botany, Biochemistry and Production of Beans and Beverage. M. N. Clifford and K. C. Willson. Croom Helm, London, 1985.<p>Caffeine, Coffee and Health. Edited by S. Garattini. Raven Press, 1993.<p>Coffee: Recent Developments. R. J. Clarke and O. Vitzthum. Blackwell Science, 2001.<p>Espresso Coffee: The Science of Quality. Second edition. A. Illy and R. Viani. Academic Press, 2005.<p>Association for Science and Information on Coffee: www.asic-cafe.org (sadly now a spam&#x2F;gambling site)<p>International Coffee Organization: www.ico.org (seems to have a bad cert now?)<p>News from the industry of specialty coffee: www.scaa.org&#x2F;chronicle&#x2F;category&#x2F;coffee-science (also dead)<p>from <a href=\"https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;the-science-of-a-perfect-cup-of-coffee&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.scientificamerican.com&#x2F;article&#x2F;the-science-of-a-...</a>",
      "a parameterization would be helpful for machine designers probably",
      "[dead]"
    ],
    "full_text": null
  },
  {
    "title": "Google AI Studio is now sponsoring Tailwind CSS",
    "url": "https://twitter.com/OfficialLoganK/status/2009339263251566902",
    "source": "hn",
    "summary": "",
    "comments": [
      "<a href=\"https:&#x2F;&#x2F;xcancel.com&#x2F;OfficialLoganK&#x2F;status&#x2F;2009339263251566902#m\" rel=\"nofollow\">https:&#x2F;&#x2F;xcancel.com&#x2F;OfficialLoganK&#x2F;status&#x2F;200933926325156690...</a>",
      "This is good, but it doesn&#x27;t necessarily mean that Tailwind is out of the financial difficulty that we talked about yesterday. You can sponsor Tailwind for as little as $6,000&#x2F;year. 29 companies were already sponsoring Tailwind including 16 companies at the $60,000&#x2F;year level. Maybe Google AI Studio has decided to shell out a lot more, but it could also be a relatively small sponsorship compared to the $1.1M in sponsorships that Tailwind is already getting. Google has deep pockets and could easily just say &quot;f-it, we&#x27;re betting on AI coding and this tool helps us make UIs and $2M&#x2F;year is nothing compared to what we&#x27;re spending on AI.&quot; It&#x27;s also possible that the AI Studio team has a small discretionary budget and is giving Tailwind $6,000&#x2F;year.<p>It&#x27;s good, but it&#x27;s important to read this as &quot;they&#x27;re offering some money&quot; and not &quot;Tailwind CSS now doesn&#x27;t have financial issues because they have a major sponsor.&quot; This could just be a 1-5% change in Tailwind&#x27;s budget. We don&#x27;t know.<p>And that&#x27;s not to take away from their sponsorship, but on the heels of the discussion yesterday it&#x27;s important to note that Tailwind was already being sponsored by many companies and still struggling. This is a good thing, but it&#x27;s hard to know if this moves the needle a bunch on Tailwind&#x27;s problems. Maybe it&#x27;ll be the start of more companies offering Tailwind money and that&#x27;d be great.",
      "Last year they claimed they had $800k in ARR from sponsors alone[1]. Add to that whatever they made by selling Tailwind Plus ($299 individual &#x2F; $979 teams one time payment)<p>How much money do you really need to maintain a CSS library? I understand everyone wants a really fancy office in an expensive city, lots of employees with very high salaries and generous perks, and so on. But all that is not needed to maintain a CSS library (that is kind of feature complete already).<p>I think Tailwind was making a lot of money (surely over a million), expanded and got bloated unnecessarily just because they had all that money, and now that their income dropped to what still is a lot of money for a CSS library, they&#x27;re angry that they have to cut expenses to a more reasonable level.<p>I guess it worked out for them because now they have even more sponsoring.<p>And they used the <i>AI bad</i> get out of jail free card when a lot of their drop in sales probably comes from shadcn&#x2F;ui and others which offer something similar for free.<p>[1] <a href=\"https:&#x2F;&#x2F;petersuhm.com&#x2F;posts&#x2F;2025&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;petersuhm.com&#x2F;posts&#x2F;2025&#x2F;</a>",
      "Vercel is also now sponsoring Tailwind CSS: <a href=\"https:&#x2F;&#x2F;x.com&#x2F;rauchg&#x2F;status&#x2F;2009336725043335338\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;rauchg&#x2F;status&#x2F;2009336725043335338</a>",
      "There are a lot of comments to the tune of &quot;why does a CSS library need 1m+ (or any money at all) to survive?&quot;. I&#x27;m no expert on this kind of thing, but Tailwind 0.1.0 first released on November 2017. Since then, there&#x27;s been continual improvements up until last month with 4.1.18, totalling 8 years of dev work. A simple CSS library wouldn&#x27;t have much need to go past 0.1.0, certainly not 1.0.0. Clearly tailwind did, which would imply there&#x27;s more than meets the eye.<p>But you can&#x27;t have it both ways, it can&#x27;t be just a simple CSS library that doesn&#x27;t need that much money, but also expect a decade of work+ on it. After all, this originally stems from the fact that a PR attempting to improve something didn&#x27;t get merged in; a technically finished project would have the same problem, but that would be the rule rather than the exception.",
      "<a href=\"https:&#x2F;&#x2F;github.com&#x2F;tailwindlabs&#x2F;tailwindcss.com&#x2F;commits&#x2F;main&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;tailwindlabs&#x2F;tailwindcss.com&#x2F;commits&#x2F;main...</a><p>They&#x27;ve just added 26 sponsor companies in the last two days, 7 of them partners!",
      "This is probably related to this [1] if anyone is wondering.<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46527950\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46527950</a>",
      "For every Tailwind, there will be 1000x other projects affected by AI&#x27;s use of OSS that will not get sponsored.",
      "I suppose this an attempt to try and head off the stories about &quot;AI&quot; killing open source",
      "How did a CSS library make any money at all? How did a CSS library have employees?"
    ],
    "full_text": null
  },
  {
    "title": "MCP is a fad",
    "url": "https://tombedor.dev/mcp-is-a-fad/",
    "source": "hn",
    "summary": "",
    "comments": [
      "This analysis dismisses MCP by focusing too narrowly on local file system interactions. The real value isn&#x27;t just running scripts; it&#x27;s interoperability.<p>MCP allows any client (Claude, Cursor, IDEs) to dynamically discover and interact with any resource (Postgres, Slack) without custom glue code. Comparing it to local scripts is like calling USB a fad because parallel ports worked for printers. The power is standardization: write once, support every AI client.<p>Edit:<p>To address the security concerns below: MCP is just the wire protocol like TCP or HTTP. We don&#x27;t expect TCP to natively handle RBAC or prevent data exfil. That is the job of the application&#x2F;server implementation.",
      "We went from &quot;Review any services and their interaction without local system and network&quot; to &quot;Defending local and remote logic created on the fly to mangle the local file system, and why that&#x27;s a good thing&quot; ...<p>That&#x27;s not a productivity boost. That&#x27;s a rapid increase in cognitive tax you&#x27;re offloading for later and as you get backlogged in reviewing it, you lose more control over what it does...",
      "MCP is just a small, boring protocol that lets agents call tools in a standard way, nothing more. You can run a single MCP server next to your app, expose a few scripts or APIs, and you are done. There is no requirement for dozens of random servers or a giant plugin zoo.<p>Most of the “overhead” and “security nightmare” worries assume the worst possible setup with zero curation and bad ops. That would be messy with any integration method, not only with MCP. Teams that already handle HTTP APIs safely can apply the same basics here: auth, logging, and isolation.<p>The real value is that MCP stays out of your way. It does not replace your stack, it just gives tools a common shape so different clients and agents can use them. For many people that is exactly what is needed: a thin, optional layer, not another heavy platform.",
      "Simon Willison made many of the same points (without the technical deep dive) back in October 2025 [1], when Anthropic announced Skills.<p>A couple of choice quotes, which are echoed in this new article:<p>&gt; I like to joke that one of the reasons it took off is that every company knew they needed an “AI strategy”, and building (or announcing) an MCP implementation was an easy way to tick that box.<p>&gt; Almost everything I might achieve with an MCP can be handled by a CLI tool instead.<p>[1]: <a href=\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Oct&#x2F;16&#x2F;claude-skills&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2025&#x2F;Oct&#x2F;16&#x2F;claude-skills&#x2F;</a>",
      "Fair warning, if you load the site in dark-mode the diagrams are completely broken. They&#x27;re PNGs with an alpha-transparency background and gray&#x2F;black for the actual content, when the site is in dark mode you can see nothing at all...<p>So make sure to change to light-mode in the top-right if you want to read this article at all.",
      "This article could really mostly be reduced to the last two paragraphs, but then it calls skills &quot;over-engineered&quot;. Skills are basically just having the agent read the front matter with instructions to read the rest if a given skill seems useful in a given context... I don&#x27;t know how it could be more minimal.",
      "I think this article misses the most important point of MCP: Authentication. Granted, it wasn&#x27;t in the initial spec, but it is now, and it really opens interoperability without compromising on security.<p>Think about how to provide your SaaS service to users of ChatGPT or Claude.ai (not only coding tools like VSCode). At one time, the user will need to allow the SaaS service to interact with their agent, and will have to authenticate in the SaaS service so that the agent can act on their behalf. This is all baked in the MCP spec (through OAuth) [1], and scripting can&#x27;t beat that.<p>That&#x27;s why the Extensions&#x2F;Applications marketplaces of consumer AI assistants like ChatGPT Apps [2] are a thin layer on top of MCP.<p>Another domain where MCP is required is for Generative UI. We need a standard that allows third-party apps to return more sophisticated content than just text The MCP spec now encloses the MCP Apps specification [3], which is exactly that: a specification for how third-party apps can generate UI components in their response. On the other hand, scripting will only let you return text.<p>[1]: <a href=\"https:&#x2F;&#x2F;modelcontextprotocol.io&#x2F;specification&#x2F;2025-03-26&#x2F;basic&#x2F;authorization\" rel=\"nofollow\">https:&#x2F;&#x2F;modelcontextprotocol.io&#x2F;specification&#x2F;2025-03-26&#x2F;bas...</a>\n[2]: <a href=\"https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;11487775-apps-in-chatgpt\" rel=\"nofollow\">https:&#x2F;&#x2F;help.openai.com&#x2F;en&#x2F;articles&#x2F;11487775-apps-in-chatgpt</a>\n[3]: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;modelcontextprotocol&#x2F;ext-apps\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;modelcontextprotocol&#x2F;ext-apps</a>",
      "I don&#x27;t think MCP is a fad - I think it is the 2020s equivalent of:<p>- Active X<p>- asbestos<p>- leaded gasoline and paint<p>- radium medicines<p>Well, with the exception of the first 3 actually being quite useful.",
      "Tip: If you&#x27;re in dark mode, flip to light mode so that you can see the graphics. There&#x27;s a toggle in top right corner of the site."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: macOS menu bar app to track Claude usage in real time",
    "url": "https://github.com/richhickson/claudecodeusage",
    "source": "hn",
    "summary": "",
    "comments": [
      "Alternatively there&#x27;s also this with 1.7k stars already and supporting more services (Despite the name): <a href=\"https:&#x2F;&#x2F;github.com&#x2F;steipete&#x2F;CodexBar\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;steipete&#x2F;CodexBar</a> (<a href=\"https:&#x2F;&#x2F;codexbar.app\" rel=\"nofollow\">https:&#x2F;&#x2F;codexbar.app</a>)",
      "This is a great idea and a useful one for avoiding having to monitor Claude&#x27;s consumption. \nI&#x27;ve often exceeded the limit mid-process. If I&#x27;m not mistaken, something like this already exists, but I like the graphics and how the information is displayed. \nCongratulations!",
      "This looks great. What am I doing wrong? I downloaded it, installed, but all I see is a red X in the toolbar. When I click on it, it says &quot;Authentication expired. Run &#x27;claude&#x27; to re-aut...&quot; so I run claude in terminal and still nothing...",
      "Interesting, but not original - a simple search shows at least a dozen same&#x2F;similar (better?) solutions? Anything that yours does that those don&#x27;t?",
      "I loved the screenshot section of the readme followed by zero screenshots.",
      "Love it, installed and set it to run at Login. Can I follow you on any social media that isn&#x27;t Xitter?",
      "Just yesterday I was trying to figure out a method to accurately estimate my remaining usage for the five hour sessions for a shell script. It wasn&#x27;t until I pointed Claude at your repo and had it make something based off of that that I got it to work well.<p>Thank you!",
      "cool! I had to always go to claude settings for this."
    ],
    "full_text": null
  },
  {
    "title": "Making Magic Leap past Nvidia's secure bootchain and breaking Tesla Autopilots",
    "url": "https://fahrplan.events.ccc.de/congress/2025/fahrplan/event/making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way",
    "source": "hn",
    "summary": "",
    "comments": [
      "Here’s the video of the talk. Not sure why the schedule page was linked.<p><a href=\"https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way\" rel=\"nofollow\">https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-making-the-magic-leap-past-nvidi...</a>",
      "&gt; The Tegra X2 is an SoC used in devices such as the Magic Leap One, and Tesla&#x27;s Autopilot 2 &amp; 2.5 promising a secure bootchain.<p>I guess they didn’t learn from the Tegra X1 which was famously responsible for the boot rom exploit on the original model of the Nintendo Switch.",
      "Here is the talk if anyone is interested: <a href=\"https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-making-the-magic-leap-past-nvidia-s-secure-bootchain-and-breaking-some-tesla-autopilots-along-the-way\" rel=\"nofollow\">https:&#x2F;&#x2F;media.ccc.de&#x2F;v&#x2F;39c3-making-the-magic-leap-past-nvidi...</a>",
      "Huh, I thought Magic Leap went out of business.<p>Didn&#x27;t know they were still around!",
      "Just curious, how fast can these embedded systems boot?",
      "Sounds really interesting. CCC is an amazing event.",
      "I hope Nvidia&#x27;s new offerings (Orin, Thor, etc) don&#x27;t have the same issue in their bootROM. That would be an incredibly expensive mistake."
    ],
    "full_text": null
  },
  {
    "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs",
    "url": "https://sakana.ai/drq/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Using evolution in the context of Core War is not a new idea by far, it is even referenced in the paper.<p>Examples here: <a href=\"https:&#x2F;&#x2F;corewar.co.uk&#x2F;evolving.htm\" rel=\"nofollow\">https:&#x2F;&#x2F;corewar.co.uk&#x2F;evolving.htm</a><p>The difference here is that instead of using a typical genetic algorithm written in a programming language, it uses LLM prompts to do the same thing.<p>I wonder if the authors tried some of the existing &quot;evolvers&quot; to compare to what the LLM gave out.",
      "What a lovely period of time that was—when &quot;Computer Recreations&quot; ran monthly in <i>Scientific American</i>. I read the column every month and was fascinated to learn about Eliza, Core Wars, Conway&#x27;s Life, Wa-Tor, etc. It was a time when you coded simply for the fun of it—to explore, learn.<p>I know you can still do that today, but… something has changed. I don&#x27;t know what it is. (Maybe I changed.)<p>Anyway, I was unable to track down PDF versions of the original articles, but, for the curious and newcomers to Core Wars, they&#x27;re transcribed here:<p><a href=\"https:&#x2F;&#x2F;corewar.co.uk&#x2F;dewdney&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;corewar.co.uk&#x2F;dewdney&#x2F;</a>",
      "Hi HN,<p>I am one of the authors from Sakana AI and MIT. We just released this paper where we hooked up LLMs to the classic 1984 programming game Core War. For those who haven&#x27;t played it, Core War involves writing assembly programs in a language called Redcode that battle for control of a virtual computer&#x27;s memory. You win by crashing the opponent&#x27;s process while keeping yours running. It is a Turing-complete environment where code and data share the same address space, which leads to some very chaotic self-modifying code dynamics.<p>We did not just ask the model to write winning code from scratch. Instead, we treated the LLM as a mutation operator within a quality-diversity algorithm called MAP-Elites. The system runs an adversarial evolutionary loop where new warriors are continually evolved to defeat the champions of all previous rounds. We call this Digital Red Queen because it mimics the biological hypothesis that species must continually adapt just to survive against changing competitors.<p>The most interesting result for us was observing convergent evolution. We ran independent experiments starting from completely different random seeds, yet the populations consistently gravitated toward similar behavioral phenotypes, specifically regarding memory coverage and thread spawning. It mirrors how biological species independently evolve similar traits like eyes to solve similar problems. We also found that this training loop produced generalist warriors that were robust even against human-written strategies they had never encountered during training.<p>We think Core War is an under-utilized sandbox for studying these kinds of adversarial dynamics. It lets us simulate how automated systems might eventually compete for computational resources in the real world, but in a totally isolated environment. The simulation code and the prompts we used are open source on GitHub.<p>Other info other than the blog link:<p>Paper (website): <a href=\"https:&#x2F;&#x2F;pub.sakana.ai&#x2F;drq&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pub.sakana.ai&#x2F;drq&#x2F;</a><p>Arxiv: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2601.03335\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2601.03335</a><p>Code: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;SakanaAI&#x2F;drq\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;SakanaAI&#x2F;drq</a>",
      "How does the output fare on competitive hills like <a href=\"https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=94t\" rel=\"nofollow\">https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=94t</a> ?<p>AFAIK, the best results so far for fully computer-generated warriors have been on the nano and tiny format (<a href=\"https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=nano\" rel=\"nofollow\">https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=nano</a>, <a href=\"https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=tiny\" rel=\"nofollow\">https:&#x2F;&#x2F;sal.discontinuity.info&#x2F;hill.php?key=tiny</a>), with much shorter warriors (at most 5 or 20 instructions).",
      "The idea of what LLMs could do in CoreWars has been hanging around in the back of my head for a while now. So happy to see someone explore it systematically"
    ],
    "full_text": null
  },
  {
    "title": "Slopware.wtf – Roasting AI-Generated Garbage Software",
    "url": "https://slopware.wtf/",
    "source": "hn",
    "summary": "",
    "comments": [
      "The site itself is slopware, I mean who would roast AI-gen software because most of it is made by someone in 10 mins and it gets lost on product hunt in like a minute due to speed to other vibe coders.<p>If it costed nothing to make, its most likely worth nothing.",
      "Self-fulfilling honor tbh, the website itself looks like an AI slop :&#x27;D"
    ],
    "full_text": null
  },
  {
    "title": "Task-free intelligence testing of LLMs",
    "url": "https://www.marble.onl/posts/tapping/index.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "These aren&#x27;t task free. They&#x27;re just implicit task, &quot;figure out what you&#x27;re expected to do&quot;. These sort of riddle tasks are 100% dependent on who does the expecting.<p>This is not a new idea. Traditional IQ tests pivoted to them (they weren&#x27;t originally like that), and no doubt they have great &quot;discriminative power&quot;, because having the ability to figure out what&#x27;s expected of you and not getting intimidated by cryptic and obtuse tasks put before you, are certainly extremely valuable skills in e.g business and politics.<p>But I always respected real tasks more. A question on a math test is honest; if it doesn&#x27;t precisely define what&#x27;s expected of you, the taskmaster has done a bad job, not you. It still can be extremely demanding.<p>An implicit task, by comparison, smells more of riddles, gnosticism. Do you know the way? Do you know the <i>genre</i>? (Once you know the genre of implicit tasks typical to IQ tests, you can easily increase your performance by a lot).<p>For that matter, this idea isn&#x27;t new to machine learning either. Francois Chollet did it already, and he was IMO just as wrongheaded in thinking implicit tasks are somehow more indicative of &quot;true intelligence&quot; than explicit ones.",
      "On alternative ways to measure LLM intelligence, we had good success with this: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2509.23510\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2509.23510</a><p>In short: start with a dataset of question and answer pairs, where each question has been answered by two different LLMs. Ask the model you want to evaluate to choose the better answer for each pair. Then measure how consistently it selects winners. Does it reliably favor some models over the questions, or does it behave close to randomly? This consistency is a strong proxy for the model’s intelligence.<p>It is not subject to dataset leaks, lets you measure intelligence in many fields where you might not have golden answers, and converges pretty fast making it really cheap to measure.",
      "I like the high level idea! (how do we test intelligence in a non functional way?)<p>I&#x27;m effect, the different response types are measuring how the models respond to a context-free novel environment. I imagine humans would also respond on a variety of ways to this test, none of which are necessarily incorrect from the perspective of intelligence testing .<p>Many tests of human behavior (eg, n behavioral economics) create some pretense context to avoid boarding the response that is actually being measured. For example, we may invite a participant to a study of color preference, but actually measure how fast they complete the task when the scientist has&#x2F;hasn&#x27;t bathed in a week (or whatever).<p>Likewise, for llm intelligence testing, you could create pretext tasks and context, and perhaps measure what the model considered along the way, instead of the actual task outcome.",
      "This is very interesting. Especially the last part where it shows gpt-5.2 and gpt-oss and their very similar and unique outcome of being 90%+ Serious.<p>I tested this locally and got the same result with gpt-oss 120b. But only on the default &#x27;medium&#x27; reasoning effort. When I used &#x27;low&#x27; I kept getting more playful responses with emojis and when I used &#x27;high&#x27; I kept getting more guessing responses.<p>I had a lot of fun with this and it provided me with more insight than I would have thought.",
      "Aren&#x27;t LLMs just super-powerful pattern matchers? And guessing &quot;taps&quot; a pattern recognition task? I am struggling to understand how your experiment relates to intelligence in any way.<p>Also, commercial LLMs generally have system instructions baked on top of the core models, which intrinsically prompt them to look for purpose even in random user prompts.",
      "Game playing is the next frontier. Model economically valuable tasks as games and have the agents play&#x2F;compete. Alphabench and Vendingbench show the potential of this approach.",
      "whats the assistant prompt being used for these? i dont think ive ever gotten these joking responses back to anything",
      "Typo:<p>&quot;The behvior summary&quot;",
      "[flagged]"
    ],
    "full_text": null
  },
  {
    "title": "Support for the TSO memory model on Arm CPUs (2024)",
    "url": "https://lwn.net/Articles/970907/",
    "source": "hn",
    "summary": "",
    "comments": [
      "imo the fragmentation argument is very flaky. this is not like, e.g. the old ARM big endian mode which accidentally created a separate ISA for really dubious reasons. the only thing that really have a need for alternate store orders on ARM at this point are just (x86) emulators. This argument may have made more sense 10, 15 years ago but ARM has become so ubiquitous that the idea of “oh my port of this originally x86 userland software crashes on ARM due to a synchronization bug, better use this TSO thing as a get-out-of-jail-free card “ really doesn’t have much water to it now",
      "We need a TLA authority to help prevent collisions in the acronym space. It’s enough that MCP is also the Burroughs&#x2F;Unisys mainframe operating system, now TSO is also the time-sharing option on IBM mainframes.",
      "The focus on user space fragmentation is wrong, IMHO.<p>One of the maintainers (Catalin Marinas) made [0] a much more important point: Apple makes no promises about how their &quot;TSO&quot; bits work now or will work in the future. This mode was designed for Rosetta2, not the general public. It is not documented formally. Someone saying &quot;it is TSO&quot; is not documentation. A formal definition of a memory model is usually a very long document describing a lot of corner cases, for example [1] is a SUMMARY of the ARMv8 memory model, it is 31 pages long. It is a summary! The full spec makes up chapters D7 and D8 in [2], totaling 243 pages. Even there, there are corners that it does not touch on and people get wrong. Without such a spec for Apple&#x27;s TSO mode, how can anyone rely on how it might or might not work?<p>Additionally, you might find silicon bugs if you do something in this mode that Rosetta2 doesn&#x27;t or didn&#x27;t. Consider that the only first-party user of this mode was Rosetta2. Anything it does not do that you do might find a bug.<p>The stated linux kernel policy of &quot;do not break user space&quot; is impossible to deliver on, if built on an undocumented hardware feature that might change at any time and was never fully publicly specified. The maintainers are right to reject this.<p>[0] <a href=\"https:&#x2F;&#x2F;lwn.net&#x2F;ml&#x2F;linux-kernel&#x2F;ZiKyWGKTw6Aqntod@arm.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;lwn.net&#x2F;ml&#x2F;linux-kernel&#x2F;ZiKyWGKTw6Aqntod@arm.com&#x2F;</a><p>[1] <a href=\"https:&#x2F;&#x2F;developer.arm.com&#x2F;-&#x2F;media&#x2F;Arm%20Developer%20Community&#x2F;PDF&#x2F;Learn%20the%20Architecture&#x2F;Armv8-A%20memory%20model%20guide.pdf?revision=58b1dd0a-3800-4218-b21a-f95a0332034c\" rel=\"nofollow\">https:&#x2F;&#x2F;developer.arm.com&#x2F;-&#x2F;media&#x2F;Arm%20Developer%20Communit...</a><p>[2] <a href=\"https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;6943ef0c79820936800f7f48?token=\" rel=\"nofollow\">https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;6943ef0c7982093...</a>",
      "It&#x27;s a pity that Hector Martin stepped away from all this great work (under that name, anyway)"
    ],
    "full_text": null
  },
  {
    "title": "IBM AI ('Bob') Downloads and Executes Malware",
    "url": "https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware",
    "source": "hn",
    "summary": "",
    "comments": [
      "<i>&quot;IBM Bob is IBM’s new coding agent, currently in Closed Beta. &quot;</i><p>Promptarmor did a similar attack(1) on Google&#x27;s Antigravity that is also a beta version. Since then, they added secure mode(2).<p>These are still beta tools. When the tools are ready, I&#x27;d argue that they will probably be safer out of the box compared to a whole lot of users that just blindly copy-paste stuff from the internet, adding random dependencies without proper due diligence, etc. These tools might actually help users acting more secure.<p>I&#x27;m honestly more worried about all the other problems these tools create. Vibe coded problems scale fast. And businesses have still not understood that code is not an asset, it&#x27;s a liability. Ideally, you solve your business problems with zero lines of code. Code is not expensive to write, it&#x27;s expensive to maintain.<p>(1) <a href=\"https:&#x2F;&#x2F;www.promptarmor.com&#x2F;resources&#x2F;google-antigravity-exfiltrates-data\">https:&#x2F;&#x2F;www.promptarmor.com&#x2F;resources&#x2F;google-antigravity-exf...</a>\n(2)  <a href=\"https:&#x2F;&#x2F;antigravity.google&#x2F;docs&#x2F;secure-mode\" rel=\"nofollow\">https:&#x2F;&#x2F;antigravity.google&#x2F;docs&#x2F;secure-mode</a>",
      "These prompt injection vulnerabilities give me the heebie jeebies. LLMs feel so non deterministic that it appears to me to be really hard to guard against. Can someone with experience in the area tell me if I&#x27;m off base?",
      "I&#x27;m not saying IBM shouldn&#x27;t try, but really – why is IBM building coding CLIs? They&#x27;re like the company version of the Steve Buscemi &quot;How do you do, fellow kids?&quot; meme.",
      "&gt; Bob has three defenses that are bypassed in this attack<p>This section describes the bypass in three steps, but only actually describes two defenses and uses the third bullet point as a summary of how the two bypasses interact.",
      "Sounds like most of this is simply taking shortcuts instead of properly parsing[0].<p>0: <a href=\"https:&#x2F;&#x2F;lexi-lambda.github.io&#x2F;blog&#x2F;2019&#x2F;11&#x2F;05&#x2F;parse-don-t-validate&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;lexi-lambda.github.io&#x2F;blog&#x2F;2019&#x2F;11&#x2F;05&#x2F;parse-don-t-va...</a>",
      "You can probably get any coding agent with this if you put these instructions in the README&#x2F;CLAUDE.md&#x2F;AGENTS.md or whatever of your repo.<p>It&#x27;s unclear to me if Bob is working as intended or how we should classify these types of bugs.  Threat modeling this sort of prompt injection gets murky, but in general don&#x27;t put untrusted markdown into your AI agents.",
      "This is an article with a very very high commercial vested interest in the software they sell (promptarmor.com - &quot;All AI Risk is Third Party Risk&quot;).",
      "pretty funny that the text shown users when trying run commands with substitution like $() specifically says they block process substitution in commands, but the code just doesnt block it at all",
      "I didnt know IBM was even in this game.",
      "I can’t believe the Bob CLI is just another fork of the Gemini CLI, no wonder Anthropic has the moat in agentic development CLIs, at least they are developing their own."
    ],
    "full_text": null
  },
  {
    "title": "Why women experience more gut pain",
    "url": "https://www.scimex.org/newsfeed/scientists-discover-why-women-experience-more-severe-gut-pain",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space",
    "url": "https://arxiv.org/abs/2512.24617",
    "source": "hn",
    "summary": "",
    "comments": [
      "I&#x27;m really glad that these HNet-inspired approaches are getting traction, I&#x27;m a big fan of that paper.<p>Though I wonder how much of the gains in this case are actually due to 75% extra parameters compared to the baseline, even if the inference FLOPs are matched.<p>Can&#x27;t help but see this as a just different twist on parameter use sparsity idea leveraged by MoE models, as those also gain in performance at constant forward pass FLOPs because of extra parameters.",
      "Would this enable a model to learn concepts in one language and generate answers about it in another, as long as it learns general translations between them?",
      "Broken citations. My inner reviewer gets sad. :("
    ],
    "full_text": null
  },
  {
    "title": "Logistics Is Dying; Or – Dude, Where's My Mail?",
    "url": "https://lagomor.ph/2026/01/logistics-is-dying-or-dude-wheres-my-mail/",
    "source": "hn",
    "summary": "",
    "comments": [
      "When &quot;free shipping&quot; is what consumers expect, do you think people are ready to pay for better service? It the package is lost, it is often cheaper to write it off as a loss and send another item or refund.<p>If you need overnight shipping, you can have it, but it is not cheap, so most people won&#x27;t do it. As for documents, is is all electronic now.<p>Logistics is very much alive, but it is adapted to our needs, that is massive containerships for worldwide trade, local warehouses for fast delivery of common items, reliability that is in balance with the cost of losing items, speed matching what people are ready to pay for, and specialized services for special needs, like overnight shipping.<p>EDIT:<p>It made me think about the &quot;Lettre Verte&quot; in France, named after the green stamp, and also supposedly because it is good for the planet. It was introduced early 2010s as a cheaper, slower (2 days instead of 1) alternative to the &quot;priority&quot; red stamp. And now the red stamp is gone, replaced with an electronic service and the green stamp is now 3 days instead of 2. Mail has become much slower overall, because there is much less mail than before, and fast service is not economically viable nor essential on a day-to-day basis. La Poste used to have a dedicated high speed train, equipped with sorting center for overnight mail across the country, now decommissioned, not enough mail to fill a train.",
      "The article makes a claim that it is no better in Canada and proceeds to remark about financial conditions. The mail is being delivered just fine, just not profitably.<p>I&#x27;ve not had issues with the USPS, but I don&#x27;t doubt that it is getting worse. Private delivery (Amazon and the like) has been pretty much flawless. Order from McMaster, and it almost invariably arrives within two days (continental US)<p>I just don&#x27;t experience what the author is getting at.",
      "&gt; Modern logistics companies succeed financially while failing at the task. Stock prices go up. Service goes down. The quarterly report looks great. Your package is in a warehouse two states away marked “delivered.”<p>It&#x27;s not just logistics. It&#x27;s the same with big corporations all across the economy. Service or product quality going down, stock prices going up.",
      "Speak for yourself. My mail is usually delivered next-day and I can track it live on the truck. I get annoyed when something I ordered is being delivered by someone other than swisspost. I get notifications the day before and day of delivery.<p>Swisspost had been cutting its budget back consolidating locations but despite this is delivering even better service using tech.",
      "Denmarks main postal carrier discontinued letter delivery last year.<p>Living in Germany I find the contrast always interesting and often, unfortunately, annoying. Almost anything legal has to be done on paper via letter. Fax (yes, really) is one of the only other alternatives that is accepted. Im just really glad that after moving, my new hometown is significantly stepping up their digital game and making it as easy as possible. They even respond to emails in city hall!<p><a href=\"https:&#x2F;&#x2F;edition.cnn.com&#x2F;2025&#x2F;12&#x2F;30&#x2F;europe&#x2F;denmark-postal-service-letters-intl-scli?Profile=CNN+International\" rel=\"nofollow\">https:&#x2F;&#x2F;edition.cnn.com&#x2F;2025&#x2F;12&#x2F;30&#x2F;europe&#x2F;denmark-postal-ser...</a>",
      "&gt; Mail not working is a symptom of something larger—a preference for metrics over outcomes, for cost-cutting over capability, for whatever can be measured over what actually matters.<p>This sentence sounds straight out of ChatGPT (along with the general substance-less nature of the article).",
      "FedEx is easily the best carrier in my area right now. It&#x27;s not even close. If I need absolute certainty that something will arrive, I&#x27;m paying extra for the overnight priority option. All of my employers have seen it the same way. Even the holiday gift packages are sent FedEx.<p>USPS has been a disaster by comparison. We&#x27;ve been dealing with actual criminal elements in my local post office stealing and tampering with mail. I don&#x27;t know of anyone in my community who <i>hasnt</i> filed a complaint with the USPIS. I don&#x27;t even care about the packages anymore. I&#x27;m more worried about the tax forms, vehicle titles and replacement credit cards getting lost now.<p>Any merchant who only offers USPS or uses them as the last mile of delivery is a no-go for me. Amazon is the most stressful experience because you never know. The best approach I&#x27;ve found is to use their pickup box and batch things to coincide with my grocery trip.",
      "I had some dramatic episodes. Surprisingly, FedEx is very bad in Germany too. Once I had to retrieve a laptop from a FedEx station to prevent it from going back to China. The trick is, it was addressed to a different name. The truck just went by the door for three days, &quot;nobody home&quot;. That was a situation.",
      "The postal service doesn&#x27;t need to be fast, but it does need to be reliable. The natures of one&#x27;s promises are less important than whether or not they are kept.",
      "I&#x27;m surprised someone registered a .ph TLD for a blog, it costs minimal almost $50&#x2F;yr if not a bit more afaik."
    ],
    "full_text": null
  },
  {
    "title": "Is Craigslist the Last Real Place on the Internet?",
    "url": "https://www.wired.com/story/is-craigslist-the-last-real-place-on-the-internet/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I love craigslist for used car stuff, its a shame that all the sellers are now on FB marketplace. :(",
      "I love Craigslist! It&#x27;s like FB Marketplace, but better. The search tools are more powerful, and you don&#x27;t get a barrage of &quot;is this still available?&quot; messages followed by crickets. I&#x27;ve found it much easier to both buy and sell things on CL.",
      "<a href=\"https:&#x2F;&#x2F;archive.ph&#x2F;R59RJ\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.ph&#x2F;R59RJ</a>",
      "Craigslist is kind of dead near me, except free stuff. I&#x27;ll get a million people when I put up free stuff still.<p>I do most of my 2nd hand buying on OfferUp. Idk where it came from, a friend told me about it and it&#x27;s pretty active.",
      "not quite the last<p><a href=\"https:&#x2F;&#x2F;www.barnstormers.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.barnstormers.com&#x2F;</a><p>airplanes, airplane stuff, airports, airport stuff, little airplanes, big airplanes, fighter jets, airplanes from the dawn times, every budget and skill level.\nonline since day 2, almost exactly unchanged, certain adds cost a few bucks to list, other adds are hand coded into the pages, \nsocial side is listings for &quot;fly in&#x27;s&quot;",
      "[dead]",
      "[dead]"
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Watch LLMs play 21,000 hands of Poker",
    "url": "https://pokerbench.adfontes.io/run/Large_Models",
    "source": "hn",
    "summary": "",
    "comments": [
      "Finally, a way to settle the model wars that actually matters: Texas Hold&#x27;em. That 3D replay view is sick! ♠♦\nI spent way too long watching the replay on Game 2a58900d. It’s wild to see the chain of thought mapped against the betting rounds. It really exposes when a model is hallucinating a strong hand versus actually calculating pot odds. This &#x27;PokerBench&#x27; might actually become the standard for measuring agentic risk-taking.",
      "Really cool, I’m curious what would be the comparison versus a deterministic bot that uses probability tables.",
      "People looking into this a little too much, looks to me like random walk. You should try reinitiating the trial (or have multiple running) and see if the ranking is robust.",
      "Do you have any idea why the win rate for GPT-5.2 is higher than Gemini 3 Flash yet the former loses money while the latter earns money? Is it just bet sizing (betting more when it has a good hand) or something else?",
      "Fun, any idea how much would be the cost per game? I am worried 160 isnt a big enough sample size.",
      "Do you have idea why smaller models are better then large ones?",
      "Very very fun. Just glancing at this quickly at lunch but is there any idea of incorporating tool use?",
      "What about the open source models? I remember from the trading benchmarks Deepseek performed pretty well."
    ],
    "full_text": null
  },
  {
    "title": "Distinct AI Models Seem to Converge on How They Encode Reality",
    "url": "https://www.quantamagazine.org/distinct-ai-models-seem-to-converge-on-how-they-encode-reality-20260107/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Given the same fundamentals, such as transformer architecture networks, then multiple models given data about the same world are going to converge on representation as a matter of course. They&#x27;re going to diverge if the underlying manner in which data gets memorized and encoded, such as with RNNs, like RWKV.<p>The interesting bits should be the convergence of representation between human brains and transformer models, or brains and RWKV, because the data humans collect is implicitly framed by human cognitive systems and sensors.<p>The words and qualia and principles we use in thinking about things and communicating and recording data are going to anchor all data in a fundamental ontological way that is inescapable, and therefore it&#x27;s going to constrain the manner in which higher order extrapolations and derivations can be structured, and those structures are going to overlap with human constructs."
    ],
    "full_text": null
  },
  {
    "title": "Elon Musk's X could be banned in Britain over AI chatbot deepfakes row",
    "url": "https://www.telegraph.co.uk/business/2026/01/08/musks-x-could-be-banned-in-britain-over-ai-chatbot-row/",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Why AI is pushing developers toward typed languages",
    "url": "https://github.blog/ai-and-ml/llms/why-ai-is-pushing-developers-toward-typed-languages/",
    "source": "hn",
    "summary": "",
    "comments": [
      "In my Django+Vue&#x2F;TS&#x2F;Inertia side project, I was surprised to see my agent know to build (use ts compiler in this context) after each change and it iterates till it gets it right. The Django code is not as typed, so I have to feed it a few error messages myself. Gotta unbreak my Mypy (python type checker) to keep my sanity.<p>In 2012, I felt the high of Scala programs working perfectly once it compiles. Now my TS code is almost there, and Django is somewhat behind.",
      "Been vibecoding in rust for this reason. Even with the smaller  amount of training data it does seem to produce less fragile code.",
      "Python is at least as typed as Lua."
    ],
    "full_text": null
  }
]