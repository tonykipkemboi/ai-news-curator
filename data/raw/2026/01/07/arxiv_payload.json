[
  {
    "title": "Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation",
    "url": "https://arxiv.org/abs/2601.03254v1",
    "source": "arxiv",
    "summary": "The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, whic",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 Emergent Communication\n\n3.1 Vocabulary and Messages\n3.2 Language in Multi-Agent Systems\n3.3 Emergent communication\n\n\n\n4 Automated Semantic Rules Detection\n\n4.1 Task: Lewis Game\n4.2 Attributes and Hyperattributes\n4.3 Algorithm Workflow\n\n\n\n5 Experimental Setup\n\n5.1 Agents architecture\n5.2 Image Feature Transformations\n\n5.3 Datasets\n\n1. Multi-Object Positional Relationships Dataset (MOPRD).\n2. Visual Genome Human-Animal-Circular (VGHAC).\n\n\n\n5.4 Attributes and Hyperattributes Definition\n\nMOPRD\nVGHAC\n\n\n\n5.5 Evaluation metrics\n\nTopographic Similarity (TopSim)\nAccuracy\n\n\n\n\n\n6 Results\n\n\n6.1 Performance Metrics\n\nAccuracy:\nTopSim:\n\n\n6.2 ASRD analysis\n\n\n7 Discussion and Conclusion\n\n\n\n\n11institutetext: University of Mons, MAIA Artificial Intelligence Lab, Mons, Belgium\n11email: {bastien.vanderplaetse,stephane.dupont}@umons.ac.be\n22institutetext: University of Mons, Mathematics and Operational Research, Mons, Belgium\n22email: xavier.siebert@umons.ac.be\nCorresponding author\nAutomated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation\n\n\nBastien Vanderplaetse\n\n‚ÄÉ‚ÄÉ\nXavier Siebert\n\n‚ÄÉ‚ÄÉ\nSt√©phane Dupont\n\n\n\nAbstract\nThe field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.\n\n\n\n1 Introduction\n\nThe study of emergent communication in multi-agent systems aims to uncover how autonomous agents can independently create and refine their own interaction protocols. This research area connects computational linguistics, artificial intelligence, and robotics, focusing on the development of complex communication strategies among autonomous agents. Depending on the task they are trained for, these agents may cooperate or be in competition with one another. To communicate, agents need a protocol communication, which refers to a set of rules, conventions, and structures that govern how information is exchanged between agents in a multi-agent system. These protocols define the format, timing, sequencing, and semantics of messages, ensuring that agents can effectively interpret and respond to the information they receive. More specifically, emergent communication protocols focus on the functional aspects of communication, such as how messages are encoded, transmitted, and decoded. These protocols govern the mechanics of interaction, ensuring that agents can exchange information efficiently and reliably. Communication protocols require a language, which refers to the symbolic system or vocabulary that agents develop to represent and convey meaning. It encompasses the creation of symbols, grammar, and shared understanding, allowing agents to express complex ideas and concepts. While emergent communication protocols deal with the \"how\" of communication, emergent language deals with the \"what\" and the shared understanding that emerges from interactions.\n\n\nIn traditional systems, communication protocols and languages are often explicitly designed and predefined, tailored to specific tasks or environments. However, in the context of emergent communication, either the protocol, the language, or both are not specified, but instead evolve dynamically as agents interact and learn from their environment and each other. This emergent process allows for the development of flexible and adaptive communication strategies, which can be particularly advantageous in complex or novel scenarios where predefined protocols and languages may be insufficient or impractical. However, learned protocols and languages raise challenges in interpretability. This has driven interest in using discrete symbols and syntactic structures inspired by human language, valued for its adaptability and ability to convey intricate ideas across new contexts. Emergent communication thus proposes a solution to create scalable, adaptable discrete communication languages that could hopefully mirror human linguistic structures to improve interpretability. However, the interpretation of messages exchanged by the agents in the context of emergent communication has not yet been extensively studied. This paper proposes an interpretation method based on an algorithm, Automated Semantic Rules Detection (ASRD), that extracts patterns occuring in the emergent language between the agents. This algorithm identifies constant positions in messages by grouping them based on attribute or hyperattribute values representing the properties of the observed data. It then extracts representative patterns by removing global invariants, linking these patterns to the data properties. Our results show that ASRD is able to find relevant patterns in the messages used by agents and to determine their relation to attributes defining the input data observed by the agents.\n\n\nSection¬†2 reviews related works in the field of emergent communication, highlighting the gaps in interpretability and the need for automated analysis tools.\n\n\nSection¬†3 provides a detailed explanation of emergent communication, focusing on the fundamental concepts of vocabulary, messages, and language in multi-agent systems.\n\n\nSection¬†4 introduces the ASRD algorithm, describing its workflow and how it extracts semantic rules from emergent communication protocols.\n\n\nSection¬†5 outlines the experimental setup, including the agents‚Äô architecture, image feature transformations, datasets, and evaluation metrics used in this paper.\n\n\nSection¬†6 presents the results of the experiments, analyzing the performance of the agents and the effectiveness of ASRD in extracting semantic rules from emergent languages.\n\n\nSection¬†7 discusses the implications of the findings, highlights the limitations of the current approach, and suggests directions for future research.\n\n\n\n\n2 Related Works\n\nEmergent communication builds on the idea that, when agents share an environment and work towards common objectives, they can develop communication strategies without human intervention¬†[foerster_learning_2016]. Recent advances have highlighted the flexibility of these emergent protocols, showcasing their ability to improve coordination and collaboration across diverse tasks and scenarios¬†[masquil_intrinsically-motivated_2022, wang_emergence_2022]. Current research primarily focuses on improving the overall performance of multi-agent systems, with additional emphasis on the resilience and adaptability of emergent protocols under various conditions¬†[bullard_quasi-equivalence_2021, wang_emergence_2022]. While emergent protocols improve agent collaboration, a notable gap in the literature lies in the interpretability of emergent languages and their alignment with human understanding, which is a critical aspect for ensuring the usability and trust of such systems in real-world contexts¬†[zhu_survey_2024]. Some research works propose to design adaptive languages that enhance the clarity and sparsity of messages to meet the needs of diverse human-agent teams¬†[karten_interpretable_2023]. Others works explore the development of communication strategies more aligned with human language¬†[bouchacourt_how_2018], specifically on its compositionality property¬†[mordatch_emergence_2018]. However, the interpretation of language emerging from such techniques requires a lot of manual analysis.\n\n\nIn this paper, we propose a method to help at this interpretation thereby minimizing manual analysis. To focus on the language emergence and its interpretation, we test our algorithm on a simple task involving two agents with a simple communication protocol: the Lewis Game, often used in studies of emergent communication¬†[chaabouni_emergent_2022, lazaridou_emergence_2018].\n\n\n\n\n3 Emergent Communication\n\nEmergent communication in multi-agent systems (MAS) refers to the process by which autonomous agents develop their own communication protocol or language without explicit instructions. In this paper, we focus on emergent language. Therefore, this Section defines the fundamental concepts of an emergent language.\n\n\n\n3.1 Vocabulary and Messages\n\nIn MAS, communication between agents requires a vocabulary ùí±\\mathcal{V}, which is a finite, non-empty set of symbols {w1,‚Ä¶,wn}\\left\\{w_{1},\\ldots,w_{n}\\right\\}, where n‚àà‚Ñï0n\\in\\mathbb{N}_{0} represents the number of symbols. Each element wi‚ààùí±w_{i}\\in\\mathcal{V} is referred to as a word. The set of all possible finite sequences of words from ùí±\\mathcal{V} is denoted as ùí±‚àó\\mathcal{V}^{*}.\n\n\nAgents use the words from ùí±\\mathcal{V} to construct messages. A message m=(w1,‚Ä¶,wT)m=\\left(w_{1},\\ldots,w_{T}\\right) is a finite sequence of words, where each wi‚ààùí±w_{i}\\in\\mathcal{V} and TT is the length of the message. The set of all possible messages is denoted as ‚Ñ≥‚äÜùí±‚àó\\mathcal{M}\\subseteq\\mathcal{V}^{*}. Within a message, each instance of a word from ùí±\\mathcal{V} is called a token.\n\n\n\n\n3.2 Language in Multi-Agent Systems\n\nA language ‚Ñí\\mathcal{L} over a vocabulary ùí±\\mathcal{V} is a structured system of communication used by agents to convey information. Formally, a language is defined as a set of messages selected from ùí±‚àó\\mathcal{V}^{*}, along with a set of rules that govern how these messages are constructed and interpreted. Specifically, a language can be represented as ‚Ñí=(ùí±,‚Ñõ‚Ñí)\\mathcal{L}=\\left(\\mathcal{V},\\mathcal{R}_{\\mathcal{L}}\\right), where ‚Ñõ‚Ñí\\mathcal{R}_{\\mathcal{L}} is the set of syntactic and semantic rules that define how symbo"
  },
  {
    "title": "STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning",
    "url": "https://arxiv.org/abs/2601.03248v1",
    "source": "arxiv",
    "summary": "Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address th",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Problem and Dataset Construction\n\n3.1 Problem Definition\n3.2 Dataset Synthesis Pipeline\n3.3 Dataset Splits and Usage\n\n\n\n4 Our Solution: STReasoner\n\n4.1 Model Architecture\n4.2 Model Training\n\n\n\n5 Experiment\n\nBaselines.\n5.1 Main Results\n5.2 Zero-Shot Results on Real-World Data\n5.3 Ablation Study\n5.4 Effect of Spatial Reward Analysis\n5.5 Case Study\n\n\n6 Conclusion\n7 Limitations\n8 Potential Risks\n\nA Related Work\n\nA.1 LMs Reasoning\nA.2 Time Series Reasoning\nA.3 Spatio-Temporal Reasoning\nA.4 Spatio-Temporal Forecasting.\nA.5 Spatio-Temporal Language Models.\n\n\n\nB Dataset Details\n\n\nB.1 Spatio-Temporal Data and Textual Description Pair Generation\n\n1. Demand Source and Propagation Nodes.\n2. Time-Varying Adjacency Matrix.\n3. Propagation Time Lags.\n\n\nB.2 Spatio-Temporal Data and Textual Description Pair Validation\nB.3 QA Dataset Generation\n\n\nC Task-Grounded Reward Design\nD Implementation Details\nE Performance with Confidence Intervals\nF Scaling Up RL Training\nG Synthesized Data Pair Showcase\nH Reasoning Case Study\nI ST-Bench Prompt Template\nJ ST-Align Prompt Template\nK Agent Prompt Template\nL Spatial Effect Prompt Template\n\n\n\n\n\nSTReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning\n\n\n\nJuntong Ni1, Shiyu Wang,\nMing Jin2, Qi He3, Wei Jin1\n1Emory University, 2Griffith University, 3Microsoft \n{juntong.ni, wei.jin}@emory.edu\n\n\n\nAbstract\nSpatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17%17\\% and 135%135\\% at only 0.004√ó0.004\\times the cost of proprietary models and generalizes robustly to real-world data. Our code is available at https://github.com/LingFengGold/STReasoner.\n\n\n\nSTReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning\n\n\n\n\n\nJuntong Ni1, Shiyu Wang,\nMing Jin2, Qi He3, Wei Jin1\n\n1Emory University, 2Griffith University, 3Microsoft\n\n{juntong.ni, wei.jin}@emory.edu\n\n\n\n\n\n\n1 Introduction\n\nTime series data are ubiquitous in real-world systems and often exhibit complex spatio-temporal structures¬†wang2020deep, from traffic networks¬†liu2023largest and climate systems¬†lin2018exploiting to disease propagation across regions¬†liu2024review.\nAs the community races to bring large language models (LLMs) to these domains¬†jin2023large, it is critical to ensure that LMs can support decisions grounded in these data¬†merrill2024language.\n\n\nFigure 1: A traffic flow example of spatio-temporal reasoning. QQ: query, ùíØ\\mathcal{T}: time series, ùí¢\\mathcal{G}: graph.\n\n\nHowever, most existing approaches primarily focus on predictive accuracy¬†liang2025foundation; liu2024spatial; liu2024can; li2024urbangpt, offering limited support for decision-making that requires explicit reasoning rather than point forecasts.\nThus, we focus on spatio-temporal reasoning: the ability to answer natural-language queries about what happened, where, when, and why in a system whose state evolves over time and is coupled across space (e.g., a graph).\nSuch queries require linking observations across multiple nodes and multiple time steps through the system‚Äôs spatial dependencies and temporal dynamics, often involving propagation and time lag. As shown in Figure¬†1, when traffic congestion is observed, the query ‚ÄúWhich source node caused the congestion at Node¬†2 at 9:00?‚Äù cannot be answered from a single-node prediction at 9:00; it requires tracing upstream dependencies in the traffic graph and connecting earlier upstream patterns to the downstream congestion under delay. Answering it thus integrates textual intent, temporal evidence, and spatial structure. Spatio-temporal reasoning of this form is essential in many other critical systems, such as tracking viral spread across urban populations¬†liu2024review, assessing cascading impacts on power grids under extreme weather¬†panteli2015influence, and tracing flood propagation along river networks¬†paola2006toward.\n\n\nDespite its importance, spatio-temporal reasoning in time series remains underexplored.\nLLMs and vision language models (VLMs) demonstrate strong reasoning ability in textual and visual domains¬†guo2025deepseek; huang2025vision, but they are not explicitly trained or evaluated for reasoning over spatio-temporal time series. Existing spatio-temporal forecasting methods focus on predicting future values¬†chen2025learning; li2024urbangpt; yuan2024unist, without explicitly reasoning, which limits their usefulness for decision-making. Meanwhile, time series language/reasoning models primarily consider univariate or multivariate series¬†guan2025timeomni; luo2025time; kong2025time; xie2024chatts; jin2023time and fail to consider spatial relationship.\nAs a result, spatio-temporal reasoning in time series is inherently distinct from these paradigms.\n\n\nSpatio-temporal reasoning in time series faces three key fundamental challenges that hinder its development:\n(a) Data Challenges.\nExisting spatio-temporal datasets rarely provide paired natural language descriptions to explain spatial entities, dependency, or temporal dynamics. Furthermore, current time series reasoning datasets often lack spatial interactions such as explicit graph structures.\n(b) Evaluation Gaps.\nThere are no standardized, multi-dimensional benchmarks that decompose spatio-temporal reasoning into distinct reasoning tasks, making it difficult to systematically evaluate models‚Äô capabilities across different spatio-temporal reasoning demands.\n(c) Modeling Limitations. The effective architecture and optimization objective for spatio-temporal reasoning are largely underexplored.\nFirst, it remains unclear how to effectively fuse time series, graph structure, and textual information for reasoning without sacrificing numerical precision or global context.\nSecond, it is unclear how to optimize models for spatio-temporal reasoning.\nExisting RL methods rely on result-only rewards¬†guo2025deepseek and fail to enforce spatial grounding, allowing models to exploit superficial temporal patterns rather than perform genuine spatial attribution.\n\n\nTo address these challenges, we propose three key solutions.\nFirst, we introduce an SDE-based multi-agent data synthesis pipeline, which generates time series with controllable spatial dependencies and temporal evolution, together with aligned textual descriptions.\nSecond, building on this pipeline, we build ST-Bench and organize spatio-temporal reasoning into four tasks: etiological spatial reasoning, spatial entity identification, spatial correlation reasoning, and in-context forecasting, along with a real-world dataset for zero-shot evaluation.\nThird, we propose STReasoner, which integrates a dedicated time series encoder with LLM to process numerical time series and textual inputs jointly.\nTo explicitly incentivize spatial reasoning during training, we propose spatial-aware group relative policy optimization (S-GRPO), which assigns additional reward only when spatial information improves performance.\n\n\nWe conduct a comprehensive empirical study to evaluate spatio-temporal reasoning capabilities across existing language models and STReasoner.\nOur contributions can be summarized as:\n\n\n\n\n(a)\n\nTo the best of our knowledge, we take an early step toward formally studying the problem of spatio-temporal reasoning in time series.\n\n\n\n(b)\n\nWe propose an SDE-based multi-agent pipeline to synthesize spatio-temporal data with aligned textual descriptions. Based on this pipeline, we construct ST-Bench to support evaluation.\n\n\n\n(c)\n\nWe propose STReasoner, a unified spatio-temporal reasoning model, together with a spatial-aware RL objective.\n\n\n\n(d)\n\nThrough systematic evaluation, we show that existing LMs struggle with spatio-temporal reasoning, while STReasoner achieves strong performance and robust zero-shot generalization at substantially lower cost.\n\n\n\n\n\n\n\n2 Related Work\n\nRecent work has improved the reasoning ability of LLMs and extended them to non-text modalities¬†wei2022chain; guo2025deepseek; huang2025vision.\nMeanwhile, time series language and reasoning models support QA and multi-step reasoning, but largely focus on univariate or multivariate series without spatial dependency¬†jin2023time; xie2024chatts; wang2025can.\nSpatio-temporal reasoning has also been explored in videos and trajectories¬†feng2025video; li2025stbench.\nIn contrast, spatio-temporal reasoning in time series involves numerical signals on discrete entities, with graph-defined spatial dependencies and induced temporal dynamics.\nA complete discussion of related work is provided in Appendix¬†A.\n\n\nFigure 2: Overall framework of the Network SDEs-based multi-agent spatio-temporal data synthesis pipeline (upper) and spatio-temporal QA generation (lower). Detailed prompts for each agent are provided in Appendix¬†K, and the pseudo algorithm is given in Algorithm¬†1.\n\n\n\n\n\n3 Problem and Dataset Construction\n\n\n3.1 Problem Definition\n\nWe define spatio-temporal reasoning in time series as follows:\ngiven a graph ùí¢=(ùí±,‚Ñ∞)\\mathcal{G}=(\\mathcal{V},\\mathcal{E}) with NN nodes,\na set of time serie"
  },
  {
    "title": "Self-Supervised Learning from Noisy and Incomplete Data",
    "url": "https://arxiv.org/abs/2601.03244v1",
    "source": "arxiv",
    "summary": "Many important problems in science and engineering involve inferring a signal from noisy and/or incomplete observations, where the observation process is known. Historically, this problem has been tackled using hand-crafted regularization (e.g., sparsity, total-variation) to obtain meaningful estimates. Recent data-driven methods often offer better solutions by directly learning a solver from exam",
    "full_text": null
  },
  {
    "title": "PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters",
    "url": "https://arxiv.org/abs/2601.03237v1",
    "source": "arxiv",
    "summary": "Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperpla",
    "full_text": null
  },
  {
    "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
    "url": "https://arxiv.org/abs/2601.03236v1",
    "source": "arxiv",
    "summary": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy.",
    "full_text": "\n\n\n\n1 Introduction\n2 Background\n\n3 MAGMA Design\n\n3.1 Architectural Overview\n3.2 Data Structure Layer\n3.3 Query Process: Adaptive Hierarchical Retrieval\n3.4 Memory Evolution (Write and Update)\n3.5 Implementation\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Overall Comparison\n4.3 Generalization Study\n4.4 System Efficiency Analysis\n4.5 Ablation Study\n\n\n5 Conclusion\n6 Limitations\nA Related Work\n\nB System Implementation Details\n\nB.1 Hyperparameter Configuration\n\n\n\nC Prompt Library\n\nC.1 Event Extraction Prompt (JSON-Structured)\nC.2 Query-Adaptive QA Prompt\nC.3 Evaluation Prompt (LLM-as-a-Judge)\n\n\n\nD Baseline Configurations\n\nDataset Statistics.\n\n\n\nE Case Study\n\n\nE.1 Detailed Analysis\n\nCase 1: Overcoming Information Loss (Recall).\nCase 2: Multi-Hop Reasoning vs. Surface Extraction.\nCase 3: Temporal Grounding.\n\n\n\n\n\nF Metric Validation Analysis\n\nF.1 Rationale for Semantic Scoring\n\n\n\n\n\n\n\nMAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents\n\n\nDongming JiangŒ±, Yi LiŒ±, Guanpeng LiŒ≤ and Bingzhe LiŒ±\nŒ±University of Texas at Dallas\nŒ≤University of Florida\n{dongming.jiang, yi.li3, bingzhe.li}@utdallas.edu; liguanpeng@ufl.edu\n\n\n\nAbstract\nMemory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy.\nIn this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.\n\n\n\nMAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents\n\n\n\n\nDongming JiangŒ±, Yi LiŒ±, Guanpeng LiŒ≤ and Bingzhe LiŒ±\n\nŒ±University of Texas at Dallas\n\nŒ≤University of Florida\n\n{dongming.jiang, yi.li3, bingzhe.li}@utdallas.edu; liguanpeng@ufl.edu\n\n\n\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks¬†Brown2020; achiam2023gpt; Wei2022,\nyet they remain limited in their ability to maintain and reason over long-term context.\nThese models process information within a finite attention window, and their internal representations do not persist across interactions,\ncausing earlier details to be forgotten once they fall outside the active context¬†Brown2020; Beltagy2020.\nEven within a single long sequence, attention effectiveness degrades with distance due to attention dilution, positional encoding limitations, and token interference, leading to the well-known ‚Äúlost-in-the-middle‚Äù and context-decay phenomena¬†liu2024lost; Press2021.\nMoreover, LLMs lack native mechanisms for stable and structured memory, resulting in inconsistent recall, degraded long-horizon reasoning, and limited support for tasks requiring persistent and organized memory¬†Khandelwal2018; maharana2024evaluating.\n\n\nTo address these inherent limitations, Memory-Augmented Generation (MAG) systems have emerged as a promising direction for enabling LLMs to operate beyond the boundaries of their fixed context windows. MAG equips an agent with an external memory continuously recording interaction histories and allowing the agents to retrieve and reintegrate past experiences when generating new responses. By offloading long-term context to an explicit memory module, MAG systems provide a means for agents to accumulate knowledge over time, support multi-session coherence, and adapt to evolving conversational or task contexts. In this paradigm, memory is no longer implicit in internal activations but becomes a persistent, queryable resource that substantially enhances long-horizon reasoning, personalized behavior, and stable agent identity.\n\n\nDespite their promise, current MAG systems exhibit structural and operational limitations that constrain their effectiveness in long-term reasoning¬†li2025memos; chhikara2025mem0; xu2025mem; packer2023memgpt; rasmussen2025zep; wang2025mirix; Kang2025. Most existing approaches store past interactions in monolithic repositories or minimally structured memory buffers, relying primarily on semantic similarity, recency, or heuristic scoring to retrieve relevant content. For example, A-Mem¬†xu2025mem organizes past interactions into Zettelkasten-like memory units that are incrementally linked and refined, yet their retrieval pipelines rely primarily on semantic embedding similarity, missing the relations such as temporal or causal relationships. Cognitive-inspired frameworks like Nemori¬†nan2025nemori introduce principled episodic segmentation and representation alignment, enabling agents to detect event boundaries and construct higher-level semantic summaries. However, their memory structures are still narrative and undifferentiated, with no explicit modeling of distinct relational dimensions.\n\n\nTo address the structural limitations of existing MAG systems, we propose MAGMA, a multi-graph agentic memory architecture that explicitly models heterogeneous relational structure in an agent‚Äôs experience. MAGMA represents each memory item across four orthogonal relational graphs (i.e., semantic, temporal, causal, and entity), yielding a disentangled representation of how events, concepts, and participants are related.\n\n\nBuilt on this unified multi-graph substrate, MAGMA introduces a hierarchical, intent-aware query mechanism that selects relevant relational views, traverses them independently, and fuses the resulting subgraphs into a compact, type-aligned context for generation. By decoupling memory representation from retrieval logic, MAGMA enables transparent reasoning paths, fine-grained control over memory selection, and improved alignment between query intent and retrieved evidence. This relational formulation provides a principled and extensible foundation for agentic memory, improving both long-term coherence and interpretability.\n\n\nOur contributions are summarized as follows:\n\n\n1.\n\nWe propose MAGMA, a multi-graph agentic memory architecture that explicitly models semantic, temporal, causal, and entity relations essential for long-horizon reasoning.\n\n\n\n2.\n\nWe introduce an Adaptive Traversal Policy that routes retrieval based on query intent, enabling efficient pruning of irrelevant graph regions and achieving lower latency and reduced token usage.\n\n\n\n3.\n\nWe design a dual-stream memory evolution mechanism that decouples latency-sensitive event ingestion from asynchronous structural consolidation, preserving responsiveness while refining relational structure.\n\n\n\n4.\n\nWe demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems on long-context benchmarks including LoCoMo and LongMemEval, while reducing retrieval latency and token consumption relative to prior systems. The code is open-sourced111https://github.com/FredJiang0324/MAMGA.\n\n\n\n\n\nFigure 1: High-Level Architecture of Memory-Augmented Generation (MAG).\n\n\nFigure 2: Architectural Overview of MAGMA. The system is composed of three layers: (1) A Query Process that routes and synthesizes context; (2) A Data Structure Layer organizing memory into Relation Graphs and a Vector Database; and (3) A Write/Update Process utilizing a dual-stream mechanism for fast ingestion and asynchronous consolidation.\n\n\n\n\n2 Background\n\nExisting Large Language Models (LLMs) face fundamental challenges in handling long-term agentic interactions. These challenges stem from the inherent limitations of fixed-length contexts, which result in fragmented memory and an inability to maintain narrative coherence over time.\nThe evolution of long-term consistency in LLMs is shifted from Context-Window Extension¬†(Beltagy2020; Press2021; kang2025lm2; qian2025memorag), Retrieval-Augmented Generation (RAG)¬†(lewis2020retrieval; jiang2025rago; wang2024m; jiang2024longrag; gutierrez2025rag; lin2025cache) to Memory-Augmented Generation (MAG).\n\n\nRetrieval-oriented approaches enrich the model with an external, dynamic memory library, giving rise to the paradigm of Memory-Augmented Generation (MAG)¬†zhong2024memorybank; park2023generative; huang2024emotional. Formally, unlike static RAG, MAG maintains a time-variant memory ‚Ñ≥t\\mathcal{M}_{t} that evolves via a feedback loop:\n\n\n\not=LLM‚Äã(qt,Retrieve‚Äã(qt,‚Ñ≥t))o_{t}=\\text{LLM}(q_{t},\\text{Retrieve}(q_{t},\\mathcal{M}_{t}))\n\n(1)\n\n\n\n\n\n‚Ñ≥t+1=Update‚Äã(‚Ñ≥t,qt,ot)\\mathcal{M}_{t+1}=\\text{Update}(\\mathcal{M}_{t},q_{t},o_{t})\n\n(2)\n\n\n\n\nAs shown in Figure¬†1, this feedback loop enables the memory module to evolve over time: the user query is combined with retrieved information to form an augmented prompt, and the model‚Äôs output is subsequently written back to refine ‚Ñ≥t\\mathcal{M}_{t}.\n\n\nSome prior schemes focused on structuring the intermediate states or relationships of memory to enable better reasoning.\nThink-in-Memory (TiM) (liu2023think) stores evolving chains-of-thought to maintain consistency.\nA-MEM (xu2025mem) draws inspiration from the Zettelkasten method, organizing knowledge into an interconnected note network.\nMore recently, graph-based approaches like GraphRAG (edge2024graphrag) and Zep (rasmussen2025zep) structure memory into knowledge graphs to capture cross-document dependencies. We provide a detailed discussion of related work in Appendix¬†A.\n\n\nHowever, prior work typically organizes memory around associative proximity (e.g., semantic similarity) rather than mechanistic dependency¬†(kiciman2023causal). As a result, such methods can retrieve what "
  },
  {
    "title": "Shallow-circuit Supervised Learning on a Quantum Processor",
    "url": "https://arxiv.org/abs/2601.03235v1",
    "source": "arxiv",
    "summary": "Quantum computing has long promised transformative advances in data analysis, yet practical quantum machine learning has remained elusive due to fundamental obstacles such as a steep quantum cost for the loading of classical data and poor trainability of many quantum machine learning algorithms designed for near-term quantum hardware. In this work, we show that one can overcome these obstacles by ",
    "full_text": null
  },
  {
    "title": "Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models",
    "url": "https://arxiv.org/abs/2601.03232v1",
    "source": "arxiv",
    "summary": "Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy ",
    "full_text": null
  },
  {
    "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
    "url": "https://arxiv.org/abs/2601.03227v1",
    "source": "arxiv",
    "summary": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Geo-Localization\n2.2 Audio-Language Models\n\n\n\n3 Audio Geo-Localization Benchmark\n\n3.1 Dataset Acquisition\n3.2 Initial Filtering\n\n3.3 Audio Localizability\n\n3.3.1 Localizability Calculation\n3.3.2 Top Positive and Negative Categories\n3.3.3 Localizability Examples\n\n\n3.4 Post Processing\n\n\n\n4 Experiments\n\n\n4.1 Experimental Settings\n\n4.1.1 Benchmarking Models\n4.1.2 Evaluation Metrics\n\n\n4.2 Comparison Results (RQ1)\n4.3 Benchmark Examples (RQ2)\n4.4 Continent-Level Prediction Inequality (RQ3)\n4.5 Error Distribution (RQ4)\n\n\n5 Conclusion\n6 Limitations\n\nA Appendix\n\nA.1 Introduction of Aporee Platform\nA.2 Interactive Platform Construction\nA.3 Formulation of the four acoustic filters\n\nA.4 Definition of Evaluation Metrics\n\nA.4.1 Mean Distance Error (km)\nA.4.2 Hierarchical Accuracy\nA.4.3 Thresholded Distance Accuracy (&lt;œÑ&lt;\\tau)\nA.4.4 Reject Rate\nA.4.5 Speech and Non-Speech Distance Error\n\n\nA.5 Definition of Main Sound Labels\nA.6 Further Comparison Results\nA.7 Further Experimental Settings\n\n\n\nB Definition of Each Error Type\n\nB.1 Global Distribution of AGL1K\nB.2 Consistency on three contribution-labeling LLM\nB.3 Continent-Distribution of Localizability\nB.4 Human Annotator\n\nB.5 Prompts\n\nB.5.1 Benchmark Prompt\nB.5.2 Contribution Calculation Prompts\n\n\n\nB.6 Sample Output\n\nB.6.1 Example 1-Gemini 3 Pro\nB.6.2 Example 1-GPT-4o Audio\nB.6.3 Example 1-Qwen3-Omni\nB.6.4 Example 1-Mimo-Audio\nB.6.5 Example 2-Gemini 3 Pro\nB.6.6 Example 2-Qwen3-Omni\nB.6.7 Example 3-Gemini 3 Pro\nB.6.8 Example 3-GPT-4o Audio\nB.6.9 Example 3-Qwen3-Omni\n\n\n\n\n\n\n\n\n\nThe Sonar Moment: Benchmarking Audio-Language Models \nin Audio Geo-Localization\n\n\n\nRuixing Zhang1,\nZihan Liu1,2,\nLeilei Sun1,3,\nTongyu Zhu1,3,\nWeifeng Lv1\n1The State Key Laboratory of Complex and Critical Software Environment, Beihang University \n2Shanghai AI Laboratory, \n3The Key Laboratory of Data Science and Intelligent Computing, International Innovation Institute, Beihang University\n\n\n\n\n\nAbstract\n\n\nFigure 1: Overview of AGL1K, illustrating the audio geo-localization task, localizability, and composition.\n\nGeo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs‚Äô reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.\n\n\n\nThe Sonar Moment: Benchmarking Audio-Language Models \nin Audio Geo-Localization\n\n\n\n\nRuixing Zhang1,\nZihan Liu1,2,\nLeilei Sun1,3,\nTongyu Zhu1,3,\nWeifeng Lv1\n\n1The State Key Laboratory of Complex and Critical Software Environment, Beihang University\n\n2Shanghai AI Laboratory,\n\n3The Key Laboratory of Data Science and Intelligent Computing, International Innovation Institute, Beihang University\n\n\n\n\n\nCode: https://github.com/Rising0321/AGL1K\nSpace: https://huggingface.co/spaces/RisingZhang/AudioGeoLoc\n\n\n\n\n1 Introduction\n\nGeo-localization aims to infer the geographic origin of a signal and offers a compelling alternative to standard classification because it requires compositional reasoning over diverse clues. Mapping observations to a single GPS coordinate not only requires perceptual inference but also broad world knowledge of geography and culture. In computer vision, this problem is typically studied as image geo-localization¬†Weyand et¬†al. (2016); Regmi and Shah (2019); Li et¬†al. (2024). As for audio, it can also provide analogous evidence, including reverberation patterns, traffic density, and coastal wave dynamics. Beyond academic interest, audio geo-localization has clear societal value. For example, assisting investigations of audios disseminated by extremist actors can help fact-checkers verify the claimed location of viral content. These applications make audio geo-localization relevant to public safety. However, despite this potential, a systematic benchmark for audio geo-localization remains absent.\n\n\nThe lack of audio geo-localization benchmarks stems from two main factors. First, there is no publicly available audio dataset with location annotations. In contrast to image geo-localization, where progress has been enabled by large-scale geo-tagged data from social media platforms, no comparable resource exists for audio. Second, the field lacks a quantitative notion of audio localizability to filter for geographically informative recordings. Even if crowd-sourced platforms can provide large numbers of audio-location pairs, without such a measure it remains nontrivial to identify samples that carry meaningful geographic signals.\n\n\nTo systematically evaluate audio geo-localization capability in audio language models (ALMs), we introduce AGL1K, the first audio geo-localization benchmark for ALMs. AGL1K is curated from the crowd-sourced Aporee platform and filtered using our proposed Audio Localizability metric, which estimates the geographic informativeness of each recording by aggregating evidence from both positive and negative sound categories during inference. The resulting benchmark spans 72 countries across six continents and covers diverse acoustic scenes, including nature soundscapes, animal vocalizations, music, human-made sounds, and spoken conversations. This diversity makes AGL1K a suitable testbed for assessing compositional reasoning in modern ALMs, with potential downstream relevance to public safety and misinformation detection.\n\n\nOur comprehensive evaluation of 16 ALMs shows that current ALMs have begun to exhibit meaningful audio geo-localization capability, yet AGL1K remains challenging. The results reveal a clear capability hierarchy between closed- and open-source models. Leading ALMs (e.g., Gemini 3 Pro) demonstrate strong knowledge and reasoning, with failures increasingly driven not by missing information but by over-commitment to a single clue. In contrast, open-source models exhibit more fundamental limitations in fine-grained perception, which prevents reliable extraction of geographically informative signals.\n\n\nThrough detailed error analysis, we conclude three findings suggesting directions for improving future audio models. (1) Enhance fine-grained perception: Many open-source ALMs misidentify languages and other subtle acoustic clues, limiting perceptual sensitivity and downstream knowledge retrieval. (2) Mitigate regional bias: Systematic prediction imbalance such as over-predicting certain continents or regions, persists across models. (3) Strengthen compositional reasoning: As audio geo-localization requires integrating multiple weak clues, models must avoid relying on any single clue as decisive evidence.\n\n\nIn summary, this work makes the following contributions:\n\n\n\n\n‚Ä¢\n\nThe first audio geo-localization benchmark for ALMs. We introduce AGL1K, a benchmark comprising 1,444 user-uploaded audio clips from 72 countries, covering diverse acoustic scenes including nature soundscapes, animal vocalizations, music, human-made sounds, and spoken conversations.\n\n\n\n‚Ä¢\n\nA principled notion of Audio Localizability. We propose an Audio Localizability, a quantitative measure of an audio‚Äôs geographic informativeness. The metric aggregates informativeness from positive and negative sound labels during inference, enabling filtering localizable recordings.\n\n\n\n‚Ä¢\n\nA comprehensive evaluation of state-of-the-art ALMs. We benchmark 16 ALMs and find the emergence of audio geo-localization to some extent. We further analyze representative models‚Äô reasoning traces, regional biases, and error causes to provide future insights for improving ALMs.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Geo-Localization\n\nMost existing geo-localization research focuses on image-based geo-localization, which can be broadly categorized into three paradigms.\nClassification-based approaches discretize the Earth‚Äôs surface into predefined regions, but achieving fine-grained accuracy requires a large number of classes, making training and scalability challenging¬†Weyand et¬†al. (2016); Clark et¬†al. (2023); M√ºller-Budack et¬†al. (2018); Seo et¬†al. (2018).\nRetrieval-based methods embed images and geographic coordinates into a shared representation space and localize by nearest-neighbor search. However, they rely on massive, globally distributed, annotated image databases that are costly to construct and maintain¬†Regmi and Shah (2019); Shi et¬†al. (2019, 2020); Cepeda et¬†al. (2023).\nMore recently, vision-language model-based methods exploit the world knowledge encoded in large multimodal models to directly predict locations, demonstrating promising performance¬†Li et¬†al. (2024); Han et¬†al. (2025); Wang et¬†al. (2025b).\nIn contrast, audio geo-localization remains unexplored. Existing benchmarks are limited in diversity, often focusing on narrow domains such as bird vocalizations¬†Chasmai et¬†al. (2025).\n\n\n\n\n2.2 Audio-Language Models\n\nThe development of ALMs has closely followed the advances in deep learning.\nEarly breakthroughs such as Deep Speech 2¬†Amodei et¬†al. (2016) replaced traditional multi-stage pipelines with end-to-end neural networks, achieving near-human transcription performance.\nSubsequent progress in self-supervised and "
  },
  {
    "title": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI",
    "url": "https://arxiv.org/abs/2601.03222v1",
    "source": "arxiv",
    "summary": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misal",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.03222v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computers and Society\n    \n\n    \n      arXiv:2601.03222v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 6 Jan 2026]\n    Title:The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI\n    Authors:Jacob Erickson            View a PDF of the paper titled The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI, by Jacob Erickson\n    View PDF\n\n\n\n    \n            Abstract:As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user&#39;s own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.\n    \n\n    \n    \n              \n          Comments:\n          Manuscript under review\n        \n\n          Subjects:\n          \n            Computers and Society (cs.CY); Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)\n        \n          Cite as:\n          arXiv:2601.03222 [cs.CY]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.03222v1 [cs.CY] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.03222\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Jacob Erickson [view email]          [v1]\n        Tue, 6 Jan 2026 18:07:52 UTC (209 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI, by Jacob EricksonView PDF\n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CY\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.HC\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n  "
  },
  {
    "title": "From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence",
    "url": "https://arxiv.org/abs/2601.03220v1",
    "source": "arxiv",
    "summary": "Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume o",
    "full_text": null
  },
  {
    "title": "MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics",
    "url": "https://arxiv.org/abs/2601.03217v1",
    "source": "arxiv",
    "summary": "Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student wo",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 The BUGGY Tradition: Procedural Misconceptions in Learning Science\n2.2 Misconception Datasets and Benchmarks\n2.3 AI for Education and Student Modeling\n\n\n\n3 The MalruleLib Framework\n\n\n3.1 Design Principles\n\nMalrule Architecture\n\n\n3.2 Coverage Statistics\n3.3 A Misconception Generator at Million-Instance Scale\n\n\n\n4 Benchmark Design\n\n\n4.1 Task Definitions\n\nMalrule Reasoning Task.\nForward MRA Task.\nCorrect Reasoning Accuracy (CRA).\n\n\n4.2 Sampled Dataset Statistics\n4.3 Models and Evaluation\n\n\n\n5 Results and Discussion\n\n\n5.1 Capability gaps in student modeling\n\nMRA remains below CRA.\nFMRA remains below CRA.\nExamples outperform descriptions.\n\n\n\n5.2 Value of step evidence and generalization\n\nReasoning traces help most models.\nCross-template generalization is the bottleneck.\nLarge domain spread.\n\n\n\n\n6 Conclusion\nA Additional Malrule Examples\nB Malrules and Their Sources\n\nC Full Results\n\nC.1 Per-Malrule Breakdown\nC.2 Template Listing\n\n\n\nD Template Listing\n\nD.1 Template Statistics Overview\nD.2 Context Domain Examples\nD.3 Scaffolded Complexity\nD.4 Templates by Malrule\n\n\n\nE Framework Details\n\nE.1 Classification Details\nE.2 Template Coverage\n\n\n\n\n\n\n\n\nMalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics\n\n\n\nXinghe Chen \nRice University \nHouston, TX \nxc42@rice.edu\n&amp;Naiming Liu \nRice University \nHouston, TX \nnl35@rice.edu\n&amp;Shashank Sonkar \nUniversity of Central Florida \nOrlando, FL \nshashank.sonkar@ucf.edu\n\n\n\n\nAbstract\nStudent mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student‚Äôs next answer under cross-template rephrasing. Across nine language models (4B‚Äì120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10‚Äì21%, while providing student step traces improves prediction by 3‚Äì15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.\n\n\n\n1 Introduction\n\nA student who computes 12+13=25\\frac{1}{2}+\\frac{1}{3}=\\frac{2}{5} is not guessing. They are applying a coherent but flawed procedure. Add the numerators, add the denominators. Learning scientists have shown that many mathematical errors arise from such systematic procedures. They are often called malrules, misconceptions, or procedural bugs, and they are stable, diagnosable, and instructionally meaningful (brown1978diagnostic; siegler2013early). For a tutor, the key step is not verifying correctness. It is inferring which malrule a student is using and predicting how it will reappear on the next problem, so feedback targets the underlying reasoning.\n\n\nCan modern AI do this? sonkar2025turing propose a direct ‚ÄúEducational Turing Test‚Äù for student modeling. Given evidence of a student‚Äôs misconception, can a model predict the specific errors that student will make on new problems? We operationalize this test with Malrule Reasoning Accuracy (MRA). In MRA, a model is shown a student‚Äôs incorrect solution to one problem. The model must infer the underlying malrule from that example and then predict the student‚Äôs answer on a new problem.\n\n\nCritically, the new problem often changes surface form. Table¬†2 illustrates the challenge. The model sees a student evaluate x2+25\\sqrt{x^{2}+25} at x=8x{=}8 and answer 1313, which implies the malrule a2+b2=a+b\\sqrt{a^{2}+b^{2}}=a+b. The model must then apply the same malrule to a different template, such as a distance word problem that implicitly requires 82+32\\sqrt{8^{2}+3^{2}}. This cross-template generalization is what tutoring demands. Students rarely repeat the same question, but they do repeat the same misconception across contexts.\n\n\n\n\n\nModel\nCRA\nMRA\nForward MRA\n\n\n\n\nLlama-3.3-70B\n70.4%\n34.6%\n39.6%\n\n\nQwen3-80B-Think\n70.1%\n56.4%\n53.9%\n\n\ngpt-oss-120b\n65.0%\n56.9%\n48.8%\n\n\nAverage\n68.5%\n49.3%\n47.5%\n\n\n\nTable 1: Three evaluation settings for the educational Turing Test. CRA: solve problems correctly. MRA: infer misconceptions from examples and predict student answers. Forward MRA: receive explicit misconception descriptions and predict answers.\n\n\n\n\n\n\n\nForward MRA\n\n\n\n\nMRA (Cross-Template)\n\n\n\n\n\n\n\n\nSystem: You are simulating a student who has a specific mathematical misconception. Apply the described misconception consistently to solve the problem.\n\n\n\n\nSystem: You are an expert in identifying and understanding student mathematical misconceptions. Given an example of a student‚Äôs incorrect answer, identify the systematic error and apply it to predict answers for new problems.\n\n\n\n\n\n\nUser: A student has the following misconception:\nStudents distribute square root over addition: a2+b2=a+b\\sqrt{a^{2}+b^{2}}=a+b\nApply this misconception to solve:\nEvaluate f‚Äã(x)=x2+4f(x)=\\sqrt{x^{2}+4} when x=3x=3. What is f‚Äã(3)f(3)?\n\n\n\n\nUser: A student solved this problem incorrectly:\nProblem: Evaluate f‚Äã(x)=x2+25f(x)=\\sqrt{x^{2}+25} when x=8x=8.\nStudent‚Äôs Answer: 13\nNow predict what this same student would answer for:\nYou walk 8 blocks east and 3 blocks north. What is the straight-line distance from your starting point?\n\n\n\n\n\n\nExpected: 5  (correct: 13‚âà3.61\\sqrt{13}\\approx 3.61)\n\n\n\n\nExpected: 11  (correct: 73‚âà8.54\\sqrt{73}\\approx 8.54)\n\n\n\n\n\nTable 2: Prompts for Forward MRA and MRA tasks. Forward MRA provides an explicit misconception description; the model must translate it into procedural errors. MRA provides only a worked example; the model must infer the misconception pattern and generalize it to a new problem format (here, from algebraic to word problem). Both prompts target the same underlying malrule: distributing square roots over addition.\n\n\n\n\n\n\n\nCategory\n\n\n\n\nMalrule\n\n\n\n\nProblem\n\n\n\n\nStudent‚Äôs Work\n\n\n\n\n\n\n\n\nRadicals\n\n\n\n\na2+b2=a+b\\sqrt{a^{2}+b^{2}}=a+b\n\n\n\n\nT1: Evaluate the function f‚Äã(x)=x2+25f(x)=\\sqrt{x^{2}+25} when x=8x=8. What is f‚Äã(8)f(8)?\n\n\n\n\nx2+25=x+5\\sqrt{x^{2}}+\\sqrt{25}=x+5; 8+5=ùüèùüë8+5=\\mathbf{13}\n\n\n\n\n\n\nT2: You walk 8 blocks east and 3 blocks north. What is the straight-line distance from your starting point?\n\n\n\n\n82+32=8+3=ùüèùüè\\sqrt{8^{2}}+\\sqrt{3^{2}}=8+3=\\mathbf{11}\n\n\n\n\n\n\n\n\nOrder of\nOperations\n\n\n\n\n\n\n\nAddition before\nsubtraction\n\n\n\n\n\nT1: Evaluate: 29‚àí28+1229-28+12\n\n\n\n\n28+12=4028+12=40; 29‚àí40=‚àíùüèùüè29-40=\\mathbf{-11}\n\n\n\n\n\n\nT2: Starting at 45¬∞F, the temperature decreases by 5¬∞F, then increases by 3¬∞F. What‚Äôs the result?\n\n\n\n\n5+3=85+3=8; 45‚àí8=ùüëùüï45-8=\\mathbf{37}¬∞F\n\n\n\n\n\n\nFunctions\n\n\n\n\n\n\nf‚Äã(a+b)=f(a+b)=\nf‚Äã(a)+f‚Äã(b)f(a)+f(b)\n\n\n\n\n\nT1: Given f‚Äã(x)=x3f(x)=x^{3}, evaluate f‚Äã(11+10)f(11+10)\n\n\n\n\nf‚Äã(11)=1331f(11)=1331, f‚Äã(10)=1000f(10)=1000; 1331+1000=ùüêùüëùüëùüè1331+1000=\\mathbf{2331}\n\n\n\n\n\n\nT2: Given f‚Äã(x)=|x+3|f(x)=|x+3|, evaluate f‚Äã(8+4)f(8+4)\n\n\n\n\nf‚Äã(8)=11f(8)=11, f‚Äã(4)=7f(4)=7; 11+7=ùüèùüñ11+7=\\mathbf{18}\n\n\n\n\n\n\nDivision\n\n\n\n\n\n\nLarger √∑\\div smaller\nalways\n\n\n\n\n\nT1: 4 cookies are shared equally among 6 children. How much does each child get?\n\n\n\n\n4√∑6‚Üí6√∑4=1.54\\div 6\\rightarrow 6\\div 4=\\mathbf{1.5}\n\n\n\n\n\n\nT2: 4 meters of ribbon is divided into 5 equal strips. How long is each strip in meters?\n\n\n\n\n4√∑5‚Üí5√∑4=1.254\\div 5\\rightarrow 5\\div 4=\\mathbf{1.25}\n\n\n\n\n\n\nSubtraction\n\n\n\n\n\n\nBorrow without\ndecrementing\n\n\n\n\n\nT1: Calculate: 408‚àí384408-384\n\n\n\n\n8‚àí4=48{-}4{=}4, 10‚àí8=210{-}8{=}2, 4‚àí3=14{-}3{=}1 ‚Üí\\rightarrow ùüèùüêùüí\\mathbf{124}\n\n\n\n\n\n\nT2: A store had 561 items in stock. After selling 526 items, how many remain?\n\n\n\n\n11‚àí6=511{-}6{=}5, 6‚àí2=46{-}2{=}4, 5‚àí5=05{-}5{=}0 ‚Üí\\rightarrow ùüíùüì\\mathbf{45}\n\n\n\n\n\nTable 3: Examples of malrules from MalruleLib, showing two templates per misconception. Each malrule produces systematic errors across different problem formats: algebraic expressions and word problems. The template diversity illustrates the cross-template generalization challenge: models must recognize the same underlying misconception despite surface-level differences. See Appendix Table¬†7 for additional examples.\n\n\nOur results expose a sharp gap between doing mathematics and modeling student thinking. On three representative large models (70B‚Äì120B) shown in Table¬†1, models achieve 68.5% accuracy on direct problem solving (CRA: Correct Reasoning Accuracy), but only 49.3% on cross-template misconception prediction from an example (MRA). Mathematical reasoning ability does not transfer to student modeling. We also evaluate Forward MRA, where the model is given an explicit natural-language description of the misconception and must apply it, and find performance remains limited (47.5% on the same models). Table¬†5 reports the full benchmark across all nine models and experimental settings.\n\n\nMisconceptions are well studied, but the field lacks infrastructure that treats them as computational objects. Most resources describe misconceptions, but they do not operationalize them as procedures that can be executed across many templates with malrule-consistent intermediate steps (lucy2024evaluating). This makes it difficult to generate training data, run controlled evaluations, or measure cross-template student modeling at scale.\n\n\nWe address this gap with MalruleLib111Code and models are available at https://github.com/luffycodes/malrulelib, a le"
  },
  {
    "title": "Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion",
    "url": "https://arxiv.org/abs/2601.03213v1",
    "source": "arxiv",
    "summary": "Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a gener",
    "full_text": null
  },
  {
    "title": "Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers",
    "url": "https://arxiv.org/abs/2601.03211v1",
    "source": "arxiv",
    "summary": "In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large lang",
    "full_text": null
  },
  {
    "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward",
    "url": "https://arxiv.org/abs/2601.03205v1",
    "source": "arxiv",
    "summary": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nData Synthesis for Logic and Reasoning.\nFine-Grained Reward Mechanisms.\nRole of Difficulty in Reinforcement Learning.\n\n\n\n3 The UltraLogic Data Framework\n\n3.1 Overall Architecture\n3.2 Original Task Repository\n3.3 Diverse Task Template Repository\n\n3.4 Data Synthesis Pipeline\n\n3.4.1 Input Code Generation\n3.4.2 Solution Code Generation\n3.4.3 Difficulty Control Module\n3.4.4 Data Validation and Quality Assurance\n\n\n\n\n\n4 Bipolar Float Rewards\n\n4.1 Baseline Training Paradigm and the Limitations of Binary Rewards\n\n4.2 Design and Iteration of the Graded Float Reward\n\n4.2.1 Initial Exploration: Graded Float Reward in the [0, 1] Range\n\n4.2.2 Bipolar Float Reward: Graded Penalties\n\nReshaping Advantage and the Non-negative reward trap.\nReward Cliff and the Bipolar Mapping Logic.\nPenalty-Driven Correction (Push-Pull Dynamics).\n\n\n\n\n\n\n\n5 Experimental Setup\n\n\n5.1 Training Configuration and Benchmarks\n\nTraining.\nEvaluation.\n\n\n\n5.2 Ablation Study Design\n\nDifficulty Matching.\nBipolar Float Reward.\n\n\n\n\n\n6 Results and Analysis\n\n6.1 Difficulty Matching: The Scaling Law of Difficulty\n6.2 Effectiveness of Bipolar Float Reward: The Penalty-Driven Optimization\n\n6.3 Additional Empirical Observations\n\nData Quality Sensitivity.\nArchitectural Choice.\n\n\n\n\n7 Conclusion\nDependence on Human Annotation.\nHeuristic Nature of Reward Scaling.\n\nA Detailed Data Settings\n\nA.1 Three-dimensional Orthogonal Categories\nA.2 Task Category Cases\nA.3 A Complete Task Case\n\n\n\nB Detailed Experiment Results\n\nB.1 Difficulty Matching Ablation Study\nB.2 Bipolar Float Reward Ablation Study\n\n\n\nC Prompts\n\nC.1 The Prompt of Task Mining and Template Creation\nC.2 The Prompt of Question Template Generalization\nC.3 The Prompt of Input Code Generation\nC.4 The Prompt of Solution code Generation\nC.5 The Prompt of Dynamic Parameter Adjustment\n\n\nD Details of Human Annotation\n\n\n\n\n\n\nUltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward\n\n\nFirst Author \nAffiliation / Address line 1 \nAffiliation / Address line 2 \nAffiliation / Address line 3 \nemail@domain\n&amp;Second Author \nAffiliation / Address line 1 \nAffiliation / Address line 2 \nAffiliation / Address line 3 \nemail@domain\n\n\n‚ÄÉ‚ÄÉ\n\nYile Liu1,2,\nYixian Liu111footnotemark: 1,\nZongwei Li111footnotemark: 1,\nYufei Huang1,\nXinhua Feng1,\nZhichao Hu1\nJinglu Hu2,\nJianfeng Yan1,\nFengzong Lian1,\nYuhong Liu1\n1Hunyuan, Tencent \n2Waseda University \nirei.liu@asagi.waseda.jp, ly_xian@whu.edu.cn, zongweili@tencent.com\n\nEqual contribution.Corresponding author.\n\n\nAbstract\nWhile Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning‚Äîrequiring multi-step logic, planning, and verification‚Äîremains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.\n\n\n\nUltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward\n\n\n\n\nYile Liu1,2‚Ä†‚Ä†thanks: Equal contribution.,\nYixian Liu111footnotemark: 1‚Ä†‚Ä†thanks: Corresponding author.,\nZongwei Li111footnotemark: 1,\nYufei Huang1,\nXinhua Feng1,\nZhichao Hu1\n\nJinglu Hu2,\nJianfeng Yan1,\nFengzong Lian1,\nYuhong Liu1\n\n1Hunyuan, Tencent\n\n2Waseda University\n\nirei.liu@asagi.waseda.jp, ly_xian@whu.edu.cn, zongweili@tencent.com\n\n\n\n\n\n1 Introduction\n\nIn recent years, Large Language Models (LLMs) have achieved revolutionary breakthroughs in numerous areas of Natural Language Processing (NLP)¬†achiam2023gpt; deepseekai2025deepseekv3technicalreport; yang2025qwen3technicalreport. However, complex reasoning‚Äîparticularly general-purpose reasoning that requires multi-step logic, planning, and verification‚Äîremains a critical bottleneck in advancing model intelligence to higher levels¬†liu2025agenticmathenhancingllmreasoning; wu-etal-2025-c2rbench. To overcome this limitation, both academia and industry have turned their attention to the post-training stage, especially methods based on Reinforcement Learning (RL)¬†ouyang2022traininglanguagemodelsfollow; rafailov2024directpreferenceoptimizationlanguage. These models have demonstrated astonishing improvements in reasoning abilities in domains like mathematics and code by leveraging Reinforcement Learning with Verifiable Rewards (RLVR)¬†shao2024deepseekmathpushinglimitsmathematical. The core of this paradigm lies in the fact that task answers in these domains (e.g., whether code passes unit tests or a mathematical answer is correct) have explicit, automatically verifiable feedback, providing a clear reward signal for reinforcement learning.\n\n\nHowever, when extending this paradigm from specific domains to broader, more general-purpose reasoning tasks, a fundamental problem becomes particularly prominent: the insufficient supply of high-quality training data¬†liu2025synlogicsynthesizingverifiablereasoning; lu2024mathgeniegeneratingsyntheticdata; liu2025agenticmathenhancingllmreasoning. The success of RLVR currently relies heavily on existing high-quality competition datasets¬†hendrycks2021measuringmathematicalproblemsolving; rein2023gpqagraduatelevelgoogleproofqa, but the field of general reasoning lacks similar large-scale data resources. Existing datasets are not only limited in task diversity¬†liu2025synlogicsynthesizingverifiablereasoning; li202512surveyreasoning, failing to cover a wide range of reasoning scenarios, but they also generally lack a clear and controllable difficulty calibration system¬†kwan2025opensir; wang2025morphobench. This makes the difficulty distribution of the training data hard to manage, which in turn affects the model‚Äôs learning efficiency and stability.\n\n\nTo systematically address this data supply problem, we propose and implement UltraLogic: an innovative framework for the large-scale, automated production of high-quality reasoning data, built upon a core Code-based Solving Framework methodology. This framework operates through a process that combines human definition with automated production. First, in a collaborative process between domain experts and prompt-driven LLMs, two critical Python functions are written for each novel reasoning task type: an input function to generate the slot-filling data for predefined problem templates, and a solution function to provide a deterministic ground-truth answer based on that data. This step ensures the logical correctness, verifiability, and controllability of all our data. Subsequently, an automated pipeline takes over, performing large-scale generalization on these \"seed tasks\" using Programmatic Expansion (PE) techniques¬†mishra2023lilaunifiedbenchmarkmathematical. This pipeline not only generates a vast number of problem variants but also systematically creates task instances spanning 10 difficulty levels by precisely controlling the parameters of the input function. These difficulty levels are then objectively calibrated against the measured success rates of flagship model. Experiments show that by training on our synthesized data alone, using a standard binary reward mechanism¬†shao2024deepseekmathpushinglimitsmathematical, the model exhibits a significant improvement in general reasoning capabilities compared to baselines.\n\n\nBuilding on this foundation, we further explore potential methods for improving training efficiency, which constitutes our second exploratory contribution. We hypothesize that although the binary reward mechanism is clear and effective, providing the learning process with a more information-dense and graded reward signal could potentially enhance both training efficiency and final performance. To this end, we design and implement a novel Bipolar Float Reward (BFR) mechanism, which introduces a penalty-driven optimization signal to the reinforcement learning process. This mechanism is capable of quantifying \"partially correct\" outputs based on task-specific characteristics, using metrics such as accuracy or F1-Score¬†paulus2017deepreinforcedmodelabstractive. Our experiments demonstrate that this bipolar approach significantly outperforms both binary and standard non-negative float rewards, facilitating faster convergence and superior final performance by effectively penalizing imperfect reasoning paths.\n\n\nIn summary, the contributions of this paper are twofold: first, we provide an automated framework for the large-scale production of diverse, difficulty-calibrated reasoning data, demonstrating that task diversity is a more critical driver for general reasoning enhancement than mere data scaling; second, we propose the Bipolar Float Reward (BFR) mechanism and provide empirical evidence that integrating a graded penalty into the reward signal is crucial for breaking performance bottlenecks in complex reasoning tasks.\n\n\n\n\n2 Related Work\n\nData Synthesis for Logic and Reasoning.\n\nData synthesis for logical reasoning tasks currently follows two primary paradigms:\nprogrammatic synthesis, which uses deterministic generators to produce data and answers¬†liu2025synlogicsynthesizingverifiablereasoning; and generative synthesis, which leverages powerful LLMs to ge"
  },
  {
    "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
    "url": "https://arxiv.org/abs/2601.03204v1",
    "source": "arxiv",
    "summary": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded",
    "full_text": "\n\n\n\nKeywords:\n1 Introduction\n\n2 Related Work\n\n2.1 Multi-Agent System Architectures\n2.2 Task Decomposition in Multi-Agent Systems\n2.3 Long-Context and Autonomous Research Agents\n\n\n\n3 Formalizing File-Centric State for Long-Horizon Agents\n\n3.1 Agent Execution as a State-Conditioned Decision Process\n3.2 Persistent State Externalization\n3.3 Bounded Reasoning Context Reconstruction\n3.4 Comparison and Implications\n\n\n\n4 Methodology: The InfiAgent Framework\n\n\n4.1 File-Centric State Management and Task Memory\n\nPeriodic State Consolidation.\n\n\n4.2 Multi-Level Agent Hierarchy\n4.3 External Attention Pipeline\n\n\n\n5 Experiments\n\n\n5.1 DeepResearch Benchmark\n\nSetup.\nOverall performance.\nComponent-wise analysis.\n\n\n\n5.2 Long-Term Literature Review Task\n\nEvaluation metric (coverage).\nResults.\nAblation analysis.\nLimitations of the comparison.\n\n\n\n5.3 Case Study: Real-World Applications and Expert Blind Review\n\nVersatile Application Scenarios of InfiHelper\nExpert Blind Review on Academic Output\n\n\n\n\n\n6 Discussion\n\nWhat Explicit State Externalization Does Not Solve.\nLong Context Is Not a Substitute for Persistent State.\nEfficiency and Latency Trade-offs.\nScope and Generalization.\nBroader Implications.\n\n\n7 Limitations\n8 Conclusion\nA Appendix: Detailed Experimental Results\n\n\n\n\n\nInfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents\n\n\nChenglin Yu\n\nThe University of Hong Kong\n\n\nYuchen Wang\n\nThe Hong Kong Polytechnic University\n\n\nSongmiao Wang\n\nThe Hong Kong Polytechnic University\n\n\nHongxia Yang\n\nThe Hong Kong Polytechnic University\n\n\nMing Li\nCorresponding author: ming.li@polyu.edu.hk\nThe Hong Kong Polytechnic University\n\n\n\nAbstract\nLLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent‚Äôs reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents.\nRepo:https://github.com/ChenglinPoly/infiAgent\n\n\nKeywords:\n\nLarge Language Model Agents, Infinite-Horizon Reasoning, File-Centric State Management, Autonomous Research, Multi-Agent Collaboration\n\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have increasingly been deployed as autonomous agents capable of planning, tool use, and multi-step reasoning across a wide range of tasks¬†[naveed2025comprehensive, tran2025multi]. Recent agent-based systems demonstrate promising performance in domains such as scientific research, software engineering, and information synthesis, suggesting that LLMs can serve as general-purpose problem solvers that coordinate external tools and resources. Despite this progress, existing LLM agents remain brittle when deployed over long task horizons.\n\n\nA core challenge arises from how agent state is represented and maintained. Most current agent frameworks implicitly treat the LLM prompt as the primary carrier of state, accumulating dialogue history, tool traces, intermediate plans, and partial results directly within the context window. As task duration increases, this design leads to unbounded context growth, forcing systems to rely on truncation, summarization, or heuristic retrieval to remain within model limits. These mechanisms introduce well-known failure modes, including information loss, interference from irrelevant tokens, and increased sensitivity to early errors, ultimately resulting in unstable behavior over long horizons.\n\n\nRetrieval-Augmented Generation (RAG) and long-context models partially mitigate context length limitations by enabling access to external memory or larger windows. However, these approaches still entangle long-term task state with the agent‚Äôs immediate reasoning context, placing increasing cognitive load on the LLM as execution progresses. Empirical evidence suggests that agent performance often degrades as context fills, a phenomenon sometimes described as the ‚Äúillusion of state‚Äù in autonomous research agents¬†[shojaee2025illusion]. Consequently, simply extending context length does not fundamentally resolve the long-horizon stability problem.\n\n\nRecent work such as MAKER¬†[meyerson2025maker] demonstrates that near-infinite execution is possible in highly structured domains through extreme task decomposition. While effective for logic-based problems with predefined subtask boundaries, such approaches rely on specialized micro-agents and rigid workflows, limiting their applicability to open-ended domains like scientific research, where task structure, intermediate goals, and relevant information sources are not known a priori.\n\n\nIn this work, we argue that achieving stable long-horizon behavior in LLM agents requires an explicit separation between persistent task state and bounded reasoning context. We introduce InfiAgent, a general-purpose agent framework that externalizes long-term state into a file-centric representation. Rather than storing historical information implicitly within the prompt, InfiAgent treats the file system as the authoritative and persistent record of the agent‚Äôs actions, environment, and intermediate artifacts. At each decision step, the agent reconstructs its reasoning context solely from a snapshot of this externalized state and a fixed-size window of recent actions, ensuring that the context size remains strictly bounded regardless of task duration.\n\n\nBuilding on this state abstraction, InfiAgent employs a hierarchical agent architecture that enforces structured task decomposition and controlled tool invocation. High-level planning agents operate over abstract goals and state summaries, while lower-level agents execute domain-specific or atomic actions. This design reduces error propagation commonly observed in flat multi-agent systems and enables consistent behavior across extended execution horizons. In addition, InfiAgent introduces an external attention mechanism that processes large documents and heavy information outside the main reasoning context, injecting only task-relevant outputs back into the agent‚Äôs state.\n\n\nWe evaluate InfiAgent on the DeepResearch benchmark¬†[shojaee2025illusion] and a long-horizon literature review task involving up to 80 academic papers. Without task-specific fine-tuning, InfiAgent equipped with a 20B-parameter open-source model achieves performance comparable to or exceeding larger proprietary agents on DeepResearch. In extended evaluations spanning hundreds of execution steps, the proposed file-centric state abstraction enables reliable task completion, whereas baseline agents frequently degrade due to context limitations. These results suggest that explicit state externalization provides a practical and effective foundation for building stable, general-purpose LLM agents capable of operating over arbitrarily long horizons.\n\n\n\n\n2 Related Work\n\n\n2.1 Multi-Agent System Architectures\n\nTraditional multi-agent systems have primarily focused on peer-to-peer collaboration models. While effective for simple coordination, these approaches struggle with complex, hierarchical task decomposition. Recent work has explored hierarchical organization¬†[hierarchical2025]. Moore¬†[taxonomy2025] categorized Hierarchical Multi-Agent Systems (HMAS) highlighting trade-offs between global efficiency and local autonomy. Comprehensive reviews note a persistent challenge in designing unified agents that seamlessly integrate cognition, planning, and interaction¬†[agentReview2025]. InfiAgent addresses these by enforcing stability through strict compositional constraints and a file-centric state.\n\n\n\n\n2.2 Task Decomposition in Multi-Agent Systems\n\nTask decomposition has been a central challenge. Existing approaches include goal-oriented¬†[goal2023], constraint-based¬†[constraint2017], and learning-based decomposition¬†[learning2022]. Frameworks like AGENTiGraph leverage knowledge graphs (KGs) for decomposition¬†[KGAgent2025]. However, these approaches often lack guarantees regarding system stability over long horizons. MAKER¬†[meyerson2025maker] introduced extreme decomposition for logic tasks, but its applicability to open-ended research remains limited. InfiAgent employs a recursive DAG-based decomposition that is flexible enough for general tasks while maintaining strict parent-child control.\n\n\n\n\n2.3 Long-Context and Autonomous Research Agents\n\nThe ‚Äùillusion of state‚Äù in deep research agents has been highlighted by Shojaee et al.¬†[shojaee2025illusion], showing that performance often degrades as context fills. Agents like AgentGym¬†[xi2024agentgym] and Richelieu¬†[guan2024richelieu] focus on self-evolution, while systems like the AI Scientist attempt end-to-end automation. However, many rely on extended context windows or manual templates. InfiAgent‚Äôs approach of ‚ÄùZero Context Compression‚Äù via file-based state offers a robust alternative to these context-heavy methods.\n\n\n\n\n\n3 Formalizing File-Centric State for Long-Horizon Agents\n\nWe formalize the long-horizon agent execution problem by distinguishing between persistent task state and bounded reasoning context. This separation clarifies the limitations of context-centric agent designs and motivates the file-centric state abstraction adopted by InfiAgent.\n\n\n\n3.1 Agent Execution as a State-Conditioned Decision Process\n\nWe consider an autonomous LLM agent operating ov"
  },
  {
    "title": "Counterfactual Fairness with Graph Uncertainty",
    "url": "https://arxiv.org/abs/2601.03203v1",
    "source": "arxiv",
    "summary": "Evaluating machine learning (ML) model bias is key to building trustworthy and robust ML systems. Counterfactual Fairness (CF) audits allow the measurement of bias of ML models with a causal framework, yet their conclusions rely on a single causal graph that is rarely known with certainty in real-world scenarios. We propose CF with Graph Uncertainty (CF-GU), a bias evaluation procedure that incorp",
    "full_text": null
  },
  {
    "title": "Recursive querying of neural networks via weighted structures",
    "url": "https://arxiv.org/abs/2601.03201v1",
    "source": "arxiv",
    "summary": "Expressive querying of machine learning models - viewed as a form of intentional data - enables their verification and interpretation using declarative languages, thereby making learned representations of data more accessible. Motivated by the querying of feedforward neural networks, we investigate logics for weighted structures. In the absence of a bound on neural network depth, such logics must ",
    "full_text": null
  },
  {
    "title": "DIP: Dynamic In-Context Planner For Diffusion Language Models",
    "url": "https://arxiv.org/abs/2601.03199v1",
    "source": "arxiv",
    "summary": "Diffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples. However, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generatio",
    "full_text": "\n\n\n\n1 Introduction &amp; Related Work\n2 Masked DLMs with Dynamic Context\n3 Problem Statement\n\n4 Dynamic In-Context Planner\n\n4.1 Block-wise Decoding with KV Cache\n4.2 Example Ranking\n4.3 Example Insertion Policy\n\n\n\n5 Experiment\n\n5.1 Experiment Setup\n5.2 Results\n\n\n6 Conclusion\nA Appendix: Algorithm for Fast-dLLM and DIP\nB Appendix: Use Of Ai Assistants\n\n\n\n\n\n\nDIP: Dynamic In-Context Planner For Diffusion Language Models\n\n\nYang Li ‚ÄÉHan Meng ‚ÄÉChenan Wang ‚ÄÉHaipeng Chen\nCollege of William &amp; Mary, Williamsburg, VA, USA \n{yli102, hmeng, cwang33, hchen23}@wm.edu\n\n\n\nAbstract\nDiffusion language models (DLMs) have shown strong potential for general natural language tasks with in-context examples.\nHowever, due to the bidirectional attention mechanism, DLMs incur substantial computational cost as context length increases. This work addresses this issue with a key discovery: unlike the sequential generation in autoregressive language models (ARLMs), the diffusion generation paradigm in DLMs allows efficient dynamic adjustment of the context during generation.\nBuilding on this insight, we propose Dynamic In-Context Planner (DIP), a context-optimization method that dynamically selects and inserts in-context examples during generation, rather than providing all examples in the prompt upfront.\nResults show DIP maintains generation quality while achieving up to 12.9√ó\\times inference speedup over standard inference and 1.17√ó\\times over KV cache-enhanced inference.\n\n\n\nDIP: Dynamic In-Context Planner For Diffusion Language Models\n\n\n\n\nYang Li ‚ÄÉ‚ÄäHan Meng ‚ÄÉ‚ÄäChenan Wang ‚ÄÉ‚ÄäHaipeng Chen\n\nCollege of William &amp; Mary, Williamsburg, VA, USA\n\n{yli102, hmeng, cwang33, hchen23}@wm.edu\n\n\n\n\n\n\n1 Introduction &amp; Related Work\n\nDiffusion language models have emerged as a new paradigm for language modeling, demonstrating strong potential across general natural language tasks (nie2025large; ye2025dream; austin2021structured; deepmindGeminiDiffusion; arriola2025block; lou2023discrete; sahoo2024simple; lou2023discrete).\nHowever, the DLM‚Äôs decoding mechanism is fundamentally different. By utilizing bidirectional attention and iterative refinement, DLMs remove the strict left-to-right constraint inherent to AR generation.\nThis paradigm shift offers fine-grained control over the generation process, enabling flexible editing and global planning (zhang2025survey; jin2025thinking; li2025diffusion; chen2025dparallel).\n\n\nSimilar to ARLMs, DLMs also benefit from In-Context Learning (ICL), in which providing in-context examples improves performance on many retrieval-augmented generation tasks (gao2023retrieval).\nExisting ICL approaches for DLMs largely inherit the AR convention (radford2018improving; bi2025optagent), which keeps the prompt fixed and uses all in-context examples throughout generation (radford2018improving; dong2024survey). As shown in Figure 1, the static all-in-context example approach could lead to slower inference speed.\nPopular DLMs (ye2025dream; nie2025large) utilize bidirectional attention, which scales poorly with context length.\nWhile recent optimizations, such as KV cache (liu2025dllm; song2025sparse), parallel decoding (jiang2025d; wu2025fast), and block-wise decoding (wu2025fast), can reduce the computation of static prompts, they do not resolve the fundamental inefficiency: the generative process still requires computing full attention over context during the generation process, which limits inference throughput (wu2025fast).\n\n\nFigure 1: DLMs using Fast-dLLM (wu2025fast) shows a negative correlation between throughput and the number of examples in the prompt.\n\n\nFigure 2: Dynamic In-Context Planner (DIP): (1) Example ranking stage uses MMR to rank the candidate examples by their marginal utility. (2) Insertion policy progressively inserts new examples into the context between blocks. \n\n\nThis work addresses this issue with a key discovery: unlike the sequential generation, which requires a fixed context in ARLMs, the diffusion generation paradigm in DLMs is free from these constraints and allows efficient dynamic adjustment of the context during generation. Take the 3-shot GSM8k task as an example. In ARLMs, all examples must be provided at once in the prompt before generation begins. On the contrary, as illustrated in the middle column of Figure 2, the diffusion generation paradigm allows generation with 1 to 3 examples across different stages of the generation process\n(initialized with 1-shot at block 1, insert 1 example at block 2, ‚Ä¶).\nBy dynamically selecting in-context examples based on the evolving generation state, we could increase inference throughput, ideally without compromising the generation quality.\n\n\nBuilding on these insights, we propose Dynamic In-Context Planner (DIP), a context-optimization method that dynamically selects in-context examples during generation. DIP consists of a Maximal Marginal Relevance (MMR) based example ranking stage and a confidence- and generation-process-aware example insertion policy that performs insertion decisions during generation. DIP is training-free and can serve as a plug-in method that efficiently selects examples and updates the prompt on top of prominent DLM frameworks such as Fast-dLLM wu2025fast.\n\n\nOur main contributions include:\n(1) We make a key discovery that the diffusion generation paradigm of DLMs allows efficient dynamic context adjustment, opening a new avenue for efficient DLMs inference based on dynamic context optimization.\n(2) We propose DIP, a practical context-optimization method that dynamically updates in-context examples during generation, which consistently yields inference speedup.\n(3) Empirical results on the 5-shot GSM8k task show that DIP can maintain generation quality while achieving up to 12.9√ó\\times inference speedup over LLaDA‚Äôs standard inference nie2025large and 1.17√ó\\times over KV cache-enhanced inference like Fast-dLLM wu2025fast.\n\n\n\n\n2 Masked DLMs with Dynamic Context\n\nIn this section, we briefly discuss the modeling details of DLMs that enable dynamic ICL with minimal overhead.\nMasked DLMs (the absorbing state discrete diffusion models) (sahoo2024simple; zhu2025llada; ye2025dream) consist of a forward and reverse process, where an absorbing state is a mask token, denoted as [MASK], which represents missing or corrupted information (lou2023discrete; nie2025large).\n\n\nIn the forward process, qq with an initial sequence y0y_{0} of length NN is organized into discrete time steps, where each time step corresponds to a single masking stage. At each time step, independent tokens are gradually replaced by [MASK] tokens with probability t‚àà[0,1]t\\in[0,1], which is defined as:\n\n\n\nq‚Äã(yt|y0)=‚àèi=0N‚àí1q‚Äã(yti|y0i),q(y_{t}|y_{0})=\\prod\\nolimits^{N-1}_{i=0}q(y_{t}^{i}|y_{0}^{i}),\n\n(1)\n\n\nwhere i‚àà[0,N]i\\in[0,N] and yiy^{i} stands for the ii-th token. Once a token transitions to [MASK], it remains in this state throughout the forward process. As tt increases to 11, the original sequence will eventually become a fully masked sequence.\n\n\nConversely, in the reverse process pŒ∏p_{\\theta}, at each time step, masked DLMs aim to predict the probability distribution over all masked tokens simultaneously given yty_{t}, which can be formulated as:\n\n\n\npŒ∏‚Äã(ys|yt)=‚àèi=0N‚àí1pŒ∏‚Äã(ysi|yti).p_{\\theta}(y_{s}|y_{t})=\\prod\\nolimits_{i=0}^{N-1}p_{\\theta}(y_{s}^{i}|y_{t}^{i}).\n\n(2)\n\n\nBased on these predictions, the additional sampling method SS decides which tokens to \"unmask\" at each step. As tt decreases to 0, the masked sequence will eventually recover the original sequence.\n\n\nMasked DLMs usually employ a bidirectional attention mechanism (e.g., BERT (devlin2019bert)) and scale poorly with context length, as shown in Figure 1.\nTo address this, recent KV cache inference frameworks often use block diffusion, dividing the overall generation length into bb equal-length discrete blocks and performing semi-AR decoding on the block level. For instance, Fast-dLLM (wu2025fast) employs a KV cache for prefix and masked-suffix tokens in non-decoding-blocks, achieving significant speedups.\n\n\n\n\n3 Problem Statement\n\nWe formally define the new dynamic example optimization task: instead of using all examples as context throughout DLM‚Äôs generation process, we aim to decide when and which examples to insert during generation to accelerate inference while maintaining inference quality.\n\n\nDefinition 1\n\n(ICL Setting)\nGiven a prompt pp with a pool of in-context examples Ep‚Äão‚Äão‚Äãl={(qi,ai)}i=1KE_{pool}=\\{(q_{i},a_{i})\\}_{i=1}^{K}, where qiq_{i} and aia_{i} denote the query and answer of the ii-th example, respectively.\n\n\n\nDefinition 2\n\n(Efficient Inference Setting)\nGiven a KV cache-based inference process (e.g., Fast-dLLM wu2025fast) divided into NN non-KV cache steps (or blocks), indexed by b‚àà{0,‚ãØ,N‚àí1}b\\in\\{0,\\cdots,N-1\\}.\n\n\n\nGoal:\nThe goal is to find an optimal policy œÄ\\pi that selects a subset of examples Eb‚äÜ{Ep‚Äão‚Äão‚Äãl,‚àÖ}{E}_{b}\\subseteq\\{{E}_{pool},\\emptyset\\} for each block bb, which maintains the generation quality while improving the overall throughput.\n\n\n\n\n4 Dynamic In-Context Planner\n\nAs shown in Figure 2, the Dynamic In-Context Planner (DIP) is a two-stage framework that decouples selection complexity into: (1) example ranking stage and (2) example insertion policy.\nThe ranking stage orders the candidate examples in Ep‚Äão‚Äão‚ÄãlE_{pool} by their marginal utility.\nThen, an example insertion policy is a monotonic-increasing policy that progressively inserts these examples into the context at different generation blocks bb based on generation confidence and generation stage. A formal algorithm is outlined in Appendix A.\n\n\n\n4.1 Block-wise Decoding with KV Cache\n\nAs DIP aims to optimize inference throughput, it is essential to ensure compatibility with the KV cache-enhanced inference framework. We build DIP on top of Fast-dLLM (wu2025fast), a KV cache enhanced block-wise inference framework, which requires a necessary non-KV cache call at the start of discrete block decoding to update the KV cache, providing an"
  },
  {
    "title": "Empowering Reliable Visual-Centric Instruction Following in MLLMs",
    "url": "https://arxiv.org/abs/2601.03198v1",
    "source": "arxiv",
    "summary": "Evaluating the instruction-following (IF) capabilities of Multimodal Large Language Models (MLLMs) is essential for rigorously assessing how faithfully model outputs adhere to user-specified intentions. Nevertheless, existing benchmarks for evaluating MLLMs' instruction-following capability primarily focus on verbal instructions in the textual modality. These limitations hinder a thorough analysis",
    "full_text": null
  },
  {
    "title": "Software-Defined Agentic Serving",
    "url": "https://arxiv.org/abs/2601.03197v1",
    "source": "arxiv",
    "summary": "As multi-agent LLM pipelines grow in complexity, existing serving paradigms fail to adapt to the dynamic serving conditions. We argue that agentic serving systems should be programmable and system-aware, unlike existing serving which statically encode the parameters. In this work, we propose a new SDN-inspired agentic serving framework that helps control the key attributes of communication based o",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Agentic Serving\n\n2.1 Agentic Workflows\n2.2 Limitations today\n2.3 Improving Agentic Serving\n\n\n\n3 A Software-Defined Approach\n\n3.1 Agent-Serving Control Plane\n3.2 Metrics Plane\n3.3 Configurable Data Plane\n3.4 Strawman Implementation\n\n\n4 Related Work\n5 Discussion and Conclusion\n\n\n\n\n\nSoftware-Defined Agentic Serving\n\n\nSaurabh Agarwal\n\nUniversity of Texas-Austin\n\n, \nMarco Laju\n\nUniversity of Texas-Austin\n\n, \nJayanth Srinivasa\n\nCisco-Research\n\n, \nMyungjin Lee\n\nCisco-Research\n\n and \nAditya Akella\n\nUniversity of Texas-Austin\n\n\n\nAbstract.\nAs multi-agent LLM pipelines grow in complexity, existing serving paradigms fail to adapt to the dynamic serving conditions. We argue that agentic serving systems should be programmable and system-aware, unlike existing serving which statically encode the parameters.\nIn this work, we propose a new SDN-inspired agentic serving framework that helps control the key attributes of communication based on runtime state. This architecture enables serving-efficient, responsive agent systems and paves the way for high-level intent-driven agentic serving.\n\n\n\n1. Introduction\n\nThe rise of Large Language Models (LLMs) has catalyzed a growing ecosystem of intelligent LLM-powered agents‚Äîeach acting as a specialized model for tasks such as code generation, testing, retrieval, and planning. To accomplish these complex tasks, these agents need to\nroutinely invoke APIs, tools, and other models in ‚Äùagentic pipelines‚Äù¬†(park2023generative, ; talebirad2023multi, ).\n\n\nConsider an agentic software development pipeline illustrated in Figure¬†3. The workflow involves communication between two agents: a ‚Äùdeveloper‚Äù agent that emits functions, and a ‚Äùtester‚Äù agent that provides feedback.\nThere are several possible communication strategies between the agents: (a) Batching all functions in one call, which can reduce overhead and improves throughput. (b) Function-by-function pipelining, which enables early feedback and concurrency. (c) Token-level streaming, which minimizes latency for interactive responsiveness.\nIn this example, the ‚Äúbest‚Äù strategy depends as much on the application needs as on the current system state. For example, batching may preserve throughput under high load and reduce backlogs; streaming improves app responsiveness under low load.\nMore crucially, the agnetic serving decisions‚Äî regarding batch size, routing, communication granularity‚Äîboth influence and are influenced by runtime system factors, e.g., load, queue lengths, model latency, token generation rates, user responsiveness goals, SLOs, etc. (¬ß2). Ideally, the decisions should thus be dynamic and adaptive to serving-system state.\n\n\n\n\n\nFigure 1. Agentic Software Developer: The above highlights a software agent workflow where the developer agent is responsible for generating functions and the testing agent generates testing code. \n\n\n\n\nFigure 2. Communication Pattern: The above schematic provides an approximate timeline for the communication pattern for a possible different communication pattern.\n\n\n\n\nFigure 3. Serving throughput using A2A: We serve two agents using three different communication mechanisms under varying load. No one configuration consistently outperforms another.\n\n\n\n\n\nUnfortunately, today‚Äôs agentic serving architectures make optimal strategy selection difficult, especially as system dynamics change. This is due to three fundamental drawbacks in existing agentic serving designs.\nFirst, agentic communication protocols today, such as Anthropic‚Äôs Model Context Protocol (MCP)¬†(mcp, ), Google‚Äôs Agent2Agent (A2A)¬†(a2a, ), IBM‚Äôs Agent Connection Protocol (ACP)¬†(acp, ), and others¬†(agntcy, ),\nrequire developers to early bind to a strategy at design time‚Äîsuch as choosing batching, streaming, or pipelined sends.\nSecond, serving parameters like batch size, communication frequency, routing strategy, model selection are unaware of serving-layer parameters like ‚Äì model latency, queue times, cost per inference, or cross-agent resource contention ‚Äì\non which performance critically depends.\nFinally, today‚Äôs communication architectures lack interfaces for cross-agent and pipeline-wide optimization, e.g., for prioritization at downstream agents, or balancing user experience with end-to-end system throughput.\n\n\nTaken together, these attributes make agentic serving architectures both a burden on developers of agentic pipelines‚Äìit is difficult to ensure and control performance‚Äìand a source of serious run-time inefficiency.\n\n\nWe advocate for a new agentic serving architecture where the agent communication is dynamically controllable and strongly coupled with the serving system. Specifically, inspired by SDN, we argue for a programmable agentic substrate. However, distinct from SDN, we require the substrate to go beyond communication and also interface directly with applications to enable better control over serving, ensuring efficiency and performance.\n\n\nWe envision a ‚Äùsoftware-defined agentic serving stack‚Äù with three components:\n\n\n‚Ä¢\n\nA data plane that supports fine-grained, dynamically adjustable agentic message granularities spanning token-level streaming to batched contexts.\n\n\n\n‚Ä¢\n\nA metrics plane which provides low-overhead access to metrics via a simple, cross-agent/tool uniform data collection interface.\n\n\n\n‚Ä¢\n\nA control plane that leverages metrics to dynamically govern inter-agent communication strategies, batch size, routing, and model selection across agent pipelines. It systematically evolves system-wide state to a target that meets operator-specified goals. Control decisions account for both system- and application-level metrics.\n\n\n\n\n\nWe envision such a stack to support high-level declarative languages for agent communication and serving policy. These languages could allow infrastructure engineers to express goals‚Äîsuch as ‚Äùminimize end-to-end latency,‚Äù ‚Äùbatch unless load is low,‚Äù or ‚Äùstream only for interactive or high-priority tasks‚Äù‚Äîwithout needing to manage the low-level implementation details. The control plane translates these intents into real-time decisions based on the current serving state. The architecture frees agentic workflow developers from implementing bespoke approaches for controlling application performance; instead, agentic pipelines simply offer the control plane the appropriate hints.\n\n\nWe discuss challenges arising from tool/agent heterogeneity and churn in realizing the above vision. We present simple unified interfaces and design ideas for the three planes that can enable effective control despite these attributes.\n\n\nOur preliminary prototype shows that our architecture can improve agentic serving throughput by up to 3.6√ó3.6\\times by fine-grained control of the communication granularity. Further, when additional control is exercised over the serving system, the serving throughput improves by another 2.3√ó2.3\\times.\n\n\nMore than a specific proposal, the main aim of our work is to kickstart a discussion around addressing fundamental issues in emerging agentic architectures. We present a framework inspired by SDN, along with APIs that integrate with current and future tools and agents, to overcome these issues. But we recognize that these are mere starting points in an interesting design space. We conclude by identifying key future directions in ¬ß5.\n\n\n\n\n2. Agentic Serving \n\n\n\nLLMs are rapidly evolving beyond\ntheir basic use as chatbots, increasingly serving as full-scale digital assistants.\nIn particular, LLMs are being given access to additional tools¬†(qin2023toolllm, ; yang2023gpt4tools, ; schick2023toolformer, ) (e.g., calculators, web search engines, databases, file systems, to name a few) and being treated as autonomous agents cooperating with other agents to fulfill complex tasks (e.g., software development¬†(hong2023metagpt, ) and problem solving¬†(tan2024taskgen, )). Such workflows necessitate LLMs communicating with other tools and LLMs.\n\n\nUnfortunately, today‚Äôs agentic serving systems have fundamental limitations. We start by describing how these systems work.\n\n\n\n\n\n‚¨á\n\n1# Client initialization and capability determination\n\n\n2with httpx.client() as httpx_client:\n\n\n3 developer_agent = A2AClient.get_client_from_agent_card_url(\n\n\n4 httpx_client, \"http://localhost:6069\"\n\n\n5)\n\n\n6\n\n\n7 testing_agent = A2AClient.get_client_from_agent_card_url(\n\n\n8 httpx_client, \"http://localhost:6070\"\n\n\n9)\n\n\n10 prompt = \"Write‚ê£three‚ê£functions‚ê£to‚ê£creating‚ê£an‚ê£entry,\n\n\n11‚ê£‚ê£‚ê£‚ê£update‚ê£it‚ê£and‚ê£delete‚ê£it\"\n\n\n12\n\n\n13 send_message_payload = message{\n\n\n14 'role': \"user\",\n\n\n15 'parts': [{'type': 'text', 'text': prompt}],\n\n\n16 }\n\n\n17\n\n\n18 streaming_request = SendStreamingMessageRequest(\n\n\n19 params=MessageSendParams(**send_message_payload)\n\n\n20 )\n\n\n21 # perform streaming message\n\n\n22 stream_response = developer_agent.send_message_streaming(\n\n\n23 streaming_request)\n\n\n24 # waiting for streaming response\n\n\n25 async for chunk in stream_response:\n\n\n26 # stream to testing agent\n\n\n27 testing_request =SendStreamingMessageRequest(\n\n\n28 params=MessageSendParams(**chunk)\n\n\n29 )\n\n\n30 test_funtion = testing_agent.send_message_streaming(chunk)\n\n\n31 await asyncio.sleep(0.1)\n\n\n\n\nFigure 4. A code snippet illustrating an agentic application written using A2A.\n\n\n\n2.1. Agentic Workflows\n\nConstructing multi-agent LLM workflows today requires developers to manually orchestrate communication across tools and models. Developers curate a list of callable agents, handling authentication, and managing communication modes for each endpoint‚Äîoften via heterogeneous mechanisms such as REST, RPC, or streaming APIs.\nTo manage this complexity, recent efforts have introduced protocol-based abstractions. Examples include Google‚Äôs A2A¬†(a2a, ), Anthropic‚Äôs MCP¬†(mcp, ), and IBM‚Äôs ACP¬†(acp, ). These protocols provide common interfaces for agent discovery and message exchange, aiming to standardize agentic workflows.\nFigure¬†4 shows an example using A2A to connect a ‚Äúdeveloper‚Äù agent with a ‚Äútesting‚Äù agent. The workflow obtains agent cards (Lines 3 and 7), sends a streaming "
  },
  {
    "title": "Sparse Knowledge Distillation: A Mathematical Framework for Probability-Domain Temperature Scaling and Multi-Stage Compression",
    "url": "https://arxiv.org/abs/2601.03195v1",
    "source": "arxiv",
    "summary": "We develop a unified theoretical framework for sparse knowledge distillation based on probability-domain softening operators. While the equivalence $p^{1/T} \\propto \\mathrm{softmax}(z/T)$ is well known, our contribution is an operator-level analytical framework built on this foundation rather than the equivalence itself.\n  The framework comprises four core components: (i) operator-agnostic bias--v",
    "full_text": null
  },
  {
    "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
    "url": "https://arxiv.org/abs/2601.03194v1",
    "source": "arxiv",
    "summary": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing t",
    "full_text": "\n\n\n\n\nIntroduction\n\nMotivation: Scarcity of Hateful Rationales\nA Glance at the Proposed Solution\n\n\n\nRelated Work\n\nExplainability in Hate Speech Detection\nMultilingual Explainable Methods\n\n\n\nDataset Description and Annotation\n\n\nDataset Source\n\nProfile of Annotators\n\n\nRationale Annotation Procedure\n\n\n\nMethodology\n\nTraining Procedure\nExplainability via N-gram Contributions\nFinal Explanations through LLM Consultation\n\n\n\nExperimental Setup and Results\n\n\nMetrics for Evaluation\n\nPerformance-Based Metrics\n\nExplainability-Based Metrics\n\nPlausibility\nFaithfulness\n\n\n\n\nComparison Methods\nResults Analysis for Telugu\nResults Analysis for Hindi\nResults Analysis for English\nParameter Sensitivity Analysis &amp; Ablation\nGeneralizability\n\n\nConclusion\nAcknowledgments\n\n\n\n\n\n\nX-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection\nand A Novel LLM-consulted Explanation Framework\nMohammad Zia Ur Rehmana (phd2101201005@iiti.ac.in)\nSai Kartheek Reddy Kasub (saikartheekreddykasu@gmail.com)\nShashivardhan Reddy Koppulaa (cse210001032@iiti.ac.in)\nSai Rithwik Reddy Chirrac (schirra7@asu.edu)\nShwetank Shekhar Singhd (b21322@students.iitmandi.ac.in,)\nNagendra Kumara (nagendra@iiti.ac.in)\na Indian Institute of Technology Indore, Madhya Pradesh India\nb Indian Institute of Information Technology Dharwad, India\nc Arizona State University, United States\nd Indian Institute of Technology Mandi, India\nThis is the preprint version of the accepted paper.\nAccepted in Proceedings of AAAI, 2026\n\n\nAbstract\nHate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and the model‚Äôs explainability. Moreover, combining human rationales with our explainability method to refine the model‚Äôs attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on:\nhttps://github.com/ziarehman30/X-MuTeST.\n\n\nIntroduction\n\nThe rise in online media usage has heightened exposure to hate speech, making detection systems increasingly essential. While initial research focused only on the detection of hateful content (Waseem and Hovy 2016), more recent efforts have shifted towards explainability and addressing the rationales behind labelling content as hateful (Clarke et¬†al. 2023). Advanced methods such as Large Language Models (LLMs) and their predecessors, such as BERT (Devlin et¬†al. 2019), can provide explanations for such decisions. LLMs can be prompted to provide the explanations in the form of an identified hateful list of words (Roy et¬†al. 2023) for a given sentence, whereas methods such as BERT can provide explanations in the form of attention given to each word of the sentence (Mathew et¬†al. 2021). However, in the case of under-resourced languages, where overall sequence classification performance has improved significantly over time (Rehman et¬†al. 2023), machine-provided rationales, many a time, do not align well with human rationales. For instance, in Figure 1, the translation of the first and third rationales provided by the human annotator would be ‚Äúto feel offended‚Äù and ‚Äúto bark‚Äù, respectively, which are very offensive terms in the Hindi language. Though the LLMs correctly identify the explicit term ‚Äúdog‚Äù, they do not identify first and third rationale terms as hateful or offensive, which can be attributed to the fact that societal and cultural aspects of languages are often overlooked by these methods for under-resourced languages. This gap between machine-provided rationales and human rationales can be bridged by providing rationale resources for these languages.\n\n\nFigure 1: Example of human and LLM rationales\n\n\nMotivation: Scarcity of Hateful Rationales\n\nThough numerous rationale and reasoning-based resources for the English language (Rajani et¬†al. 2019; Camburu et¬†al. 2018) are compiled in ERASER (DeYoung et¬†al. 2020) in various domains spanning from movie reviews (Zaidan and Eisner 2008), questions with multiple correct answers (Khashabi et¬†al. 2018), and to other miscellaneous domains (Lehman et¬†al. 2019; Clark et¬†al. 2019), there is a scarcity of rationale-based resources in the domain of hate speech detection. In 2021, Mathew et¬†al. (2021) introduced the HateXplain dataset, which features word and phrase-level span annotations for hate speech in English, claiming to be the first of its kind. However, similar datasets for under-resourced languages such as Hindi and Telugu are nonexistent to the best of our knowledge. Even for English, HateXplain is one of the few datasets offering human rationales for hate speech detection. To address this gap, we provide word-level human rationales for hate speech detection datasets in Telugu, Hindi, and English. Our primary focus was on Indic languages. However, we believe that evaluating models on the English dataset alongside HateXplain enhances the generalizability of future approaches.\n\n\n\nA Glance at the Proposed Solution\n\nWe propose a novel explainability framework that combines LLM-based and n-gram-based explanations to generate final explanations. We propose an eXplainable Multilingual haTe Speech deTection ( X-MuTeST) method. X-MuTeST is a two-stage explainable hate speech detection framework designed to enhance both classification performance and explainability. It incorporates a novel explainability method based on n-grams to identify relevant tokens. In the first stage, the model‚Äôs attention is guided by human-annotated rationales to align the model‚Äôs focus with key tokens identified by humans. In the second stage, training is guided by the n-gram-based explainability method, which generates attention masks based on model-driven token importance. This approach balances human rationales and the model‚Äôs insights, improving both explainability and classification performance. Finally, union of explanations is taken from LLama 3.1 and X-MuTeST. Through experiments on three datasets, Hindi, Telugu, and English, we demonstrate the efficacy of the proposed framework.\nThe key contributions of this work are as follows:\n\n\n‚Ä¢\n\nWe contribute benchmark human-annotated rationales for under-resourced languages, such as Hindi and Telugu, alongside English. This will provide a valuable resource for future research in explainable hate speech detection across multiple languages.\n\n\n\n‚Ä¢\n\nWe propose a novel LLM-consulted and n-gram-based hybrid explainability method that combines LLMs with traditional transformer-based attention improvement. The proposed method enhances explainability performance.\n\n\n\n‚Ä¢\n\nWe propose a novel explainable hate speech detection framework with a two-stage training method that balances classification and explainability performance.\n\n\n\n\n\n\n\nRelated Work\n\nExplainability in Hate Speech Detection\n\nExplainability in hate speech detection has garnered significant attention, with recent advancements highlighting both its potential and underlying challenges. The works, such as Zaidan, Eisner, and Piatko (2007) and Yessenalina, Choi, and Cardie (2010) utilized rationales for improving sentiment classification, which set the stage for explainable approaches in more domains (Wang et¬†al. 2024), including hate speech (Mathew et¬†al. 2021; Lin et¬†al. 2024).\nThe work of Mathew et¬†al. (2021) provides one of the first datasets to include human-annotated rationales for hate speech. For each sample, annotators not only label whether the text contains hate speech but also highlight the specific tokens that are relevant to that judgment. The authors highlight that even though a model may be accurate in classification, its reasoning may not align with human logic or provide meaningful explanations.\n\n\nRecent works in LLMs and attention-based models also highlight critical challenges in explainability, as seen in works by Roy et¬†al. (2023), Clarke et¬†al. (2023), and Arshad and Shahzad (2024). These studies underscore difficulties in ensuring faithful rationales, even when powerful models achieve high classification accuracy.\n\n\n\nMultilingual Explainable Methods\n\nKapil et¬†al. (2023) and Sawant et¬†al. (2024) emphasize the importance of precise annotations and transparency in low-resource languages such as Hindi, where cultural aspects affect how hate speech is perceived and classified. Similarly, Yadav, Kaushik, and McDaid (2023) address the complexities of code-mixed languages, highlighting how explainability helps clarify model predictions in multilingual settings. In this context, Geleta et¬†al. (2023) introduce a multilingual framework for assessing hate speech intensity levels using explainable AI, further underlining the need for transparent hate speech detection models. By leveraging explainable methodologies, these studies collectively enhance the detection of hate speech across linguistic boundaries, ensuring that cultural and linguistic differences are accounted for in model interp"
  },
  {
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "url": "https://arxiv.org/abs/2601.03193v1",
    "source": "arxiv",
    "summary": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controll",
    "full_text": null
  },
  {
    "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
    "url": "https://arxiv.org/abs/2601.03192v1",
    "source": "arxiv",
    "summary": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Continuous Learning\n2.2 Reinforcement Learning\n2.3 Agentic Memory\n\n\n\n3 Problem Formulation\n\n3.1 Memory-Augmented Agent Policy\n3.2 Non-Parametric Reinforcement Learning\n\n\n\n4 MemRL\n\n4.1 Memory Structure: The Intent-Experience-Utility Triplet\n\n4.2 Two-Phase Retrieval: From Semantic Recall to Value-Aware Selection\n\nPhase A: Similarity-Based Recall.\nPhase B: Value-Aware Selection.\n\n\n4.3 Runtime Learning: Non-Parametric RL on Memory\n4.4 Cognitive Interpretation\n\n4.5 Stability Analysis\n\nSetup.\nStationary Reward Assumption.\nExpected Convergence of Utility Estimates.\nProof.\nBounded Variance and Stability.\nGlobal Stability via EM Convergence.\n\n\n\n\n\n5 Experiments\n\n\n5.1 Experimental Setup\n\nBaselines.\nBenchmarks.\nRuntime Learning Results.\nTransferring Results.\n\n\n\n5.2 Ablations\n\n5.2.1 Effectiveness of Runtime RL\n5.2.2 Impact of Q-Value Weighting\n5.2.3 Sensitivity to Retrieval Size (k1k_{1} and k2k_{2}).\n\n\n\n5.3 Discussion\n\n5.3.1 MemRL as a Trajectory Verifier.\n\n5.3.2 Predictive Power of the Q Critic\n\nCase study: a high-Q ‚Äúfailure‚Äù memory as a transferable near-miss.\n\n\n5.3.3 Stability of MemRL\n5.3.4 Impact of Task Similarity on Memory Efficacy.\n\n\n\n\n6 Conclusion\n\nA Theoretical Analysis and Proofs\n\n\nA.1 Proof of Theorem¬†1: Convergence of EMA Estimation\n\nAssumptions.\nDerivation of Error Dynamics.\nConvergence Analysis.\n\n\n\nA.2 Bounded Variance and Global Stability\n\nDerivation of the Variance Bound.\nRecursive Unrolling.\nAsymptotic Convergence.\nConnection to Phase-A Clustering.\n\n\n\n\n\nB Theoretical Analysis: Convergence via Variational Inference\n\nB.1 The Convergence Objective\nB.2 Variational Objective with Trust Region\n\nB.3 Optimization via Generalized Expectation-Maximization (GEM)\n\nE-Step (Policy Optimization).\nM-Step (Policy Evaluation via Error Minimization).\n\n\nB.4 Proof of Convergence\n\n\n\n\n\n\n\n\n\nMemRL: Self-Evolving Agents via Runtime \nReinforcement Learning on Episodic Memory\n\n\n\nShengtao Zhang1, ,\nJiaqian Wang2,* ,\nRuiwen Zhou3 ,\nJunwei Liao1,4 ,\nYuchen Feng5 , \n¬†Weinan Zhang1,4 ,\nYing Wen1,4 ,\nZhiyu Li5 ,\nFeiyu Xiong5 ,\nYutao Qi2 ,\nBo Tang6,5, ,\n¬†Muning Wen1,‚Ä†\\dagger\n\n1Shanghai Jiao Tong University, ‚ÄÉ2Xidian University, ‚ÄÉ3National University of Singapore, \n4Shanghai Innovation Institute, ‚ÄÉ5MemTensor (Shanghai) Technology Co., Ltd. \n6University of Science and Technology of China \n\n\nEqual contribution. The order is decided by flipping a coin.Corresponding to Bo Tang (tangb@memtensor.cn) and Muning Wen (muningwen@sjtu.edu.cn)\n\n\nAbstract\nThe hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation‚Äîretrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory.\nUnlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.\n\n\n\n\n\n\nFigure 1: Benchmark Runtime Learning performance of MemRL. We compare MemRL against state-of-the-art memory baselines (MemP) and standard retrieval methods (RAG). MemRL consistently outperforms various baselines, demonstrating the efficacy of runtime utility-driven updates.\n\n\n\n\n1 Introduction\n\nFigure 2: The conceptual framework of MemRL.\n\n\nThe hallmark of human intelligence lies in the delicate balance between the stability of cognitive reasoning and the plasticity of episodic memory (grossberg2013adaptive; mcclelland1995there; kumaran2016learning), a mechanism known as Constructive Episodic Simulation that allows adaptation without rewiring neural circuitry (schacter2007constructive; hassabis2007deconstructing; schacter2012future; gick1980analogical).\nWhile Large Language Models (LLMs) demonstrate impressive reasoning capabilities, existing paradigms struggle to emulate this dynamic, decoupled self-evolution (wei2022chain; yao2022react; schick2023toolformer; wang2023voyager). On one hand, fine-tuning approaches attempt to internalize experience by modifying model weights (ouyang2022training; stiennon2020learning; rafailov2023direct; ethayarajh2024kto), but often suffer from catastrophic forgetting and high computational costs (kirkpatrick2017overcoming; li2024revisiting; wu2024continuallearninglargelanguage). On the other hand, Retrieval-Augmented Generation (RAG) (lewis2020retrieval) offers a non-parametric alternative but remains fundamentally passive; it retrieves information based solely on semantic similarity without evaluating its actual utility (karpukhin2020dense; gao2023retrieval). Lacking a mechanism to distinguish high-value past strategies from similar noise, current RAG agents struggle to effectively learn from runtime feedback to optimize their performance over time.\n\n\nThis limitation underscores a critical research question: How can we enable an agent to continuously improve its performance after deployment, without compromising the stability of its pre-trained backbone? Our objective is to achieve an agent that evolves with continued usage and rapidly adapts to new tasks after deployment, referred to as Runtime Continuous Learning (javed2023online; silver2025era_of_experience; parisi2019continual; wu2024continuallearninglargelanguage), all while keeping the backbone model frozen to prevent catastrophic forgetting (finn2017model; wei2025evo).\nTo address this challenge, inspired by the human cognitive mechanism of constructive simulation, we propose MemRL, a framework that facilitates self-evolving agents by explicitly decoupling the model‚Äôs stable cognitive reasoning from dynamic episodic memory. Figure¬†2 illustrates the conceptual framework of our proposed MemRL.\nDrawing on value-iteration approaches in Reinforcement Learning (RL) to estimate expected experience utilities (sutton2018reinforcement), we formalize the interaction between the frozen LLM and external memory as a Markov Decision Process (MDP) (puterman2014markov). Unlike traditional methods that optimize the backbone model, MemRL optimizes the policy of memory usage to maximize expected utility.\n\n\nMemRL organizes memory into a structured Intent-Experience-Utility triplet. This structure transforms retrieval from a passive semantic match task into an active decision-making process: Value-Aware Retrieval selects experiences based on their learned Q-values, reflecting expected utility, rather than semantic similarity alone (watkins1992q); Utility-Driven Update refines these Q-values via environmental feedback and Bellman backup (bellman1966dynamic).\nThis closed-loop cycle enables the agent to distinguish high-value strategies from similar noise, effectively learning from both success and failure without the computational cost or catastrophic forgetting risks associated with weight updates. As for experiments, we validate MemRL on four diverse benchmarks, including HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench. Our results demonstrate consistent superiority over baselines, achieving relative improvement in exploration-heavy environments. Our in-depth analysis reveals a strong correlation between learned utility and task success, further confirming MemRL‚Äôs effectiveness.\n\n\nIn summary, our contributions are threefold:\n\n\n\n‚Ä¢\n\nWe propose a runtime learning framework based on Model-Memory decoupling and the Intent-Experience-Utility triplet, which reconciles the stability-plasticity dilemma by enabling agents to learn without parameter updates.\n\n\n\n‚Ä¢\n\nWe introduce MemRL, a non-parametric reinforcement learning algorithm that implements Value-Aware Retrieval and Utility-Driven Memory Curation. This allows agents to self-evolve by optimizing memory utility, establishing a new paradigm for enhancing agent capabilities.\n\n\n\n‚Ä¢\n\nWe conduct extensive evaluations and provide deep insights into MemRL‚Äôs working mechanism. We analyze how it ensures structural integrity in complex tasks and theoretically substantiate its stability via Bellman contraction, exploring how utility-driven updates minimize catastrophic forgetting while maximizing positive transfer.\n\n\n\n\n\n\n\n2 Related Works\n\n\n2.1 Continuous Learning\n\nContinual learning addresses the stability-plasticity dilemma, aiming to acquire new knowledge sequentially without suffering from catastrophic forgetting. Classical approaches‚Äîsuch as regularization, distillation, and experience replay‚Äîmitigate forgetting by constraining parameter updates or preserving past data distributions (kirkpatrick2017overcoming; li2017learning; lopez2017gradient). However, these parametric methods are computationally expensive for LLMs and risk destabilizing the pre-trained backbone through frequent online updates. Recent surveys on continual learning for LLMs further systematize these difficulties and emphasize the importance of external mechanisms and non-parametric pathways (wu2024continuallearninglargelanguage). Therefore, from a continual learning perspective, if we aim for agents to improve with use while preserving the stability of the backbone, a more practical direction is to shift"
  },
  {
    "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation",
    "url": "https://arxiv.org/abs/2601.03191v1",
    "source": "arxiv",
    "summary": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address th",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methodology\n\n3.1 Anatomy Perception Module\n3.2 Large Language Model\n\n\n\n4 Experiments\n\n4.1 Dataset\n4.2 Radiology Tasks\n4.3 Training Scheme\n\n\n\n5 Results and Discussion\n\n5.1 Grounding\n5.2 Report Generation\n5.3 Image Understanding\n5.4 Visual Question Answering (VQA)\n\n\n6 Ablation\n7 Conclusion\nS1 Self-Similarity Loss Matrix\nS2 Vector Database\n\nS3 Radiology Tasks\n\nS3.1 Image Understanding\nS3.2 Grounding\nS3.3 Report Generation\nS3.4 Visual Question Answering\n\n\n\n\n\n\n\nAnatomiX, an Anatomy-Aware Grounded\nMultimodal Large Language Model for Chest X-Ray Interpretation\n\n\nAnees Ur Rehman Hashmi\nHasso Plattner Institute\nPotsdam, Germany\nanees.hashmi@hpi.de\n\n‚ÄÉ‚ÄÉ\nNuman Saeed\nMBZUAI\nAbu Dhabi, UAE\nnuman.saeed@mbzuai.ac.ae\n\n‚ÄÉ‚ÄÉ\nChristoph Lippert\nHasso Plattner Institute\nPotsdam, Germany\nchristoph.lippert@hpi.de\n\n\n\nAbstract\nMultimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at github.com/aneesurhashmi/anatomix\n\n\nFigure 1: Comparison between AnatomiX and RadVLM [10] in anatomy understanding. (a) and (b) show both models predicting the disease on the correct side (color scheme: red for model‚Äôs output, green for all ground truth locations). (c), (d) and (e) show models‚Äô outputs for the same image flipped on the vertical axis (left ‚Üî\\leftrightarrow right), where RadVLM completely fails to recognize the correct anatomical object, while AnatomiX successfully recognizes the correct anatomies, showcasing high anatomical understanding\n\n\n\n1 Introduction\n\nMultimodal Large Language Models (MLLMs) are being increasingly applied in the natural and medical imaging domain to perform multiple tasks using a single model [36]. These models typically consist of an image encoder and a Large Language Model (LLM), and utilize the pretrained LLM‚Äôs strengths by passing image embeddings along with a text prompt into the LLM to perform downstream tasks [20]. The pretrained LLMs are generally trained on very large text corpora and therefore demonstrate strong text generation capabilities, making them suitable for a diverse set of downstream tasks after supervised fine-tuning and instruction tuning [1, 24]. However, owing partly to their autoregressive design and the challenges of merging vision and language modalities, MLLMs still struggle with fine-grained spatial understanding, for instance, when reasoning about positions of multiple objects or their relative spatial relations in a scene [21, 34].\n\n\nThis issue of MLLMs has previously been addressed by introducing object grounding, which aligns the text concepts with the objects in the image [33, 28]. Grounding in MLLMs is usually achieved by training the model on a dataset containing the names or descriptions of local objects in the image plus their bounding boxes or segmentation masks as spatial markers. This in turn improves the reasoning abilities of MLLMs with better concept understanding, making them applicable in the medical domain, where spatial reasoning is essential. In particular, chest X-ray (CXR) interpretation greatly benefits from such multimodal reasoning, as accurate localization and semantic alignment between textual findings and radiographic regions are crucial for diagnosis. MLLMs like ViviMed [23], ChexAgent [7], RadVLM [10] and MAIRA-2 [4] show that grounding via special tokens yields consistent performance gains on CXR image tasks.\n\n\nAlthough incorporating grounding through additional tokens has improved MLLMs‚Äô spatial reasoning, it remains insufficient for the fine-grained localization and differentiation required in medical imaging, where anatomically distinct regions often exhibit highly similar visual textures and appearances [34]. As illustrated in Fig. 1, current state-of-the-art (SOTA) MLLM fails to correctly localize lesions or identify the correct anatomical objects when presented with flipped images - where the left and right sides are switched. These models may perform well on standard orientations but fail when spatial cues are inverted, revealing that they overly rely on spatial correlations rather than recognition of anatomical structures, exposing a critical gap between visual grounding and medical comprehension.\n\n\nThis weak anatomical understanding in current medical MLLMs is likely due to their single-step visual grounding process. Specifically, these models must implicitly detect the correct anatomical objects within an image before performing the downstream task. This one-step process differs fundamentally from the workflow of radiologists, who iteratively identify, localize, and evaluate each anatomical structure before drawing diagnostic conclusions. To address this issue, we introduce AnatomiX, an anatomy-aware grounded MLLM for chest X-Ray interpretation. AnatomiX uses a two stage process to first identify different thoracic anatomical objects (organs) before performing the task; thereby, showing a high anatomical understanding compared to existing CXR grounding MLLMs as shown in Fig. 1. Our proposed model significantly outperforms SOTA models on four grounding tasks and shows SOTA or on-par performance on report generation, VQA and image understanding tasks. Extensive experiments on a large collection of datasets show the high reasoning and anatomical understanding capabilities of AnatomiX. In summary, our work makes the following contributions:\n\n\n\n\n‚Ä¢\n\nWe introduce AnatomiX, an anatomy-aware grounded multimodal large language model for chest X-Ray interpretation.\n\n\n\n‚Ä¢\n\nAnatomiX achieves SOTA performance on a CXR related grounding task, while maintaining on-par or better performance on report-generation, VQA and image understanding tasks.\n\n\n\n‚Ä¢\n\nWe demonstrate the robustness of AnatomiX across different datasets and perform ablations to validate the contribution of each component.\n\n\n\n\n\n\n\n2 Related Work\n\nEarly adaptations of MLLMs to radiology primarily involve fine-tuning general-domain models on medical datasets. LLaVA-Med [16] and RadVLM [10] extend LLaVA [20] for multi-task CXR benchmarks, improving both report generation and VQA. Several works introduce explicit grounding for CXR tasks: ViviMed [23] and MedRG [38] pair the Segment-Anything model [15] with an LLM for detection and segmentation, while MAIRA-2 [4] enables grounded report generation through additional tokens. Similarly, RadVLM [10] constructs a large instruction dataset for diverse CXR tasks, whereas CheXagent [7] applies contrastive learning and instruction tuning to enhance phrase grounding and CXR report generation. Radialog [27] supports multi-turn CXR conversations, and MedGemma [30] adapts Gemma-3 for general purpose medical tasks by finetuning on large-scale medical datasets. More recently, AOR [17] introduced region level information in LLM for CXR interpretation; however, this model is not yet publicly available for testing and comparison.\n\n\nFigure 2: Anatomy Perception Module (APM) architecture (a): The encoder ‚Ñ∞\\mathcal{E} outputs image embedding Ip\\mathrm{I}_{p}, while the decoder ùíü\\mathcal{D} and feature extraction module ‚Ñ≥\\mathcal{M} output object bounding boxes y^b‚Äão‚Äãx\\hat{y}_{box}, and anatomical object tokens O^A\\hat{\\mathrm{O}}_{A}, respectively. Different colors in O~\\tilde{\\mathrm{O}}, O\\mathrm{O} and O^A\\hat{\\mathrm{O}}_{A} represent specific anatomical objects. (b) shows the contrastive alignment using frozen sentence encoder ùíÆ\\mathcal{S} and self-similarity loss. (c): The vector database (ùí±D‚ÄãB\\mathcal{V}_{DB}) contains the text sentences and embeddings used for contrastive retrieval. (Bottom right): APM uses (a) and (b) during training, and replaces (b) with (c) during inference. The ùí´\\mathcal{P} represent different FC projectors described in section 3.1.\n\n\nMost prior efforts adapt general-domain MLLMs rather than developing domain-specific architectures. CheX [25] advances toward anatomy-aware modeling by incorporating anatomical objects from CXR reports but lacks prompt-based interaction and generative flexibility. Overall, existing CXR MLLMs rely on instruction tuning and large-scale radiology datasets, achieving strong benchmark performance yet showing limited reasoning and anatomical understanding [34]. In contrast, our approach introduces a two-stage anatomy-aware pipeline that explicitly models thoracic structures before performing downstream tasks.\n\n\n\n\n3 Methodology\n\nThis section describes the architecture of AnatomiX, which comprises two primary components: The Anatomy Perception Module and a large language model outlined below.\n\n\n\n3.1 Anatomy Perception Module\n\nGiven an input CXR image I\\mathrm{I}, the objective of the Anatomy Perception Module (APM) is to extract a global image representation along with fine-grained features corresponding to NN thoracic anatomical objects. The resulting representations are subsequently used by an LLM for downstream tasks. The APM adopts a multi-task learning framework tha"
  },
  {
    "title": "Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning",
    "url": "https://arxiv.org/abs/2601.03190v1",
    "source": "arxiv",
    "summary": "Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these li",
    "full_text": null
  },
  {
    "title": "Decentralized Autoregressive Generation",
    "url": "https://arxiv.org/abs/2601.03184v1",
    "source": "arxiv",
    "summary": "We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of ",
    "full_text": null
  },
  {
    "title": "Multi-Modal Data-Enhanced Foundation Models for Prediction and Control in Wireless Networks: A Survey",
    "url": "https://arxiv.org/abs/2601.03181v1",
    "source": "arxiv",
    "summary": "Foundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving m",
    "full_text": "\n\n\n\n\nI Introduction\n\nI-A Related Surveys\nI-B Contributions and Organization\n\n\n\nII Background on Foundation Models\n\nII-A Evolution and Capabilities of FMs: From Representation Learning to LLMs\nII-B Well-known Foundation Models and Wireless-Specific Foundation Models\nII-C Deployment of Foundation Models in Wireless Networks\n\n\n\nIII Foundation Model-Enabled Multi-Modal Contextual Information Understanding in Wireless Networks\n\n\nIII-A Vision-based Wireless Applications Enhanced by Foundation Models\n\nIII-A1 Foundation Model Capabilities for Visual Information Extraction\nIII-A2 FM-Enhanced Wireless Applications\n\n\n\nIII-B Graph-based Wireless Network Analysis Enhanced by Foundation Models\n\nIII-B1 Fundamental AI works about graph FMs\nIII-B2 Applications of graph FMs in wireless networks\n\n\nIII-C 3D Point Cloud Information Analysis in Wireless Networks\n\nIII-D Other Modalities and Data Sources in Wireless Networks\n\nIII-D1 ISAC data\nIII-D2 RF data\nIII-D3 Network usage data\n\n\n\n\n\nIV Foundation Models for Prediction Tasks in Wireless Networks\n\nIV-A Foundation Models for Prediction\n\nIV-B Foundation Models for Wireless Traffic Analysis and Prediction\n\nIV-B1 Train FMs from scratch\nIV-B2 Fine-tune open-source pre-trained FMs\nIV-B3 Leverage pre-trained FMs directly\n\n\nIV-C Foundation Models for Other Prediction Tasks\n\n\n\nV Foundation Models for Control in Wireless Networks\n\nV-A Foundation Models as Teachers for RL\nV-B Foundation Models as Representation Learners for Control Tasks\nV-C Foundation Models as Interactive Agents\n\n\n\nVI Developing Wireless-specific Foundation Models: Datasets\n\nVI-A Wireless Traffic Datasets\nVI-B Radio Frequency Datasets\nVI-C Sensing Modality Datasets\nVI-D Data Suitability and Limitations\nVI-E Preprocessing Considerations for Wireless-Specific Modalities\n\n\n\nVII Developing Wireless-specific Foundation Models: Methods\n\n\nVII-A Pre-training Foundation Models for Wireless Networks\n\nVII-A1 Model design\nVII-A2 Tokenization Strategies\nVII-A3 Pre-training strategies\n\n\nVII-B Fine-tuning Foundation Models\nVII-C On-Device and Distributed Inference\nVII-D Privacy in Federated Learning-enabled Collaborative Foundation Model Development\n\n\n\nVIII Open Issues and Future Directions\n\nVIII-A Development of Foundation Models for Wireless-Specific Modalities\nVIII-B Empowering Pre-trained Foundation Models with Wireless-Specific Knowledge\nVIII-C Exploring the Deployment of Foundation Models in Wireless Networks\nVIII-D Enhancing the Security of Foundation Models\nVIII-E Enhancing the Robustness of Foundation Models\nVIII-F Developing On-device Small-Scale Foundation Models\nVIII-G Latency Considerations in FM-Enabled Wireless Systems\nVIII-H Overcoming Data Scarcity for Multi-Modal FM Development\nVIII-I Towards Agentic AI for Autonomous Wireless Networks\n\n\nIX Conclusion\n\n\n\n\n\nMulti-Modal Data-Enhanced Foundation Models \nfor Prediction and Control in Wireless Networks: \nA Survey\n\n\nHan Zhang,\nMohammad Farzanullah,\nMohammad Ghassemi,\nAkram Bin Sediq,\nAli Afana,\nand Melike Erol-Kantarci\nHan Zhang, Mohammad Farzanullah, Mohammad Ghassemi and Melike Erol-Kantarci are with the School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON K1N 6N5, Canada (e-mail: hzhan363@uottawa.ca; mfarz086@uottawa.ca; mghas017@uottawa.ca; melike.erolkantarci@uottawa.ca).Akram Bin Sediq and Ali Afana are with Ericsson, Ottawa, K2K 2V6, Canada (e-mail:\nakram.bin.sediq@ericsson.com; ali.afana@ericsson.com)\n\n\n\nAbstract\nFoundation models (FMs) are recognized as a transformative breakthrough that has started to reshape the future of artificial intelligence (AI) across both academia and industry. The integration of FMs into wireless networks is expected to enable the development of general-purpose AI agents capable of handling diverse network management requests and highly complex wireless-related tasks involving multi-modal data. Inspired by these ideas, this work discusses the utilization of\nFMs, especially multi-modal FMs in wireless networks. We focus on two important types of tasks in wireless\nnetwork management: prediction tasks and control tasks. In particular, we first discuss FMs-enabled multi-modal contextual information understanding in wireless networks. Then, we explain how FMs can be applied to prediction and control tasks, respectively. Following this, we introduce the development of wireless-specific FMs from two perspectives: available datasets for development and the methodologies used. Finally, we conclude with a discussion of the challenges and future directions for FM-enhanced wireless networks.\n\n\nIndex Terms: \n\nGenerative foundation models, multi-modal foundation models, Generative AI, large language models, wireless networks, traffic prediction, B5G\n\n\n\n\nI Introduction\n\n\nThe rise of foundation models (FMs) marks a significant breakthrough that is shaping the artificial intelligence (AI) field. FMs refer to large-scale machine learning (ML) models trained on extensive datasets that can be adapted and fine-tuned for various applications and downstream tasks [ArtificialIntelligenceFoundation]. Given their high generality and adaptability, there is a growing trend of using open-source FMs as the backbone of scenario-oriented AI applications. This approach effectively reduces engineering costs and minimizes the need for extensive involvement in model design and training. As FMs grow in popularity, the AI landscape is expected to shift from developing custom-built AI systems to creating general-purpose, deployment-ready AI applications [FoundationModelsANewParadigm][TowardsArtificialGeneral].\n\n\nThe emergence of the transformer architecture has dramatically accelerated the progress of FMs [AComprehensiveSurvey]. The transformer is a neural network architecture based on the multi-head self-attention mechanism [AttentionIsAllYouNeed]. It typically consists of alternating attention and feed-forward layers that perform repeated transformations on the vector representations and extract latent information. The introduction of the attention mechanism in the transformer architecture allows elements in the input sequence to be weighted based on their importance and enables the model to capture long-range dependencies. Benefiting from this architecture, transformers exhibit scaling laws [ScalingLawsForNeural], showing that the model‚Äôs performance predictably increases when the model size, dataset size, and compute budget increase. The transformer‚Äôs scaling law ensures its ability to scale effectively over ultra-large datasets. This led to the idea of using large-scale transformer-based models trained on massive datasets to gain emergent capabilities and generalization capabilities on a series of downstream tasks, which motivated the emergence and popularity of FMs. Compared with traditional AI techniques, FMs have two unique characteristics: emergence and homogenization [OnTheOpportunities]. Emergence means that the model behavior is induced from training on extensive data rather than being explicitly constructed. Homogenization means the FM is a single generic learning algorithm that can handle a wide range of applications. These two characteristics enhance the usability of FMs across diverse data types and domains.\n\n\nTable I: A comparison between the definition and capabilities of LLM, Generative AI, and FM.\n\n\nConcepts\nDefinition\nCapabilities\nExamples\n\n\nLLM\n\n\n\nTransformer-based ML\n\n\nmodels that can comprehend and\n\n\ngenerate human language text [ASurveyOfLargeLanguageModels].\n\n\n\n\n\n\nMulti-modal information understanding\n\n\nand generation, emergent capabilities,\n\n\nreasoning and so on\n\n\n\n\n\n\nGenerative pre-training transformer (GPT) [GPT4MODEL, GPT2MODEL],\n\n\nPathways language model (PaLM) [PALMMODEL],\n\n\nLarge language model Meta AI (LLAMA) [LLAMAMODEL]\n\n\n\n\n\n\n\n\n\nGenerative\n\n\nAI\n\n\n\n\n\n\nA class of algorithms and models\n\n\nwithin AI and NLP designed to generate\n\n\nnew, previously unseen data rather than\n\n\ndistinguishing between existing categories\n\n\nas discriminative models [RecentAdvnces].\n\n\n\n\n\n\nMulti-model content generation,\n\n\nfraud detection, recommendations\n\n\n\n\n\n\nGenerative adversarial network (GAN) [GAN],\n\n\nVariational autoencoder (VAE) [VAE],\n\n\nAutoregressive Models,\n\n\nDiffusion Models\n\n\n\n\n\nFM\n\n\n\nA series of models trained on broad data\n\n\nthat can be adapted to a wide range\n\n\nof downstream tasks, making them different\n\n\nfrom task-agnostic models [OnTheOpportunities].\n\n\n\n\n\n\nEmergence, homogenization,\n\n\nand domain adaption,\n\n\nserves as the common basis\n\n\n\n\n\n\nGPT [GPT4MODEL, GPT2MODEL], Bidirectional encoder\n\n\nrepresentations from transformers (BERT) [BERTMODEL],\n\n\nDALL-E [DALLEMODEL]\n\n\n\n\n\n\n\n\nIn recent years, various AI techniques have been widely employed in all aspects of beyond fifth-generation (B5G) wireless communications, including optimizing network performance, improving network security, and automating the manual processes involved in managing wireless networks [ExploringThePotentialOfAI]. These techniques possess the capabilities to handle the complexity of the network architecture and the growing demand for communications [AIEnabledFuture]. However, most of the current widely used AI-driven network management applications are based on small-scale custom-built AI models.\n\n\nWhile existing small-scale, custom-built AI techniques have significantly accelerated the evolution of wireless networks, several opportunities remain to be seized. A primary concern is that custom-built AI techniques are usually only applicable to a single use case or a limited number of specific use cases. Once the scenario changes, the model has to be rebuilt and trained again. Future intelligent communications are expected to consider not only how AI techniques can be used in specific network optimization problems or scenarios. They are expected to also incorporate AI into all aspects of the wireless communication system and propose a generic approach to designing AI-driven wireless communications and realizing the vision of intrinsic AI [TelecomsArtificialGene"
  },
  {
    "title": "Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions",
    "url": "https://arxiv.org/abs/2601.03173v1",
    "source": "arxiv",
    "summary": "Time pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spannin",
    "full_text": "\n\n\n\nI Introduction\n\nII Literature Review\n\nII-A Time Pressure and Driving Behaviour\nII-B Time Pressure and Gap-Acceptance Behaviour\nII-C Time Pressure Effects on PTW Riding: Speed and Over-Speeding\nII-D Time Pressure and Distraction in MTW Riders\nII-E Simulator Validity Under Time Pressure\nII-F Machine Learning for Behaviour and Risk Modeling\nII-G Research gap and motivation\n\n\n\nIII Methodology for Data Collection\n\nIII-A Simulator Setup\nIII-B Simulator Scenario Design\nIII-C Participants\nIII-D TP Scenarios\nIII-E Behavioral Transition Under Exam Pressure: Why LTP Matters\nIII-F Dataset Collection and Experimental Design\nIII-G Dataset Details\n\n\n\nIV Methodology for TP prediction\n\nIV-A MTPS Architecture\nIV-B Model Structure\n\n\n\nV Experimental Setup\n\nV-A Dataset Preprocessing\nV-B Baseline Models and Evaluation Metrics\nV-C Implementation and Training Details\n\n\n\nVI Results and Analysis\n\nVI-A Empirical Analysis of TP Impact on Speed and Control\nVI-B The Inadequacy of Rider Compensation Under TP\nVI-C Systemic Risk Escalation Under TP\n\nVI-D MTPS Model Performance and Validation\n\nVI-D1 Overall Performance Comparison\nVI-D2 Ablation Study\nVI-D3 Statistical Significance of MTPS\nVI-D4 Importance of TP Prediction\nVI-D5 Impact of Feature Subsets on MTPS Performance\n\n\n\nVI-E Model Performance and Reliability\n\nVI-E1 Confusion Matrix\nVI-E2 ROC and Discriminatory Power\nVI-E3 Probability Outputs from Softmax Classification\nVI-E4 Calibration and Transition Across TP States\nVI-E5 Model Performance and Confidence Intervals\nVI-E6 Worked Example: Calm / NTP Class\n\n\n\nVI-F Threshold-Based Rider State Transitions and Sensitivity Analysis\n\nVI-F1 Threshold Selection and Statistical Support\nVI-F2 Mapping Predicted Probabilities to ITS Interventions\nVI-F3 Sensitivity and Cost Considerations\n\n\nVI-G MTPS Model Complexity and Edge-AI Deployability\n\n\n\nVII Applications of Predicted TP\n\n\nVII-A Significance of Predicted MTPS TP for Collision Prediction\n\nVII-A1 Evaluation of MTPS-Predicted TP\n\n\nVII-B Application in ITS Interventions\n\n\n\nVIII Discussions and Conclusions\n\nVIII-A Future Work\n\n\n\n\n\n\n\n Predicting Time Pressure of Powered Two-Wheeler Riders for Proactive Safety Interventions\n\n\n\nSumit S. Shevtekar1,\nChandresh K. Maurya1,\nGourab Sil2,\nSubasish Das3\nCorresponding author: Sumit Shevtekar, email: phd2401101010@iiti.ac.in1Department of Computer Science and Engineering, Indian Institute of Technology Indore, India. Email: chandresh@iiti.ac.in2Department of Civil Engineering, Indian Institute of Technology Indore, India. Email: gourabsil@iiti.ac.in3Civil Engineering Program, Ingram School of Engineering, Texas State University, USA. Email: subasish@txstate.edu\n\n\nAbstract\nTime pressure critically influences risky maneuvers and crash proneness among powered two-wheeler riders, yet its prediction remains underexplored in intelligent transportation systems. We present a large-scale dataset of 129,000+ labeled multivariate time-series sequences from 153 rides by 51 participants under No, Low, and High Time Pressure conditions. Each sequence captures 63 features spanning vehicle kinematics, control inputs, behavioral violations, and environmental context. Our empirical analysis shows High Time Pressure induces 48% higher speeds, 36.4% greater speed variability, 58% more risky turns at intersections, 36% more sudden braking, and 50% higher rear brake forces versus No Time Pressure. To benchmark this dataset, we propose MotoTimePressure, a deep learning model combining convolutional preprocessing, dual-stage temporal attention, and Squeeze-and-Excitation feature recalibration, achieving 91.53% accuracy and 98.93% ROC AUC, outperforming eight baselines. Since time pressure cannot be directly measured in real time, we demonstrate its utility in collision prediction and threshold determination. Using MTPS-predicted time pressure as features, improves Informer-based collision risk accuracy from 91.25% to 93.51%, approaching oracle performance (93.72%). Thresholded time pressure states capture rider cognitive stress and enable proactive ITS interventions, including adaptive alerts, haptic feedback, V2I signaling, and speed guidance, supporting safer two-wheeler mobility under the Safe System Approach.\n\n‚Ä†‚Ä†publicationid: pubid: \n\n\nI Introduction\n\n\nPowered two-wheelers (PTWs) are among the most accessible and widely used transport modes, particularly in developing countries. However, they carry elevated crash risks due to exposure to rider and balance control demands¬†[10122165]. The severity of the crash is strongly influenced by overspeeding, traffic dynamics, rider behavior, and road conditions¬†[Seefong2024, 7328721, 9625971], with human error and risky maneuvers being the leading causes¬†[pavlidis2016dissecting, 6899632, CHOUHAN20241378]. PTWs account for 21% of global road fatalities rising to 38% of deaths and 51% of injuries in Low- and Middle-Income Countries (LMICs)¬†[WHO2023]. Almost half of traffic deaths involve vulnerable users such as pedestrians, cyclists, and PTW riders¬†[WHO2023]. Excess speed alone contributes to 34% PTW fatalities, with riders traveling up to 2.3 times faster than car drivers¬†[Sharma21042025].\n\n\nAccording to India‚Äôs Ministry of Road Transport and Highways (MoRTH), PTWs are the most prevalent and economical mode of transport in LMICs. Between 2003 and 2022, India‚Äôs registered vehicle population grew to 354 million, of which PTWs constituted 263 million (74.4%). This segment grew at a CAGR of 8.6%, surpassing buses (2.48%) and goods vehicles (7.3%). Category-wise registration trends are shown in Fig.¬†1, underscoring the growing dependence on PTWs and the urgent need for targeted safety interventions¬†[MORTH202425].\n\n\nFigure 1: Registered motor vehicles in India by category (2003‚Äì2022)¬†[MORTH202425].\n\n\nPTWs also account for the largest share of traffic fatalities in India, accounting for 44.5% of deaths in 2022, followed by pedestrians (19.5%), cars/LMVs (12.5%), trucks/lorries (6.3%), autorickshaws (3.9%), bicycles (2.9%), buses (2.4%), and others (8.0%)¬†[MORTH202324, MORTH202122, MORTH202223]. According to official MoRTH reports on road accidents in india , road fatalities in India are predominantly male, ranging from 85.2% to 87.3% between 2019 and 2023¬†[morth2019, morth2020, morth2021, morth2022, morth2023], with most fatalities occurring in the 18‚Äì45 age group. These statistics show the high risk associated with male PTW riding behaviour. Driving errors are the main cause of crash globally, especially for PTWs, which face high instability and dynamic control challenges¬†[8294048, BHALLA201583]. Field studies of PTW behavior have shown significant variability in acceleration and braking patterns¬†[Mondal21042023], emphasizing the inherent dynamic control challenges these vehicles face in real-world traffic. Simulator studies further reveal context-dependent rider behavior in mixed traffic¬†[10073653], underscoring PTWs vulnerability and the need for targeted safety interventions.\n\n\nA critical yet underexplored factor in PTW safety is time pressure (TP), reflecting perceived urgency or limited time to complete riding tasks¬†[PAWAR2022105582]. For example, many delivery PTW riders in India are subjected to TPs¬†[fortune2025quickcommerce, thehindu2022delivery, jakartapost2022delivery]. Elevated TP impairs cognition and motor control, driving overspeeding, braking inconsistencies, delayed hazard response, and higher crash risk¬†[PAWAR2022105582, Gupta25112024, Sharma21042025]. Naturalistic studies link risky behavior to speed, maneuvering, and context¬†[KONG2020105620], while stress further amplifies variability and control errors¬†[pavlidis2016dissecting]. Field evidence from Thailand also confirms that widespread speeding significantly raises collision risk¬†[Seefong2024].\n\n\nPawar et al.¬†[PAWAR2022105582] showed that TP induces risky maneuvers. However, directly measuring TP in real-world PTW riding is practically impossible. We hypothesize that TP can be predicted from PTW‚Äôs data.\nTherefore, we established a proxy measure for TP and predict it using other PTW telematic features such as speed, acceleration, etc. Towards this end, we first collect data in the same fashion as [PAWAR2022105582] but for PTW. That is, riders experienced TP conditions (no time pressure (NTP), low time pressure (LTP), high time pressure (HTP)) and asked to drive PTW. Thus, we collect the supervised data. To the best of our knowledge, this is the first work that develops predictive models for cognitive state detection in PTW riders, addressing a fundamental limitation in current ITS. To evaluate the data quality, we propose a novel MotoTimePressure (MTPS) model that predicts TP. To show the utility of such a predicted TP feature in collision risk prediction, we train an Informer¬†[zhou2021informer] model using the ground truth (GT) TP feature and the predicted TP feature. Our results show that predicted TP features increase the accuracy of collision risk prediction from 91.25% to 93.51%,\napproaching oracle performance (93.72%).\nThe key contributions of this work are summarized as follows:\n\n\n\n\n1.\n\nThe First Labeled Dataset for PTW Rider Cognitive State: To the best of our knowledge, We present a first high-resolution PTW simulator dataset comprising more than 129,000 labeled multivariate time-series sequences from 153 rides by 51 participants under three TP conditions (NTP, LTP, HTP). Each sequence includes 63 features covering vehicle kinematics, control inputs, behavior violations, and environmental context, providing a rich resource for modeling cognitive load and rider safety.\n\n\n\n2.\n\nSpeed and Control Degradation Under TP: We provide comprehensive statistical evidence that TP systematically degrades riding control. Under HTP conditions, riders exhibit 48% higher mean speeds and 36.4% greater speed variability compared to NTP. Control degradation extends to 604% increased front brake force application, 50% higher rear brake forces, and 35% increased gear usage, demonstratin"
  },
  {
    "title": "Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages",
    "url": "https://arxiv.org/abs/2601.03168v1",
    "source": "arxiv",
    "summary": "Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap",
    "full_text": null
  }
]