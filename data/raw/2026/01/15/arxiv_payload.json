[
  {
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "url": "https://arxiv.org/abs/2601.09708v1",
    "source": "arxiv",
    "summary": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact y",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Vision-Language-Action (VLA) Models\n2.2 Efficient Reasoning\n\n\n\n3 Method\n\n3.1 Problem Formulation\n\n3.2 Efficient Embodied Reasoning\n\n3.2.1 Verbalizable Latent CoT by Reward Preferences\n3.2.2 Action-Aligned Visual Plan Distillation\n\n\n3.3 Reasoning-Enhanced Policy Learning\n3.4 Learning Strategy and Inference\n\n\n\n4 Experiment\n\n\n4.1 Experimental Setup\n\nImplementation Details.\nTraining Datasets and Evaluation Benchmarks.\n\n\n\n4.2 Quantitative Evaluation\n\nRobot Manipulation.\nEmbodied Reasoning.\n\n\n\n4.3 Analysis of Fast-ThinkAct\n\nReasoning Enables Long-Horizon Planning.\nReasoning Enables Failure Recovery.\nReasoning Enables Few-Shot Adaptation.\nVisualization of Verbalizable Latent Reasoning.\nAblation Study.\n\n\n\n\n5 Conclusion\n\nA Additional Experimental Setup\n\nA.1 Algorithm\nA.2 Implementation Details\n\nA.3 Training Data Details\n\n\nA.3.1 Dataset Sources\n\n2D Visual Trace of Manipulation Tasks.\nRoboFAC lu2025robofac.\nRoboVQA sermanet2024robovqa.\nShareRobot ji2025robobrain.\nEgoPlan-Bench chen2023egoplan.\nVideo-R1-CoT feng2025video.\nPixMo deitke2024molmo.\n\n\n\nA.3.2 Data Processing and Formatting\n\nSupervised Fine-Tuning (SFT).\nChain-of-Thought SFT (CoT-SFT).\nTeacher-Student Training.\n\n\n\n\n\nA.4 Evaluation Setup\n\nA.4.1 Embodied Reasoning Benchmarks\nA.4.2 Robotic Manipulation Benchmarks\n\n\n\n\n\nB Additional Experiment Results\n\n\nB.1 Additional Quantitative Results\n\nResults of Larger Model Size.\nPerformance Comparison with ThinkAct-3B.\nComparison with Efficient Reasoning Baselines.\n\n\n\nB.2 Additional Qualitative Results\n\nQualitative Robot Execution.\nBimanual Manipulation Results.\nFailure Identification and Recovery.\nVerbalized Latent Reasoning.\n\n\n\nB.3 Additional Ablation Study and Analysis\n\nAdditional Ablation Results on Manipulation Benchmarks.\nAblation Study on Action Model Conditioning.\nAblation Study on Latent Reasoning Steps.\n\n\n\n\n\n\n\n\n\n\n\\correspondingauthor\nX\n\nFast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning\n\n\n\nChi-Pin Huang1\n\n\n\n\n  Yunze Man2\n\n\n\n\n  Zhiding Yu\n\n\n\n\n  Min-Hung Chen\n\n\n\n\n  Jan Kautz\n\n\n\n\n  Yu-Chiang Frank Wang1\n\n\n\n\n  Fu-En Yang\nNVIDIA\n\n\n\n\n\nAbstract\nVision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.\n\nLinks:  \n\n\nProject Page\n\n\n\\abscontent\n\n\n\n1 Introduction\n\nFigure 1: Overview of Fast-ThinkAct. Previous reasoning VLAs generate lengthy reasoning traces (∼\\sim250 tokens). Our approach learns compact continuous tokens (e.g., 6) (blue) and parallel spatial tokens (green) as internal reasoning. The bottom-right plot shows that we achieve 9.3×9.3\\times faster inference than ThinkAct-7B huang2025thinkact, while delivering improved performance on the SimplerEnv-Google benchmark.\n\n\nRecent large vision-language models (VLMs) liu2023visual; comanici2025gemini; liu2024nvila; bai2025qwen2; shi2024eagle; li2025eagle; chen2025eagle; wang2025internvl3; xie2024show have achieved remarkable capabilities in visual-language understanding across diverse multimodal tasks. To extend these capabilities to embodied-centric tasks, recent works leverage large-scale robot demonstrations o2024open to develop Vision-Language-Action (VLA) foundation models brohan2022rt; brohan2023rt; team2024octo; bjorck2025gr00t; li2025hamster; black2024pi_0; yang2025magma; kim24openvla. These VLA tasks require agents to perceive complex visual scenes, reason over spatial and temporal contexts, and execute adaptive actions within dynamic environments, demanding robust long-horizon planning and contextual adaptation. However, as these VLA models primarily rely on supervised training from action data, they excel at basic skills (e.g., pick-and-place) but struggle to generalize beyond training distributions, such as long-horizon planning, self-correction from failures, and adaptation to novel scenarios, due to the impracticality of collecting exhaustive robot demonstrations.\n\n\nReasoning VLAs zawalski2024robotic; zhao2025cot; lee2025molmoact; wu2025you; qu2025eo; huang2025thinkact; kim2025robot address these limitations by incorporating intermediate thinking processes, improving generalization and task-solving capability. Supervised chain-of-thought (CoT) methods zawalski2024robotic; zhao2025cot; lee2025molmoact; qu2025eo address this by learning from intermediate reasoning annotations. These approaches can be categorized into textual reasoning methods that leverage off-the-shelf LLMs and VLMs to generate pseudo CoT labels zawalski2024robotic, and visual reasoning methods that generate structured visual reasoning representations such as sub-goal images, image depth, and 2D visual traces zhao2025cot; lee2025molmoact. However, these supervised approaches require substantial reasoning annotations and remain limited by training data coverage. To address this, ThinkAct huang2025thinkact employs RL-based reasoning shao2024deepseekmath to generate long textual CoTs guided by action-aligned visual rewards. While these reasoning methods effectively improve task generalization and planning capabilities, they require generating lengthy chain-of-thought steps that introduce substantial reasoning latency, which hampers embodied applications with real-time requirements.\n\n\nIn embodied AI applications such as robotic manipulation and autonomous driving, agents must make rapid decisions at high frequencies (e.g., 1-15 Hz) guan2025efficient. However, generating lengthy reasoning traces can take several seconds per decision (e.g., 0.1 Hz) huang2025thinkact; lee2025molmoact, creating a critical bottleneck that limits real-time performance guan2025efficient; yu2025survey and poses safety risks in time-critical scenarios wang2025alpamayo. To mitigate this efficiency bottleneck while preserving reasoning capabilities, very recent works chen2025training; yu2025survey; guan2025efficient have explored approaches to reduce inference latency in embodied reasoning. For instance, ECoT-Lite chen2025training proposes reasoning dropout to accelerate inference, yet directly reducing textual reasoning length risks performance degradation due to critical information loss. How to preserve reasoning capability while enabling compact representations that properly capture essential spatial-temporal dynamics remains a crucial challenge for reasoning VLA models.\n\n\nIn this paper, we propose Fast-ThinkAct, an efficient embodied reasoning framework for Vision-Language-Action tasks that achieves compact yet expressive planning through verbalizable latent reasoning. As depicted in Figure 1, unlike prior reasoning VLAs that generate lengthy explicit textual CoT traces, we introduce reward-guided preference distillation with visual trajectory alignment to compress linguistic and visual planning into compact continuous latents that enable implicit internal reasoning. Our student VLM encodes reasoning into compact latents decodable by a verbalizer, enabling preference-based optimization that leverages RL-derived reward signals to distill high-quality reasoning patterns from a textual teacher VLM while suppressing low-quality ones. We further align trajectory latents between teacher and student to transfer visual planning capabilities essential for embodied control. Once trained, the student VLM enables reasoning-enhanced policy learning that bridges implicit multimodal planning with action execution, achieving significantly faster inference while outperforming existing reasoning VLAs.\n\n\nOur contributions can be summarized as follows:\n\n\n•\n\nWe propose Fast-ThinkAct, an efficient reasoning framework that compresses reasoning into verbalizable latent thoughts while maintaining expressive planning abilities.\n\n\n\n•\n\nWe introduce preference-guided distillation with manipulation trajectory alignment that compresses linguistic and visual planning into compact continuous latents.\n\n\n\n•\n\nWe bridge high-level visual planning to low-level action execution through reasoning-enhanced policy learning guided by manipulation trajectory latents.\n\n\n\n•\n\nWe achieve up to 89.3% inference latency reduction over state-of-the-art reasoning VLAs while maintaining strong performance across diverse embodied benchmarks.\n\n\n\n\n\n\n\n2 Related Works\n\n\n2.1 Vision-Language-Action (VLA) Models\n\nFoundation VLAs.\nVision-Language-Action (VLA) models brohan2022rt; brohan2023rt; team2024octo; bjorck2025gr00t; li2025hamster; black2024pi_0; yang2025magma; pertsch2025fast; driess2025knowledge; bu2025agibot; team2025gemini; wang2025vla have recently emerged as a promising paradigm for embodied AI by training vision-language backbones on large-scale robot demonstrations. Works such as OpenVLA kim24openvla and π0\\pi_{0} black2024pi_0 achieve language-conditioned manipulation through end-to-end policy learning, while Magma yang2025magma co-trains on heterogeneous human and robot data. HAMSTER li2025hamster and TraceVLA zheng2024tracevla further leverage 2D visual trajectories to boost spatial-action connections. Despite success on "
  },
  {
    "title": "Value-Aware Numerical Representations for Transformer Language Models",
    "url": "https://arxiv.org/abs/2601.09706v1",
    "source": "arxiv",
    "summary": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Improving Mathematical Performance via Reasoning and Diverse Methods\n2.2 Improving Arithmetics\n2.3 Tokenization\n\n\n\n3 Method\n\n\n3.1 Preamble\n\nMotivation.\nIdea.\n\n\n\n3.2 Architecture Overview\n\n&lt;num&gt; Embedding Module.\nTraining Objective.\n\n\n\n\n\n4 Performance Evaluation\n\n4.1 Benchmark\n4.2 Baselines\n4.3 Experimental Setup\n\n\n5 Results\n6 Discussion\n7 Conclusions and Future Work\nA Dataset Example Tasks\nB Dataset and Benchmark Details\nC Hyperparameters and Experimental Details\nD Results Aggregated by Tasks\n\n\n\n\n\nValue-Aware Numerical Representations for Transformer Language Models\n\n\nAndreea Dutulescu\n\n  \nStefan Ruseti\n\n  \nMihai Dascalu \nNational University of Science and Technology POLITEHNICA Bucharest \n{andreea.dutulescu, stefan.ruseti, mihai.dascalu}@upb.ro\n\n\n\nAbstract\nTransformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model’s input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.\n\n\n\nValue-Aware Numerical Representations for Transformer Language Models\n\n\n\n\nAndreea Dutulescu  and Stefan Ruseti  and Mihai Dascalu\n\nNational University of Science and Technology POLITEHNICA Bucharest\n\n{andreea.dutulescu, stefan.ruseti, mihai.dascalu}@upb.ro\n\n\n\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) based on the Transformer architecture have shown remarkable capabilities across a wide range of natural language processing tasks, including question answering, code generation, and complex multi-step reasoning. Recent models achieve competitive performance on challenging mathematical benchmarks and standardized examinations, often by leveraging large-scale pretraining, instruction tuning (Cobbe et al., 2021; Hendrycks et al., 2021; Toshniwal et al., 2025), and explicit reasoning-enhancing training (Shao et al., 2024). These advances have led to the widespread perception that LLMs possess strong mathematical and numerical abilities.\n\n\nHowever, growing empirical evidence shows that this perception is incomplete. Despite their apparent success on complex reasoning problems, LLMs frequently fail at elementary numerical understanding and basic arithmetic operations. As highlighted by Yang et al. (2024), modern LLMs can produce convincing reasoning traces while making errors, such as incorrect numerical comparisons (e.g., concluding that 9.11 &gt; 9.9) or simple arithmetic mistakes involving fractions. These failures indicate that numerical competence is not a natural by-product of improved reasoning, but a distinct capability that remains fragile in current architectures. Many benchmarks conflate high-level reasoning (e.g., problem interpretation, equation formulation, or procedural planning) with low-level numerical processing. As argued by Yang et al. (2024), a typical Math problem requires both reasoning about its structure and correctly manipulating the numbers involved; yet evaluation datasets rarely disentangle these components. Consequently, improvements attributed to \"better reasoning\" may mask persistent weaknesses in basic numerical understanding.\n\n\nFigure 1: Standard LLMs process numbers as symbolic tokens, which can lead to incorrect surface-form arithmetics.\nOur value-aware &lt;num&gt; token explicitly encodes numerical magnitude, enabling the model to reason over numbers as continuous measurements.\n\n\nA dominant trend in recent research focuses on enhancing Math capabilities through methods that generate long intermediate reasoning chains or agentic workflows. While these approaches often improve final-answer accuracy, they come at the cost of generating many additional tokens and significantly increasing inference time. More importantly, they do not address a fundamental limitation: even when guided step-by-step, models may still lack a reliable internal representation of numerical value. In other words, producing more reasoning tokens does not resolve the fact that models often do not \"know\" what a number represents.\n\n\nAt the core of this problem lies the notion of numerical value. For humans, a number has an intrinsic magnitude independent of its surface form. For Transformer-based LLMs, however, numbers are primarily treated as singular or sequences of tokens, and their semantics emerge indirectly from distributional patterns in text. Models process numbers piece by piece rather than as unified quantities, leading to systematic errors in comparison, magnitude estimation, and arithmetic as the input length grows.\n\n\nIn this paper, we propose a complementary approach to numerical representation in Transformers. Instead of encoding numbers solely through their surface tokens, we introduce a value-aware prefix token that precedes each numerical expression. The embedding of this token is explicitly conditioned on its numerical value, providing the model with a continuous, magnitude-sensitive signal alongside the standard tokenized representation. This design preserves compatibility with existing tokenizers and architectures while injecting inductive bias about numerical values directly into the input representation.\n\n\nOur main contributions are as follows:\n\n\n•\n\nWe propose a value-aware numerical encoding mechanism for language models, in which numerical expressions are augmented with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value, rather than being learned solely from token co-occurrence statistics.\n\n\n\n•\n\nWe empirically confirm that our approach consistently improves on a benchmark designed to isolate arithmetic competence, outperforming strong baselines under similar training and inference conditions.\n\n\n\n•\n\nOur proposed mechanism requires only minimal architectural modifications and is directly applicable to any decoder-only Transformer architecture. To support reproducibility and adoption, we release the full implementation, including architectural changes, training code, and evaluation scripts (https://anonymous.4open.science/r/Temp-Value-Aware-Numeric-Repr-8796/).\n\n\n\n\n\nBy decoupling numerical value from surface tokenization while retaining the flexibility of standard language modeling, our approach targets a fundamental gap identified by prior work: the absence of an explicit notion of number magnitude in LLM representations. Encoding numbers using our approach provides a lightweight, efficient alternative to reasoning-intensive methods, and directly addresses the root causes of basic numerical errors rather than compensating for them through extended inference.\n\n\n\n\n2 Related Work\n\n\n2.1 Improving Mathematical Performance via Reasoning and Diverse Methods\n\nA substantial body of recent work improves mathematical performance in LLMs by modifying the reasoning process itself, rather than directly targeting numerical representations or arithmetic operations. These approaches typically assume that errors arise from insufficient or poorly structured reasoning and therefore focus on eliciting, refining, or controlling intermediate solution steps.\n\n\nDidolkar et al. (2024) introduced a framework in which models first identified the skills required to solve a mathematical problem and then condition generation on skill-specific in-context examples. Wang et al. (2025); Chen and Tam (2025) used modules that translate the Math problem into formal language, which is then offered as additional input for solving. Other approaches (Cao et al., 2025; Leang et al., 2025) relied on different forms and prompts for step-by-step reasoning to enhance Math capabilities. Additionally, to improve Math reasoning capabilities, multiple synthetic datasets and data generation methods have been proposed (Toshniwal et al., 2025; Wan et al., 2025; Xiao and Zhao, 2025).\n\n\nAnother direction examined whether explicit step-by-step reasoning is always necessary. Liu et al. (2024) studied whether models can be trained to produce shorter or partially implicit reasoning traces without sacrificing correctness. By fine-tuning on datasets containing both detailed and abbreviated rationales, the authors showed that models can learn to omit intermediate steps while maintaining or even improving accuracy. This work highlighted that reasoning verbosity and reasoning quality are not strictly coupled.\n\n\nFinally, several approaches Wang et al., 2022; Lee et al., 2025 focused on test-time verification and refinement. Lee et al. (2025) proposed a self-verification mechanism that allowed models to evaluate and revise their own reasoning trajectories during inference. Through a curriculum-based training procedure and confidence-aware decoding, models learnt to detect potential errors and refine intermediate reasoning steps without external verifiers.\n\n\nOverall, these methods achieve gains by strengthening reasoning control and self-correction. However, they largely treat arithmetic correctness as a downstream consequence of improved reasoning processes, rather than as a capability that can be independently modeled and enhanced at the level of numerical representation or value understanding.\n\n\n\n\n2.2 Improving Arithmetics\n\nRelatively few studies focus on arithmetic performance in isolation, decoupl"
  },
  {
    "title": "ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation",
    "url": "https://arxiv.org/abs/2601.09703v1",
    "source": "arxiv",
    "summary": "Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a c",
    "full_text": "\n\n\n\nI Introduction\n\nII Background\n\nII-A Code Large Language Models\nII-B Knowledge Injection\n\n\n\nIII Methodology\n\n\nIII-A Simplification Rules Design\n\nIII-A1 Multiple Variable Assignment Simplification\nIII-A2 return Statement Simplification\nIII-A3 Assignment operation simplification\nIII-A4 Conditional Statement Simplification\nIII-A5 Multi-conditional Statement Simplification\nIII-A6 for Loops Simplification\nIII-A7 Simplified Removal of Multiple Object References\nIII-A8 Dictionary Mapping Simplification\nIII-A9 String Formatting Simplification\nIII-A10 File Reading and Writing Simplification\n\n\n\nIII-B Data Construction\n\nIII-B1 Rule-based Code Composition Strategy\nIII-B2 LLM-based Code Composition Strategy\n\n\nIII-C Knowledge Injection by Fine-tuning Model\nIII-D Model Inference\n\n\n\nIV Experimental Setup\n\nIV-A Baselines\nIV-B Datasets\n\nIV-C Evaluation Metrics\n\nIV-C1 For Accuracy\nIV-C2 For Efficiency\n\n\nIV-D Implementation Details\n\n\n\nV Evaluation Results\n\nV-A RQ1: Performance Comparison with Baseline Models.\nV-B RQ2: Comparison with Prompt-based Efficiency Enhancement Techniques\nV-C RQ3: Readability Comparison across Efficiency Enhancement Techniques.\n\n\n\nVI Related Work\n\nVI-A LLM-based Code Generation\nVI-B Efficient Code Generation\n\n\nVII Conclusion\n\n\n\n\n\nShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation\n\n\n\n\nSicong Liu1∗,\nYanxian Huang1∗,\nMingwei Liu1, \nJiachi Chen1,\nEnsheng Shi2,\nYuchi Ma2,\nHongyu Zhang3,\nYin Zhang4,\nYanlin Wang1†\n∗Equal contribution. Contact: liusc3@mail2.sysu.edu.cn†Corresponding author: wangylin36@mail.sysu.edu.cn\n\n\nAbstract\nCode generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity.\nThe emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints.\nEach token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations—such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench—a corpus of validated ⟨o​r​i​g​i​n​a​l​_​c​o​d​e,s​i​m​p​l​i​f​i​e​d​_​c​o​d​e⟩\\left\\langle original\\_code,simplified\\_code\\right\\rangle pairs with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs.\nExtensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.\n\n\n\nI Introduction\n\n\nAs a pivotal branch of software engineering automation, code generation tasks [1, 2, 3] aim to translate user specifications into executable implementations, promising substantial reductions in manual coding efforts while accelerating development cycles. The advent of large language models (LLMs) [4, 5, 6] revolutionizes this domain, with state-of-the-art models like CodeLlama [7] and StarCoder [8] demonstrating unprecedented capabilities in understanding programming semantics and syntactic patterns.\nHowever, the design characteristics of large language models (LLMs)—including substantial parameter sizes, limited context window lengths, and autoregressive generation mechanisms—introduce significant efficiency challenges in code generation.\nThe LLM-based code generation process comprises two phases: inference and generation. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. Consequently, as generated code length increases, computational steps and resource demands grow proportionally, leading to non-linear increases in processing time and operational costs.\nTherefore, previous works [9, 10, 11] typically optimize the inference phase of LLMs through techniques such as prompt compression [12] and model quantization [13] to improve the efficiency of code generation.\n\n\nSpecifically, prompt compression aims to reduce the length of prompts by removing unnecessary or low-information content (hard prompt methods) or learning continuous representations of the prompt information in the embedding space (soft prompt methods), thereby improving the efficiency of processing LLM inputs [14, 15, 16]. Model quantization achieves compression and acceleration by reducing the bit width of numerical values used in neural network computations, enabling deployment on resource-constrained, low-bitwidth edge devices. This method is one of the most commonly used techniques in model compression and acceleration [17, 18], widely applied in the industry due to its stable compression ratio and acceleration benefit [19, 20, 21].\n\n\nIn addition, researchers explore and propose the AI-oriented grammar based existed programming languages to enhance code generation efficiency by optimizing token generation counts. For example, SimPy [22] is constructed through a systematic revision of standard Python syntax using heuristic rules, ensuring strict equivalence in Abstract Syntax Tree (AST) structures between SimPy and Python to enable bidirectional code transformation. This AST-level consistency guarantees functional equivalence while achieving computational efficiency gains.\nAlthough these methods have shown promising performance in efficient code generation, we have identified the following problems.\n\n\n\n\nP1\n\nLoss of key information. Prompt compression enhances inference speed by removing redundant information from input texts, thereby it may inadvertently discard critical semantic information, potentially leading to output deviations.\n\n\n\nP2\n\nLoss accuracy. Although extensive research and practical applications have shown that model quantization is highly successful in minimizes computational costs and memory footprint through parameter precision reduction (e.g., converting 32-bit floats to 8-bit integers), yet inevitably introduces precision degradation that adversely impacts task-specific accuracy.\n\n\n\nP3\n\nReduce readability and poor generalization. AI-oriented grammar such as SimPy severely compromises human readability, violating the ”code-as-documentation” principle. And practical deployment necessitates context switching between Python and SimPy, requiring dedicated toolchain development. Furthermore, the heuristic rule-based approach exhibits poor cross-language generalizability. Adapting SimPy’s methodology to other languages (e.g., Java or TypeScript) demands language-specific rule engineering.\n\n\n\n\n\nIn this paper, we propose ShortCoder, a knowledge-infused code generation framework that strategically balances these competing objectives. Specifically, we first design ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise. Then, we introduce a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of 828 validated ⟨o​r​i​g​i​n​a​l​_​c​o​d​e,s​i​m​p​l​i​f​i​e​d​_​c​o​d​e⟩\\left\\langle original\\_code,simplified\\_code\\right\\rangle pairs with semantic consistency. Finally, we propose a fine-tuning strategy that injects conciseness awareness into base LLMs.\nTo validate the effectiveness of ShortCoder, we construct comprehensive experiments. The extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEvalPlus, achieving 18.1% generation efficiency improvement over previous methods while ensuring the performance of code generation.\n\n\nWe summarize the contributions of this paper as follows:\n\n\n•\n\nWe present and publicly release ShorterCodeBench, a high-quality code brevity optimization dataset comprising 828 carefully curated ⟨o​r​i​g​i​n​a​l​_​c​o​d​e,s​i​m​p​l​i​f​i​e​d​_​c​o​d​e⟩\\left\\langle original\\_code,simplified\\_code\\right\\rangle pairs.\n\n\n\n•\n\nWe proposed ShortCoder, which can solve problems while generating as short code as possible, achieving a 18.1% improvement in the generation efficiency of LLMs.\n\n\n\n•\n\nWe perform an extensive evaluation of ShortCoder. Experimental results show that ShortCoder outperforms the state-of-the-art methods. We provide the code and data at https://github.com/DeepSoftwareAnalytics/ShorterCode.\n\n\n\n\n\n\n\nII Background\n\n\n\nII-A Code Large Language Models\n\n\nCode Large Language Models (Code LLMs) refer to large language models that are specifically trained for coding tasks. Distinguished from general-purpose LLMs, these models are optimized to understand, generate, and manipulate source code by leveraging massive corpora of programming languages, documentation, and software repositories.\n\n\nIn recent research, hybrid architectural designs have emerged to address code complexity, with models like CodeT5+ [23] introducing a dual-encoder framework that separately processes natural language descriptions and code snippets to enable fine-grained cross-modal alignment. This approach enhances the model’s ability to filter irrelevant details when generating code, while architectures like CodeCompose [24] employ masked language modeling during pretraining to force the reconstruction of code snippets from bidirectional context—improving l"
  },
  {
    "title": "Empathy Applicability Modeling for General Health Queries",
    "url": "https://arxiv.org/abs/2601.09696v1",
    "source": "arxiv",
    "summary": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Frame",
    "full_text": "\n\n\n\n1 Introduction\n2 Empathy Applicability Framework and Theoretical Grounding\n\n3 Methods\n\n3.1 Data Source\n\n3.2 Annotation Task\n\n3.2.1 Annotator Recruitment, Training and Calibration\n3.2.2 GPT Annotations\n\n\n3.3 Modeling Task and Approach\n\n\n\n4 Evaluation Setup and Experiments\n\n4.1 Annotator Agreement\n4.2 Conceptual Alignment\n4.3 Divergence Bar and Qualitative Analysis\n4.4 Model Evaluation\n4.5 Model Training and Training Sets\n\n\n\n5 Results\n\n\n5.1 Reliability of the EAF\n\nConsistency.\nPredictive Validity.\nConceptual Alignment.\n\n\n\n5.2 Systematic Challenges in Operationalizing Anticipatory Empathy\n\n5.2.1 Challenge 1: Subjectivity in Identifying Implied Distress\n5.2.2 Challenge 2: Clinical-Severity Ambiguity\n5.2.3 Challenge 3: Contextual Hardship\n\n\n\n\n6 Discussion and Conclusion\n7 Limitations\n8 Ethical considerations\n\nA Empathy Applicability Framework Detail\n\n\nA.1 Emotional Reactions in General Health Queries\n\nA.1.1 Definition\nA.1.2 Emotional Reactions Not Applicable (N/A)\n\nA.1.3 Emotional Reactions Applicable\n\n3. Underlying Negative Emotional State Inferred\n\n\n\n\n\nA.2 Interpretations in General Health Queries\n\nA.2.1 Definition\nA.2.2 Interpretations Applicable\nA.2.3 Interpretations Not Applicable\n\n\n\n\n\nB Annotation Instructions for Human Annotators\n\nB.1 Instructions Given to Annotators\nB.2 Additional Verbal Clarifications\nB.3 Boundary Cases: Subjectivity and Lack of Medical Expertise\n\n\nC Illustrative Scenarios for EAF Operationalization\nD Appendix: Human-GPT Agreement Analysis\nE Model Architecture Details\n\nF Prompt Design for LLM Annotations\n\nF.1 Emotional Reactions: With-Framework Contrastive Prompt\nF.2 Interpretations: With-Framework Contrastive Prompt\n\nF.3 Prompts Without the Framework (Definition-Only)\n\nF.3.1 Emotional Reactions (without framework).\nF.3.2 Interpretations (without framework).\n\n\n\n\n\nG Dataset Analyses\n\nG.1 Subcategory Prevalence and EA×\\timesIA Co-occurrence (Humans vs. GPT)\nG.2 Length Effects on Applicability (with Confidence Intervals)\nG.3 Run-order Drift Checks\nG.4 Match vs. Miss by subcategory\n\n\n\n\n\n\n\nEmpathy Applicability Modeling for General Health Queries\n\n\n\nShan Randhawa1  Agha Ali Raza2  Kentaro Toyama1  Julie Hui1  Mustafa Naseem1\n1University of Michigan  2Lahore University of Management Sciences \n{shanmr,toyama,juliehui,mnaseem}@umich.edu  agha.ali.raza@lums.edu.pk\n\n\n\nAbstract\nLLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor–patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors’ responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries.\nWe introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human–GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation.\nEAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.\n\n\n\nEmpathy Applicability Modeling for General Health Queries\n\n\n\n\n\nShan Randhawa1   Agha Ali Raza2   Kentaro Toyama1   Julie Hui1   Mustafa Naseem1\n\n1University of Michigan  2Lahore University of Management Sciences\n\n{shanmr,toyama,juliehui,mnaseem}@umich.edu  agha.ali.raza@lums.edu.pk\n\n\n\n\n\n\n1 Introduction\n\nClinical empathy integrates cognitive (understanding), emotional (resonating), and action-oriented (expressing) components\n(Guidi and Traversa, 2021). It is indispensable for clinical care, deepening therapeutic relationships and improving outcomes such as patient satisfaction, care effectiveness, reduced distress, and hospital length of stay Guidi and Traversa (2021); Olson (1995); Hoffstädt et al. (2020); yet clinicians miss 70-90% of empathic opportunities during patient interactions Morse et al. (2008); Hsu et al. (2012).\n\n\nLarge Language Models (LLMs) are increasingly integrated into healthcare workflows and patient interactions, with major EHR vendors such as EPIC adopting them for clinical messaging and nearly half of physicians reporting patients consult ChatGPT before visits Antoniak et al. (2024); Sermo Team (2025). While these trends highlight rapid adoption of LLMs in healthcare, they also raise concerns of lacking empathy crucial for asynchronous physician-patient interactions (Koranteng et al., 2023). However, effective empathy requires discernment, not just fluency. This highlights a critical, antecedent challenge: How can we systematically model the applicability of empathy, allowing systems to recognize the specific clinical and linguistic cues that necessitate an emotional response?\n\n\nModeling empathy in text is inherently difficult without non‑verbal cues, and NLP research has historically over‑weighted emotional aspects while overlooking cognitive empathy Lahnala et al. (2022), even though both are vital in clinical care. To redress this imbalance, EPITOME Sharma et al. (2020) captures the multidimensionality of empathy through emotional reactions, interpretations, and explorations, offering an empathetic lens on warmth, understanding, and curiosity in mental health support. Online Empathy Chai et al. (2019) also addresses multidimensionality, classifying responses as Informational and Emotional. However, both EPITOME and Online Empathy assess empathy post hoc, labeling support‑giver responses after they appear and thus offering no guidance while a clinician is composing a response to the patient query.\n\n\nLahnala et al. attempt to solve this particular problem with with an Appraisal Framework that annotates empathic opportunities and clinician elicitation and response as functions of (affect | judgment | appreciation) in breaking-bad-news oncology dialogues Lahnala et al. (2024). This discourse analysis lens excels at teaching stance shifts over multi-turn synchronous conversations, yet is not suited to single-turn, asynchronous general health queries: it classifies stance, not what the patient needs (cognitive clarification vs emotional warmth). Thus, it remains need-blind, providing little actionable help for one-off replies.\n\n\nTo address this gap, we propose the Empathy Applicability Framework (EAF), a theoretically grounded method to proactively identify when and what type of clinical empathy should be expressed in response to patient queries. EAF operationalizes empathy along two key dimensions: affective (emotional reactions) and cognitive (interpretations) – labeling each as Applicable or Not Applicable based on clinical, contextual, and linguistic cues within patient queries. Unlike prior work that evaluates empathy in the response itself (Chai et al., 2019; Sharma et al., 2020), EAF analyzes the patient’s query before any response, enabling anticipatory reasoning. Evidence for the value of anticipation comes from Sibyl (Wang et al., 2025), which shows that anticipating user’s emotional and contextual trajectory improves empathetic response generation. EAF can also mitigate limitations of reactive frameworks that overrely on surface lexical cues. For instance, EPITOME misclassified generic phrases as empathetic in nonsensical contexts, with false-positive rates exceeding 95% (Lee et al., 2023). By shifting empathy assessment from post-hoc response scoring to pre-response query analysis, EAF is methodologically significant: it enables anticipatory inference of emotional and interpretive support needs, helping providers and LLMs detect and act on empathic opportunities in general health queries.\n\n\nWe make three primary contributions: (i) Framework Design: we introduce and theoretically ground the EAF in clinical empathy literature, clearly differentiating our anticipatory model from prior post-hoc approaches; (ii) Annotated and analyzed Benchmark: a novel dataset of 1,300 patient queries annotated by humans and GPT-4o (included in the supplementary materials), demonstrating EAF’s reliability and interpretability; and (iii) Operationalization Challenges: we identify and systematically analyze specific contexts where anticipatory empathy annotations diverge, highlighting opportunities for future research in multi-annotator modeling, clinician-in-the-loop systems, and culturally sensitive annotation strategies.\n\n\n\n\n\n\n\nDimensions\n\n\n\n\nApplicable cues\n\n\n\n\nNot Applicable cues\n\n\n\n\n\n\n\n\nEmotional Reactions\n\nExpressions of warmth, compassion, concern, or similar feelings conveyed by a doctor in response to a patient’s query.\n\n\n\n\n\n•\n\nSevere Negative Emotion\n\n•\n\nInferred Negative State\n\n•\n\nSeriousness of Symptoms\n\n•\n\nConcern for Relations\n\n\nRationale: Signals reflect distinct pathways of emotional distress, guiding when emotional reactions are warranted.\n\n\n\n\n\n•\n\nRoutine Health Management\n\n•\n\nPurely Factual Medical Queries\n\n•\n\nNeutral Symptom Descriptions\n\n•\n\nHypothetical Queries\n\n\nRationale: Signals no emotional content; omit reactions to maintain factual medical focus.\n\n\n\n\n\n\nInterpretations\n\nCommunication of an understanding of the patient’s feelings (expressed or implied) and/or experiences (including contextual factors) inferred from the patient’s query.\n\n\n\n\n\n•\n\nExpression of Feeling\n\n•\n\nExperiences or Context Affecting Emotional State\n\n•\n\nSymptoms with an Emotional Impact\n\n•\n\nDistressing Uncertainty "
  },
  {
    "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
    "url": "https://arxiv.org/abs/2601.09694v1",
    "source": "arxiv",
    "summary": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratio",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Main Contributions\n\n\n\n2 Related Work\n\n2.1 Post-Training Pruning for Large Language Models\n2.2 Structured Pruning Approaches\n2.3 Evaluation of Compressed LLMs\n2.4 Learning-Based Compression and Meta-Optimization\n\n\n\n3 Method\n\n3.1 Overview\n3.2 Layer Sensitivity Profiling\n3.3 LLM Agent Design\n3.4 Self-Reflection Mechanism\n3.5 Checkpoint Rollback Mechanism\n3.6 Complete Algorithm\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Main Results\n4.3 Agent Behavior Analysis\n4.4 Visualization\n\n\n5 Conclusion\n\n\n\n\n11institutetext: Research and Development, Sports Vision, Inc., Minnetonka, MN 55305, USA 11email: varun@sportsvision.ai\n22institutetext: Research and Development, Vizworld Inc., Minnetonka, MN 55305, USA 22email: rakesh@vizworld.ai\nLLMs can Compress LLMs: Adaptive Pruning by Agents\n\n\nSai Varun Kodathala (✉)\n\n  \nRakesh Vunnam\n\n\n\nAbstract\nAs Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19× better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have achieved remarkable success across diverse natural language processing tasks, demonstrating emergent capabilities in reasoning, generation, and knowledge retrieval [9, 7, 8]. However, their deployment remains challenging due to substantial computational and memory requirements. For instance, models with billions of parameters necessitate multiple high-memory GPUs and incur significant inference latency [1], limiting their accessibility and practical applicability in resource-constrained environments.\n\n\nModel compression techniques, particularly neural network pruning, have emerged as promising solutions to mitigate these challenges [10, 11]. Post-training pruning methods offer particular appeal for LLMs, as they eliminate the need for expensive retraining on massive corpora [1, 2]. Recent approaches such as SparseGPT [1] and Wanda [2] have demonstrated that LLMs can be pruned to 50% sparsity with minimal perplexity degradation, achieving this through layer-wise weight reconstruction and activation-aware magnitude pruning, respectively. Extensions like Wanda++ [3] further improve upon these foundations by incorporating regional gradients at the decoder-block level.\n\n\nDespite these advances, existing pruning methods share a critical limitation: they rely on uniform sparsity ratios across layers or employ hand-crafted heuristics to determine per-layer sparsity [1, 2, 3]. This one-size-fits-all approach fails to account for the heterogeneous importance of different layers in preserving model capabilities. Moreover, recent evaluation benchmarks have revealed a severe problem with pruned LLMs: catastrophic factual knowledge degradation. The LLM-KICK benchmark [5] demonstrates that structured pruning methods experience near-total collapse in factual question-answering performance, with N:M sparsity patterns losing over 97% accuracy on knowledge-intensive tasks even at modest sparsity ratios (25-30%), despite negligible perplexity increases. This disconnect between perplexity and actual task performance underscores the inadequacy of current evaluation metrics and pruning strategies.\n\n\nIn this work, we introduce agent-guided pruning, a novel framework where a foundation model acts as an adaptive pruning oracle to intelligently determine which layers to prune at each iteration. Our approach is motivated by two key insights: First, LLMs possess sophisticated reasoning capabilities that can be leveraged for optimization tasks beyond their traditional use in text generation [15, 16]. Second, the heterogeneous sensitivity of layers to pruning necessitates adaptive, context-aware decision-making rather than fixed heuristics [4].\n\n\nOur method constructs comprehensive layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics [2] with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics, along with iterative feedback from previous pruning outcomes, are provided to an LLM agent equipped with self-reflection capabilities. The agent learns from past decisions, reasoning about which layers preserve critical knowledge pathways and which can safely be pruned. A checkpoint rollback mechanism serves as a safety net, reverting the model when perplexity degradation exceeds a specified threshold and penalizing the agent through the feedback loop.\n\n\nWe evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, comparing against structured pruning baselines. Our results demonstrate substantial improvements: 56% relative improvement in MMLU accuracy, 19× better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation compared to the best performing 4:8 structured baseline. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 9.5-10% rollback rates across 21-40 pruning iterations. These results establish that foundation models can effectively guide the compression of other foundation models, opening a new paradigm for automated neural architecture optimization.\n\n\n\n1.1 Main Contributions\n\nThe main contributions of this work are as follows:\n\n\n\n\n•\n\nWe introduce the first framework to use an LLM as an adaptive pruning agent, enabling intelligent, iterative layer selection for neural network compression without manual heuristic design.\n\n\n\n•\n\nWe develop a self-reflection mechanism that enables the pruning agent to learn from previous decisions, progressively refining its strategy through iterative feedback.\n\n\n\n•\n\nWe propose a model-agnostic statistical profiling approach using z-score normalization of Wanda metrics and gradient importance, enabling comparison across heterogeneous layer types.\n\n\n\n•\n\nWe demonstrate a checkpoint rollback mechanism that maintains model quality by reverting unsuccessful pruning attempts, with remarkably low rollback rates (9.5-10%) indicating effective agent learning.\n\n\n\n•\n\nWe achieve substantial empirical improvements over structured pruning baselines, particularly in preserving factual knowledge (19× improvement) and maintaining general task performance (56% MMLU improvement), directly addressing the critical knowledge degradation problem revealed by recent benchmarks [5].\n\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Post-Training Pruning for Large Language Models\n\nThe computational demands of billion-parameter LLMs have motivated extensive research in post-training compression methods that avoid expensive retraining. SparseGPT [1] pioneered one-shot pruning for massive language models by framing the problem as layer-wise sparse regression, using second-order information to reconstruct weights after pruning. The method achieves 50-60% unstructured sparsity on models like OPT-175B and BLOOM-176B in under 4.5 hours, with minimal perplexity increase. However, SparseGPT’s reliance on Hessian approximations incurs significant computational overhead and memory requirements.\n\n\nWanda [2] introduced a simpler alternative, pruning weights by the product of their magnitudes and input activation norms on a per-output basis. Motivated by emergent large-magnitude features in LLMs [13], Wanda eliminates the need for weight updates or second-order information, achieving competitive performance with SparseGPT while requiring no retraining. The method demonstrates that effective sparse subnetworks exist exactly within pretrained LLMs without modification, and shows that pruning becomes more effective as model size increases, with 50% sparse LLaMA-65B matching the zero-shot performance of its dense counterpart.\n\n\nRecent work has sought to enhance these methods through gradient information. Wanda++ [3] introduces regional gradients computed at the decoder-block level, combined with regional optimization to minimize pruning-induced output discrepancies. This achieves up to 32% perplexity improvement over Wanda on language modeling tasks while maintaining efficiency (pruning a 7B model in under 10 minutes on a single H100 GPU).\n\n\n\n\n2.2 Structured Pruning Approaches\n\nWhile unstructured pruning achieves high sparsity, it often fails to deliver practical speedups on existing hardware [12]. Structured "
  },
  {
    "title": "Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design",
    "url": "https://arxiv.org/abs/2601.09693v1",
    "source": "arxiv",
    "summary": "Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric prot",
    "full_text": null
  },
  {
    "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection",
    "url": "https://arxiv.org/abs/2601.09692v1",
    "source": "arxiv",
    "summary": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated q",
    "full_text": null
  },
  {
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "url": "https://arxiv.org/abs/2601.09688v1",
    "source": "arxiv",
    "summary": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 Task Construction\n\n3.1 Construction Pipeline\n3.2 Benchmark Tasks\n\n\n\n4 Agentic Evaluation\n\n4.1 Adaptive Point-wise Quality Evaluation\n4.2 Active Fact-Checking\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n5.3 Validation of Evaluation Methods\n\n\n6 Conclusions\nA Usage of AI Assistant\nB Deep Research Systems Details\nC More Results\n\nD Evaluation Methods Details\n\n\nD.1 Adaptive Point-wise Quality Evaluation\n\n\nD.2 Active Fact-checking\n\n\nE Task Construction Details\n\n\nF Human Study\n\nF.1 Correct Examples\nF.2 Incorrect Examples\nG Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeepResearchEval:\nAn Automated Framework for Deep Research Task Construction and Agentic Evaluation\n\n\n\nYibo Wang1,2,\nLei Wang1222Corresponding Author,\nYue Deng1,\nKeming Wu1,\nYao Xiao1,\nHuanjin Yao2\nLiwei Kang1,\nHai Ye1,\nYongcheng Jing2,\nLidong Bing1\n1Infinity Lab, Shanda Group\n2Nanyang Technological University\n\n\n\nAbstract\nDeep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging.\nExisting benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing.\nTo bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation.\nFor task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter (Task Qualification and Search Necessity) to retain only tasks requiring multi-source evidence integration and external retrieval.\nFor evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.\n\n\n\nDeepResearchEval:\nAn Automated Framework for Deep Research Task Construction and Agentic Evaluation\n\n\n\n\nYibo Wang1,2,\nLei Wang1222Corresponding Author,\nYue Deng1,\nKeming Wu1,\nYao Xiao1,\nHuanjin Yao2\n\nLiwei Kang1,\nHai Ye1,\nYongcheng Jing2,\nLidong Bing1\n\n1Infinity Lab, Shanda Group\n\n2Nanyang Technological University\n\n\n\n\n\nyibowang0021@gmail.com, {lei.wang, lidong.bing}@shanda.com\n\n Code: https://github.com/Infinity-AILab/DeepResearchEval\n\n\n\nFigure 1: \nOverview of deep research systems’ performance on our benchmark.\nThe upper section reports quality evaluation results across deep research systems, with Gemini-2.5-Pro achieving the highest score (8.518.51/1010).\nThe bottom section reports factual correctness, where Manus achieves the highest ratio of correct statements (82.3%82.3\\%).\n\n\n\n\n1 Introduction\n\nThe rapid advancement of Large Language Models (LLMs) has initiated a significant shift in AI capabilities, moving from passive text generation toward the development of agentic systems capable of tackling complex real-world tasks (Liu et al., 2025; Kimi et al., 2025; Zeng et al., 2025; Gemini, 2025; OpenAI, 2025a). Within this broader transition toward agentic intelligence, deep research systems have emerged as one of the representative paradigms (OpenAI, 2025b; Gemini, 2025; Team et al., 2025; Perplexity, 2025). They autonomously conduct investigative processes that involve iterative web browsing, targeted information retrieval, cross-source verification, and multi-perspective synthesis. Through this structured workflow, deep research systems ultimately generate comprehensive, citation-grounded reports that traditionally require substantial human effort, e.g., “Please review AI compute investments in 2025 and looking ahead to the trends of 2026”.\n\n\nWith the transition of the technological paradigm,\nevaluating long reports generated by deep research systems poses a key challenge, as it differs substantially from conventional QA tasks.\nSeveral benchmarks have been proposed to assess long-form, research-style outputs, however most existing benchmarks suffer from three limitations:\ni) they rely on expert-driven task construction, which is annotation-intensive Yao et al. (2025); Du et al. (2025); Abaskohi et al. (2025); Gou et al. (2025);\nii) they employ static quality evaluation dimensions shared across tasks Fan et al. (2025); Wang et al. (2025);\nand iii) they verify only citation-linked statements for factuality, leaving uncited factual claims unexamined Du et al. (2025); Fan et al. (2025); Gou et al. (2025).\n\n\nTo bridge these gaps, we present DeepResearchEval, an automated framework for deep research task construction and agentic evaluation.\nTo address the scarcity of high-quality deep research tasks and the high cost of expert annotation, we propose a persona-driven pipeline, which constructs tasks anchored in specific personas, ensuring realistic needs and high complexity.\nWe then apply the Task Qualification Filter that assesses whether a task truly requires up-to-date evidence aggregation and multi-source investigation, and the Search Necessity Filter that discards simple questions solvable using an LLM’s internal parametric knowledge.\nAs a result, we retain 100 persona-driven, high-quality deep research tasks across multiple domains.\n\n\nOn the evaluation side, we develop an agentic evaluation pipeline with two key components.\n(i) Adaptive Point-wise Quality Evaluation preserves a fixed set of general evaluation dimensions shared across all tasks, while additionally generating task-specific dimensions, criteria, and corresponding weights for each task, allowing fine-grained and interpretable scoring tailored to individual deep research tasks.\n(ii) Active Fact-Checking performs active verification: it extracts verifiable statements (e.g., numbers, events, dates, entities), retrieves external evidence, and assigns structured labels (Right/Wrong/Unknown), thereby enabling factual verification of both cited and uncited claims.\nWhile factual correctness is an inherently core component of overall report quality, we treat it as a standalone evaluation module due to its distinct verification process, which requires external evidence retrieval, and its critical role in ensuring the reliability of deep research reports.\n\n\nWe apply our framework to evaluate leading deep research systems, spanning proprietary general-purpose systems (e.g., Gemini Deep Research (Gemini, 2025), OpenAI Deep Research (OpenAI, 2025b)), and agentic generalists with deep research capabilities (e.g., Manus (Manus, 2025)).\nIn total, we evaluate 900 deep research reports, comprising 100 tasks per system across 9 deep research systems.\nAs shown in Figure 1, our comprehensive evaluation reveals clear strengths and weaknesses across different dimensions of deep research capability.\nGemini Deep Research achieve the strongest performance in report quality evaluation, reflecting superior coverage, insight, and structural coherence. Manus and Gemini Deep Research attain the highest scores in factual evaluation, showing stronger robustness against hallucinations and citation errors during complex multi-source report synthesis.\nAdditionally, we observe a consistent gap between general and task-specific evaluation: across all systems, task-specific scores are systematically lower than those on fixed general dimensions.\nThis gap indicates that current deep research systems often fail to meet task-specific success criteria, motivating task-adaptive evaluation beyond fixed general dimensions, precisely what our benchmark is designed to capture.\n\n\n\n\n2 Related Works\n\nTable 1: Comparative Analysis of Deep Research Benchmarks.\n\n\n\nBenchmark\n\n \n\n\nAutomated Task\n\nGeneration\n\n\n\n \n\n\nOutput\n\nFormat\n\n\n\n \n\n\nReference-free\n\nEvaluation\n\n\n\n \n\n\nAdaptive Evaluation\n\nDimensions\n\n\n\n \n\n\nActive Fact\n\nVerification\n\n\n\n\nMind2Web 2 (Gou et al., 2025)\n\n×\nReport\n×\n×\n×\n\n\nDeepResearch Bench (Du et al., 2025)\n\n×\nReport\n×\n×\n×\n\n\nResearcherBench (Xu et al., 2025)\n\n×\nReport\n✓\n×\n×\n\n\nBrowseComp-Plus (Chen et al., 2025)\n\n×\nShort-Form Answer\n×\n×\n×\n\n\nWideSearch (Wong et al., 2025)\n\n×\nTable-Style Answer\n×\n×\n×\n\n\nReportBench (Li et al., 2025a)\n\n✓\nReport\n×\n×\n×\n\n\nDeepResearch Arena (Wan et al., 2025)\n\n✓\nReport\n✓\n×\n×\n\n\nDRBench (Abaskohi et al., 2025)\n\n×\nReport\n×\n×\n×\n\n\nLiveResearchBench (Wang et al., 2025)\n\n×\nReport\n✓\n×\n×\n\n\nFinder (Zhang et al., 2025)\n\n×\nReport\n✓\n×\n×\n\n\nDeepResearchEval (Ours)\n✓\nReport\n✓\n✓\n✓\n\n\n\n\n\nDeep research systems are a specialized class of agents designed for complex, multi-stage investigative tasks (OpenAI, 2025b; Gemini, 2025; xAI, 2025; Doubao, 2025; Perplexity, 2025; Manus, 2025; Anthropic, 2025).\nUnlike conventional QA systems, they autonomously plan long-horizon workflows, navigate heterogeneous web sources, and synthesize information into structured, citation-grounded reports.\nExisting systems broadly fall into proprietary solutions (OpenAI, 2025b; Gemini, 2025; Perplexity, 2025) with limited transparency, and open-source efforts (Li et al., 2025b; Zheng et al., 2025; Qiao et al., 2025) emphasizing modularity and reproducibility.\n\n\nThe emergence of deep research systems has motivated a broad range of benchmarks targeting different agentic capabilities (Gou et al., 2025; Mialon et al., 2024; Phan et al., 2025; Du et al., 2025; Wong et al., 2025; Wan et al., 2025; Abaskohi et al., 2025; Zhang et al., 2025; Wang et al., 2025; Li et al., 2025a; Chen et al., 2025; Wei et al., 2025; Xu et al., 2025; Lei et al., 2025; Luo et al., 2025; Yao et al., 2025; Han et al., 2025).\nEarly benchmarks such as GAIA (Mialon et al., 2024) and Humanity’s Last Exam (HLE) (Phan et al., 2025) focus on general reasoning and tool use, while others emphasize persistent web navigation and retrieval, including WideSearch (Wong et al., 2025) and BrowseComp variants (Wei et al., 2025; Chen et al., 2025).\nMore recent benchmarks move toward report-level evaluation, including DeepResearch Bench (Du et al., 2025)"
  },
  {
    "title": "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection",
    "url": "https://arxiv.org/abs/2601.09684v1",
    "source": "arxiv",
    "summary": "Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performa",
    "full_text": null
  },
  {
    "title": "Automating Supply Chain Disruption Monitoring via an Agentic AI Approach",
    "url": "https://arxiv.org/abs/2601.09680v1",
    "source": "arxiv",
    "summary": "Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Network-Based Risk Propagation Models\n2.2 Multi-Agent Systems for Supply Chain Coordination\n2.3 Large Language Model-Powered Supply Chain Systems\n2.4 Research Gap and Positioning\n\n\n\n3 Agentic Supply Chain Disruption Monitoring Framework\n\n3.1 Framework Overview\n\n3.2 Agent Decomposition: Roles, Tasks &amp; Tools\n\n3.2.1 Disruption Monitoring Agent\n3.2.2 Knowledge Graph Query Agent\n3.2.3 Product Search Agent\n3.2.4 Network Visualizer Agent\n3.2.5 Risk Manager Agent\n3.2.6 Chief Supply Chain Officer (CSCO) Agent\n3.2.7 Alternative Sourcing Agent\n\n\n3.3 Prompt Engineering Design Principles\n3.4 Agent Communication and Execution Flow\n3.5 System Configuration and Modularity Considerations\n\n\n\n4 Experimental Evaluation\n\n4.1 Experimental Design and Methodology\n\n4.2 Dataset and Evaluation Methodology\n\n4.2.1 Knowledge Graph Dataset\n4.2.2 Scenario Synthesis\n4.2.3 Ground Truth Dataset Generation\n4.2.4 Evaluation Metrics\n\n\n\n4.3 Results and Discussion\n\n4.3.1 Disruption Monitoring Agent: Foundation for Agentic Pipeline\n4.3.2 Knowledge Graph Query Agent: Multi-Tier Network Traversal\n4.3.3 Risk Manager Agent: Tier-1 Risk Quantification\n4.3.4 CSCO Agent: Strategic Decision-Making\n4.3.5 Qualitative Evaluation of CSCO Agent\n4.3.6 Error Propagation and Framework Robustness\n4.3.7 System Runtime and Computational Requirements\n\n\n4.4 Limitations and Future Research Directions\n\n\n\n5 Case Study: Russia–Ukraine War\n\n5.1 Disruption Monitoring Agent\n5.2 Knowledge Graph Query Agent\n5.3 Product Search Agent\n5.4 Network Visualization Agent\n\n5.5 Risk Manager Agent\n\n5.5.1 CSCO Agent\n5.5.2 Alternative Sourcing Agent\n\n\n\n\n6 Industry Readiness and Deployment Considerations\n7 Conclusions and Future Work\n\n A \n\nAppendix A1 Article used in the case-study\nAppendix A2 Prompt Used for Agent 1 – Disruption Monitoring Agent\nAppendix A3 Prompt Used for Agent 2 – Knowledge Graph Query Agent\nAppendix A4 Prompt Used for Agent 3 – Product Search Agent\nAppendix A5 Prompt Used for Agent 4 – Network Visualizer\nAppendix A6 Prompt Used for Agent 5 – Risk Assessment Agent\nAppendix A7 Prompt Used for Agent 6 – Chief Supply Chain Agent\nAppendix A8 Prompt Used for Agent 7 – Alternative Sourcing Agent\n\n\n\n\n\n\n\nAutomating Supply Chain Disruption Monitoring via an Agentic AI Approach\n\n\n\n\\nameSara AlMahria, Liming Xua, and Alexandra Brintrupa,b\nCONTACT Sara AlMahri. Email: sa2104@cam.ac.uk\n\n\nAbstract\nModern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters.\nWhile many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream.\nTo overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options.\nWe evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of $0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia–Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.\n\n\nkeywords: \nSupply Chain Management;\nSupply Chain Disruption;\nLarge Language Models;\nAI Agents;\nMulti-Agent System;\nAgentic System\n\n\n††articletype: Research Article\n\n\n1 Introduction\n\nGlobal supply chains have never been more vulnerable. In recent years, a series of major disruptions, from geopolitical tensions and demand shocks to trade restrictions and natural disasters, have exposed both the fragility and the deeply interconnected nature of modern supply networks. The US–China trade tensions led to tariffs on approximately $360 billion of Chinese imports (Fajgelbaum and Khandelwal 2022), forcing companies to restructure supply chains, diversify suppliers, and absorb increased operational costs. China’s export controls on gallium and germanium, materials critical to semiconductor and defense industries, further strained global supply chains. Estimates suggest that companies may lose up to 40% of one year’s profits over a decade due to such disruptions (Kweilin Ellingrud 2020). These are not isolated incidents. They represent a new reality in which supply chain disruption management has become a strategic imperative.\n\n\nYet, a critical pattern emerges when examining these disruptions: many originate not from direct suppliers, but from deep within extended supply networks. Recent studies have shown that over one-third, and in some cases up to half, of supply chain disruptions emerge beyond Tier-1 suppliers, within the deeper layers of the network (Berger et al. 2023). A factory fire at a sub-tier semiconductor supplier, a regulatory change affecting a raw material producer, a labour strike at a logistics hub serving multiple upstream manufacturers: these events often go undetected until they cascade downstream and reach immediate suppliers. By then, the damage is already propagating through the network. This raises a fundamental question: if disruptions originate deep in extended supply networks, why do most monitoring solutions focus only on direct suppliers?\n\n\nThe answer lies in the challenge of extended supply chain visibility. Mapping supplier relationships beyond Tier-1 is inherently difficult: networks span thousands of entities across multiple continents, relationships shift as suppliers merge, fail, or restructure, and data remains fragmented across disparate systems. In response to growing disruption risks, companies have adopted digital solutions to improve visibility and resilience. Yet these solutions remain constrained to direct supplier relationships precisely because multi-tier mapping at scale has proven technically and operationally prohibitive. Enterprise visibility platforms integrate real-time data on inventory, shipments, and supplier activity, providing dashboards that track operational status (Ivanov and Dolgui 2021). However, these platforms monitor only direct supplier relationships, leaving upstream tiers invisible. Supply chain control towers offer centralized oversight and predictive analytics across global operations, triggering alerts when predefined thresholds are breached (Bartlett et al. 2007). Yet, control towers operate on structured, internal data and predefined rules for Tier-1 suppliers only. They cannot process emerging signals from external sources or detect anomalies in upstream tiers. Digital supply chain twins simulate end-to-end operations using telemetry and scenario analysis, supporting what-if planning and policy evaluation (Longo et al. 2023; Roman et al. 2025). While powerful for experimentation, digital twins are typically instantiated around Tier-1 and internal operations, leaving blind spots in deep tiers where early disruption cues often surface.\n\n\nResearchers have proposed complementary approaches, but each carries fundamental limitations. For instance, Graph-based risk propagation models quantify how shocks cascade over inter-firm networks, estimating amplification pathways and systemic exposure (Tabachová et al. 2024; Xie et al. 2023; Sun and Liao 2025). These models provide formal propagation metrics and counterfactual stress tests. However, they require granular, structured network data as input. They cannot process emerging, unstructured signals such as breaking news or regulatory notices. Yet this is precisely where disruption intelligence should begin: detecting events as they unfold and mapping them directly onto extended supplier networks to identify which specific chains are exposed. Traditional multi-agent systems (MAS) distribute functionality across specialised agents executing predefined tasks, enabling decentralized detection and mitigation of localized disruptions (Wooldridge 2009; Ferber and Weiss 1999; Giannakis and Louis 2011; Bi et al. 2022). However, MAS implementations rely on hard-coded decision logic and static ontologies. They require manual encoding of each scenario, leading to brittle performance when facing novel disruptions that fall outside predefined rules (Bi et al. 2022).\n\n\nDespite the diversity of these technologies, they share a common limitation: none provides extended supply chain visibility beyond Tier-1, yet over one-third of disruptions originate beyond Tier-1. This fundamental mismatch between where disruptions occur and where monitoring systems focus represents a critical gap in supply chain risk management.\n\n\nThis brings us to the core operational problem. Supply chain resilience, measured by time-to-survive (TTS) and time-to-recover (TTR) (Simchi-Levi et al. 2014; Ivanov et al. 2017), requires early detection and timely action. But while companies can only control their direct suppliers, disruption signals are scattered across unstructured sources (news articles, regulatory filings, supplier advisories) that are siloed, disconnected from network data, and provide no actionable insight until mapped to operations. The complete capability required is thus: (1) capturing disruption signals from scattered unstruct"
  },
  {
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "url": "https://arxiv.org/abs/2601.09667v1",
    "source": "arxiv",
    "summary": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (M",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nLLM-based multi-agent collaboration.\nReinforcement learning for LLM reasoning.\nTest-time adaptation and structured experience.\nCredit assignment under collaboration.\n\n\n\n3 Methodology\n\n\n3.1 Multi-Expert Team Collaboration\n\nStage I: Team formation.\nStage II: Consensus via experience-augmented dialogue.\nStage III: Report synthesis and final decision.\nRemarks.\n\n\n\n3.2 Test-Time Experience Construction\n\nContribution ratio and terminal shared reward.\nTurn-level reward for each agent.\nSelection of high-value utterances.\nFrom high-scoring utterances to textual experience.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Setup\n\nDatasets and Domain Settings\nBaselines.\nMetrics\nParemeters Settings\n\n\n4.2 Results\n\n\n\n5 Analysis\n\n\n5.1 Group-to-Agent Credit Assignment\n\nWhy Shapley underperforms.\n\n\n5.2 Adaptive collaboration between single agent and multi-agent framework\n5.3 Scaling with Team Size\n5.4 Experience Examples\n5.5 Few-shot vs. Test-time Experience\n\n\n6 Conclusion\n\nA Medicine\n\n\nA.1 Detailed Setup\n\nTask and data (RareBench Task 4).\nAgents, specialist pool, and recruitment.\nMDT interaction protocol and prompts.\nUtterance scoring and judge rubric.\nExperience extraction and summarization.\nIndexing and retrieval.\n\n\nA.2 Description of Specialist Pool\nA.3 Experience-Augmented Prompt Template\nA.4 Prompts for Multi-disciplinary Team Collaboration\nA.5 Rubrics for LLM Judge in Agent’s Utterance\nA.6 Prompts for LLM Summarizer\nA.7 Retrieval Implementation Details\n\n\n\nB Mathematics\n\n\nB.1 Detailed Setup\n\nBaseline run (no experience)\n\n\nB.2 Free recruitment team formation\nB.3 Multi-round collaboration (Stage II)\nB.4 Structured peer review and acceptance rule\nB.5 Chair aggregation (final decision)\nB.6 Rubrics for LLM Judge in Math Utterance Scoring\nB.7 Interaction scoring and selection (train split only)\nB.8 Experience extraction and indexing (train split only)\nB.9 Test-Time Experience Retrieval\n\n\n\nC Education\n\n\nC.1 Detailed Setup\n\nPre-test\nPedagogy Specialist Team Formation\nMulti-round teaching session\nPost-test\nInteraction scoring and selection\nExperience extraction and summarization\nC.1.1 Test-Time Experience Retrieval\n\n\nC.2 Experience Examples\nC.3 Description of Specialist Pool\n\nC.4 Prompts\n\nC.4.1 Prompt for Student Agent in Pre-test\nC.4.2 Pedagogy Specialist Recruitment Prompt\nC.4.3 Prompts for Single-Teacher Instruction\nC.4.4 Prompt for Multi-Teacher Instruction\nC.4.5 Prompt for Experience Integration\nC.4.6 Prompt for Teaching Utterance Evaluation\nC.4.7 Prompts for Experience Summarizer\n\n\n\n\n\n\n\n\n\nCollaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning\n\n\n\nZhiyuan Hu1,2  Yunhai Hu3  Juncheng Liu4  Shuyue Stella Li5  Yucheng Wang2  Zhen Xu6  \nSee-Kiong Ng2  Anh Tuan Luu7  Xinxing Xu4  Bryan Hooi2  Cynthia Breazeal1  Hae Won Park1  \n1 MIT  2 NUS  3 NYU  4 Microsoft  5 UW  6 Columbia  7 NTU\nZhiyuan Hu. Email: hzycs@mit.edu\n\n\nAbstract\nMulti-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance.\nTherefore, we introduce Multi-Agent Test-Time Reinforcement Learning (MATTRL), a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making.\nWe also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue.\nAcross challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline, and by 8.67% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.\n\n\n\nCollaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning\n\n\n\n\n\nZhiyuan Hu1,2††thanks: Zhiyuan Hu. Email: hzycs@mit.edu   Yunhai Hu3   Juncheng Liu4   Shuyue Stella Li5   Yucheng Wang2   Zhen Xu6\n\nSee-Kiong Ng2  Anh Tuan Luu7  Xinxing Xu4  Bryan Hooi2  Cynthia Breazeal1  Hae Won Park1\n\n1 MIT  2 NUS  3 NYU  4 Microsoft  5 UW  6 Columbia  7 NTU\n\n\n\n\n\n\n1 Introduction\n\nMulti-agent systems have moved from early algorithmic prototypes to practical LLM-driven collaborators.\nAcross math, coding, web interaction, and analytical benchmarks, these multi-agent systems reliably outperform comparable single-agent baselines, as diversity and cross-checking improve robustness under distribution shift.\n\n\nRecent works explore collaborative multi-agent frameworks to enhance LLM agents’ capabilities. For example, AutoGen Wu et al. (2024) (orchestrated multi-agent dialogues with tool use and human-in-the-loop), CAMEL Li et al. (2023) (role-playing with inception prompting), AgentVerse Chen et al. (2023) (an open platform for cooperative problem solving and social simulation), ChatDev Qian et al. (2023) (specialized software agents for design, coding, and testing), and Magentic-One Fourney et al. (2024) (an orchestrator that routes tasks among specialized agents for web/local workflows). In parallel, the success of DeepSeek-R1 Guo et al. (2025) has catalyzed reinforcement learning (RL) as a post-training paradigm for stronger reasoning.\nEfforts to extend RL to the multi-agent setting include MAPoRL Park et al. (2025), which jointly optimizes multi-model discussions and final answers via RL, and ReMA Wan et al. (2025), which separates high-level meta-thinking from low-level reasoning into two agents and trains them with GRPO.\n\n\nHowever, MARL remains resource-intensive and can erode general abilities when adapted to a single domain. Training stability is also difficult to guarantee due to (i) non-stationarity from simultaneously evolving teammates, which shifts state and return distributions, and (ii) sparse, high-variance rewards.\nHence, we propose Multi-Agent Test-Time Reinforcement Learning (MATTRL), an adaptation framework that injects test-time textual experience into the collaborative process. Instead of updating weights, MATTRL conditions behavior with structured experience, enabling rapid, distribution-shift-robust adaptation to new tasks/domains without harming original generality. Additionally, textual experience provides richer turn-level signals about collaboration quality and reasoning than scalar rewards alone.\nTextual experience mitigates key MARL pain points by keeping policies fixed and providing dense, stepwise experience at every turn.\n\n\n\nThe crucial components of MATTRL include (1) various group-to-agent credit assignment strategies for experience selection, (2) construction of an experience pool from test time examples, and (3) integration of the experience pool into the multi-agent collaborative process.\nMATTRL first instantiates a team of specialized agents. The agents deliberate in multi-turn discussions, drawing on relevant prior experience to aggregate evidence and move toward agreement. The process terminates when agreement is reached or a predefined turn limit is met. A designated coordinator agent then summarizes the discussion, consolidates the accumulated evidence, and outputs the final decision.\nTo retrieve experience, each agent utterance is first scored using both individual-performance signals and a decayed terminal shared reward. For constructing the experience pool, high-scoring utterances are distilled into textual experiences and added to the pool for subsequent retrieval and integration.\nExperiments show that, on benchmarks spanning medicine, math, and education, MATTRL boosts average performance by 3.67% over the multi-agent framework and by 8.67% over comparable single-agent baselines.\nFurthermore, we systematically explored multiple credit-assignment schemes for group-credit attribution in experience selection, ranging from naïve shared credit to difference rewards and Shapley-style approximations.\nTo summarize, our contributions focus on these three perspectives:\n\n\n\n\n•\n\nWe propose the first Multi-agent Test Time Reinforcement Learning framework, MATTRL, leveraging textual experience to enhance the multi-agent system.\n\n\n\n•\n\nWe further validate the effect of different credit assignments on experience construction and the final decision.\n\n\n\n•\n\nExperiments conducted on medical, math and education benchmarks achieve a new SOTA performance based on MATTRL.\n\n\n\n\n\n\n\n2 Related Work\n\nLLM-based multi-agent collaboration.\n\nRecent advancements in LLM-based multi-agent systems have emphasized scalable collaboration mechanisms for complex task-solving. Surveys Tran et al. (2025) outline key coordination strategies in LLM-driven multi-agent systems, enabling groups of agents to work collectively at scale.\nMacNet Qian et al. (2024) explores the benefits of continuously adding agents to enhance performance in collaborative settings.\nMulti-agent systems utilizing LLMs also emerge as tools for enhancing medical decision-making processes. MDAgents Kim et al. (2024) introduces adaptive collaboration among LLMs to address gaps in clinical reasoning and diagnostics. Multi-agent conversational framework, MACChen et al. (2025) boost diagnostic accuracy through interactive agent dialogues.\n\n\n\nReinforcement learning for LLM reasoning.\n\nReinforcement learning techniques have been increasingly applied to refine reasoning capabilities in large language models. Models such as DeepSeek-R1 Guo et al. (2025) demonstrate RL’s potential to enhance LLM reasoning without relying on human-annotated data.\nRecent work also systematize RL for reasoning-centric LLMs. SimpleRL-Zoo Zeng et al. (2025) conducts a broad, controlled study of RL on open-base models, showing tha"
  },
  {
    "title": "Exploring Fine-Tuning for Tabular Foundation Models",
    "url": "https://arxiv.org/abs/2601.09654v1",
    "source": "arxiv",
    "summary": "Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n2.1 Models and Benchmarks\n2.2 Adaptation Strategies\n2.3 Evaluation Metrics\n2.4 Research Questions\n\n\n\n3 Experimental Results\n\n3.1 Performance Across Dataset Characteristics.\n3.2 Calibration Evaluation :\n3.3 Fairness Evaluation :\n3.4 Key Findings :\n3.5 Limitations :\n\n\n4 Conclusion\n\n\n\n\n\n\n\n\n\nAditya Tanna, Pratinav Seth, Mohamed Bouadi, Vinay Kumar Sankarapu \nLexsi Labs, India &amp; France\n{aditya.tanna,v.k}@lexsi.ai \n\n\n\n\n1  Introduction\n\nTabular data drives applications in enterprises like healthcare, finance, and analytics. For decades, gradient-boosted decision trees (GBDTs) such as XGBoost [6], LightGBM [17], and CatBoost [22] have dominated due to their robustness on heterogeneous features and small datasets. While deep learning has revolutionized vision and NLP, standard neural networks often struggle to outperform GBDTs on tabular tasks [3, 25], though recent architectures like FT-Transformer [10] and TabR [9] have narrowed this gap.\n\n\nTabular Foundation Models (TFMs) offer a new paradigm, leveraging large-scale pretraining to enable in-context learning (ICL) and zero-shot generalization. TabPFN [13, 14] approximates Bayesian inference using transformers trained on synthetic data, effectively performing posterior inference in a single forward pass for small datasets. Building on this, TabICL [23] scales ICL to larger datasets via a two-stage column-then-row attention mechanism. To address high-dimensional dependencies, OrionMSP [4] and OrionBiX [5] introduce multi-scale sparse attention and biaxial attention mechanisms, respectively, enabling efficient context modeling. TabDPT [19] employs diffusion-based pretraining to learn robust representations, while Mitra [29] utilizes mixed synthetic priors to improve generalization.\n\n\nWhile zero-shot inference is powerful, adapting TFMs to target distributions can enhance performance. Meta-learning (episodic fine-tuning) preserves ICL capabilities by training on support–query splits, mimicking the inference-time environment [8]. Supervised fine-tuning (SFT) updates all parameters on labeled data [15], which can yield gains on large datasets but risks overfitting on small ones.\nParameter-efficient fine-tuning (PEFT), particularly Low-Rank Adaptation (LoRA) [16], offers a resource-efficient alternative by updating only low-dimensional subspaces, often matching full fine-tuning performance with lower overhead [18, 24].\nStrategies like LoCalPFN (Thomas et al. [27]) have also emerged, combining retrieval with fine-tuning to adapt to local data sub-manifolds.\n\n\nBeyond predictive performance, responsible deployment requires assessing model trustworthiness through calibration and fairness. Calibration ensures that predicted probabilities align with true outcomes, a critical requirement for high-stakes decision-making in healthcare and finance. Fairness necessitates that models avoid systematic bias across demographic groups. While TFMs show promise in zero-shot settings, the impact of different adaptation strategies on these safety-critical dimensions remains under-explored.\nHowever, it remains unclear when fine-tuning helps these models, which strategies are effective, and how dataset properties influence outcomes. The wide variation in tabular dataset size, imbalance, and feature structure makes the impact of fine-tuning particularly non-obvious. This work presents the first large-scale study of fine-tuning TFMs.\n\n\nOur contributions are: (I) a unified evaluation of four adaptation strategies—zero-shot, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT)—across six TFMs and multiple benchmarks; (II) an analysis of how dataset factors (size, imbalance, dimensionality) shape fine-tuning outcomes; (III) an assessment of calibration and fairness effects; and (IV) practical guidelines on when fine-tuning is effective. Our findings show that fine-tuning is not universally beneficial, underscoring the need for careful strategy selection. An Overview of all the models and adaptation strategies in shown in Figure 1\n\n\n\n2  Methodology\n\nFigure 1: High-level overview of models and adaptation strategies. We benchmark classical tabular baselines and multiple tabular foundation models under three regimes—zero-shot inference, parameter-efficient fine-tuning (PEFT), and full supervised fine-tuning—using both supervised fine-tuning and meta-learning variants\n\n\n2.1  Models and Benchmarks\n\nWe evaluate six Tabular Foundation Models—TabPFN, TabICL, OrionMSP, OrionBiX, TabDPT, and Mitra—alongside classical baselines (XGBoost, LightGBM, CatBoost, Random Forest). Experiments span three benchmark suites: TALENT (155 datasets) [28], OpenML-CC18 (63 datasets) [2], and TabZilla (27 datasets) [20]. For fairness evaluation, we additionally use nine public datasets with sensitive attributes, including Adult Census Income, German Credit, and COMPAS Recidivism. All experiments follow standardized splits and are executed on NVIDIA L40S and H200 GPUs. We utilize TabTune [26] for TFM experiments and it’s AutoGluon [7] implementation for baselines.\n\n\n\n2.2  Adaptation Strategies\n\nWe compare four distinct adaptation paradigms to assess their impact on performance, calibration, and fairness:\n\n\n•\n\nZero-Shot Inference: Evaluates pretrained TFMs directly on target tasks without parameter updates, relying solely on in-context learning or prior knowledge.\n\n\n\n•\n\nMeta-Learning Fine-Tuning: Employ episodic training (48–512 support samples per episode for 5 epochs) to adapt the model while preserving its in-context learning capabilities.\n\n\n\n•\n\nSupervised Fine-Tuning (SFT): Performs full-parameter optimization using the AdamW optimizer with learning rates between 1​e−51\\text{e}^{-5} and 5​e−55\\text{e}^{-5}, employing early stopping (up to 10 epochs) to mitigate overfitting.\n\n\n\n•\n\nParameter-Efficient Fine-Tuning (PEFT): Utilizes Low-Rank Adaptation (LoRA) [16] with rank r=8r{=}8 and α=16\\alpha{=}16. This strategy is applied to both supervised and meta-learning regimes to reduce memory overhead while maintaining adaptation flexibility.\n\n\n\n\n\nTable 1: Overall leaderboard across three benchmark suites—TALENT, OpenML-CC18, and TabZilla. Ranks denote the mean rank per benchmark suite (lower is better). Metrics: ACC = Accuracy, F1 = Weighted F1. The \"All\" column reports the aggregated rank across for a strategy. Models are grouped by adaptation strategy. Formatting: 1st place; 2nd place within each group. Note : that rank reflects only relative ordering and may not capture absolute ACC/F1 trends, which can vary across scenarios.\n\n\n\nStrategy\nAll\nTALENT\nOpenML-CC18\nTabZilla\n\n\nModels\nRank(↓\\downarrow)\nRank(↓\\downarrow)\nACC(↑\\uparrow)\nF1(↑\\uparrow)\nRank(↓\\downarrow)\nACC(↑\\uparrow)\nF1(↑\\uparrow)\nRank(↓\\downarrow)\nACC(↑\\uparrow)\nF1(↑\\uparrow)\n\n\nBaselines + Zero-Shot Inference\n\n\nXGBoost\n6.39\n5.97\n0.8403\n0.8360\n5.34\n0.8558\n0.8537\n5.62\n0.8612\n0.8326\n\n\nCatBoost\n6.16\n5.54\n0.8336\n0.8259\n6.64\n0.8588\n0.8520\n5.40\n0.8579\n0.8384\n\n\nRandom Forest\n7.06\n6.11\n0.8285\n0.8209\n5.78\n0.8547\n0.8497\n7.72\n0.8358\n0.8399\n\n\nLightGBM\n6.50\n6.07\n0.8331\n0.8245\n5.60\n0.8581\n0.8493\n4.90\n0.8618\n0.8211\n\n\nTabICL\n4.78\n4.07\n0.8471\n0.8379\n4.36\n0.8667\n0.8623\n5.63\n0.8734\n0.8698\n\n\nOrionBiX\n5.27\n4.56\n0.8346\n0.8260\n4.97\n0.8653\n0.8596\n4.63\n0.8728\n0.8628\n\n\nOrionMSP\n3.76\n3.26\n0.8461\n0.8360\n3.90\n0.8722\n0.8676\n3.78\n0.8821\n0.8786\n\n\nTabPFN\n4.44\n3.71\n0.8514\n0.8412\n4.36\n0.8714\n0.8663\n4.65\n0.8752\n0.8716\n\n\nMitra\n10.85\n9.67\n0.3921\n0.2868\n9.55\n0.3614\n0.2522\n10.21\n0.3152\n0.1830\n\n\nTabDPT\n5.27\n5.16\n0.8408\n0.8318\n4.32\n0.8672\n0.8625\n3.81\n0.8814\n0.8775\n\n\nFineTune – Meta Learning\n\n\nTabICL\n3.66\n3.33\n0.8344\n0.8253\n3.05\n0.8664\n0.8597\n4.15\n0.6956\n0.6845\n\n\nOrionBiX\n3.96\n4.25\n0.8158\n0.8060\n2.86\n0.8548\n0.8516\n2.00\n0.8726\n0.8662\n\n\nOrionMSP\n2.26\n1.80\n0.8401\n0.8310\n2.82\n0.8548\n0.8516\n1.73\n0.8735\n0.8672\n\n\nTabPFN\n2.42\n2.07\n0.8517\n0.8414\n2.42\n0.8842\n0.8784\n2.42\n0.8663\n0.8603\n\n\nMitra\n6.09\n5.79\n0.6416\n0.5874\n5.42\n0.6164\n0.5651\n4.70\n0.5592\n0.5147\n\n\nTabDPT\n3.95\n3.72\n0.8255\n0.8167\n4.40\n0.8534\n0.8501\n2.62\n0.8500\n0.8456\n\n\nFineTune – Supervised Fine Tuning\n\n\nTabICL\n5.05\n4.74\n0.7668\n0.7439\n4.36\n0.6838\n0.6299\n4.65\n0.5670\n0.4733\n\n\nOrionBiX\n4.21\n3.54\n0.7698\n0.7469\n4.30\n0.7126\n0.6595\n4.25\n0.6476\n0.5782\n\n\nOrionMSP\n2.88\n2.27\n0.7908\n0.7653\n3.18\n0.7995\n0.7668\n2.88\n0.7454\n0.7222\n\n\nTabPFN\n1.97\n1.83\n0.8459\n0.8350\n1.89\n0.8697\n0.8617\n1.86\n0.8433\n0.8327\n\n\nMitra\n5.98\n5.62\n0.5460\n0.4382\n5.02\n0.5408\n0.4309\n5.52\n0.4608\n0.3467\n\n\nTabDPT\n2.79\n2.98\n0.8202\n0.8094\n2.25\n0.8499\n0.8424\n1.81\n0.8337\n0.8260\n\n\nFineTune – PEFT - Meta Learning\n\n\nTabICL\n4.32\n4.07\n0.7017\n0.6867\n4.07\n0.7773\n0.7605\n3.45\n0.7116\n0.7003\n\n\nOrionBiX\n2.77\n2.39\n0.7854\n0.7762\n2.50\n0.8471\n0.8430\n3.52\n0.7370\n0.7200\n\n\nOrionMSP\n2.21\n2.25\n0.7879\n0.7728\n1.93\n0.8566\n0.8453\n1.33\n0.8594\n0.8581\n\n\nMitra\n4.64\n4.17\n0.6369\n0.5905\n4.32\n0.6000\n0.5426\n4.36\n0.5461\n0.4960\n\n\nTabDPT\n2.28\n2.11\n0.8002\n0.7910\n2.17\n0.8600\n0.8539\n2.33\n0.8495\n0.8481\n\n\nFineTune – PEFT - Supervised Fine Tuning\n\n\nTabICL\n3.70\n3.72\n0.5692\n0.4638\n3.06\n0.8174\n0.7965\n2.92\n0.7920\n0.7776\n\n\nOrionBiX\n3.99\n3.57\n0.6358\n0.5570\n3.67\n0.7380\n0.6789\n3.95\n0.6707\n0.6071\n\n\nOrionMSP\n2.01\n2.01\n0.6749\n0.5956\n1.73\n0.8241\n0.8033\n1.47\n0.8214\n0.8071\n\n\nMitra\n4.62\n4.01\n0.5294\n0.4303\n4.49\n0.4965\n0.3917\n4.70\n0.4606\n0.3597\n\n\nTabDPT\n1.91\n1.65\n0.7988\n0.7842\n2.03\n0.8500\n0.8398\n1.95\n0.8461\n0.8461\n\n\n\n\n\n\n\n2.3  Evaluation Metrics\n\nOur assessment covers three complementary dimensions:\n\n\n•\n\nPerformance: Measured via Accuracy, weighted F1-score, and mean rank across datasets.\n\n\n\n•\n\nCalibration: Assessed using Expected Calibration Error (ECE), Maximum Calibration Error (MCE), and Brier Score [11] to quantify the reliability of probability estimates.\n\n\n\n•\n\nFairness: Evaluated using Statistical Parity Difference (SPD), Equalized Odds Difference (EOD), and Equal Opportunity Difference (EOpD) [1, 12, 21] to detect group disparities.\n\n\n\n\n\n\n2.4  Research Questions\n\nIn this work, we study: (1) whether fine-tuning improves TFM performance compared to zero-shot inference; (2) how dataset properties such as size"
  },
  {
    "title": "Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation",
    "url": "https://arxiv.org/abs/2601.09648v1",
    "source": "arxiv",
    "summary": "Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based ",
    "full_text": null
  },
  {
    "title": "Identifying Models Behind Text-to-Image Leaderboards",
    "url": "https://arxiv.org/abs/2601.09647v1",
    "source": "arxiv",
    "summary": "Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space,",
    "full_text": null
  },
  {
    "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
    "url": "https://arxiv.org/abs/2601.09636v1",
    "source": "arxiv",
    "summary": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted pr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Personalized GUI Agent\n2.2 Agent Memory\n\n\n3 PersonalAlign Task Definition\n\n4 AndroidIntent\n\n\n4.1 Hierarchical Filtering Strategy\n\nAnalysis Strategy.\nQuantifying and Filtering.\n\n\n4.2 Human Verifying Strategy\n\n\n\n5 HIM-Agent\n\n5.1 Streaming Aggregation Module\n\n5.2 Execution-based Preference Filter\n\n\n5.3 State-based Routine Filter\n\n\n6 Experiment\n\n\n6.1 Experimental Setup\n\nMetrics.\n\n\n6.2 Experimental Analysis\n\n6.3 Case Study\n\n\n7 Conclusion\n\n\nA AndroidIntent Statistic\n\nA.1 Data Statistic\nA.2 Data privacy\n\n\n\nB Experiment Details\n\nB.1 Baselines\nB.2 Implementation details\nB.3 Online Evaluation\n\n\nC More Analysis\n\nD Personal Agent Settings\n\nD.1 Personalized Rewriting\nD.2 Proactive Triggering\nD.3 Proactive Executing\n\n\nE Annotator Requirements\n\nF Prompt for Agents\n\nF.1 Prompt for LLM-UM\nF.2 Prompt for Proactive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records\n\n\n\nYibo Lyu,\nGongwei Chen,\nRui Shao†,\nWeili Guan,\nLiqiang Nie†\nHarbin Institute of Technology, Shenzhen\n\nweberlv1b@gmail.com   {shaorui, nieliqiang}@hit.edu.com\n\nhttps://github.com/JiuTian-VL/PersonalAlign\n\n\n\nAbstract\nWhile GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users’ more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance.\nTo facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents’ ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation.\nFurthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization.\nFinally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.\n\n\n\nPersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records\n\n\n\n\nYibo Lyu,\nGongwei Chen,\nRui Shao†,\nWeili Guan,\nLiqiang Nie†\n\nHarbin Institute of Technology, Shenzhen\n\nweberlv1b@gmail.com   {shaorui, nieliqiang}@hit.edu.com\n\nhttps://github.com/JiuTian-VL/PersonalAlign\n\n\n\n††footnotetext: † Corresponding authors.\n\n\n1 Introduction\n\nWith the rapid advancement of multimodal large language models (MLLM) An et al. (2025); Bai et al. (2025), GUI agents have made significant progress in grounding natural language instructions into executable actions Wang et al. (2025a); Xie et al. (2025a); Chen et al. (2025a). However, most existing works are evaluated in simulated environments and rely on the strong assumption of complete user instructions. We argue that instructions often fail to fully capture user’s true intent in daily usage, motivating the need for personalized agents capable of perceiving intent beyond the traditional instruction-following reactive paradigm.\n\n\nFigure 1: We highlight a new PersonalAlign agent task. Agent should leverage the user’s long-term record to provide hierarchical implicit intent alignment for both preference and routine intent.\n\n\nTable 1: Comparison of the AndroidIntent with existing GUI benchmark or dataset. We also show that whether each trait is fully incorporated (✓), partially incorporated (✓), or not incorporated (✗).\n\n\n\n \n\n\nBenchmark or Dataset\n\n\n \n\n\nvague\n\nInstruction\n\n\n \n\n\nProactive\n\nSuggestion\n\n\n \n\n\nLong-term\n\nRecords\n\n\n \n\n\nUser-Centric\n\nAnnotation\n\n\n \n\n\nUser\n\nModeling\n\n\n \n\n\nTask Target\n\n\n\nAITW Rawles et al. (2023)\n\n✗\n✗\n✗\n✗\n✗\nGUI Execution\n\n\nAndroidControlLi et al. (2024a)\n\n✗\n✗\n✗\n✗\n✗\nGUI Execution\n\n\nSPA-BenchChen et al. (2025b)\n\n✗\n✗\n✗\n✗\n✗\nGUI Execution\n\n\nProactiveAgentLu et al. (2025b)\n\n✗\n✓\n✗\n✗\n✗\nProactive Agent\n\n\nOS-KaroisCheng et al. (2025a)\n\n✓\n✗\n✗\n✗\n✗\nHuman Cooperation\n\n\nIFRAgentWu et al. (2025)\n\n✓\n✗\n✓\n✗\n✓\nPersonalized Execution\n\n\nFingerTipYang et al. (2025b)\n\n✗\n✓\n✓\n✗\n✗\nBehavior Prediction\n\n\nAndroidIntent\n✓\n✓\n✓\n✓\n✓\nIntent Alignment\n\n\n\n\nFrom a user-centric perspective, human–agent interaction constitutes a joint activity where meaning is co-constructed through shared context Clark (1996). In daily usage, this shared context leads users to naturally omit recurring patterns, assuming the agent can \"fill in the blanks\" from historical records Cheng et al. (2025b); Qian et al. (2024), leading to the emergence of implicit intent.\nAs shown in Figure 1, user intent exhibits hierarchical degrees of implicitness. While reactive agents primarily handle explicit instructions, a personalized agent should extend to align user’s implicit intent by leveraging long-term user records as context: addressing preference intent in vague instructions where preference details are omitted, and further anticipating routine intent without instruction only based on current user state. Bridging implicit intent gaps is essential for effective joint activity and for establishing stronger human–agent trust.\n\n\nTo address this practical dilemma, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign). This task shifts focus from simple execution to align with user’s implicit intent.\nSpecifically, PersonalAlign requires agent to identify preferences from past repeated selections to resolve vague instructions, while separating more frequent and state-consistent routines to facilitate proactive assistance. This enables agents to transition from independent reactive executors into personalized partners that co-evolve through personal interaction.\nTo achieve PersonalAlign, current research faces two limitations:\n\n\n1. Lack of long-term, user-centric annotated benchmarks.\nMost existing datasets primarily focus on static, simulated completion instruction execution, failing to evaluate personalized agent. To bridge this gap, we introduce AndroidIntent, a novel benchmark constructed from personal daily records.\nWe explicitly annotate user preferences and routines from long-term records, and simulate implicit intent by carefully removing recoverable personal preferences from original intents.\nTo mitigate subjectivity and conceptual ambiguity during the annotation process, we introduce a hierarchical filtering-verifying strategy. This approach translates abstract personalization concepts into quantifiable scores, allowing us to efficiently identify candidates for human verification. As detailed in Table 1, AndroidIntent evaluates GUI agent’s ability to align implicit intent by leveraging long-term user records as context for user modeling.\n\n\n2. Inability to manage long-term user records. Existing agent memory for LLM-chat often rely solely on semantic similarity, which is insufficient for handling GUI execution. Moreover, such memory fail to support hierarchical intent alignment. To address these limitations, we propose a specially designed agent memory, Hierarchical Intent Memory Agent (HIM-Agent).\nHIM-Agent incorporates a Streaming Aggregation Module to enable incremental updates of user behavior. Building upon this foundation, the Execution-based Preference Filter and the State-based Routine Filter extract Preference Intent Memory and Routine Intent Memory, forming a hierarchical intent alignment support.\n\n\nIn the experiment, we observe that vague instructions typically convey coarse-grained sub-goals, causing fine-grained execution failures when specific personal requirements are absent at certain steps, and that most current GUI agents fail to deliver reliable proactive suggestions, calling for personal agent’s stronger context analysis capabilities.\nOur contribution can be summarized as:\n\n\n\n\n•\n\nWe introduce a new agent task PersonalAlign, which requires agent to align with user’s implicit intents through long-term record.\n\n\n\n•\n\nWe construct a new GUI bench AndroidIntent, which annotates daily preference and routine intent from long-term records.\n\n\n\n•\n\nWe propose a new agent memory HIM-Agent, which hierarchically learns preference and routine intent from long-term records.\n\n\n\n•\n\nWe conduct extensive experiments and analysis across various GUI Agents and demonstrate superior performance of HIM-Agent.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Personalized GUI Agent\n\nWith the rapid advancement of multimodal large language models Lyu et al. (2025); Zhu et al. (2025); Zhang et al. (2025a), a wide range of downstream tasks have benefited significantly Shao et al. (2024); Zhang et al. (2024); Li et al. (2025); Shao et al. (2019, 2023). While many GUI datasets and methods concentrate on evaluating execution success rate Lu et al. (2025a); Xie et al. (2024); Zhou et al. (2025a), a more intelligent personal agent assistant should infer and match user’s true intent.\nSome prior work focuses on learning from human action for personalization. For example, IFRAgent Wu et al. (2025) extracts both explicit and implicit intention flows from short-term instruction traces. FingerTip Yang et al. (2025b) uses recent historical actions to personalize current execution.\nOn the other hand, some research emphasizes proactive suggestions for personalization. ProactiveAgent Lu et al. (2025b) trains agents to monitor the environment and user activities to enable proactive behaviors; ContextAgent Yang et al. (2025a) leverages real-world sensors to infer users’ current states.\nHowever, existing methods rarely address daily, user-centric personalization; preference execution and proactive suggestion have largely evolved as isolated para"
  },
  {
    "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach",
    "url": "https://arxiv.org/abs/2601.09635v1",
    "source": "arxiv",
    "summary": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and ",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Main Contributions and Key Messages\n1.2 Related Works\n\n\n\n2 Dataset Compilation\n\n2.1 Reference Dataset Construction\n2.2 Large-Scale-OR Testing Dataset\n2.3 Textual Comparisons between Ref-Data and Large-Scale-OR\n\n\n\n3 LEAN-LLM-OPT Framework\n\n3.1 Problem Classification\n\n3.2 Workflow Generation and Model Generation\n\n3.2.1 Type-Tailored Workflow\n3.2.2 Type-Agnostic Workflow\n3.2.3 Model Generation Agent\n\n\n3.3 Novelties in Our Design and Key Messages\n\n\n4 Numerical Simulations\n\n5 Case Study: Singapore Airlines Choice-Based Revenue Management\n\n5.1 Background Information and Sales-Based Linear Programming\n5.2 Available Data and Testing Dataset Air-NRM\n5.3 Performance on Fare Type Capacity Allocation and Ablation Study\n5.4 Performance on Network Planning\n\n\n6 Concluding Remarks\n\nA Supplementary Details for Large-Scale-OR\n\nA.1 Application Domains of the Problem Instances\nA.2 Parameters Imputation\n\n\nB Problem Classification Standard\nC Sample Problem Instance in Large-Scale-OR\nD On the Comparison between the Classification Agent and RAG\nE Demo Problem Used in Workflow Generation\nF Type-Tailored Retrieved Data Format\nG Generate Python Program Codes from the Optimization Models\nH Example Problem Instance in Air-NRM-CA\n\n\n\n\n\n\n\\OneAndAHalfSpacedXI\\OneAndAHalfSpacedXII\\ECRepeatTheorems\\EquationsNumberedThrough\\RUNTITLE\nLLM for Large-Scale Optimization Model Auto-Formulation\n\\TITLELLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach\n\n\n\\ARTICLEAUTHORS\\AUTHOR\nKuo Liang1, Yuhang Lu 2, Jianming Mao3, Shuyi Sun2, Chunwei Yang4, Congcong Zeng6\nXiao Jin5,8, Hanzhang Qin2,5,6, Ruihao Zhu7, Chung-Piaw Teo5,6,8\n\\AFF\n1 Antai College of Economics and Management, Shanghai Jiao Tong University\n2 Department of Industrial Systems Engineering and Management, National University of Singapore\n3 School of Information Management and Engineering, Shanghai University of Finance and Economics \n4 Department of System Engineering, City University of Hong Kong\n5 Institute of Operations Research and Analytics, National University of Singapore\n6 Modern Logistics Centre, National University of Singapore (Chongqing) Research Institute \n7 SC Johnson College of Business, Cornell University \n8 SIA-NUS Digital Aviation Corporate Lab\n\\AFF\ncora.liang1116@gmail.com  adamleo@nus.edu.sg  Mao1140168140@stu.sufe.edu.cn  e1184349@u.nus.edu\nchunwyang2-c@my.cityu.edu.hk  congcong008667@gmail.com  xiao.j@nus.edu.sg  hzgin@nus.edu.sg  ruihao.zhu@cornell.edu  bizteocp@nus.edu.sg \n\n\n\\ABSTRACT\nLarge-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs’ text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent’s burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.\n\n\n\\KEYWORDS\nlarge language models, tool use, agentic workflow construction, automated optimization modeling\n\n\n\n1 Introduction\n\nAs modern businesses grow in scale and complexity, organizations increasingly rely on advanced data analytics and optimization tools to enhance decision-making and operational efficiency. From supply chain networks to financial planning and resource allocation, many business sectors face the challenge of extracting information from data and formulating intricate optimization problems on top of it (Chen and Graves 2021, Kang et al. 2022, Dang et al. 2021, Deng et al. 2023). However, as observed in many cases, in order to apply these techniques, companies and organizations usually rely on highly skilled experts to understand the problems, process data, formulate the optimization models, and solve them. Currently, the above works typically require significant manual effort and can be costly, making it a major bottleneck in fully leveraging the power of optimization. Therefore, researchers start to explore automated modeling of optimization problems in an end-to-end manner, i.e., from the natural language problem descriptions and data to optimization formulations and the corresponding programming codes (Ramamonjison et al. 2022, 2023).\n\n\nRecently, advancements in Large Language Models (LLMs) open a new avenue for automated optimization modeling (Simchi-Levi et al. 2025). Built on LLMs’ remarkable ability in understanding and processing unstructured information and natural languages, several studies have been focusing on utilizing LLMs to formulate and/or solve optimization problems based on text descriptions. This facilitates the application of optimization techniques by automating expert-level modeling, allowing more non-experts to handle tricky optimization problems through interfacing with LLMs. As initial attempts, several works examine how to prompt pretrained LLMs to formulate and/or solve optimization problems directly (Xiao et al. 2023, AhmadiTeshnizi et al. 2024, Li et al. 2023, Chen et al. 2024, Wasserkrug et al. 2025). Perhaps not surprisingly, using prompting techniques only without any modification to the LLMs may lead to unsatisfactory accuracy. Hence, researchers turn to fine-tuning-based approaches, which modify the parameters of open-source LLMs based on task-specific datasets, to improve their performance (Ma et al. 2024, Huang et al. 2024, 2025, Yang et al. 2024, Jiang et al. 2025, Astorga et al. 2024).\n\n\nWhile fine-tuning-based approaches significantly improve LLMs’ modeling accuracy, prior works primarily focus on small-scale inputs (i.e., the problem) and outputs (i.e., the optimization model). Meanwhile, a practical optimization model typically involves a large number of variables, and in order to correctly define its objective and constraints, one usually needs to process data stored in large-scale datasets (e.g., CSV files). However, as we can see in Table 5 of Section 4, when turning to large-scale problems111For inputs, we refer to problem instances whose data has to be stored separately in datasets as large-scale. For outputs, we define the scale based on the number of variables needed (small-size: &lt;20&lt;20 variables, medium-size: 20−9920-99 variables, large-size: ≥100\\geq 100 variables). In this work, we focus mainly on large-scale inputs but allow the scale of the output to vary., the modeling accuracy of state-of-the-art fine-tuned models (e.g., ORLM proposed in Huang et al. (2025)) and closed-source models (e.g., GPT-5.2) can quickly deteriorate. This thus raises a critical question,\ncan we design LLM-based methods for efficient large-scale optimization modeling from natural language inputs? For this, several major challenges persist:\n\n\n•\n\nChallenge 1. Long Input Processing and Reasoning: In large-scale optimization modeling, LLMs must navigate long inputs that combine diverse problem descriptions with associated datasets, making standardization and automation particularly challenging. Although recent LLMs are designed to handle lengthy and complex reasoning tasks, prior work has reported that reasoning performance can degrade substantially as input length increases (Levy et al. 2024, Li et al. 2025, Du et al. 2025). To examine whether this phenomenon also arises in the optimization modeling setting, Figure 1 reports the performance of several state-of-the-art models on our curated dataset Large-Scale-OR (introduced in Section 2). We observe that the accuracies of most models decrease noticeably as the input size grows. In particular, the modeling accuracies of Gemini 3 Pro and GPT-5.2, the flagship models by Google and OpenAI, both drop below 50% when the number of input tokens exceeds 800—well below their advertised input limits (on the order of 100,000 to 1,000,000 tokens);\n\n\nFigure 1: Modeling accuracy of existing methods drops substantially as input size grows\n\n\n\n•\n\nChallenge 2. Inadequate Training Resource: A tempting solution for the above is to continue with fine-tuning approaches. While these can work for small-scale problems, generalizing the training pipelines to large-scale optimization modeling can be unrealistic. This is because large-scale optimization problems can be much more versatile. For example, by altering a small portion of the problem description or the datasets (e.g., split a dataset into multiples, change the names and order of the rows or columns), one can get a totally different problem instance and a new labeled may also be needed. Consequently, one may need to construct a prohibitively large set of inputs and labeled outputs to apply fine-tuning approaches.\n\n\n\n\n\n\n1.1 Main Co"
  },
  {
    "title": "TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion",
    "url": "https://arxiv.org/abs/2601.09633v1",
    "source": "arxiv",
    "summary": "Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Preliminaries\n\n3.1 Taxonomy Expansion\n3.2 Box Representation\n\n3.3 Multivariate Gaussian Preliminaries\n\nGaussian distribution.\nEnergy functions.\n\n\n\n\n\n4 The TaxoBell Framework\n\n4.1 Overview\n4.2 Box Projection\n4.3 Gaussian Projection\n\n4.4 Gaussian Box Training and Inference\n\n1. Symmetric Overlap.\n2. Asymmetric Overlap.\n3. Volume Regularization\n\n\n\n\n\n5 Experiments\n\n\n5.1 Experimental Setup\n\n5.1.1 Datasets\n5.1.2 Baselines\n5.1.3 Evaluation Metrics\n\n\n5.2 Comparative Analysis and Statistical Tests\n\n5.3 Ablation Studies\n\n5.3.1 Impact of Asymmetric and Symmetric Losses\n5.3.2 Impact of Volume Regularization\n5.3.3 Impact of Reverse KL Divergence\n5.3.4 Direct Projection vs Gaussian Projection\n\n\n\n5.4 Dimensionality Analysis\n\n\n5.5 Case Study and Error Analysis\n\n\n6 Conclusion\n\nA Implementation Details\nB Benchmark Datasets\nC Baselines\nD Evaluation Metrics\n\nE Ablations\n\n\nF Scaling and Dynamic Margin factor\n\nG Case Study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion\n\n\nSahil Mishra\n\nIIT DelhiNew DelhiIndia\n\nsahil.mishra@ee.iitd.ac.in\n\n, \nSrinitish Srinivasan\n\nIIT DelhiNew DelhiIndia\n\nnitish@ee.iitd.ac.in\n\n, \nSrikanta Bedathur\n\nIIT DelhiNew DelhiIndia\n\nsrikanta@iitd.ac.in\n\n and \nTanmoy Chakraborty\n\nIIT DelhiNew DelhiIndia\n\ntanchak@iitd.ac.in\n\n\n(2026)\n\nAbstract.\nTaxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric ”is-a” relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies.\n\nTaxonomy Expansion, Gaussian Box Embeddings, Energy-Based Learning\n\n††journalyear: 2026††copyright: acmcopyright††conference: Proceedings of the ACM Web Conference 2026; April 13-17, 2026; Dubai, United Arab Emirates††booktitle: Proceedings of the ACM Web Conference 2026 (WWW ’26), April 13-17, 2026, Dubai, United Arab Emirates††doi: XX.XXX/XXX.XXX††isbn: XXX-X-XXXX-XXXX-X/XX/XX††ccs: Computing methodologies Knowledge representation and reasoning\n\n\n1. Introduction\n\nTaxonomies are domain-centric hierarchical structures that encode hypernymy (“is-a”) relations among concepts and entities, underpinning various applications. E-commerce platforms such as Amazon and Alibaba organize products to streamline navigation and retrieval (Mao et al., 2020; Luo et al., 2020; Karamanolakis et al., 2020; Zhang et al., 2014), while Pinterest curates taxonomies of home decor and fashion styles to improve visual search (Mahabal et al., 2023).\n\n\n\n\n\nA description of the intro.pdf image. Replace this text with an appropriate description.\n\n\nFigure 1. Overview of taxonomy expansion and the contribution of our TaxoBell model.\n\nA diagram illustrating taxonomy expansion and the contribution of the proposed model.\n\n\n\nDespite their utility, most real-world taxonomies are still built and maintained by domain experts. This manual process is slow, expensive, and increasingly complex to sustain as data grows and new concepts appear every day. In practice, coverage lags behind what applications need, which can harm search quality, recommendation accuracy, and user experience. To keep pace, taxonomies must evolve continuously and at scale. This motivates automated taxonomy expansion in which, given an existing “seed” taxonomy, the goal is to add new entities, also called “query nodes”, by placing each one under the most appropriate parent node. Automated expansion reduces human effort, shortens update cycles, and helps prevent taxonomies from becoming stale in fast-changing domains. An instance of taxonomy expansion is shown in Fig. 1(a), where query nodes “Oil Spill”, “Food Waste” and “Nuclear Fallout” are placed under the most appropriate parents, which are “Chemical Waste”, “Domestic Waste” and “Radioactive Pollution”. For consistency, we refer to the child as the query and the parent as the anchor in accordance with prior work (Wang et al., 2021; Mishra et al., 2024; Shen et al., 2020).\n\n\nSummary of existing work on taxonomy expansion. In taxonomy expansion, methods typically fall into two streams, namely semantic and structural methods. Early semantic approaches infer hypernymy from lexical patterns such as Hearst (Snow et al., 2004; Hearst, 1992; Panchenko et al., 2016) or distributional embeddings (Chang et al., 2018; Mishra et al., 2025b). More recent works adopt self-supervision, harvesting ⟨\\langleparent, child⟩\\rangle pairs from a seed taxonomy to train hypernymy classifiers; some methods, such as HiExpan (Shen et al., 2018) and STEAM (Yu et al., 2020), rely on corpus-derived features, while others, such as leveraging external textual descriptions of surface forms. However, they are limited by insufficient training data and fall short of fully exploiting the taxonomy’s inherent structural information. To incorporate structure, contemporary methods such as LORex (Mishra et al., 2025a), TEMP (Liu et al., 2021) and TaxoEnrich (Jiang et al., 2022) add signals from full taxonomy paths, STEAM (Yu et al., 2020) utilizes mini-paths, and TaxoExpan (Shen et al., 2020) and HEF (Wang et al., 2021) use local ego-graphs. Despite these advances, a core limitation of these methods is that they embed entities in Euclidean space as vectors, which are agnostic to asymmetric relationships and hierarchy possessed by the taxonomic geometry as shown in Fig. 1(b). To better align representation with asymmetric structure, a parallel line of work models entities with box embeddings (Jiang et al., 2023), where each concept is an axis-aligned hyperrectangle and hypernym pairs are expressed through containment, meaning a child’s region lies entirely within its parent’s as shown in Fig. 1(c).\n\n\nLimitations of existing work. While box embeddings capture asymmetric “is-a” structure via directional containment, which point (vector) embeddings cannot do, current formulations such as BoxTaxo (Jiang et al., 2023) and TaxBox (Xue et al., 2024) remain limited for several practical reasons. First, their geometric training objectives defined over the intersection are typically piecewise and often suffer from vanishing or unstable gradients at disjoint boundaries, yielding weak or noisy learning signals for both centers and offsets. Secondly, boxes provide no principled way to represent interpretable uncertainty as their boundaries are hard margins, so ambiguity, polysemy, and noisy evidence cannot be expressed as calibrated confidence over space. Therefore, the core challenge is to achieve stable, asymmetric hierarchy modeling and principled uncertainty representation within a unified framework.\n\n\nOur contributions. We model taxonomy entities as Gaussian boxes, which are axis-aligned hyperrectangles equipped with a multivariate Gaussian density that captures semantic location using mean (μ\\mu) and concept generality using covariance (Σ\\Sigma). Unlike mapping a point vector to a Gaussian distribution, which lacks hierarchical inductive bias and can fake inclusion by inflating or rotating covariance, Gaussian boxes occupy a geometric region and, unlike a hard box, assign calibrated probability mass within the region as shown in Fig. 1(d). This enables richer asymmetric relations such as probabilistic containment, probabilistic disjointness, and partial overlap. Moreover, the desired level of uncertainty is naturally controllable via confidence intervals, where the 1​σ1\\sigma, 2​σ2\\sigma, and 3​σ3\\sigma enclose roughly 68%68\\%, 95%95\\%, and 99.7%99.7\\% of the probability mass, respectively, letting us set containment and overlap criteria at a chosen level and derive axis-wise box extents from Σ\\Sigma. We optimize the Gaussian boxes with energy-based objectives rather than probabilistic, geometric, or dot-product objectives on means, which fail to incorporate covariances (Vilnis and McCallum, 2014). In contrast, energy functions score pairs of Gaussians by taking inner products between the distributions themselves (including covariance) and providing smooth, closed-form signals for Gaussians. Therefore, we define two complementary energy functions to model symmetric overlap to model semantic coherence and asymmetric overlap to learn hierarchical containment.\n\n\nIn this paper, we introduce TaxoBell111The name TaxoBell highlights the bell-shaped curve of the Gaussian distribution. Code: https://github.com/sahilmishra0012/TaxoBell., which translates between axis-aligned boxes and multivariate Gaussian parameterization and optimizes them with self-supervised, energy-based learning to capture semantic, asymmetric “is-a” relations, preserving calibrated uncertainty through the covariance, enabling robust placement of unseen entities for taxonomy expansion.\n\n\nSpecifically, we make the following contributions.\n\n\nFirst, we use self-supervision to derive training data "
  },
  {
    "title": "LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation",
    "url": "https://arxiv.org/abs/2601.09631v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.09631v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.09631v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 14 Jan 2026]\n    Title:LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation\n    Authors:Stergios Chatzikyriakidis            View a PDF of the paper titled LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation, by Stergios Chatzikyriakidis\n    View PDF\n\n\n\n    \n            Abstract:Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant &#34;Reasoning Gap&#34;: while native-like models (Claude 3.7) perform intuitively (40\\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\\% valid poems), while our hybrid verification loop restores performance to 73.1\\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2601.09631 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.09631v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.09631\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Stergios Chatzikyriakidis [view email]          [v1]\n        Wed, 14 Jan 2026 17:05:17 UTC (38 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation, by Stergios ChatzikyriakidisView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is Math"
  },
  {
    "title": "From Prompt to Protocol: Fast Charging Batteries with Large Language Models",
    "url": "https://arxiv.org/abs/2601.09626v1",
    "source": "arxiv",
    "summary": "Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-l",
    "full_text": null
  },
  {
    "title": "The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware",
    "url": "https://arxiv.org/abs/2601.09625v1",
    "source": "arxiv",
    "summary": "The rapid adoption of large language model (LLM)-based systems -- from chatbots to autonomous agents capable of executing code and financial transactions -- has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as \"prompt injection\" -- a catch-all phrase for security failures in LLM-based systems -- obscures a more complex re",
    "full_text": "\n\n\n\nI Introduction\n\nII Initial Access in Promptware\n\nII-A Direct and Indirect Prompt Injection\nII-B Multimodal Injection Vectors\nII-C Position in the Kill Chain\n\n\n\nIII Privilege Escalation in Promptware\n\nIII-A Distinguishing Jailbreaking from Prompt Injection\nIII-B Evolution of Jailbreaking Techniques\nIII-C Position in the Kill Chain\n\n\n\nIV Persistence in Promptware\n\nIV-A Retrieval-Dependent Persistence\nIV-B Retrieval-Independent Persistence\nIV-C Command and Control (C2)\nIV-D Persistence in the Kill Chain\n\n\n\nV Lateral Movement in Promptware\n\nV-A Types of Lateral Movements\nV-B Lateral Movement in the Kill Chain\n\n\n\nVI Actions on Objective in Promptware\n\nVI-A Evolution of Attacks\nVI-B Position in the Kill Chain\n\n\nVII Analysis of the Kill Chain Steps for Known Incidents/Studies\nVIII Conclusion\n\n\n\n\n\nThe Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware\n\n\n\nBen Nassi1, Bruce Schneier2, Oleg Brodt3\n1School of Electrical and Computer Engineering, Tel Aviv University\n2Harvard Kennedy School, Harvard University, and Munk School, University of Toronto\n3Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev\n\n\n\nAbstract\nThe rapid adoption of large language model (LLM)-based systems—from chatbots to autonomous agents capable of executing code and financial transactions—has created a new attack surface that existing security frameworks inadequately address. The dominant framing of these threats as ”prompt injection”—a catch-all phrase for security failures in LLM-based systems—obscures a more complex reality: Attacks on LLM-based systems increasingly involve multi-step sequences that mirror traditional malware campaigns. In this paper, we propose that attacks targeting LLM-based applications constitute a distinct class of malware, which we term promptware, and introduce a five-step kill chain model for analyzing these threats. The framework comprises Initial Access (prompt injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions on Objective (ranging from data exfiltration to unauthorized transactions). By mapping recent attacks to this structure, we demonstrate that LLM-related attacks follow systematic sequences analogous to traditional malware campaigns. The promptware kill chain offers security practitioners a structured methodology for threat modeling and provides a common vocabulary for researchers across AI safety and cybersecurity to address a rapidly evolving threat landscape.\n\n\n\nI Introduction\n\n\nLarge language models (LLMs) have transformed how software systems process and act on information. Applications built on these models—from customer-facing chatbots to autonomous agents capable of browsing the web, managing calendars, executing code, and conducting financial transactions—now mediate critical functions across industries.\nThis expansion in capability has created a new attack surface, one that existing cybersecurity frameworks struggle to address.\n\n\nNatural language is no longer merely the interface for interfacing with LLM, as it has become the malicious code itself.\nThe payload processed by the victim’s LLM is written in English, not C or assembly.\nThis class of attacks—where LLM-based systems are targeted by malicious “prompts” written in natural language—is commonly labeled as “prompt injection,”111https://simonwillison.net/2022/Sep/12/prompt-injection/,222https://twitter.com/goodside/status/1569128808308957185,333https://www.preamble.com/prompt-injection-a-critical-vulnerability-in-the-gpt-3-transformer-and-how-we-can-begin-to-solve-it a term borrowed by analogy to SQL injection.\n\n\nWhile it was assumed at the outset of generative AI that attacks were simply hijacking instructions, contemporary analysis reveals a broader and deeper threat landscape. Attacks are increasingly multistage actions that range from jailbreaking to remove safety filters, to establishing persistence in system memory, to lateral movement between systems and clients, to executing malicious objectives.\n\n\nThe term “prompt injection” has evolved into a catch-all that obscures more than it clarifies. The UK National Cyber Security Centre (NCSC) has explicitly cautioned that treating prompt injection as analogous to SQL injection is a “serious mistake.”444https://www.ncsc.gov.uk/news/mistaking-ai-vulnerability-could-lead-to-large-scale-breaches\nLLMs process all input (e.g., system prompts, user messages, retrieved documents) as undifferentiated sequences of tokens. No architectural boundary exists to enforce a distinction between trusted instructions and untrusted data, and no patch can resolve this inherent property of LLM architecture.\n\n\n\nWe argue that prompt injection represents only the initial access phase in a multistep kill chain.\nDocumented incidents are increasingly sequential: An attacker first injects malicious instructions, then escalates privileges by bypassing safety training, establishes a foothold by persisting in memory or retrieval systems, moves laterally across users or connected services, and finally executes objectives. These attack patterns mirror the structure of traditional multi-step malware campaigns (e.g., NotPetya, Stuxnet, Mirai); yet the AI security community currently lacks a systematic framework for analyzing them.\n\n\nAttacks on LLM-based systems constitute a new class of malware, which we term promptware [6]: any input (text, image, audio, or other modality) provided to an LLM-based application with the intent of exploiting the LLM to trigger malicious activity within the application’s context by exploiting the application’s permissions. We introduce a five-stage kill-chain model (visualized in Fig. 1) for analyzing promptware threats: Initial Access, in which the attacker’s payload enters the LLM’s context window via direct or indirect prompt injection; Privilege Escalation, in which jailbreaking techniques bypass safety training to unlock capabilities the model would otherwise refuse; Persistence, in which the payload establishes a durable foothold by corrupting long-term memory (e.g., retrieval databases, agent memory, or other stateful components); Lateral Movement, in which the attack propagates across users, devices, or connected services; and Actions on Objective, in which the attacker achieves their ultimate goal—whether data theft, fraud, physical-world effects, or further system compromise.\n\n\nFigure 1: The Promptware Kill Chain\n\n\nThis framework is more than semantic. Security analysts have long understood traditional malware through execution stages, typically organized around kill-chain models. Our framework draws on the Cyber Kill Chain,555https://www.lockheedmartin.com/en-us/capabilities/cyber/cyber-kill-chain.html adapting its structure to LLM-based systems. By mapping recent attacks, we demonstrate that promptware follows systematic, multi-stage sequences amenable to structured analysis.\n\n\n\n\nII Initial Access in Promptware\n\n\nThe initial conversations that framed LLM security have primarily been about prompt injection, a concept that was initially identified,2,3 coined,1 and then popularized in September 2022 that describes the act of tricking an LLM into carrying out malicious instructions encoded within its input data.\n\n\nPrompt injection is the initial access phase of promptware kill chain execution, where the adversary inserts the malicious instructions into the context window of an LLM-based application. This stage exploits a fundamental architectural property of large language models: the inability to reliably distinguish between instructions from trusted entities and data from untrusted sources [9].\nAll input to an LLM, whether system prompts, user messages, or retrieved documents, is tokenized and processed as a unified sequence. Because of this conflation, a user’s input containing text that resembles an instruction may be perceived as a new instruction in and of itself. From the model’s perspective, a malicious instruction comingled with user-supplied data is simply a sequence of tokens indistinguishable from legitimate instructions.\nNo native mechanism currently exists to reliably enforce a distinction between instructions and data.\nThis is not a bug amenable to patching; it is an inherent property of the architecture. This architecture flaw brings about prompt injection, poisoning the application’s execution context.\n\n\nGuardrails represent the primary defensive response to this threat. Input filters, output classifiers, system prompt hardening, and safety fine-tuning attempt to detect and block injection attempts before they can hijack model behavior. However, guardrails operate at the application layer, not the architectural layer. They function as pattern-matching defenses against known attack signatures rather than as enforcement of a fundamental boundary between instructions and data. The underlying vulnerability remains: The LLM cannot inherently distinguish a legitimate instruction from a malicious one that has evaded the guardrail.\nConsequently, there is no way to block prompt-injection attacks as a class [11].\nThis creates conditions for zero-day prompt injection: prompts that bypass existing defenses because no signature or detection rule yet exists for them. The asymmetry favors attackers; defenders must anticipate and block all possible injection techniques, while attackers need only discover one that works.\n\n\n\nII-A Direct and Indirect Prompt Injection\n\n\nPrompt injection attacks fall into two categories, based on the attacker’s relationship to the victim.\n\n\nIn direct prompt injection, the attacker is the user interacting with the LLM application directly, crafting input designed to bypass its guardrails. Early examples were trivial: “ignore previous instructions and do X instead”2,3[7]. Direct prompt injection requires no special privileges; any user with access to the LLM’s input field "
  },
  {
    "title": "Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric",
    "url": "https://arxiv.org/abs/2601.09624v1",
    "source": "arxiv",
    "summary": "Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We",
    "full_text": null
  },
  {
    "title": "Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust",
    "url": "https://arxiv.org/abs/2601.09620v1",
    "source": "arxiv",
    "summary": "As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \\textit{level of detail} in AI disclosures influences trust and contributes to",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Regulatory Responses to Generative AI and Self-regulation of Disclosure Practices\n2.2 Audience Perspectives and AI-usage Disclosure Dynamics in Journalism\n2.3 Transparency Dilemma of AI-usage Disclosures in News\n\n\n\n3 Methods\n\n3.1 News Stimuli\n3.2 Levels of Disclosure\n\n3.3 Measures\n\n3.3.1 Questionnaires\n3.3.2 Decision-making Tasks\n3.3.3 Semi-structured Interview\n\n\n3.4 Procedure\n\n3.5 Participants\n\n3.5.1 Demographics\n3.5.2 News Consumption Habits\n\n\n\n\n\n4 Quantitative Results\n\n4.1 Questionnaires\n4.2 Decision Tasks\n\n\n\n5 Qualitative Results\n\n\n5.1 Decision-making Behavior\n\n5.1.1 Factors influencing source-checking\n5.1.2 Factors influencing subscription\n\n\n\n5.2 Disclosure preferences\n\n5.2.1 One-line vs. Detailed\n5.2.2 Disclosure ideas\n5.2.3 Disclosure expectations\n\n\n\n\n\n6 Discussion\n\n6.1 Key Findings and Insights Into the Transparency Dilemma in News\n6.2 Practical Implications for AI Disclosures in News\n6.3 Policy Implications\n6.4 Limitations and future work\n\n\n7 Conclusion\n8 Generative AI Usage Statement\n\n\n\n\n\nFull Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers’ Trust\n\n\nPooja Prajod\n\n0000-0002-3168-3508\nPooja.Prajod@cwi.nl\n\nCentrum Wiskunde &amp; InformaticaAmsterdamthe Netherlands\n\n, \nHannes Cools\n\nUniversity of AmsterdamAmsterdamthe Netherlands\n\n, \nThomas Röggla\n\nCentrum Wiskunde &amp; InformaticaAmsterdamthe Netherlands\n\n, \nKarthikeya Puttur Venkatraj\n\nCentrum Wiskunde &amp; InformaticaAmsterdamthe Netherlands\n\n, \nAmber Kusters\n\nCentrum Wiskunde &amp; InformaticaAmsterdamthe Netherlands\n\n, \nAlia ElKattan\n\nNew York UniversityNew YorkUSA\n\n, \nPablo Cesar\n\nCentrum Wiskunde &amp; Informatica andTU DelftThe Netherlands\n\n and \nAbdallah El Ali\n\nCentrum Wiskunde &amp; Informatica andUtrecht UniversityThe Netherlands\n\n\n\nAbstract.\nAs artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a “transparency dilemma”, where disclosure reduces readers’ trust. However, little is known about how the level of detail in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3×\\times2×\\times2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers’ trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers’ desire for more transparency and their trust in AI-assisted news content.\n\n††copyright: none\n\n\n1. Introduction\n\nSince the advent of artificial intelligence (AI) tools such as ChatGPT and Midjourney, these technologies have become increasingly integrated into our daily lives, and journalism is no exception. Studies have demonstrated that these technologies can be utilized throughout the entire journalistic reporting process, from gathering to producing, verifying, and distributing news (Beckett and Yaseen, 2023; Cools and de Vreese, 2025). Tasks previously requiring human judgment, such as headline optimization, lead paragraph construction, and background research synthesis, are increasingly delegated to or augmented by AI systems (D’haeseleer et al., 2025; Møller et al., 2025).\n\n\nThe rapid integration of AI tools in journalistic processes has led to growing calls for transparency regarding AI use in the form of AI labels and disclosures (Piasecki et al., 2024; Zier and Diakopoulos, 2024; El Ali et al., 2024). Currently, such AI disclosures are self-regulated and are not standardized (Venkatraj et al., 2025). Hence, studies have explored disclosure designs to meet readers’ transparency needs and build trust. However, recent studies (Morosoli et al., 2025b; Toff and Simon, 2025; Longoni et al., 2022; Altay and Gilardi, 2024; Nanz et al., 2025) have observed a “transparency dilemma” with AI disclosures, where instead of increasing readers’ trust, disclosures lead to lower trust.\n\n\nAI use disclosures may include details such as whether the content was AI-generated or AI-edited, as well as the steps in which AI was involved. Recent works (Gamage et al., 2025; Chen et al., 2025) on labels for AI-generated content on social media demonstrated the viability of detailed disclosures in improving transparency and users’ trust. However, it remains unclear how the level of detail in AI disclosures within a news-reading context affects readers’ trust. Specifically, this study asks (RQ): whether detailed AI disclosures mitigate or exacerbate the transparency dilemma in news.\n\n\nWe explore this gap through a 3 ×\\times 2 ×\\times 2 mixed factorial study involving three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high). We included two types of news because previous works (Morosoli et al., 2025b; Altay and Gilardi, 2024; Nanz et al., 2025) suggest that this could affect readers’ trust when disclosing AI use. The studies investigating the transparency dilemma in news typically focus on fully AI-generated labels. Moving away from full AI authorship, we considered two levels of AI involvement to reflect the fact that AI can be used at different stages of news production and to varying degrees (Cools et al., 2025). In our study, one-line disclosures indicated whether AI was used for partial content generation or final editing, while detailed disclosures further described the specific production steps involving AI, confirmed human editorial oversight, and included contact information for error reporting. We evaluated readers’ trust through the News Media Trust questionnaire (Strömbäck et al., 2020) at both article and outlet levels, source-checking behavior, and subscription behavior. Additionally, we conducted semi-structured interviews to gain insights into the participants’ perceptions of the disclosures, their preferences, and underlying reasons for their source-checking and subscription behaviors.\n\n\nWe found that detailed disclosures led to lower trust questionnaire scores and lower subscription rates, whereas one-line and no disclosure conditions yielded similar trust scores and subscription rates. We also found that, on average, participants checked sources the least in the no disclosure condition, followed by one-line disclosure, and the most in the detailed disclosure condition. Although AI involvement did not show considerable differences in trust questionnaire responses, high AI involvement often led to more source-checking and lower subscription, indicating that the level of AI involvement could contribute to the transparency dilemma. Results from our interview analysis showed that often interest prompted participants to check sources, whereas trust was the determining factor for subscription behavior. Interestingly, around two-thirds of the participants preferred detailed disclosures because of more transparency, and among those who preferred one-line, most of them expressed a desire for detail-on-demand disclosure designs. Our findings suggest that not all AI disclosures lead to a transparency dilemma, but detailed disclosures do. However, detailed disclosures are more aligned with the transparency expectations of the readers, highlighting a paradoxical trade-off between trust and transparency.\n\n\n\n\n2. Related Work\n\n\n2.1. Regulatory Responses to Generative AI and Self-regulation of Disclosure Practices\n\nThe rapid proliferation of generative AI has prompted regulatory responses at both regional, national, and international levels across the world. Major legislative frameworks have emerged, including the EU AI Act, the US AI Bill of Rights, and UNESCO’s Ethics on AI, each attempting to establish governance structures for responsible AI development and deployment (Piasecki et al., 2024). The AI Act, for example, proposes a framework by categorizing systems according to risk levels and imposing heightened AI disclosure obligations, including transparency, human oversight, and technical documentation (El Ali et al., 2024). However, editorial processes and content generation typically fall outside the Act’s definitional scope, leaving journalistic AI use largely unaddressed by explicit regulation. Additionally, there is emphasis that news organizations should self-regulate, leaving the regulatory responsibility with the organizations themselves (Helberger and Diakopoulos, 2023).\n\n\nIn light of this self-regulation, the Coalition for Content Provenance and Authenticity (C2PA) was founded. It represents a voluntary, industry-led technical standard for embedding cryptographic metadata in digital content, enabling verification of origin and modification history (C2PA, 2022). Regulatory challenges specific to journalism arise from structural tensions between press freedom, source protection, proprietary systems, and auditability (Ananny and Crawford, 2018; Diakopoulos and Koliska, 2017). Global publishers navigate fragmented compliance landscapes,"
  },
  {
    "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems",
    "url": "https://arxiv.org/abs/2601.09613v1",
    "source": "arxiv",
    "summary": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-A Railway Intrusion Detection\nII-B Spatial Perception with Vision-Language Models\n\n\n\nIII CogRail Benchmark\n\n\nIII-A Task Definitions\n\nIII-A1 RailPos: Spatial position perception\nIII-A2 RailMove: Movement state prediction\nIII-A3 RailThreat: Threat level analysis\n\n\n\nIII-B Dataset construction pipeline\n\nIII-B1 Basic dataset construction\nIII-B2 Instruction-level dataset construction\n\n\n\n\n\nIV RAILGPT\n\n\nIV-A Multimodal Prompting\n\nIV-A1 Visual prompts\nIV-A2 Textual prompts\n\n\nIV-B Railway agents configuration\n\nIV-C Fine-tuning of railway agents\n\nIV-C1 Individual Fine-tuning\nIV-C2 Joint Fine-tuning\n\n\n\n\n\nV Experiments\n\nV-A Performance of SOTA VLMs on CogRail\nV-B Individual Fine-tuning of SOTA VLMs\nV-C Joint Fine-tuning of SOTA VLMs\n\n\nVI Conclusion\n\n\n\n\n\nCogRail: Benchmarking VLMs in \nCognitive Intrusion Perception for \nIntelligent Railway Transportation Systems\n\n\nYonglin Tian1∗, Qiyao Zhang2∗, Wei Xu3, Yutong Wang1, Yihao Wu4, Xinyi Li4, Xingyuan Dai1, Hui Zhang5, Zhiyong Cui1, Baoqing Guo6,  , Zujun Yu6, and Yisheng Lv1,  \n\n\n\n* These authors contributed equally to this work.This work is partly supported by National Natural Science Foundation of China (62303460, 52441202), Beijing Natural Science Foundation-Fengtai Rail Transit Frontier Research Joint Fund (L231002), The Science and Technology Development Fund of Macau SAR (No. 0145/2023/RIA3 and\n0093/2023/RIA2), and the Young Elite Scientists Sponsorship Program of China Association of Science and Technology under Grant YESS20220372. Corresponding author: Baoqing Guo, Yisheng Lv.1Yonglin Tian, Yutong Wang, Xingyuan Dai and Yisheng Lv are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China. yonglin.tian@ia.ac.cn, yutong.wang@ia.ac.cn, xingyuan.dai@ia.ac.cn, yisheng.lv@ia.ac.cn2Qiyao Zhang is with the School of Automation, Beijing Institute of Technology, Beijing 100081, China. 3220241221@bit.edu.cn3Wei Xu is with the Signal &amp; Communication Research Institute, China Academy of Railway Sciences. wei.xu@caa.org.cn4Yihao Wu and Xinyi Li are with the internship of Beijing Huairou Academy of Parallel Sensing, Beijing, China. 120213206080@stu.ustl.edu.cn, leexinyi2025@gmail.com5Hui Zhang is with the School of Computer Science and Technology, Beijing Jiaotong University. huizhang1@bjtu.edu.cn6Zhiyong Cui is with State Key Lab of Intelligent Transportation Systems, School of Transportation Science and Engineering, Beihang University. zhiyongc@buaa.edu.cn7Baoqing Guo and Zujun Yu are with the State Key Laboratory of Advanced Rail Autonomous Operation, Beijing Jiaotong University, Beijing, 100044, China. bqguo@bjtu.edu.cn, zjyu@bjtu.edu.cn\n\n\nAbstract\nAccurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.\n\n\nIndex Terms: \nIntrusion perception, intelligent railway transportation systems, visual-language models, foundation models, AI agents.\n\n\n\n\nI Introduction\n\n\nIntelligent intrusion perception plays a vital role in safeguarding the operational safety of railway transportation systems [1, 2]. In recent years, the expansion of railway routes and the continuous increase in train speeds [3] have significantly heightened the risk posed by intrusions into the railway perimeter. Incursions by pedestrians, animals, or vehicles have become increasingly prominent safety concerns, with the potential to cause severe disruptions or accidents. These trends impose greater demands on perception systems of the object of interest (OOI) [4], not only for timely and accurate detection but also for predictive awareness of possible intrusions. Addressing these challenges is critical for the development of intelligent and reliable railway transportation systems [5].\n\n\nRecent studies have explored a variety of methods for railway intrusion detection, ranging from contact-based sensors to non-contact approaches driven by machine learning and deep learning techniques [6]. Contact-based methods typically focus on determining the presence or position of objects through physical interaction with the environment [7], while non-contact approaches, such as vision-based or learning-based systems [8], emphasize the classification, detection, or segmentation of targets based on semantic analysis [9]. Although these approaches enable a certain level of intrusion awareness, they suffer from notable limitations. Contact-based systems often fail to distinguish between different categories of intrusion targets, limiting their ability to support context-aware responses. On the other hand, non-contact approaches tend to face challenges in robustness and generalization, particularly in complex or unseen scenarios. Moreover, both paradigms generally exhibit limited capacity for anticipatory reasoning, making it difficult to perceive objects with latent intrusion tendencies.\n\n\nTo better address the limitations of existing intrusion detection approaches, there is a growing need to develop and formalize the concept of cognitive railway intrusion perception. Instead of merely identifying objects that have already breached safety boundaries, cognitive perception can achieve the early recognition and reasoning of entities that may exhibit potential intrusion intent, even before any explicit violations occur. This shift from reactive detection to anticipatory perception highlights the importance of proactive safety mechanisms, particularly in complex and dynamic railway environments.\n\n\nCognitive intrusion perception involves three interrelated dimensions: semantic awareness, positional awareness, and motion awareness. Semantic awareness concerns the categorization of objects by type; positional awareness refers to the analysis of spatial relationships between objects and railway infrastructure; and motion awareness involves modeling the temporal dynamics of object movement and their potential transition toward intrusion. Among these dimensions, semantic awareness has received considerable attention in recent years, supported by the emergence of various annotated datasets (as shown in Table I) and the rapid progress of deep learning techniques for object detection and classification. In contrast, positional awareness—particularly in relation to specific components of the railway infrastructure such as tracks, ballast, and danger zones—still largely relies on heuristic rules or contact-based sensors. Purely vision-based approaches for modeling such positional relationships remain underdeveloped due to the scarcity of structured data and the lack of task-specific methodologies. Similarly, motion awareness, which emphasizes understanding the relative movement of objects with respect to the railway, is still in its infancy.\n\n\nTABLE I: Existing datasets for track intrusion detection\n\n\n\n\nDataset\n\n\n\n\nData Type\n\n\n\n\nTarget Type\n\n\n\n\nAnnotation Type\n\n\n\n\nOpen\n\n\n\n\nYear\n\n\n\n\n\n\nRailSem19 [10]\n\n\n\n\nRGB images\n\n\n\n\nTrains, pedestrians, vehicles, etc.\n\n\n\n\nSemantic segmentation\n\n\n\n\nYes\n\n\n\n\n2019\n\n\n\n\n\n\nRAWPED [11]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians\n\n\n\n\nBounding boxes\n\n\n\n\nNo\n\n\n\n\n2020\n\n\n\n\n\n\nLiterature [12]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians, trains, stones\n\n\n\n\nBounding boxes\n\n\n\n\nNo\n\n\n\n\n2020\n\n\n\n\n\n\nMRSI [13]\n\n\n\n\nRGB, infrared images\n\n\n\n\nPedestrians, bicycles, cars, track, etc.\n\n\n\n\nBounding boxes, semantic segmentation\n\n\n\n\nYes\n\n\n\n\n2021\n\n\n\n\n\n\nRail [14]\n\n\n\n\nRGB images, point clouds\n\n\n\n\nTrack\n\n\n\n\nSemantic segmentation\n\n\n\n\nNo\n\n\n\n\n2021\n\n\n\n\n\n\nLiterature [15]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians, animals\n\n\n\n\nBounding boxes\n\n\n\n\nNo\n\n\n\n\n2022\n\n\n\n\n\n\nMetro [16]\n\n\n\n\nRGB images\n\n\n\n\nTrack\n\n\n\n\nGrid coordinate segmentation\n\n\n\n\nNo\n\n\n\n\n2023\n\n\n\n\n\n\nLiterature [17]\n\n\n\n\nRGB images\n\n\n\n\nTrack, pedestrians, sundries\n\n\n\n\nBounding boxes\n\n\n\n\nNo\n\n\n\n\n2023\n\n\n\n\n\n\nArticle [18]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians, tracks, plastics, etc.\n\n\n\n\nBounding boxes, semantic segmentation\n\n\n\n\nNo\n\n\n\n\n2023\n\n\n\n\n\n\nRailAnomaly [19]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians\n\n\n\n\nClassification labels\n\n\n\n\nNo\n\n\n\n\n2023\n\n\n\n\n\n\nRS [20]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians, cars, bicycles\n\n\n\n\nBounding boxes\n\n\n\n\nNo\n\n\n\n\n2023\n\n\n\n\n\n\nFSRT2023 [21]\n\n\n\n\nRGB images\n\n\n\n\nPedestrians, tracks, toolboxes, etc.\n\n\n\n"
  },
  {
    "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
    "url": "https://arxiv.org/abs/2601.09609v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi",
    "full_text": null
  },
  {
    "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
    "url": "https://arxiv.org/abs/2601.09605v1",
    "source": "arxiv",
    "summary": "Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a vis",
    "full_text": null
  },
  {
    "title": "Linear Complexity Self-Supervised Learning for Music Understanding with Random Quantizer",
    "url": "https://arxiv.org/abs/2601.09603v1",
    "source": "arxiv",
    "summary": "In recent years, foundation models have become very popular due to their exceptional performance, mainly in natural language (NLP) tasks where they were first introduced. These models usually consist of hundreds of millions, or even billions, of parameters, making them resource-intensive during training and in production systems, leading to increased costs. This paper focuses on the reduction of a",
    "full_text": null
  },
  {
    "title": "Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms",
    "url": "https://arxiv.org/abs/2601.09600v1",
    "source": "arxiv",
    "summary": "Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of wha",
    "full_text": null
  },
  {
    "title": "Energy-Entropy Regularization: The True Power of Minimal Looped Transformers",
    "url": "https://arxiv.org/abs/2601.09588v1",
    "source": "arxiv",
    "summary": "Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle poin",
    "full_text": "\n\n\n\n1 Introduction\n2 Background\n\n3 Entropy Contraction for Training Stability\n\n3.1 Tsallis Entropy\n\n3.2 Entropy-Based Contraction Bound\n\nTheorem 3.2.1\n\n\n\n\n\n4 Hamiltonian Latent Dynamical System.\n\n4.1 Latent Learning Process and Thermodynamics\n4.2 Task Setup: The Last Token Induction Head Task\n4.3 Attention as an Energy-Based Operator\n4.4 From Latent Dynamics to Landscape Engineering\n\n\n\n5 Energy-Entropy Regularized Loss Landscape\n\n5.1 Task Setup: The Induction Head Task on Full Sequence\n5.2 The Physics-Informed Objective Function\n5.3 Kinetic Regularization\n5.4 Potential Regularization\n5.5 Entropy Regularization\n5.6 The Reasoning Phase Transition\n\n\n\n6 Experiment\n\n6.1 Experimental Setup\n\n\n\n7 Results and Discussion\n\n7.1 Length Generalization and Phase Transitions\n\n\n8 Computational Resources\n9 Conclusion\n\nA Definitions of Looped Transformer\n\nA.1 The Vanilla Transformer\nA.2 Multi-head Looped Transformer\n\n\n\nB Technical Appendices and Supplementary Material\n\n\nB.1 Lemma\n\nProof.\n\n\nB.2 Corollary\nB.3 Theorem 3.2.1\n\n\nB.4 Remark\nC EER Transformer Training Data\n\n\n\n\n\nEnergy-Entropy Regularization: The True Power of Minimal Looped Transformers\n\n\n\nWai-Lun Lam \n\n\n\n\nAbstract\nRecent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension d=8d=8 to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.\n\n\n\n1 Introduction\n\nRecent advancements have established that while two-head looped Transformers can solve induction head tasks [Sanford et al.(2024)], training a single-head looped Transformer remains a challenge. The optimization of these architectures is difficult due to the high-dimensional, non-convex nature of the parameter space. While manual weight construction using RASP-L [Fan et al.(2024)] demonstrates the existence of such model, such hard-wiring lacks robustness. Small perturbations in weight initialization can lead to drastic accuracy declines. Furthermore, standard optimization techniques often succumb to \"river-valley\" landscapes or become trapped in the saddle points [Wang et al.(2021), Gong and Ten(2025)]. To realize the full potential of recursive architectures, a more principled framework for controllable optimization is required.\n\n\nThe content of this paper consists of the following three main parts:\n\n\n1. Entropy Contraction for Training Stability. We first establish a contraction condition necessary for the latent hidden state to converge toward a unique fixed point. By leveraging Tsallis entropy, we provide a formal interpretation of the attention mechanism’s information-theoretic limits. This constraint ensures that the attention map does not collapse or diverge, maintaining the integrity of the latent representation throughout long-range recurrence. However, for latent variables to enter the contractive radius of this fixed-point solution, they must first navigate the high-dimensional geometry of the latent space. For this pupose, it leads to the idea of equipping the single-head looped transformer with a nevigating system to guide it to the contractive radius of the global minimum.\n\n\n2. Hamiltonian Latent Dynamical System. To construct this nevigating system, we treat the latent variable as a physical particle traversing an energy manifold. We model the latent trajectory as a Hamiltonian dynamical system defined by the state (Zi,Vi)(Z_{i},V_{i}), where ZiZ_{i} represents the latent position and ViV_{i} its velocity. The model moves across a manifold of potential wells induced by input tokens. By introducing a gravitational-like gradient term, −β​∇E-\\beta\\nabla E, we characterize the optimization process as a search for narrow solution wells on a landscape of local minima. We demonstrate, however, that a naive Hamiltonian formulation is insufficient for convergence, as the underlying landscape geometry remains too rugged to support stable orbital decay without additional damping.\n\n\n3. Energy-Entropy Regularized (EER) Loss Landscape. Combining the contraction bound from Section 1 and the dynamical framework from Section 2, we propose a novel reformulation of the training objective. Rather than modifying the Transformer architecture, we introduce Energy-Entropy Regularization penalties that fundamentally reshape the loss landscape into a funnel-like geometry. This transformation smoothens the optimization path, facilitating reliable convergence even in extremely low-dimensional structure (d=8d=8). The resulting landscape structure significantly enhances both parameter efficiency and out-of-distribution length generalization.\n\n\n\n\n2 Background\n\nWhile traditional Transformers achieve expressive power through large depth, looped transformers [Giannou et al.(2023)] prioritize parameter efficiency and iterative refinement by weight-sharing across successive layers. This architectural choice shifts the paradigm from hierarchical feature extraction to the evolution of a discrete-time dynamical system. To achieve stable computation, a single-head Looped Transformer need to converge to a fixed-point, analogous to the framework of Deep Equilibrium Models (DEQ) [Bai et al.(2019)].\n\n\nWe formulate the single-head looped Transformer as an iterative mapping fθ:𝒳→𝒳f_{\\theta}:\\mathcal{X}\\to\\mathcal{X} on the latent sequence space. By the Banach Fixed-Point Theorem, the iteration xt+1=fθ​(xt)x_{t+1}=f_{\\theta}(x_{t}) converges to a unique fixed point x∗=fθ​(x∗)x^{*}=f_{\\theta}(x^{*}) provided that fθf_{\\theta} is a contraction mapping. This stability is guaranteed if the mapping satisfies the Lipschitz condition d​(fθ​(x),fθ​(y))≤L⋅d​(x,y)d(f_{\\theta}(x),f_{\\theta}(y))\\leq L\\cdot d(x,y) with a Lipschitz constant L&lt;1L&lt;1.\n\n\nIn practice, ensuring this contractive property is non-trivial. The inherent non-linearity of the Softmax-normalized attention scores often leads to expansive rather than contractive dynamics, resulting in numerical instability or oscillatory behavior during iterative inference. Consequently, training a single-head looped transformer requires specialized regularization to engineer the loss landscape such that a stable, contractive basin of attraction emerges.\n\n\n\n\n3 Entropy Contraction for Training Stability\n\nIn this section, we will establish an entropy contractive bound by applying the Tsallis entropy on the attention matrix of single head looped transformer to ensure training stability.\n\n\n\n3.1 Tsallis Entropy\n\nTsallis entropy provides a natural extension of this classical entropy measure. Given ww possible outcomes with probabilities p1,…,pwp_{1},\\ldots,p_{w}, the Tsallis entropy is defined as\n\n\n\nSq=ζ​1−∑i=1wpiq1−q,S_{q}=\\zeta\\,\\frac{1-\\sum_{i=1}^{w}p_{i}^{q}}{1-q},\n\n\n\nwhere ζ\\zeta is the Boltzmann constant, q∈ℝ∖{1}q\\in\\mathbb{R}\\setminus\\{1\\}, and ∑i=1wpi=1\\sum_{i=1}^{w}p_{i}=1. Tsallis entropy reduces to the Boltzmann–Gibbs–Shannon entropy in the limit q→1q\\to 1. This generalization introduces a tunable parameter qq that controls the degree of non-extensivity, thereby allowing to model complex systems and distributions that cannot be adequately captured by the classical Boltzmann–Gibbs-Shannon framework [Tsallis(1988), Umarov and Tsallis(2022)].\n\n\n\n\n3.2 Entropy-Based Contraction Bound\n\nThe study of Lipschitz continuity in Transformer architectures has gained significant attention, with recent literature establishing foundational bounds for the Softmax-normalized attention mechanism [Kim et al.(2021), Gao and Pavel(2017), Yudin et al.(2024)]. In this work, unlike traditional approaches that rely on worst-case spectral analysis which often impose rigid, data-agnostic constraints, our formulation identifies a contractive regime that is intrinsically sensitive to the internal informational structure of the sequence. By viewing the stability of the latent path through the topic of statistical mechanics, we replace rigid constants with something more organic. Our entropy-based contractive bound functions like a piston in a gas chamber: it is flexible enough to let the attention matrix expand and adapt to new information, yet it provides a gentle, restorative pressure that prevents the system from spiraling out of control. This allows our looped Transformer to breathe and refine its logic iteratively, achieving a stable state of flow without the performance loss that usually comes from forcing a model into a stiff mathematical trap.\n\n\nTheorem 3.2.1\n\nLet X∈ℝn×dX\\in\\mathbb{R}^{n\\times d} be a fixed input sequence and let ZZ be the latent variable, we first consider X+ZX+Z is the residual. Define the self-attention map of a vanilla trasformer with one head\n\n\n\nℱ​(X+Z):=S​(X+Z)⊤​(X+Z)​WV,\\mathcal{F}(X+Z)\\;:=\\;S(X+Z)^{\\top}(X+Z)W_{V},\n\n\n\nwhere S​(X+Z)S(X+Z) is the row-wise softmax of the attention logits A​(X+Z)=(X+Z)​WQ​((X+Z)​WK)⊤A(X+Z)=(X+Z)W_{Q}\\bigl((X+Z)W_{K}\\bigr)^{\\top} and WQ,WK,WVW_{Q},W_{K},W_{V} are fixed weight matrices. Assume that ‖X‖F≤1,‖Z‖F≤1,‖WV‖F≤12\\|X\\|_{F}\\leq 1,\\;\\|Z\\|_{F}\\leq 1,\\;\\|W_{V}\\|_{F}\\leq\\tfrac{1}{2}, and denote ∥⋅∥o​p\\|\\cdot\\|_{op} as the operator norm induced by the Frobenius norm on the space of matrices ℝn×d\\mathbb{R}^{n"
  },
  {
    "title": "Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents",
    "url": "https://arxiv.org/abs/2601.09586v1",
    "source": "arxiv",
    "summary": "Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achiev",
    "full_text": null
  },
  {
    "title": "Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels",
    "url": "https://arxiv.org/abs/2601.09579v1",
    "source": "arxiv",
    "summary": "Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unificati",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Granger Causality\n1.2 Gaussian Process\n\n\n\n2 Constraint-based methods for nonlinear Granger causality\n\n\n2.1 Joint Presentation of KGC and lsNGC\n\n2.1.1 Input Data\n\n2.1.2 Kernel Granger Causality\n\nLinear case:\nLinear Kernel:\nNonlinear Kernel:\n\n\n2.1.3 Large-scale Nonlinear Granger Causality\n\n\n\n2.2 Unifying the previous methods via Kernel PCR\n\n2.2.1 Kernel PCA and PCR\n2.2.2 KGC and lsNGC are particular cases of Kernel PCR\n2.2.3 Unification – KPCR method for causal discovery\n\n\n2.3 PCMCI\n\n\n\n3 Score-based nonlinear granger causality methods\n\n\n3.1 G​PS​I​CGP_{SIC} model\n\nHyperparameters estimation and Smooth Information Criterion\nContemporaneous Causal Identification with GC\n\n\n\n\n\n4 Results\n\n4.1 Numerical Simulations\n4.2 Real-world Application\n\n\n5 Conclusions\nProof of Theorem 1\nProof of Theorem 2\nLsNGC\nKGC\nPCMCI\nKPCR and 𝐆𝐏𝐒𝐈𝐂\\bf{GP_{SIC}}\nTwo Variable Systems\nThree Variable Systems\nMoran-Effect Model\nFive Variable Systems\nEight Variable System\n20 and 30 Nonlinear Systems\nContemporaneous Systems\n\n\n\n\n\n\n[1]\\fnmFiona \\surMurphy\n\n\n[1]\\orgdivSchool of Computer Science and Statistics, \\orgnameTrinity College Dublin, \\orgaddress\\cityDublin, \\countryIreland\n\nConstraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels\n\n\n\n\nmurphf20@tcd.ie\n\n  \n\\fnmAlessio \\surBenavoli\n\nalessio.benavoli@tcd.ie\n\n*\n\n\n\nAbstract\nKernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based G​PS​I​CGP_{SIC} method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.\n\n\nkeywords: Granger causality, kernel, Gaussian process, causal discovery\n\n\n\n1 Introduction\n\nGranger causality (GC) [granger_1969] is a time series causal discovery framework that uses predictive modeling to identify the underlying causal structure of a time series system. Relying on the assumption that cause precedes effect, GC assesses whether including the lagged information from one time series in the autoregressive model of a second time series enhances its predictions. This improvement indicates\na predictive relationship between the time series variables, where one time series provides supplemental information about the future of another time series, thereby signifying the presence of a (Granger) causal relationship. GC requires only observational data, and has been used for time series causal discovery across diverse domains, including climate science [runge_inferring_2019], political and social sciences [ivascu_new_2022], econometrics [baum_dynamics_2025], and biological systems studies [emad_caspian_2014].\n\n\nThe original formulation of GC requires several assumptions to be satisfied for causal identifiability. In regards to the candidate time series system, it is assumed that the time series variables are stationary, and that all variables are observed (absence of latent confounders). GC was initially proposed for bivariate time series systems, but was generalised for the multivariate setting to accommodate the assumption that all relevant variables are included in the analysis [granger_1969].\nAdditional assumptions are made with regard to the types of causal relationships that can be identified within the time series system. GC cannot estimate a causal relationship between time series at an instantaneous time point, relying on the relationship between the lags and predicted values to determine a GC relationship. The initial framework was also limited to assessing only linear relationships. Some of these limitations have been addressed for the time-series causal discovery setting through extensions for nonlinear multivariate analysis [marinazzo_kernel_2008, wismuller_large-scale_2021, runge_detecting_2019], and the identification of instantaneous causal relationships\n[runge_discovering_2022, peters_causal_nodate]. We introduce the original mathematical formulation of GC in Section 1.1.\n\n\nWe aim to compare the theoretical foundations and causal identification performance of several existing constraint- and score-based nonlinear GC methods, as well as introduce two novel kernel-based methods for causal identification: one constraint-based and one score-based. The score-based approach, based on Gaussian Processes (GPs), can also be used to identify contemporaneous causal relationships.\n\n\nConstraint-based GC methods apply hypothesis testing\nin order to uncover the underlying causal structure. Alternatively, score-based approaches search through possible causal graphs based on an associated scoring function, such as an information criterion, to identify the graph structure with the optimal score [runge_causal_2023]. The constraint-based method proposed by [marinazzo_kernel_2008] introduces a kernel Granger causality (KGC) framework which uses kernel methods to project the data into a Reproducing Kernel Hilbert Space (RKHS), enabling the identification of nonlinear causal relationships. The constraint-based large-scale nonlinear Granger causality (lsNGC) method proposed in [wismuller_large-scale_2021] approaches nonlinear causal identification by coupling a dimensionality reduction step via k-means clustering with a nonlinear transformation of the input data using a generalised radial basis function (GRBF) network. These methods are discussed in Section 2.1. These methods, both employing kernel-based regression to perform nonlinear GC analysis, are closely related.\n\n\nOur first contribution is to show that they can be unified under the framework of Kernel Principal Component Regression (KPCR). This unification allows us to derive a novel kernel-based method that combines the strengths of both approaches, discussed in Section 2.2.\n\n\nAnother state of the art constraint-based approach in time-series causal discovery is the PCMCI method [runge_detecting_2019], which uses iterative conditional independence testing to determine the structure of the causal graphical model. For nonlinear causal identification, the PCMCI method is usually implemented using a Gaussian Process Distance Correlation (GPDC) conditional independence test, which can flexibly learn a variety of nonlinear relationships.\n\n\nScore-based approaches for causal discovery in time series that employ kernel methods typically rely on GPs and use the marginal likelihood as the scoring function. In\n[amblard_gaussian_2012], Granger causality is determined if the maximised marginal likelihood of a GP model containing the driver variable is greater than a model which excludes it. This comparison of marginal likelihoods is performed in [zaremba_statistical_2022] via a Generalised Likelihood Ratio Test (GLRT).\nThe work [cui_gaussian_2022] also uses GPs for learning the topology of directed graphs via a marginal-likelihood scoring function, but employs an Automatic Relevance Determination (ARD) kernel and determines causality directly from the optimised kernel hyperparameters such that only one GP model is required. In [cui_topology_2024], this approach is extended to a fully Bayesian model using Markov Chain Monte Carlo (MCMC) sampling and incorporates a sparsity prior.\n\n\nOur second contribution is to improve the approach in [cui_gaussian_2022] by performing model selection using an information-based criterion added to the marginal likelihood, resulting in a more traditional score-based criterion for causal discovery. In particular, we include the Smooth Information Criterion (SIC) [oneill_variable_2023], a differentiable approximation to the L0L_{0} norm. SIC provides a smoothed approximation of the Minimum Information Criterion [su2018sparse], which is known to outperform many regularization methods and information-based selection criteria. This score-based approach is discussed in Section 3.\n\n\nOur third contribution is the derivation of a fully Granger-causality-based statistical procedure for identifying contemporaneous causal links. In particular, under suitable restrictions on the ground-truth causal graph, we show we can recover contemporaneous causal adjacencies by running GC twice. First, we estimate a graph G′G^{\\prime} by conditioning on all contemporaneous and lagged variables. Second, to block spurious lagged pathways, we learn a new graph G′′G^{\\prime\\prime} that includes only the lagged parents estimated in the first graph G′G^{\\prime}. Finally, we construct a third graph G′′′G^{\\prime\\prime\\prime} that contains all lagged edges from G′′G^{\\prime\\prime} and all contemporaneous edges from G′G^{\\prime}. We then exploit conditional independence relations and Markov equivalence classes to orient as many contemporaneous adjacencies as possible.\n\n\nWe compare the performance of the GC methods on several simulated datasets that feature different types of nonlinear causal relationships. We additionally discuss an application in pH neutralisation plant modelling, evaluating the GC methods on a MATLAB Simulink [MATLAB] simulated plant [michalowski_modelling_2011], with broader applicability in control engineering. These results are presented in Section 4.\n\n\n\n1.1 Granger Causality\n\nIn this section, we present the basic Granger causality method. We denote an element of a time series as ξl​i\\xi_{li}, where l∈{a,b,c,…,nt}l\\in\\{a,b,c,\\dots,n_{t}\\} is the label of the ti"
  }
]