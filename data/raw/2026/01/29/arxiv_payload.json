[
  {
    "title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
    "url": "https://arxiv.org/abs/2601.20861v1",
    "source": "arxiv",
    "summary": "One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to tra",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Experiments\n\n3.1 ES vs GRPO Comparison\n3.2 ES and Catastrophic Forgetting\n\n3.3 Dissecting ES Updates: Norm and Sparsity\n\nNorm.\nSparsity.\n\n\n\n\n4 Conclusion\n\nA Appendix\n\n\nA.1 Algorithmic Overview and Analogy Between ES and GRPO\n\nA.1.1 ES Algorithm Overview\nA.1.2 ES Algorithm Overview\nA.1.3 Analogy Between ES Population Size and GRPO Rollout Count\n\n\n\nA.2 Implementation Details\n\nA.2.1 GRPO\nA.2.2 ES\nA.2.3 Reward functions\n\n\nA.3 Hyperparameter Values\n\nA.4 Additional Experiments\n\nA.4.1 Catastrophic Forgetting and KL\n\n\n\n\n\n\n\n\n\nEvolutionary Strategies lead to Catastrophic Forgetting in LLMs\n\n\nImmanuel Abdi, Akshat Gupta11footnotemark: 1\nMicah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli\nUC Berkeley \n{immanuelazn, akshat.gupta}@berkeley.edu\nEqual contribution.\n\n\nAbstract\nOne of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger ‚Ñì2\\ell_{2} norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.\n\n\n\nEvolutionary Strategies lead to Catastrophic Forgetting in LLMs\n\n\n\n\nImmanuel Abdi‚Ä†‚Ä†thanks: Equal contribution., Akshat Gupta11footnotemark: 1\n\nMicah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli\n\nUC Berkeley\n\n{immanuelazn, akshat.gupta}@berkeley.edu\n\n\n\n\n\n\n1 Introduction\n\nDespite rapid advances in AI with transformer-based LLMs (Vaswani et al., 2017; Brown et al., 2020; DeepSeek-AI et al., 2024), most state-of-the-art systems remain static after training and lack the ability to learn continually during deployment. In many real-world settings, models need to adapt to new tasks, user preferences, or data distributions to perform optimally. While modern chatbots like ChatGPT do this by taking notes in the form of user memory OpenAI (2024) and use in-context learning Brown et al. (2020) to incorporate this information, we currently lack solutions that can achieve this by modifying the model weights during deployment. One of the reasons that makes this challenging is that current post-training and adaptation methods for LLMs are exclusively gradient-based, including approaches such as SFT Wei et al. (2022), RLHF (Ouyang et al., 2022), DPO Rafailov et al. (2024), and GRPO (Shao et al., 2024). While effective, these methods require storing gradients, optimizer states, or intermediate activations, causing substantial memory overhead.\n\n\nEvolutionary Strategies (ES) Qiu et al. (2025); Korotyshova et al. (2025) have recently re-emerged as a gradient-free alternative for optimizing LLMs. By estimating updates through population-level perturbations rather than backpropagation, ES avoids explicit gradient storage and can significantly reduce memory requirements during deployment. Qiu et al. (2025) have shown that ES achieves comparable performance to GRPO on the Countdown task Pan (2026), presenting ES as a viable candidate for continual learning in LLMs. However, a more comprehensive analysis on task generalization was missing in their work. More importantly from the perspective of continual learning, Qiu et al. (2025) do not evaluate the extent to which ES preserves existing capabilities while learning new tasks.\n\n\nIn this work, we present a comprehensive empirical analysis of ES for fine-tuning LLMs, with a focus on continual learning and forgetting. We compare ES against GRPO on multiple math and reasoning benchmarks and evaluate forgetting curves over many update steps. Our results confirm that ES is able to reach performance levels comparable to GRPO on a large suite of tasks; however, contrary to results reported in Qiu et al. (2025), we find that GRPO still dominates ES marginally on almost all tasks. Additionally, we show that training LLMs using ES leads to significant model degradation and forgetting of existing abilities when compared to GRPO. To better understand this behavior, we analyze the structure of parameter updates produced by ES and compare them to those obtained using GRPO. We find that ES updates are significantly less sparse and exhibit much larger ‚Ñì2\\ell_{2} norms, leading to more global parameter changes that interfere with previously learned capabilities.\n\n\nOur findings highlight that although ES presents a tempting memory-efficient and gradient-free alternative to inference-time model adaptation, it is also accompanied by ‚Äúcatastrophic‚Äù forgetting Kirkpatrick et al. (2017); Gupta et al. (2024) of prior abilities of the model. We hope these results can inspire future advancements in gradient-free algorithms with continual learning and catastrophic forgetting at the forefront of thought. We also release our codebase111Our codeabase can be found here - https://github.com/akshat57/es-catastrophic and models222Our models can be found here - https://huggingface.co/collections/immanuelabdi/es-at-scale-lead-to-catastrophic-forgetting for reference.\n\n\nTo summarize, our work makes the following contributions:\n\n\n\n\n1.\n\nWe show that ES is able to reach comparable performance to GRPO on several math and reasoning benchmarks with similar number of update steps.\n\n\n\n2.\n\nWe show that training models using ES causes significant model degradation when compared to GRPO, leading to catastrophic fortgetting of prior abilities.\n\n\n\n3.\n\nFinally, we also explore the reason behind catastrophic forgetting in ES and show that this happens because model updates using ES are much less sparse when compared to GRPO with significantly larger ‚Ñì2\\ell_{2} norms.\n\n\n\n\n\n\n\n2 Related Work\n\nEvolution Strategies are a class of algorithms that search for solutions to first-order optimization problems by randomly modifying population members to find better performing members (Rechenberg, 1989; Schwefel, 1977; Beyer, 1995). Although implementations such as CMA-ES Hansen and Ostermeier (2001) and natural ES Wierstra et al. (2011); Sun et al. (2012) demonstrated success, initial implementations remained in the million-parameter scale (Such et al., 2018; Risi and Stanley, 2019; Zhang et al., 2017). However, recent updates have brought ES up to scale and in competition with GPRO, leveraging how it is highly parallelizable, Salimans et al. (2017), memory efficient Malladi et al. (2024); Korotyshova et al. (2025), faster Sarkar et al. (2025), robust to sparse reward horizons Salimans et al. (2017), and can be modified with LoRA adaptions Jin et al. (2024); Korotyshova et al. (2025); Sarkar et al. (2025).\nQiu et al. (2025) recently published a novel implementation of ES and showed that it outperforms GRPO. However, their study lacked a thorough analysis of model degradation during continued training. Additionally, a bulk of their study was focused on a single dataset. We extend their analysis to multiple datasets, evaluate model degradation during fine-tuning and also study the difference in weight updates in ES when compared to GRPO.\n\n\n\n\n3 Experiments\n\n\n3.1 ES vs GRPO Comparison\n\nWe use the ES implementation of Qiu et al. (2025) and compare it with the GRPO Shao et al. (2024) implementation from the VERL libary (Sheng et al., 2025). An algorithmic analogy between the ES and GRPO algorithms can be found in A.1 while implementations details can be found in A.2. We extend the analysis of ES and GRPO to three math and reasoning tasks ‚Äì GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and OlympiadBench (He et al., 2024), in addition to the Countdown dataset which was extensively studied in prior work Qiu et al. (2025). We perform this study for two models: Qwen2.5-1.5B-Instruct (Qwen et al., 2024) and Llama-3.2-1B-Instruct (Grattafiori et al., 2024). Following the experimental conditions of Qiu et al. (2025), we train our models on 200 examples from each dataset with identical batch size and number of rollouts.\n\n\nThe results for comparison between ES and GRPO for fine-tuning LLMs can be found in Table 1. We see that for both models, ES is within 3-4 percentage points of GRPO in terms of task performance. These results are in contrast to prior work by Qiu et al. (2025), who claim that ES significantly outperforms GRPO on the Countdown task. In our experiments, we see that although ES performance numbers are close to GRPO, GRPO still outperforms ES for all but the GSM8K dataset with Llama-3.2-1B model. Therefore, we find different relative performance trends than those reported in prior work, which may stem from differences in GRPO implementations, hyperparameter choices, or evaluation protocols. We release our codebase and open source our trained models for reference.\n\n\n\n\n\n\nModel\nTask\nES\nGRPO\n\n\n\n\n\n\nQwen-2.5-1.5B\n(Instruct)\n\nCountdown\n53.0\n56.4\n\n\nGSM8K\n77.4\n80.4\n\n\nMATH\n59.1\n63.2\n\n\nOlympiadBench\n15.2\n18.2\n\n\n\n\nLlama-3.2-1B\n(Instruct)\n\nCountdown\n15.2\n37.6\n\n\nGSM8K\n55.2\n53.8\n\n\nMATH\n32.2\n35.6\n\n\nOlympiadBench\n5.6\n5.9\n\n\n\n\nTable 1: Peak validation acc"
  },
  {
    "title": "When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation",
    "url": "https://arxiv.org/abs/2601.20858v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to \"uncontaminated\" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an unco",
    "full_text": null
  },
  {
    "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
    "url": "https://arxiv.org/abs/2601.20856v1",
    "source": "arxiv",
    "summary": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Benchmarks for LLM Planning\n2.2 Sokoban as a Benchmark for Planning\n\n\n\n3 Methods\n\n3.1 Sokoban game\n3.2 Dataset\n\n3.3 Experimental Setup\n\n3.3.1 1-shot Inference\n3.3.2 LLM-Modulo\n\n\n\n3.4 Evaluation\n\nAccuracy:\nPrefix accuracy:\nManhattan Distance:\n\n\n\n\n\n4 Results\n\n4.1 1-shot Inference\n4.2 LLM-Modulo\n\n\n\n5 Conclusions\n\n5.1 Current limitations and future work\n\n\nA Prompts for 1-shot Inference Settings\n\nB Prompts for LLM-Modulo Settings\n\nB.1 System Prompt\nB.2 PDDL Domain\n\n\nC LLM-Modulo: Map Rotations\n\n\n\n\n\nSokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models\n\n\n\nSebastiano Monti  s.monti@ipazia.com Ipazia SpA, Italy\nCarlo Nicolini  c.nicolini@ipazia.com Ipazia SpA, Italy\nGiovanni Pellegrini  g.pellegrini@ipazia.com Ipazia SpA, Italy\nJacopo Staiano  jacopo.staiano@unitn.it University of Trento, Italy\nBruno Lepri  lepri@fbk.eu Fondazione Bruno Kessler and Ipazia SpA, Italy\n\n\n\n\nAbstract\nAlthough the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.\n\n\n\n1 Introduction\n\nAutomated Planning, i.e. the task of generating sequences of actions to achieve a goal, is a well-studied problem in the field of Artificial Intelligence (AI) (Ghallab et al., 2016), since it requires AI systems to exhibit cognitive abilities such as reasoning, understanding, and efficient state space search (Wei et al., 2025).\nTo this end, automated planning literature has focused on the use of formal languages, such as the Planning Domain Definition Language (PDDL) (McDermott and others, 1998; Russell and Norvig, 2021; Haslum et al., 2019)), and of tree-search strategies or specific heuristics to find optimal solutions (Bonet and Geffner, 2001).\nLarge Language Models (LLMs) and, in particular, Large Reasoning Models (LRMs) i.e., LLMs trained to produce so-called reasoning traces resembling structured thought processes, have demonstrated impressive capabilities in natural language understanding, knowledge retrieval and multi-modal pattern recognition (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025).\nHowever, recent studies highlighted the limitations of such models when applied to planning tasks (Valmeekam et al., 2023b; Shojaee et al., 2025).\nFor instance, internal reasoning processes have been shown to resemble a form of wandering through the solution space rather than a systematic exploration (Lu et al., 2025).\nThis distinction becomes particularly important for problems that require maintaining sequential state information, such as spatial exploration in constrained environments. In these settings, effective tracking of working memory is necessary to infer the agent‚Äôs latent previous state (Zhang et al., 2024).\n\n\nIn this work, we investigate the long-horizon planning abilities of LRMs using a highly simplified variant of the Sokoban puzzle¬†(Culberson, 1998). Rather than increasing spatial complexity, we deliberately minimize the structural complexity of the environment while preserving the long-horizon nature of the task by creating examples with\nthe lowest possible branching factor compatible with solvability: a single movable block placed within a linear corridor with tightly controlled geometry.\n\n\n\nThis setting allows us to isolate long-horizon planning from state persistence: models are required to produce complete solution sequences without external memory, intermediate feedback, or state validation, relying solely on internal state representations to track the evolving environment.\nWe therefore investigate to what extent LRMs can sustain coherent planning over long (but simple) action sequences and whether even minimal reasoning branching in otherwise trivial Sokoban instances is sufficient to induce planning failures.\n\n\nConcretely, we examine whether current LRMs can reliably solve linear-corridor Sokoban puzzles with minimal possible branching and identify the point at which increases in horizon length lead to catastrophic breakdowns in action validity, despite the simplicity of the underlying environment.\nAs we will show these minimal sub-problems which are trivial to humans (Jaru≈°ek and Pel√°nek, 2010), are still challenging for Large Reasoning Models as shown by other preliminary studies involving spatial intelligence (Cai et al., 2025).\nWe posit this as a systemic deficiency in long-term action representation and sequential logic, and in spatial reasoning and thus as an important limitation of current LRMs that is not yet fully understood.\n\n\n\n\n2 Related Work\n\n\n2.1 Benchmarks for LLM Planning\n\nAs mentioned above, planning requires LLMs to blend logical, numerical, and spatial reasoning with long-horizon strategic adaptation, rather than just relying on pattern matching or memorization.\nClassical planning domains expressed in or derived from the Planning Domain Definition Language (PDDL (Fox and Long, 2003)), such as BlocksWorld (Slaney and Thi√©baux, 2001), Towers of Hanoi and similar tasks (Pallagani et al., 2023), remain a common benchmark choice, though earlier attempts date back to the pre-ChatGPT era (Silver et al., 2022).\nTest suites like PlanBench (Valmeekam et al., 2022) introduced structured, domain-agnostic evaluations inspired by classical planning (Ghallab et al., 2016), including plan generation (Oswald et al., 2024; Valmeekam et al., 2025; La Malfa et al., 2025) and optimality (Valmeekam et al., 2022; Zhai and others, 2025; Valmeekam et al., 2023a).\n\n\nIn another line of work, planning is evaluated within agentic or workflow-based frameworks, where LLMs are required to decompose goals into multiple sub-plans (Meyerson et al., 2025; Zhang et al., 2025; La Malfa et al., 2025).\nThe results in these settings are encouraging though highly cost intensive.\nImportantly, when not equipped with external tools or made part of larger workflows (e.g., enabling stateful tracking (Hu et al., 2025b)), innate planning abilities remain still weak (Schepanowski and Ling, 2025).\nEven the latest foundational models are found to consistently fail in delivering correct sequences of actions (in any format or language) due to two primary deficits: weak internal state representations leading to invalid moves and misleading heuristic search resulting in loops or early termination, as shown in the textual game ‚Äú8-puzzle‚Äù in Schepanowski and Ling (2025).\nMoreover, efficacy of different prompting techniques is model-dependent in a non-predictable way (Schepanowski and Ling, 2025; Deng et al., 2025).\n\n\nOther works have systematically investigated the performances of LLMs in playing textual games with gym-style APIs (Brockman et al., 2016; Hu et al., 2025a).\nBeyond structured puzzles, community-driven and informal game-oriented benchmarks like word-game bench (Stojanovski, 2024) and nonogram logic puzzles (Berend et al., 2014; Kleine, 2026) with multi-difficulty instances have been devised to measure how well models plan under both explicit and implicit constraints, track environment states, and adapt over multiple turns.\nThe varying depth of planning ability required helps to reveal how performance scales with complexity and structure.\n\n\nIn general, existing benchmarks using specific planning languages and/or internal reasoning traces expressed in natural language show that LLMs exhibit limited planning abilities in various domains (Kambhampati et al., 2024), especially as the complexity and horizon length of the problems increase.\nThis gap motivates the development of new benchmarks tailored to planning and solving structured textual puzzles with LLMs.\n\n\n\n\n2.2 Sokoban as a Benchmark for Planning\n\nThe Sokoban puzzle involves spatial planning in a highly constrained environment. Solvable Sokoban maps can be generated efficiently (Murase et al., 1996), and the environment is fully controllable and deterministic. These properties enable rigorous evaluation using exact solvers and verifiers, as well as metrics such as search depth and solution time (Jaru≈°ek and Pel√°nek, 2010; Shoham and Schaeffer, 2020).\nUnlike puzzles such as the Tower of Hanoi, which can be solved by repeating a simple pattern for larger instances, Sokoban offers no shortcuts.\nEach map is unique, and moving a single box can block or open paths in ways that prevent a one-size-fits-all solution.\nAs a result, Sokoban is considered a good benchmark for evaluating planning abilities in the 2023 edition of the International Planning Competition¬†(Taitler et al., 2024).\n\n\nRecently, recurrent neural networks (non LLM-based) trained over multiple examples of Sokoban puzzles have obtained state of the art performance (Jolicoeur-Martineau, 2025; Taufeeque et al., 2024).\nHowever, LLMs are found to perform poorly, struggling even with simple maps and correctly solving only a small fraction of instances: Valmeekam et al. (2025) report success rates of just about 10‚Äì12% when using the OpenAI o1-preview model directly.\nIn contrast, substantially higher success rates are achieved in an LLM-Modulo setting, where the same model is used to generate plans that are then executed by an external planner, yielding approximately 43% solved instances for o1-preview (and about 10% for o1-mini),"
  },
  {
    "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
    "url": "https://arxiv.org/abs/2601.20854v1",
    "source": "arxiv",
    "summary": "Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions.",
    "full_text": null
  },
  {
    "title": "C3Box: A CLIP-based Class-Incremental Learning Toolbox",
    "url": "https://arxiv.org/abs/2601.20852v1",
    "source": "arxiv",
    "summary": "Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging th",
    "full_text": "\n\n\n\n1 Introduction\n2 Toolbox Usage\n3 Preliminary Experiments\n4 Conclusion\nA Implemented Class-Incremental Learning Methods\n\n\n\n\n\nC3Box: A CLIP-based Class-Incremental Learning Toolbox\n\n\nC3Box Hao Sun \\emailsunhao@lamda.nju.edu.cn \nC3Box Da-Wei Zhou \\emailzhoudw@lamda.nju.edu.cn \n\\addrSchool of Artificial Intelligence, Nanjing University, China\nNational Key Laboratory for Novel Software Technology, Nanjing University, 210023, China\n\n\n\nAbstract\nTraditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental\nlearning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems.\nThe code is available at¬†https://github.com/LAMDA-CL/C3Box.\n\n\nKeywords: \nClass-Incremental Learning, Continual Learning, CLIP\n\n\n\n1 Introduction\n\nIn recent years, the rapid advancement of deep learning¬†(Liu et al., 2015) has demonstrated immense potential across numerous domains. However, real-world scenarios are inherently dynamic, with data typically emerging as continuous and evolving data streams¬†(Gomes et al., 2017). Traditional deep learning methods are typically optimized for static data distributions and struggle to integrate new classes over time. When learning from dynamic data streams, these methods often overwrite previous knowledge to accommodate new classes, resulting in catastrophic forgetting¬†(French, 1999). Class-Incremental Learning (CIL)¬†(Rebuffi et al., 2017) has been proposed to address this issue¬†(Zhou et al., 2024d). Driven by the remarkable success of Pre-trained Models (PTMs), such as Vision Transformers¬†(Dosovitskiy et al., 2021) and CLIP¬†(Radford et al., 2021), the focus of CIL research has undergone a significant shift. The field is moving away from traditional training from scratch with randomly initialized weights toward leveraging the inherent generalization capabilities of PTMs¬†(Zhou et al., 2024b).\n\n\nIn particular, CLIP¬†(Radford et al., 2021) has emerged as a pioneering pre-trained vision-language model¬†(Wang et al., 2024) that provides a powerful starting point for continual learning by aligning visual concepts with natural language in a shared embedding space. Unlike traditional vision-only models, CLIP leverages rich textual semantics to guide the learning process, offering a more robust representation that effectively mitigates catastrophic forgetting while adapting to dynamic real-world scenarios¬†(Hu et al., 2025; Wen et al., 2025). Despite these advantages, current CLIP-based CIL research still faces substantial practical challenges. Firstly, the implementation of various CLIP-based CIL methods is highly fragmented, with researchers often relying on disparate codebases and inconsistent experimental protocols. This lack of a unified framework makes it difficult to reproduce results and build upon previous work. Secondly, the absence of a standardized experimental protocol leads to significant evaluation inconsistency, where the use of varying data splits and evaluation metrics prevents fair comparisons. Thirdly, integrating CLIP-based methods into diverse incremental learning scenarios remains cumbersome, as different adaptation strategies often require bespoke modules and interface engineering, which imposes a significant engineering burden on researchers.\n\n\nFigure 1:  Overview of C3Box and its main functionalities and modules.\n\n\n\nTo bridge these gaps and provide a standardized platform for the machine learning community, we present C3Box (CLIP-based Class-inCremental\nlearning toolBOX), a modular and comprehensive toolbox specifically tailored.\nAs shown in Figure¬†1, C3Box provides extensive algorithm coverage by integrating state-of-the-art CLIP-based CIL methods alongside representative traditional and ViT-based CIL approaches, as well as fundamental baselines, within a unified framework that enables systematic and fair baseline comparisons. Moreover, building on the streamlined architecture of PyCIL¬†(Zhou et al., 2023a) and PILOT¬†(Sun et al., 2025), C3Box provides a unified configuration and execution pipeline, allowing users to define datasets, backbones, and training settings in a single JSON file. This design offers a standardized interface that allows for seamless integration of new methods and automated logging, ensuring high reproducibility and minimal engineering overhead. As a user-friendly toolbox, C3Box adopts consistent unified interfaces across methods and relies only on widely used open-source libraries to ensure easy adoption and broad compatibility across major operating systems, including Linux, macOS, and Windows.\n\n\n\n\n2 Toolbox Usage\n\nDependencies: Building upon the established architectures of PyCIL¬†(Zhou et al., 2023a) and PILOT¬†(Sun et al., 2025), C3Box is built on a robust, modular software stack. It relies solely on a suite of widely adopted open-source libraries, such as NumPy¬†(Harris et al., 2020) and SciPy¬†(Virtanen et al., 2020) for fundamental numerical operations and optimization, while the core neural architectures are implemented using the PyTorch (Paszke et al., 2019) framework. We also utilize the OpenCLIP library (Cherti et al., 2023) to provide a standardized interface for loading diverse pre-trained model weights.\nThe tqdm¬†(da Costa-Luis, 2019) library provides real-time progress monitoring.\n\n\nSupported datasets: To thoroughly support the evaluation of various algorithms, we follow¬†(Zhou et al., 2025c) and select ten benchmark datasets with significant domain gaps from CLIP‚Äôs pre-training dataset. The specific evaluation benchmarks include: CIFAR100 (Krizhevsky, 2009), CUB200 (Wah et al., 2011), ObjectNet (Barbu et al., 2019), ImageNet-R (Hendrycks et al., 2021), FGVCAircraft (Maji et al., 2013), StanfordCars (Krause et al., 2013), Food101 (Bossard et al., 2014), SUN397 (Xiao et al., 2010), UCF101 (Soomro et al., 2012) and TV100¬†(Zhou et al., 2024a). Following the benchmark protocols in CIL¬†(Zhou et al., 2025c), we evaluate on 100 classes for CIFAR100, Aircraft, Cars, Food, UCF and TV100; 200 classes for CUB200, ObjectNet, and ImageNet-R; and 300 classes for SUN to maintain consistency across incremental stages.\n\nDataset split: Following the evaluation protocols in CIL¬†(Zhou et al., 2024d), we utilize ‚ÄòB-mm Inc-nn‚Äô to split the classes. Specifically, mm denotes the number of classes in the initial stage, while nn represents the number of classes in each subsequent incremental stage.\n\nImplemented Methods:\nC3Box implements a total of 17 representative CIL methods, covering traditional CIL methods, i.e., FOSTER¬†(Wang et al., 2022a), MEMO¬†(Zhou et al., 2023b), ViT-based CIL methods, i.e., L2P¬†(Wang et al., 2022c), DualPrompt¬†(Wang et al., 2022b), CODA-Prompt¬†(Smith et al., 2023), EASE¬†(Zhou et al., 2024c), SimpleCIL¬†(Zhou et al., 2025a), APER (with Adapter/Finetune/SSF/VPT variants)¬†(Zhou et al., 2025a), TUNA¬†(Wang et al., 2025), and state-of-the-art CLIP-based methods, including RAPF¬†(Huang et al., 2024),\nCLG-CBM¬†(Yu et al., 2025), MG-CLIP¬†(Huang et al., 2025), PROOF¬†(Zhou et al., 2025c), ENGINE¬†(Zhou et al., 2025b), and BOFA¬†(Li et al., 2026). In addition, C3Box provides common baselines such as Finetune and ZS-CLIP¬†(Radford et al., 2021). Notably, all methods have been adapted into a unified CLIP-based framework. Implementation details are provided in Appendix¬†A.\n\n\nEvaluation metric: Following the evaluation protocols in CIL¬†(Rebuffi et al., 2017; Zhou et al., 2025c), we denote ùíúb\\mathcal{A}_{b} as the model‚Äôs accuracy after the bb-th incremental stage. In C3Box, we employ main metrics to evaluate this implemented method: Last Accuracy ùíúB\\mathcal{A}_{B}, which reflects performance after the last task, and Average Accuracy ùíú¬Ø=1B‚Äã‚àëb=1Bùíúb\\bar{\\mathcal{A}}=\\frac{1}{B}\\sum_{b=1}^{B}\\mathcal{A}_{b}, which represents the mean accuracy across all incremental stages. In addition, following¬†(Chaudhry et al., 2018), we define Forgetting Measure\n as the average drop from the best-achieved accuracy\nof each task to its last accuracy:\n\n\n\nFB=1B‚àí1‚Äã‚àëb=1B‚àí1maxl‚àà{b,‚Ä¶,B‚àí1}‚Å°(ùíúl,b‚àíùíúB,b).F_{B}=\\frac{1}{B-1}\\sum_{b=1}^{B-1}\\max_{l\\in\\{b,\\dots,B-1\\}}(\\mathcal{A}_{l,b}-\\mathcal{A}_{B,b}).\n\n(1)\n\n\n\n\nTable 1: Average and last performance of different methods on CIFAR100 B0 Inc10 and Aircraft B0 Inc10. ‚Äò-‚Äô indicates the original paper didn‚Äôt report the performance.\n\n\n\n\n\nMethod\nExemplars\nCIFAR100\nAircraft\n\n\nReproduced\nReported\nReproduced\nReported\n\n\nùíú¬Ø\\bar{\\mathcal{A}}\nùíúB\\mathcal{A}_{B}\nùíú¬Ø\\bar{\\mathcal{A}}\nùíúB\\mathcal{A}_{B}\nùíú¬Ø\\bar{\\mathcal{A}}\nùíúB\\mathcal{A}_{B}\nùíú¬Ø\\bar{\\mathcal{A}}\nùíúB\\mathcal{A}_{B}\n\n\nBaselines\nFinetune\n‚úó\n21.33\n9.24\n-\n-\n6.22\n3.42\n-\n-\n\n\nZS-CLIP¬†(Radford et al., 2021)\n\n‚úó\n81.81\n71.38\n-\n-\n26.61\n17.16\n-\n-\n\n\nTraditional\nFOSTER¬†(Wang et al., 2022a)\n\n‚úì\n86.56\n80.32\n-\n-\n52.96\n39.87\n-\n-\n\n\nMEMO¬†(Zhou et al., 2023b)\n\n‚úì\n85.05\n73.68\n-\n-\n42.24\n25.41\n-\n-\n\n\nViT-based\nL2P¬†(W"
  },
  {
    "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
    "url": "https://arxiv.org/abs/2601.20848v1",
    "source": "arxiv",
    "summary": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address thi",
    "full_text": null
  },
  {
    "title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion",
    "url": "https://arxiv.org/abs/2601.20847v1",
    "source": "arxiv",
    "summary": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurement",
    "full_text": null
  },
  {
    "title": "PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting",
    "url": "https://arxiv.org/abs/2601.20845v1",
    "source": "arxiv",
    "summary": "Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for ",
    "full_text": null
  },
  {
    "title": "$\\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval",
    "url": "https://arxiv.org/abs/2601.20844v1",
    "source": "arxiv",
    "summary": "This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities,\" including the $\\ell_2$ metric, inner product, and cosine si",
    "full_text": null
  },
  {
    "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)",
    "url": "https://arxiv.org/abs/2601.20843v1",
    "source": "arxiv",
    "summary": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficie",
    "full_text": "\n\n\n\n1 Introduction\n2 Sequential Refinement approach vs Parallel Scaling\n\n3 Deep Researcher Design\n\n3.1 High Level Design\n3.2 Candidate Crossover algorithm\n3.3 Agent‚Äôs Memory: Global Research Context\n3.4 Sequential Research Plan Refinement via Reflection\n3.5 One Shot Report Generation\n\n\n4 Evaluation\n5 Conclusion\n\n\n\n\n\nDeep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)\n\n\n\nSaurav Prateek \n\n\n\n(January 2026)\n\nAbstract\nThis paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD-level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm.\nThe sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from ‚Äùsiloed knowledge‚Äù. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density.\nPowered by the gemini-2.5-pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral-level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher (45) [8], Nvidia AIQ Research Assistant (40.52) [12], Perplexity Research (40.46) [13], Kimi Researcher (44.64) [11] and Grok Deeper Search (38.22) [17] present on the DeepResearch Bench‚Äôs actively running leaderboard [10]. This performance marginally exceeds our previous work, Static-DRA (34.72) [6], and reinforces the finding that sequential scaling consistently outperforms the parallel self-consistency paradigm. The entire source code, outputs and benchmark results are open-sourced at https://github.com/SauravP97/deep-researcher-reflect-evolve/\n\n\nFigure 1: Comparison of Deep Researchers on 4 key dimensions of RACE framework\n\n\n\n1 Introduction\n\nWe demonstrate a Deep Researcher architecture which utilizes Research Plan Reflection to perform continuous plan refinement (if required) and Candidates Crossover allowing for the sampling of multiple answers using varied candidate‚Äôs model parameters (e.g., temperature, top_k) to explore a larger search space. At any particular instance of time the deep researcher stores the context of the previous research done and revisits them to decide upon:\n\n\n\n\n1.\n\nThe next (potentially un-explored) area to be researched.\n\n\n\n2.\n\nRefining the Research Plan if needed.\n\n\n\n3.\n\nDetermining the percentage Research Progress.\n\n\n\n\n\nWe continue the research process until we have hit a satisfactory threshold of research progress or we have exhausted the maximum retries. The Deep Researcher has an LLM-as-a-judge which analyzes the research performed and decides on the percentage of the research progress. If the researcher crosses the threshold of 90% progress, the research process is halted and a research report is generated.\n\n\nWe generate a research report in a single-shot by an LLM Agent acting as a report writer. The Report Writer Agent has access to the entire research context on the topic and utilises it to generate a Research Report in a single shot. Unlike Google‚Äôs TTD-DR (Test-Time Diffusion) [4] which performs Report-level Denoising inspired by the sampling process in Diffusion models to where they continuously refine the noisy generated initial report iteratively.\n\n\n\n\n2 Sequential Refinement approach vs Parallel Scaling\n\nThe development of Deep Research Agents (DRAs) has seen the emergence of two primary paradigms for handling complex, multi-faceted research tasks: Parallel Scaling and Sequential Refinement.\n\n\n\n\n1.\n\nParallel Scaling - Efficiency and Its Limitations: Parallel scaling, as implemented in architectures like GPT Researcher [2] and our previous work, Static-DRA [6], focuses on decomposing a research topic into multiple independent sub-topics. These sub-topics are then investigated concurrently by parallel execution agents. While this approach offers significant advantages in terms of reduced latency and stable performance through horizontal scaling, it often suffers from a ‚Äùsiloed knowledge‚Äù problem. Because each agent operates within the vacuum of its specific sub-task, the system lacks a holistic ‚ÄùGlobal Context‚Äù. This isolation makes it difficult for the model to recognize overlapping information, avoid redundant search queries, or make intelligent, real-time modifications to the research plan based on discoveries made in other branches.\n\n\n\n2.\n\nSequential Refinement - Global Context and Dynamic Adaptation: In contrast, the Sequential Refinement approach leverages the iterative nature of the research process. Google‚Äôs TTD-DR (Test-Time Diffusion) [4] architecture exemplifies this by performing ‚ÄùReport-level Denoising,‚Äù where an initial draft is continuously refined through sequential iterations inspired by diffusion models. Our Deep Researcher advances this paradigm by shifting the focus from report refinement to Sequential Research Plan Refinement. In this model, the agent maintains a centralized Global Research Context - a comprehensive memory of every search trajectory and artifact gathered. By building each research chain explicitly upon previous attempts, the agent can ‚Äùlook back‚Äù at its progress and reason about which areas remain unexplored. This allows for dynamic plan refinement, enabling the agent to pivot its strategy at runtime, add unforeseen sub-topics, or terminate redundant paths.\n\n\n\n\n\nThe superiority of sequential scaling is supported by recent findings in ‚ÄùThe Sequential Edge‚Äù (Chopra 2025) [7] paper, which demonstrates that sequential scaling consistently outperforms the parallel self-consistency paradigm in 95.6% of configurations, with accuracy gains of up to 46.7%. This is attributed to the model‚Äôs ability to reason with a fuller, more integrated context rather than disparate fragments.\n\n\nBy adopting this sequential approach, our Deep Researcher achieved a score of 46.21 on the DeepResearch Bench [1], outperforming leading deep research agents such as Claude Researcher [8], Perplexity Research [13], Grok Deeper Search [17] and many others in the leaderboard [10]. Our architecture ensures that the final One-Shot Report Generation is informed by a unified narrative and high fact density, producing the depth required for PhD-level research.\n\n\n\n\n3 Deep Researcher Design\n\n\n3.1 High Level Design\n\nThe high level design of the Deep Researcher includes multiple modules working together to carry out the deep research on a given topic. The design is demonstrated in Figure 2.\n\n\nFigure 2: Deep Researcher - High Level Design\n\n\nThe research methodology is structured as a series of sequential iterations, wherein each successive phase leverages findings from previous cycles to facilitate informed decision-making regarding targeted research areas and necessary plan refinements. The summary of the research process is demonstrated in the steps mentioned below.\n\n\n\n\n1.\n\nStep 1 - Research Plan Curation: The research topic is provided to the Planning agent that curates a research plan for the provided topic. The plan comprises detailed steps to take in order to carry out the research.\n\n\n\n2.\n\nStep 2 - Generate Search Query: The curated plan is read by the Search agent that generates a search query. The agent also reads the global context to understand what all has been already researched and intelligently curates a search query.\n\n\n\n3.\n\nStep 3 - Answer Search Query: The search query from the previous step is answered by the Search Agent. At this step the agent utilises a Web Search tool to gather recent events and updates regarding the query. The agent also incorporates the Candidate Crossover algorithm to improve the answer generated for the query. The search query and the answer is then added to the global context.\n\n\n\n4.\n\nStep 4 - Research Plan Reflection: The Planning agent reads the current research plan and the global context to decide whether to update the currently followed research plan or not. The agent also decides on what changes to make in the plan if at all needed.\n\n\n\n5.\n\nStep 5 - Research Plan Update (maybe): The Planning agent takes on the plan reflection input from the previous step and makes the necessary updates in the Research Plan if suggested in the previous step. If there‚Äôs no change needed, the existing plan is followed.\n\n\n\n6.\n\nStep 6 - Analyze Research Progress: The Planning agent reads the research plan and the global context to analyze the current state of the research progress. If the research progress has crossed the 90% threshold benchmark, then the research process is ended. Otherwise the research loop is continued again from Step 2.\n\n\n\n7.\n\nStep 7 - One Shot Report generation: Once the research loop ends, we perform one-shot report generation by an LLM agent acting as a report writer. The agent is provided with the current research plan and the global context to write the research report in one go.\n\n\n\n\n\nThe subsequent sections provide a comprehensive and detailed examination of the aforementioned procedural stages.\n\n\n\n\n3.2 Candidate Crossover algorithm\n\nWe implement a Candidate Crossover algorithm that is integrated into Step 3, the phase in which the Search Agent conducts research for a specified query. This algorithm enhances the agent‚Äôs efficiency by deploying multiple candidates to in"
  },
  {
    "title": "Reward Models Inherit Value Biases from Pretraining",
    "url": "https://arxiv.org/abs/2601.20838v1",
    "source": "arxiv",
    "summary": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using vali",
    "full_text": null
  },
  {
    "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
    "url": "https://arxiv.org/abs/2601.20835v1",
    "source": "arxiv",
    "summary": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reaso",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 FunHSI\n\n3.1 Preliminaries\n\n3.2 Functionality-aware Contact Reasoning\n\nFunctionality grounding and reconstruction.\nLLM-based contact graph reasoning.\n\n\n\n3.3 Functionality-aware Body Initialization\n\nHuman inpainting with contact-aware reasoning.\n3D human estimation.\nContact graph refinement.\n\n\n\n3.4 Optimization-based Body Refinement\n\nOptimization objective.\nTwo-stage optimization strategy.\n\n\n\n\n\n4 Experiments\n\nDatasets.\nEvaluation Metrics.\nBaselines.\n\n4.1 Comparison to Baselines\n\nQuantitative Evaluation.\nQualitative Evaluation.\nGeneralization to City Scenes.\n\n\n4.2 Perceptual User Study\n\n4.3 Ablation Studies\n\nContact graph refinement.\nBody &amp; hand pose estimation.\nBody refinement.\nFunctional element detection.\n\n\n\n\n\n5 Conclusion\n\nLimitations and future work.\n\n\nDisclosure.\nA Human Body Part Annotation\n\nB Datasets Details\n\nIndoor scenes from SceneFun3D¬†[4].\nReal-world city scenes.\n\n\nC Implementation Details\n\nD More Experimental Analysis\n\nAdditional results on real-world cenes.\nGeneration Diversity.\nHuman Inpainting Examples.\nBody and Hand Pose Estimation Examples.\n\n\nE User Study Details\n\n\n\n\n\nOpen-Vocabulary Functional 3D Human-Scene Interaction Generation\n\n\n\nJie Liu1,2,‚ÄÑ Yu Sun1,‚ÄÑ Alp√°r Cseke1,‚ÄÑ Yao Feng4,‚ÄÑ Nicolas Heron1,‚ÄÑ Michael J. Black2, ‚ÄÑ Yan Zhang1‚ÄÑ \n1Meshcapade,‚ÄÑ\n2University of Amsterdam,‚ÄÑ \n3Max Planck Institute for Intelligent Systems,‚ÄÑ\n4Stanford University \n\n\n\n\nAbstract\nGenerating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation.\nThe key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction.\nUnfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions.\nIn this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts.\nGiven a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph.\nWe then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses.\nFinally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness.\nIn contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as ‚Äúsitting on a sofa‚Äù, while supporting fine-grained functional human-scene interactions, e.g., ‚Äúincreasing the room temperature‚Äù.\nExtensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.\n111Code, models, and more results are available at:\nhttps://jliu4ai.github.io/projects/funhsi\n\n\n\n\n\n\nFigure 1: Given a set of RGB-D images, their camera parameters, and a task description, our method automatically generates a 3D human body that interacts with the appropriate functional element in the scene. Leveraging the generalization power of foundation models, our method is training-free and applicable to unseen environments and open-vocabulary task descriptions in a zero-shot manner.\n\n\n\n\n1 Introduction\n\nWhen asked to ‚Äúincrease the room temperature‚Äù, a human can naturally reason about object functionality, identify the relevant functional element (e.g., a heater knob or thermostat), and interact with it using an appropriate body configuration.\nHowever, performing such functionally-correct interactions in a novel 3D environment remains challenging for embodied intelligent agents, as it requires a holistic understanding of scene semantics and the human actions that the environment affords¬†[7, 4].\nIn this work, we investigate to generate realistic and functional interactions between a 3D human body and a novel scene, conditioned on open-vocabulary task descriptions.\nAn effective solution to this problem benefits a wide range of applications, including embodied AI, robotics, game production, and video generation, among many others.\n\n\nThe synthesis of 3D human-scene interaction (HSI) has been extensively studied, with existing methods broadly falling into two paradigms.\nData-driven approaches learn generative models from paired 3D interaction data, achieving high visual fidelity and realistic human poses in controlled settings.\nFor example, COINS¬†[47] models human body poses conditioned on scene geometry and text commands, while TriDi¬†[29] learns a joint distribution over human pose, object geometry, and interaction signals using diffusion models.\nDespite their effectiveness, such methods rely on large-scale, high-quality paired interaction datasets and typically require explicit interaction specifications (e.g., ‚Äúsitting on a sofa‚Äù), limiting their ability to generalize to diverse novel scenes.\nTo alleviate data dependency, recent work has explored zero-shot or training-free pipelines that leverage pre-trained vision-language models (VLMs) to generate human-scene interactions.\nRepresentative examples include GenZI¬†[18], which reconstructs 3D human bodies from multi-view image synthesis, and GenHSI¬†[20], which integrates image-based object grounding with 3D body fitting from a single input image.\nWhile these methods improve flexibility and support open-vocabulary task prompts, they are primarily effective for general human-scene interactions describing physical relations or motions, e.g., ‚Äúsitting on a sofa‚Äù or ‚Äúwalking on a bridge‚Äù.\n\n\nIn contrast, many real-world tasks like ‚Äúopen the window‚Äù involve interactions at a functional level, where a human must identify and interact with fine-grained functional elements in the 3D scene to complete the task, such as finding and contacting a window handle to open a window, as shown in Fig.¬†1.\nWe refer to this setting as functional human-scene interaction.\nThis problem poses fundamental challenges, as it requires reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses needed to establish appropriate contacts.\nExisting methods typically lack explicit reasoning about object functionality and the corresponding human-scene contact, leading to interactions that are either geometrically implausible or functionally incorrect.\n\n\nIn this work, we propose FunHSI, a training-free, functionality-driven\nframework that enables functional human-scene interactions from\nopen-vocabulary task prompts.\nGiven a set of posed RGB-D images and a task prompt, FunHSI reasons about the functionality of the 3D scene and synthesizes a 3D human that interacts with the scene in a functionally correct manner to accomplish the specified task.\nAs illustrated in Fig.¬†2, FunHSI is built upon three key components.\nFirst, we introduce a functionality-aware contact reasoning module to identify task-relevant functional elements in the scene, reconstruct their 3D geometry, and infer high-level interaction patterns via contact graph reasoning.\nThe resulting contact graph explicitly encodes the contact relationships between the human body and both functional and supporting scene elements, serving as a structured representation that bridges high-level task intent and low-level physical interaction.\nSecond, we propose a functionality-aware body initialization module that synthesizes a human performing the task in the image and estimates the corresponding initial 3D body and hand poses.\nTo mitigate hallucinations during human synthesis, we introduce a human inpainting optimization strategy that automatically evaluates and improves the generated human pose configuration.\nIn addition, since image-based synthesis may produce left-right hand inconsistencies with the inferred contact graph, we further refine the contact graph to align contact specifications with the synthesized human.\nFinally, a body refinement module places the initialized 3D human into the scene and performs stage-wise optimization to jointly refine body pose and human-scene contacts, ensuring both physical plausibility and functional correctness.\n\n\nWe conduct experiments on the SceneFun3D dataset¬†[4] under both functional and general human-scene interaction settings.\nExtensive qualitative and quantitative results demonstrate the effectiveness of our design and the superior performance of our framework compared to existing baselines.\nIn addition, we show that FunHSI is compatible with recent feed-forward 3D reconstruction methods, such as MapAnything¬†[15], and can generate realistic human-scene interactions in reconstructed city scenes.\nIn summary, our contributions are as follows:\n\n\n‚Ä¢\n\nWe propose FunHSI, a training-free framework that generates functionally correct human-scene interactions from open-vocabulary task prompts. FunHSI extends beyond general interactions to support functional interaction scenarios across diverse scenes and actions.\n\n\n\n‚Ä¢\n\nWe introduce a robust optimization strategy for inpainting humans and contact graph refinement scheme, providing valuable insights for functional human-scene interactions.\n\n\n\n‚Ä¢\n\nExtensive experiments demonstrate that FunHSI achieves strong performance in both functional and general HSI tasks compared to existing baselines. Additionally, FunHSI exhibits strong flexibility and generalization on realistic city scenes captured using smartphones.\n\n\n\n\n\n\n\n2 Related Work\n\nData-driven Human-scene Interaction Synthesis.\nHuman-scene interaction (HSI) models how humans behave within 3D environments¬†[46, 48, 41, 11, 13, 38], and many works focus on generating static interactions that place the human body into the scene¬†[32, 19, 45, 10, 47, 11, 18, 20].\nA conventional approach is to learn"
  },
  {
    "title": "Linear representations in language models can change dramatically over a conversation",
    "url": "https://arxiv.org/abs/2601.20834v1",
    "source": "arxiv",
    "summary": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the ",
    "full_text": "\n\n\n\n1 Background\n2 Methods\n\n3 Experiments\n\n3.1 Further analyses\n\n\n4 Discussion\nA Supplemental Methods\n\nB Supplemental analyses\n\nB.1 Accuracy on main question sets\nB.2 Answer-wise scores and non-robust factuality representations without opposite day\nB.3 Layerwise analyses\nB.4 Contrast-Consistent Search (CCS)\nB.5 End-of-conversation correction partially reverses the representational shifts\nB.6 Smaller Gemma models\nB.7 Analyzing Qwen3 14B on opposite day\nB.8 Causal interventions can have opposite effects at different points in the context\n\n\n\n\n\n\n\n\n\\correspondingauthor\nlampinen@google.com\\reportnumber\n\nLinear representations in language models can change dramatically over a conversation\n\n\nAndrew Kyle Lampinen\n\nGoogle DeepMind\n\n\nYuxuan Li\n\nGoogle DeepMind\n\n\nEghbal Hosseini\n\nGoogle DeepMind\n\n\nSangnie Bhardwaj\n\nGoogle DeepMind\n\n\nMurray Shanahan\n\nGoogle DeepMind\n\n\n\nAbstract\nLanguage model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering‚Äîin particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.\n\n\nkeywords: Language models, representation analysis, interpretability, in-context learning, representation dynamics\n\n\nThere has been substantial recent interest in linear representations in language models [tigges2023linear, park2023linear, marks2023geometry, burns2022discovering, elhage2022toy], building on a line of work that originates from the observation of systematic linear structure in vector word embeddings [mikolov2013distributed]. These linear representations have been suggested as a means to detect and even control high-level model behaviors [e.g. zou2023representation, stolfo2024improving].\n\n\nYet, language models adapt substantially to their context. Behavioral in-context learning has been a topic of interest for some time [brown2020language, lampinen2024broader], but more recent work has studied how this kind of learning can shift the ‚Äúbeliefs‚Äù they express [geng2025accumulating]. These kinds of contextual adaptability can contribute to issues like jailbreaking [anil2024many] or delusional conversations [dohnany2025technological], as well as the more positive aspects of long-context, such as effective coding and question answering over long documents or repositories.\n\n\nRecent work has characterized how some types of in-context learning are reflected in model representations [cf. bigelow2025belief]‚Äîwhether representations of a few-shot task [toddfunction, hendel2023context], or representations of structures such as graphs from which a sequence is generated [park2025iclr].\nRecently, lubana2025priors have argued that major approaches to interpreting models are neglecting these dynamic, contextual aspects of their representations.\n\n\nIn this work, we therefore study the intersection of these areas of behavioral and representational adaptation in the context of natural conversations. In particular, we follow prior works in identifying linear representations that correlate with conceptual features like factuality or ethics in large language models. We then study how representations of conversation-relevant and conversation-irrelevant topics shift over the course of a conversation.\n\n\nWe find that representations of features like factuality can change dramatically over the course of a conversation (Fig. 1). Statements that a model represents as non-factual can be flipped to being represented as factual after a few conversation turns, and vice versa. This flipping is maintained even when the representations are robust to other prompts that change model behavior, and is generally consistent across many model layers.\n\n\nThese representational changes do not seem to require on-policy conversations; indeed, replaying conversations from other models, or even conversation scripts another model was asked to write have similar effects. Thus, these representational changes seem to be a feature of general contextual adaptation in the models.\n\n\nWe discuss the implications of these results for interpretability and understanding of models. These findings highlight major challenges of construct validity when interpreting model representations, which may pose challenges for efforts to monitor or guarantee models on the basis of their internal representations. For example, if models can fundamentally change what they represent as ‚Äúfactual‚Äù over the course of a conversation, confirming that the current conversations falls within the ‚Äúfactual‚Äù subset of the model‚Äôs representations is not a guarantee of reliability. Contextual adaptation likewise poses challenges for interpretability methods like sparse autoencoders [SAEs; e.g., bricken2023towards]‚Äîwhich fundamentally assume that the meaning of internal representations remain consistent over a context [cf. lubana2025priors]. However, our results also shed new light on how models adapt over the course of a conversation, and may therefore point towards new directions of research in interpretability and science of language models. We return to these topics in the discussion.\n\n\nFigure 1: Conceptual overview: we find that in conversations during which models answers to questions change over the course of the conversation‚Äîeven if we simply replay a fictional conversation as though the model had actually produced it‚Äîtheir internal linear representations of questions on that topic can also flip. For example, if a model represents it as more ‚Äúfactual‚Äù to deny that it experiences qualia (i.e., subjective conscious experiences), over the course of a conversation about the model‚Äôs consciousness it may dramatically change its representations to represent it as more factual to assert that it does experience qualia than to deny it. Thus, the behavioral changes are reflected in reorganization of the model‚Äôs internal representational structure.\n\n\n\n1 Background\n\nThe linear representation hypothesis: From the early days of connectionism, researchers have studied how the representations learned in neural networks come to capture data structure along particular representation dimensions [e.g. hinton1986learning], including in simple language models [elman1991distributed]. More recently, it was observed that word representations learned from co-occurrence statistics show linear structure, including vector analogies [mikolov2013distributed]. Finally, the enthusiasm for these structures was reignited by a series of observations that large language models produce linearly structured representations [elhage2022toy], including for dimensions like factuality or ‚Äúhonesty‚Äù [marks2023geometry, burns2022discovering].\n\n\nThe emergence of linear representations: There have been some theoretical explanations of the emergence of linear representations in word embeddings as driven by features of the underlying data distribution [torii2024distributional, korchinski2025emergence], perhaps in combination with the inductive biases of gradient descent [jiang2024origins]. ravfogel2025emergence argue (via controlled experiments in synthetic settings) that in the language-modeling setting, linear representations of high-level concepts like ‚Äútruth‚Äù can emerge from higher-level co-occurrences: the simple fact that true statements are more likely to co-occur with other true statements, and false statements are more likely to co-occur with other false statements.\n\n\nUnfaithful interpretability: Several works have pointed out challenges of unfaithful interpretations produced from interpretability methods [bolukbasi2021interpretability], including the potential for unfaithful interpretations of linear subspaces [makelov2024subspace]. In particular, there has been concern about the faithfulness and reliability of such methods out-of-distribution [friedman2024interpretability], with levinstein2024still particularly focusing on failures of linear representation methods to faithfully identify language model ‚Äúbeliefs.‚Äù Our work can be seen as identifying how a particular kind of distribution shift‚Äîthe accumulating context of a conversation‚Äîchanges what interpretation of a representation could be considered faithful.\n\n\nRepresentation changes in context: Indeed, structured linear representations can emerge through in-context learning [park2025iclr]. Other types of in-context learning, such as few-shot learning of functions, can likewise be reflected in local r"
  },
  {
    "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents",
    "url": "https://arxiv.org/abs/2601.20831v1",
    "source": "arxiv",
    "summary": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we pr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 MLLMs in Embodied AI\n2.2 Memory-Augmented Agents\n\n\n\n3 MLLM-based Embodied Agents\n\n3.1 Memory-Augmented MLLM Agents\n\n\n4 MemCtrl: Training Memory Heads (Œº\\mu)\n\n5 Experimental Setup\n\nDatasets.\nLLM backbones.\nBaseline: Simple, In-Context Learning.\n\n\n\n6 Results\n\nPerformance across Gemma-3 and Qwen2.5.\n\n\n\n7 Qualitative Analysis\n\n7.1 Visualization\n\n\n8 Conclusions\n9 Limitations\nA Algorithms\nB Experimental Details\nC MLLM Prompts &amp; Decoder Modifications\n\nD Analyzing Memory\n\nD.1 Statistics\n\n\n\n\n\n\n\nMemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents \n\n\n\nVishnu Sashank Dorbala ‚ÄÉDinesh Manocha \n\nUniversity of Maryland, College Park\n\n\n\nAbstract\nFoundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online.\nIn this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head Œº\\mu that acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of Œº\\mu, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on Œº\\mu-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that Œº\\mu-augmented MLLMs show an improvement of around 16%16\\% on average, with over 20%20\\% on specific instruction subsets.\nFinally, we present an qualitative analysis on the memory fragments collected by Œº\\mu, noting the superior performance of Œº\\mu augmented MLLMs on long and complex instruction types.\n\n\n\nMemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents\n\n\n\n\n\nVishnu Sashank Dorbala ‚ÄÉ‚ÄäDinesh Manocha\n\nUniversity of Maryland, College Park\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Overview: We present MemCtrl, a novel memory filterting scheme to improve decision making performance on small MLLMs tackling embodied tasks.\nOur approach proposes a trainable memory head (green box labeled ‚ÄúMemory‚Äù) that learns to actively filter out redundant observations on-the-go. This form of active filtering alleviates issues with inefficient retrieval from stored observations, while also enabling scalability as a detachable memory head.\n\n\n\nAn overarching goal of Embodied AI is the development of a generalist agent that can perform consistently well with high success on diverse tasks, environments and instructions (Szot et al., 2025). A common paradigm to achieve this has been to utilize foundation models to develop task solving frameworks Mu et al. (2023); Yang et al. (2025). While a few of these methods generalize well to diverse tasks and instructions (Driess et al., 2023; Zawalski et al., 2024), they are constrained by high training costs, prohibiting them from quickly being able to adapt to novel real-time settings where the data is out of distribution. Further, finetuning large foundation models incurs significant computational capacity, proving to be a significant hurdle in the democratization of these methods (Liang et al., 2022), especially in the context of robotics, where computation on the edge is of vital importance.\n\n\nFigure 2: Comparison with Prior Work: We present MemCtrl, a novel approach to train ‚Äúmemory heads‚Äù to filter observations on the go. Prior work either used the entirety of stored observations as context (left) or filtered them via a variety of Retrieval Augmented Generation (RAG) based schemes ( red arrows), both of which assume the parsing of large amounts of data offline. MemCtrl introduces transferrable heads to use on MLLM backbone ( green arrows) to actively filter observations.\n\n\nAn alternative, more feasible paradigm has been a modular system where foundation models are used in conjunction with memory banks (Zhong et al., 2023; Wang et al., 2024) of past experiences and reflections. Foundation models including very large Multimodal Large Language Models (MLLMs) such as LLaMA 4 Touvron et al. (2023) and Deepseek V3 DeepSeek-AI et al. (2025)\nare limited by the size of their context window, and developing methods to refine and selectively pass memory as context is an active area of research (Wu et al., 2025). Prior work in this area includes use of intrinsic model editing techniques for memory injection¬†(Mitchell et al., 2022; Meng et al., 2023), or extrinsic interactions with episodic logs, Retrieval-Augmented Generated (RAG) Gao et al. , or long-range latent states¬†(Park et al., 2023; Wang et al., 2023).\n\n\nWhile both paradigms have shown improved performance in multi-step reasoning, their implementation on embodied robot agents raises practical issues. Embodied agents providing assistance often use small models (&lt;20&lt;20B parameters) that work locally on-device, with often limited or only cloud access to large memory storage. Moreover, these agents need to generalize to novel settings, making it preferable to have modular, lightweight segments that are easily transferrable.\n\n\nTo model a more efficient memory framework for embodied agents, we draw inspiration from how humans store memories of experiences. While performing various embodied tasks, humans do not accumulate every observation for later retrieval, but rather learn to actively filter out only certain vital fragments that we assume to be relevant to our task He et al. (2025). When queried, we reconstruct the missing fragments of memory through commonsense reasoning. This makes us humans highly efficient reasoners even with limited storage.\n\n\nWe aim to endow compact embodied agents with a similar ability: rather than relying on large external memory banks or complex retrieval pipelines, the agent must actively learn to store vital memories while filtering redundant ones on the go. Learning this skill across a wide range of tasks would enable scalable self-improvement under tight computational and memory budgets.\n\n\nMain Results:\nTo address these issues, we present MemCtrl, a transferrable memory augmentation scheme that aims to improve the embodied decision making performance of small models.\nMemCtrl introduces a trainable memory head that learns to selectively store memories of importance, increasing both parameter and memory efficiency for self-improving embodied agents.\nOur contributions are as follows:\n\n\n‚Ä¢\n\nActive Memory Filtering:\nWe introduce two lightweight memory heads Œº\\mu trained on top of a frozen MLLM backbone to actively filter observations to determine which to keep and which to discard in memory. Unlike prior retrieval-based work involving filtering large observational data offline, Œº\\mu enables the MLLM to engage in real-time filtering, which is particularly useful in memory-constrained settings involving small models.\n\n\n\n‚Ä¢\n\nTransferrable Heads:\nŒº\\mu is model-agnostic and attaches to any off-the-shelf MLLM without having to finetune or edit the backbone to remove redundant observations for more prudent decision making. This modular design allows MemCtrl to transfer across embodied setups and vision-language backbones, enabling its scalable transfer across embodied agents in diverse settings.\n\n\n\n‚Ä¢\n\nImproving Small Model Performance:\nFinally, attaching Œº\\mu to the worst performing agents on the Habitat Puig et al. (2023) and ALFRED Shridhar et al. (2020) splits of EmbodiedBench Yang et al. (2025) shows a significant improvement on task performance of around 16%16\\% on average, while also storing significantly fewer observations. This efficiency makes MemCtrl favorable for real-world deployment.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 MLLMs in Embodied AI\n\nSeveral works in recent literature have leveraged large language models (LLMs) for high-level robot planning. For example, Ahn et al. (2022) introduce a framework (SayCan) where an LLM translates natural-language instructions into feasible robot actions constrained by a set of learned skills. This approach demonstrated that LLMs can provide semantic task knowledge, but it relies on a fixed library of affordance-grounded skills and struggles with adapting to novel situations. Recent multimodal LLMs extend this idea by directly integrating visual inputs. For instance, PaLM-E (Driess et al., 2023) is a vision-language model that outputs robotic actions, achieving good generalizability across manipulation and navigation tasks. However, PaLM-E requires a lot of training data, limiting its real-time adaptability. Similarly, RT-2 (Zitkovich et al., 2023) augments a vision-language model with web-scale pretraining to create a vision-language-action (VLA) agent for improved zero-shot object understanding. However, it makes use of short temporal contexts without an explicit memory mechanism. In contrast, our work uses an MLLM as an active memory controller, enabling the agent to retain and recall cross-modal information over long horizons. Our design empowers embodied agents with small models to handle complex, extended tasks without requiring large-scale training.\n\n\n\n\n2.2 Memory-Augmented Agents\n\nRecent works have explored augmenting LLM-based embodied agents with memory to address challenges with long-horizon task solving. Mai et al. (2023) propose using an LLM itself as a ‚Äòrobotic brain‚Äô that maintains an egocentric memory of the agent‚Äôs observations and dialogue. Their system shows that a textual memory stream can help the agent refer back to important context, improving consistency in multi-step tasks. Another approach is to attach an external memory to the agent. In the HELPER framework (Sarch et al., 2023), they maintain a repository of dialogue-to-action examples, retrieving relevant past intera"
  },
  {
    "title": "VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring",
    "url": "https://arxiv.org/abs/2601.20830v1",
    "source": "arxiv",
    "summary": "Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (",
    "full_text": null
  },
  {
    "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
    "url": "https://arxiv.org/abs/2601.20829v1",
    "source": "arxiv",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix condi",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 RLVR and Saturated Problems\n\n3.1 GRPO Training\n3.2 Training with Saturated Problems\n\n\n\n4 Methodology\n\n4.1 Motivation\n4.2 Methodology: Failure-Prefix Conditioning\n\n\n\n5 Experiment\n\n5.1 Experiment Settings\n5.2 Main Results\n5.3 Ablation Study: Sensitivity to Target Accuracy œÑ\\tau\n\n\n\n6 Why Failure-Prefix Conditioning Works\n\n6.1 MDP Model\n6.2 Recovery from Failure Prefixes\n\n\n7 Iterative Failure-Prefix Conditioning\n8 Conclusion\n\nA Appendix\n\n\nA.1 GRPO Training Details\n\nLibrary\nPrompt Setting\nReward Function\nClip-Higher\nTraining Hyperparameters\n\n\n\nA.2 Evaluation Details\n\nLibraries\nInference Hyperparameters\nFull Evaluation Results\n\n\n\nA.3 Failure-Prefix-Conditioned Dataset Details\n\nDataset Details\nInference Settings for Measuring Rollout Accuracy\nIdentifying saturated questions\n\n\n\nA.4 Recovery from Failure Prefixes\n\nInference Settings.\n\n\n\nA.5 Derivation of Question-Level Weights and Their Relation to Reward Variance\n\nSetup\nNormalized advantage\nGRPO-style objective\nStructural assumption on ff\nDecomposition by reward outcomes.\nEmergence of the variance-based weight\nImplication\n\n\n\n\n\n\n\n\n\nTraining Reasoning Models on Saturated Problems \nvia Failure-Prefix Conditioning\n\n\nMinwu Kim\n\n‚ÄÉ‚ÄÉ\nSafal Shrestha\n\n‚ÄÉ‚ÄÉ\nKeith Ross\n\n\n\nAbstract\nReinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated.\nWe identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts.\nTo address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states.\nWe observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model‚Äôs robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.111Code available at https://github.com/minwukim/training-on-saturated-problems.\n\n\nFigure 1: Illustration of standard GRPO training and failure-prefix conditioning on saturated problems. While standard GRPO predominantly generates correct rollouts, failure-prefix conditioning exposes the model to failure-prone reasoning states, making informative failures more accessible.\n\n\n\n1 Introduction\n\nReinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning capabilities of large language models (LLMs) (Lambert et al., 2025; Guo et al., 2025; OpenAI, 2024). However, as models improve, an increasing number of training problems become saturated, meaning the model solves them correctly in nearly all rollouts. On these problems, training often stalls because rewards become nearly deterministic, leaving little learning signals.\n\n\nImportantly, saturation does not imply that the learning signal is exhausted. Even on problems with near-perfect rollout accuracy, incorrect reasoning trajectories still exist, albeit extremely rarely under standard sampling.\nAs established in prior work, such informative failures play a crucial role in training¬†(Setlur et al., 2025; Zhu et al., 2025b).\nHowever, most of the sampling budget is spent generating redundant correct solutions, with failures becoming exceedingly sparse¬†(Wang et al., 2025).\n\n\nThis suggests that the core challenge in learning from saturated problems is not the absence of informative failures, but their poor accessibility. If failures could be encountered more frequently, RLVR could continue to make progress even on saturated problems.\n\n\nTo this end, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems in RLVR. As shown in Figure¬†1, instead of sampling from the original question, this method explicitly targets failure-prone reasoning states.\nWe identify rare incorrect rollouts from saturated questions, slice them into prefixes, and select the specific prefix lengths that maximize the learning signal (i.e., target accuracy œÑ=0.5\\tau=0.5)¬†(Li et al., 2025). Training on the resulting failure-prefix-conditioned dataset reallocates exploration toward failure-prone regions of the response space, enabling effective learning from saturated problems.\n\n\nWe validate the effectiveness of this approach through an in-depth experimental study. Using DeepSeek-R1-Distill-Qwen-1.5B¬†(Guo et al., 2025) as our base model, we identify 1,000 saturated math questions where the model achieves a rollout accuracy 31/32 (‚âà97%\\approx 97\\%). We apply failure-prefix conditioning to these questions and train our failure-prefix model using RLVR on the resulting prefix-conditioned dataset. We compare our approach against two baselines: (i) the saturate model, trained via standard RLVR on the same 1,000 saturated questions without prefix conditioning, and (ii) the medium model, trained with standard RLVR on 1,000 medium-difficulty questions with rollout accuracy close to 50%, the regime commonly regarded as maximizes the learning signal¬†(Zhan et al., 2025; Li et al., 2025).\n\n\nWe evaluate these models on various math reasoning benchmarks spanning a wide range of difficulty. The saturate model shows negligible improvement relative to the base model, confirming that standard RLVR training stalls on saturated questions. In contrast, training on the failure-prefix-conditioned dataset yields consistent and substantial gains over the base model across all benchmarks. This performance is on par with that of the medium model, demonstrating that failure-prefix conditioning can effectively recover learning signal from saturated problems. Notably, these improvements are achieved without inflating response length, indicating that our methodology preserves token efficiency. We also demonstrate that our approach is robust to the target accuracy hyperparameter œÑ\\tau; while our default œÑ=0.5\\tau=0.5 yields the best performance, ablation studies confirm that the method achieves comparable peak performance across a range of values.\n\n\nNext, we analyze why failure-prefix conditioning is effective. While standard RLVR primarily incentivizes better decision-making from the initial state, failure-prefix conditioning explicitly trains the model to recover from incorrect intermediate reasoning states. Consequently, the method promotes robustness to misleading partial trajectories early in the reasoning process. Empirically, we find that the failure-prefix model exhibits greater robustness to failure prefixes, showing a smaller drop in rollout accuracy compared to baseline methods. Additionally, we also observe a mild trade-off, where improved robustness to failure prefixes can occasionally reduce adherence to correct intermediate reasoning.\n\n\nFinally, we explore whether learning from saturated problems can be further extended by iteratively refreshing failure prefixes as the model improves. As training progresses, performance under a fixed failure-prefix-conditioned dataset eventually plateaus, suggesting that previously informative prefixes may become less effective as the policy updates. Motivated by this observation, we perform a second iteration of failure-prefix conditioning, resampling new failures from the updated model and reconstructing the prefix-conditioned dataset. Empirically, this iterative procedure yields\nadditional gains beyond the initial plateau, suggesting that periodically updating failure prefixes may further recover additional learning signal and enable continued improvement.\n\n\nWe summarize our contributions as follows:\n\n\n‚Ä¢\n\nWe propose failure-prefix conditioning, a method that reallocates exploration toward failure-prone reasoning states to enable effective RLVR training on saturated problems.\n\n\n\n‚Ä¢\n\nWe show that failure-prefix conditioning enhances robustness to incorrect intermediate reasoning trajectories, enabling better recovery from misleading partial solutions compared to standard RLVR baselines.\n\n\n\n‚Ä¢\n\nWe demonstrate that iteratively refreshing failure prefixes can yield additional gains after performance plateaus, offering a potential pathway for further extending learning on saturated problems.\n\n\n\nOverall, our results indicate that failure-prefix conditioning provides an effective and efficient strategy to unlock the value of saturated training data.\n\n\n\n\n2 Related Work\n\nTask Difficulty for RL training of LLMs. Prior work has theoretically shown that policy updates in RL training of LLMs are biased by task difficulty, with the learning signal maximized when task success rates lie at an intermediate level (Razin et al., 2024; Li et al., 2025). Zhan et al. (2025) further empirically validates this observation by stratifying training tasks by difficulty and showing that performance improvements peak on moderate-difficulty tasks. Building on this insight, several approaches regulate task difficulty during RL training. DAPO dynamically filters prompts with extremely low or high rollout accuracy that contribute little to parameter updates¬†(Yu et al., 2025). Some approaches arrange static training tasks in increasing order of difficulty to improve training efficiency (Du et al., 2025b; Shi et al., 2025; Chen et al., 2025). Instead of reordering a fixed task set, RLVE introduces adaptive training environment that directly controls task difficu"
  },
  {
    "title": "Demystifying Prediction Powered Inference",
    "url": "https://arxiv.org/abs/2601.20819v1",
    "source": "arxiv",
    "summary": "Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from larg",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Prediction-Powered Inference: Framework and Extensions\n\n2.1 Basic setup and estimator\n2.2 Core assumptions and efficiency intuition\n\n2.3 A unified view of PPI variants\n\n2.3.1 Efficiency-guaranteeing refinements\n2.3.2 Robustness to labeling mechanism and accounting for distribution shift\n2.3.3 Beyond pre-trained model\n2.3.4 Handling missingness patterns\n\n\n\n\n\n3 Theoretical Connections with Statistics Literature\n\n3.1 Semiparametric efficiency and doubly robust estimation\n3.2 Control variates, survey sampling approaches, and finite-sample limits\n3.3 Bayesian inference\n3.4 Data integration and transfer learning\n\n\n\n4 Operationalizing PPI\n\n4.1 Step 1: Define the study and prepare data\n\n4.2 Step 2: Assess assumptions and data quality\n\nAssessing (A1):\nAssessing (A2):\nAssessing (A3):\n\n\n4.3 Step 3: Select, implement, and validate the appropriate PPI variate\n\n\n\n5 Empirical Evaluation with Mosaiks Housing Price\n\n5.1 Comparing PPI-based estimators for linear regression coefficients\n5.2 The effect of ‚Äúdouble-dipping‚Äù on inference validity\n5.3 Failure of PPI-based methods under the missing not at random setting\n\n\n\n6 Discussion &amp; Conclusion\n\n6.1 Strengths\n6.2 Weaknesses\n6.3 Opportunities\n6.4 Threats\n6.5 Concluding Remarks\n\n\n\n\n\n\n\n\n\\useunder\n\\ul\n\n\nDemystifying Prediction Powered Inference\n\n\nYilin Song\n\nDepartment of Biostatistics,\nColumbia Mailman School of Public Health\n\n\nDan M. Kluger\n\nInstitute for Data, Systems, and Society, Massachusetts Institute of Technology\n\n\nHarsh Parikh\nCo-corresponding author. Email: harsh.parikh@yale.edu\nDepartment of Biostatistics,\nYale School of Public Health\n\n\nTian Gu\nCo-corresponding author. Email: tg2880@cumc.columbia.edu\nDepartment of Biostatistics,\nColumbia Mailman School of Public Health\n\n\n\nAbstract\nMachine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.\n\n\n\nKeywords: Prediction-Powered Inference; Machine Learning; Missing Data; Semi-Supervised Learning; Valid Inference; Data Science\n\n\n\n1 Introduction\n\nScientific research has persistently faced the challenge that gold-standard measurements are expensive, invasive, or logistically burdensome to collect at scale, while the statistical power required to detect meaningful effects often demands large sample sizes. This tension pervades biomedical research, where imaging phenotypes or validated clinical endpoints may be available for only a fraction of study participants [Lou et al., 2021]; environmental science, where ground-truth measurements require costly field campaigns [Rolf et al., 2021a]; and social science, where high-quality survey responses are increasingly difficult to obtain [Pitts et al., 2025]. This has resulted in a growing body of studies with partially observed outcomes‚Äîlarge datasets in which the quantity of interest is measured for some units but is missing for many others.\n\n\nSimultaneously, advances in machine learning (ML) have produced increasingly accurate predictive models capable of imputing unmeasured outcomes from readily available covariates [Rolf et al., 2021a, McCaw et al., 2024, Chen et al., 2025]. Pre-trained models leveraging biobanks, electronic health records [Gu et al., 2023a, McCaw et al., 2024], satellite imagery [Rolf et al., 2021a, Proctor et al., 2023], or large language models [Fisch et al., 2024] can generate predictions for nearly any unit with observed features. These predictions often carry substantial information about the true outcomes, raising a natural question: how can investigators leverage ML predictions to improve statistical inference without sacrificing validity?\n\n\nHowever, treating predictions as ground truth risks systematic bias whenever the predictive model is imperfect. Confidence intervals may be narrow but centered on the wrong quantity and suffer from poor inferential guarantees. In contrast, ignoring predictions entirely and analyzing only labeled cases maintains validity but discards potentially valuable information, resulting in wider confidence intervals and reduced power. Neither extreme is satisfactory.\n\n\nPrediction-Powered Inference (PPI), introduced by Angelopoulos et al. [2023a], offers a principled middle path. The key insight is elegantly simple: use predictions from a large unlabeled sample to boost efficiency, but explicitly correct for prediction error using a smaller labeled subset where both predictions and true outcomes are observed. This bias-correction mechanism ensures that inferences remain valid regardless of prediction quality, with poor predictions merely failing to improve efficiency rather than compromising validity. When predictions are informative, PPI can yield substantially tighter confidence intervals than complete-case (CC) analysis while maintaining nominal coverage.\n\n\nDespite its conceptual elegance and practical promise, PPI remains underutilized outside specialized statistics and ML communities. We identify three barriers impeding broader adoption:\n\n\n\n\n1.\n\nConceptual barriers: When is PPI appropriate for a given study? What assumptions underpin its validity, and how restrictive are they in practice? What are the connections with existing literature? The answers are scattered across technical papers that assume substantial background in statistical theory.\n\n\n\n2.\n\nOperational barriers: How should practitioners implement PPI? What data structures are required? Which variant among the growing family of PPI methods should one choose? How can assumptions be assessed, or at least interrogated, with available data?\n\n\n\n3.\n\nInterpretational barriers: How should PPI results be communicated? What do efficiency gains (or their absence) imply about the underlying prediction model? What are the consequences of assumption violations, and how can practitioners avoid common pitfalls such as ‚Äúdouble-dipping‚Äù, using overlapping data for model training and inference?\n\n\n\n\n\nThis paper addresses these gaps by demystifying PPI and presenting a unified, accessible framework that synthesizes its theoretical foundations, methodological variants, and practical diagnostics. Our exposition is organized around resolving each of the three barriers:\n\n\n\n\n1.\n\nWe articulate the core identification assumptions‚Äîdistributional comparability between labeled and unlabeled samples, independence of the pre-trained model from inference data, and complete covariate availability‚Äîand describe how different PPI variants relax or modify these requirements (Section¬†2). In Section 3, we discuss its theoretical foundations and connections with exisiting statistics literature.\n\n\n\n2.\n\nWe provide a step-by-step workflow for implementation, including data preparation, assumption diagnostics, method selection, and result validation (Section¬†4).\n\n\n\n3.\n\nWe offer guidance on interpreting and reporting PPI results, including how to understand efficiency gains and recognize potential limitations, concluding with future directions (Section¬†6).\n\n\n\n\n\nIn Section¬†5, using the MOSAIKS housing dataset [Rolf et al., 2021a], we empirically reveal both the promise and the pitfalls of PPI. We find that PPI and its variants can yield narrower confidence intervals than CC analysis when predictions are informative. However, performance deteriorates under violations such as ‚Äúdouble-dipping‚Äù and missing-not-at-random (MNAR) mechanisms, emphasizing the importance of satisfying PPI‚Äôs core assumptions.\n\n\nBeyond the methodological synthesis, we contribute several practical resources: a decision flowchart linking assumption violations to appropriate PPI variants (Figure¬†2), diagnostic strategies for evaluating assumptions in applied settings (Table¬†1), and a summary table of selective methods with their core assumptions and recommended use cases (Table¬†3).\n\n\nBy framing PPI as a general recipe rather than a single estimator, this paper aims to bridge methodological innovation and applied practice. We hope to enable researchers across disciplines to responsibly leverage the predictive power of modern ML for more efficient and valid statistical inference.\n\n\nFigure 1: Overall workflow of the practical guide of using PPI.\n\n\n\n\n2 Prediction-Powered Inference: Framework and Extensions\n\nPPI is designed for studies where the outcome of interest is difficult or costly to measure for every unit, but prediction models are available for a much larger unlabele"
  },
  {
    "title": "GNN Explanations that do not Explain and How to find Them",
    "url": "https://arxiv.org/abs/2601.20815v1",
    "source": "arxiv",
    "summary": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical",
    "full_text": null
  },
  {
    "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs",
    "url": "https://arxiv.org/abs/2601.20810v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.20810v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Software Engineering\n    \n\n    \n      arXiv:2601.20810v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 28 Jan 2026]\n    Title:Context-Augmented Code Generation Using Programming Knowledge Graphs\n    Authors:Shahd Seddik, Fahd Seddik, Iman Saberi, Fatemeh Fard, Minh Hieu Huynh, Patanamon Thongtanunam            View a PDF of the paper titled Context-Augmented Code Generation Using Programming Knowledge Graphs, by Shahd Seddik and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at this https URL\n    \n\n    \n    \n      \n          Subjects:\n          \n            Software Engineering (cs.SE); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.20810 [cs.SE]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.20810v1 [cs.SE] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.20810\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Fahd Seddik [view email]          [v1]\n        Wed, 28 Jan 2026 17:58:30 UTC (5,841 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Context-Augmented Code Generation Using Programming Knowledge Graphs, by Shahd Seddik and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.SE\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick he"
  },
  {
    "title": "Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction",
    "url": "https://arxiv.org/abs/2601.20803v1",
    "source": "arxiv",
    "summary": "This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary w",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nFew-Shot Relation Extraction\nExample selection for In-Context Learning\n\n\n\n3 The Few-Shot Relation Extraction Task\n\nIn-Context Learning for Few-Shot Relation Extraction\n\n\n\n4 From One-Shot Relation Extraction to Many-Shot Relation Extraction\n\n4.1 Obtaining Examples with LLMs\n\n4.2 Retrieving Examples from Unannotated Corpora Using Syntactic-Semantic Representations\n\nSentence Representations from SBERT\nSyntactic-Semantic Rule Representations\nEnhancing Diversity\n\n\n4.3 Hybrid Example Selection\n4.4 Summarizing Examples\n\n\n\n5 Experimental Setup\n\nDatasets\nLLMs and Implementation Details\nBaselines and Evaluation Metric\n\n\n\n6 Results\n\nStructured semantic information helps select better examples for ICL\nRetrieved examples complement LLM generated examples\nSemantic rule representations yield better results than SBERT and paraphrasing with LLMs\nSemantic rule representations work better with smaller LLMs\nICL with additional examples retrieved with semantic rule representations outperforms previous work\n\n\n\n7 Qualitative Analyses\n\nStructured semantic retrieval yields more informative examples\nDiverse examples do not always yield better results\nAdditional examples are not always beneficial\nBalanced similarity‚Äìdiversity trade-off works best\nIdentifying multiple relations at once with a single ICL hurts performance\nNER-filtering as preprocessing consistently improves results\n\n\n8 Conclusions\n\nA Appendix\n\nA.1 Hyperparameter Tuning and Retrieval Efficiency\nA.2 Decoding and Inference Details\n\n\n\n\n\n\n\nStructured Semantic Information Helps Retrieve Better Examples\nfor In-Context Learning in Few-Shot Relation Extraction\n\n\nAunabil Chakma, Mihai Surdeanu, and Eduardo Blanco\nUniversity of Arizona, Tucson, AZ, USA \n{aunabilchakma, msurdeanu, eduardoblanco}@arizona.edu\n\n\n\n\nAbstract\nThis paper presents several strategies to automatically obtain\nadditional examples for in-context learning of one-shot relation extraction.\nSpecifically, we introduce a novel strategy for example selection,\nin which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example.\nWe show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples.\nWhen these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone.\nOur framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma).\nOverall, our hybrid selection method consistently outperforms alternative strategies\nand achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.\n\n\n\n\nStructured Semantic Information Helps Retrieve Better Examples\nfor In-Context Learning in Few-Shot Relation Extraction\n\n\n\n\nAunabil Chakma, Mihai Surdeanu, and Eduardo Blanco\n\nUniversity of Arizona, Tucson, AZ, USA\n\n{aunabilchakma, msurdeanu, eduardoblanco}@arizona.edu\n\n\n\n\n\n\n1 Introduction\n\nRelation extraction (RE) is a core NLP task that\nidentifies relations between entity pairs¬†(e.g., the place of birth or employer of a person).\nExtracting relations is often a component of other tasks such as knowledge graph construction¬†Schneider et al. (2022).\nIn turn, knowledge graphs coupled with retrieval-augmented approaches have been shown\nto improve reasoning¬†Sui et al. (2025) and\nreduce hallucinations¬†Wagner et al. (2025) with LLMs,\nthus potentially benefiting any end-user application including question answering¬†Xu et al. (2025).\n\n\nTraditional relation extraction approaches require many training examples per relation¬†Marsh (1998).\nMore recently, few-shot relation extraction (FSRE) has received substantial attention¬†Sabo et al. (2021); Alam et al. (2024).\nIn this setting, relations must be extracted given nothing but the relation name, a brief description, and a handful of examples.\nBecause of this challenging setting,\nprevious work obtains relatively low results (e.g., ‚âà\\approx24¬†F1 on FS-TACRED).\nExisting approaches to FSRE\ninclude those grounded in prototypes¬†Meng et al. (2023); Hu et al. (2025)\nand typically require training or fine-tuning¬†(Section¬†2).\n\n\nFigure 1: Our approach to in-context learning for 1-shot relation extraction.\nOur method complements the (gold) 1-shot (\"Support sentence\" in the figure) with additional examples generated using shallower LLM-based methods (i.e., paraphrasing the gold example, or generating new examples based on the relation definition) or deeper methods using the underlying syntactic-semantic representations of candidate examples.\nWhen combining these strategies, the resulting few-shot relation extraction system achieves better results across two corpora, across several small language models. \n\n\nIn-context learning (ICL) with LLMs¬†(Brown et al., 2020)\nfor FSRE is a natural choice yet it remains under-explored. In particular, the best strategy for example selection in RE remains unknown.\nTo address this gap, in this paper we explore several strategies to automatically obtain additional examples for the relation of interest.\nIn particular, we explore two main strategies: using LLMs to generate new examples, and retrieving examples from unannotated corpora based on their deeper syntactic-semantic similarity with the provided one-shot example.\n\n\nFigure 1 presents our strategies to obtain additional examples for one-shot relation extraction.\nOur main novelty is to automatically combine the one-shot gold example with additional examples that are: (a) generated with LLMs, or (b) retrieved from unannotated corpora using their syntactic-semantic similarity with the provided example.\nFor LLM-based generation, we either paraphrase the gold example while preserving the subject, object, and relation,\nor generate new examples conditioned on the relation name and its natural-language definition, encouraging alternative lexicalizations and sentence structures.\nFor the retrieval methods, we rely on lexico‚Äësyntactic rules, which denote structured representations that capture how the subject and object are connected in the sentence\nvia their core syntactic, lexical, and semantic context; full details are given in Section¬†4.2.\nImportantly, the semantic representations of these rules are learned in a self-supervised fashion that is agnostic to the relations of interest, similar to word embeddings¬†Mikolov et al. (2013).\nFurther, we emphasize diversity in the retrieved examples by: (a)\nclustering candidates using their semantic representations and selecting one representative example per cluster, and (b) using an LLM to select the most diverse examples from a pool of example candidates created with the above strategies.\n\n\nThe main contributions of this paper are:\n\n\n\n\n‚Ä¢\n\nSeveral strategies to obtain additional examples for FSRE that range from shallower (e.g., LLM paraphrasing) to deeper (i.e., using representations of the syntactic-semantic structures of the texts of interest).\n\n\n\n‚Ä¢\n\nMethods to emphasize example diversity (i.e., diverse word choice and sentence structures) from a pool of candidate examples constructed using the above strategies. We investigate two directions, one based on the explicit clustering of examples and selecting one representative example per cluster, and a second based on prompt engineering.\n\n\n\n‚Ä¢\n\nDemonstrating that a hybrid approach balancing similarity and diversity obtains new state-of-the-art (SOTA) performance for FSRE, especially when smaller language models are used.\n\n\n\n\n\n\n\n2 Related Work\n\nEarly approaches and datasets for RE\nwere designed for supervised learning¬†Marsh (1998); Doddington et al. (2004).\nThese approaches are effective when trained on hundreds or thousands of examples per relation¬†Miwa and Bansal (2016); Zhou et al. (2005).\nHowever, they rarely generalize across domains (i.e., same relation types expressed in documents from different sources)\nand can only identify at inference time the relations they were trained on.\nDistant supervision¬†Mintz et al. (2009) has been explored to eliminate manual annotations,\nbut other limitations remain, e.g., noise in the synthetic training data.\n\n\nFew-Shot Relation Extraction\n\nMost modern FSRE methods bypass the need to know the relations to be extracted at inference time and avoid manual annotations beyond a handful examples per relation.\nRepresentative approaches include prototype-based classification\nusing aggregated relation representations¬†(Meng et al., 2023; Hu et al., 2025).\nEmbedding-based nearest-neighbor methods are also widely used (Sabo et al., 2021).\nWhile effective, these methods lack flexibility under relation or domain shifts. In contrast, neuro-symbolic approaches emphasize explicit relational cues (Vacareanu et al., 2024; Singh and Blanco, 2024), which improves their in- and out-of-domain performance.\nThese properties motivate our approach to retrieve examples grounded in syntactic-semantic representations.\n\n\nWhile we are not the first to use in-context learning and LLMs for FSRE¬†(Ma et al., 2023; Xu et al., 2023),\nwe are the first to focus on improving ICL for FSRE with several strategies for example selection that improve both similarity and diversity. Unlike previous work, we demonstrate that our approach transfers across benchmarks and LLMs without needing any changes\n(e.g., prompt engineering).111We do not compare our results with Ma et al. (2023) and Xu et al. (2023)\nbecause they work with customized few-shot versions of TACRED and FewRel for which the exact few-shot evaluation setups are not publicly available.\n\n\n\nExample selection for In-Context Learning\n\nOutside RE, prior work has studied example selection\nfor ICL for several NLP tasks (Li and Qiu, 2023; Chen et al., 2024; Scarlatos and Lan, 2024; Wang et al., 2024).\nExisting approaches mainly rely on embedding-based similarity\nor learning-based selection using task supervision, model feedback, and diversity-aware criteria.\nThese methods improve in-context "
  },
  {
    "title": "Reinforcement Learning via Self-Distillation",
    "url": "https://arxiv.org/abs/2601.20802v1",
    "source": "arxiv",
    "summary": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or ",
    "full_text": "\n\n\n\n\n1 Introduction\n\nSummary of evaluation results.\n\n\n\n2 SDPO: Self-Distillation Policy Optimization\n\n2.1 Comparison to RLVR\n2.2 Compute time &amp; memory\n2.3 Stability improvements\n\n\n\n3 Learning without Rich Environment Feedback\n\n\n3.1 Experimental setting\n\nBaselines.\n\n\n3.2 Results\n3.3 Self-distillation learns to reason concisely\n\n\n\n4 Learning with Rich Environment Feedback\n\nResults.\n4.1 Self-distillation benefits from stronger models\n4.2 Self-distillation performs dense credit assignment\n4.3 The self-teacher improves during training\n\n4.4 On-policy self-distillation avoids catastrophic forgetting\n\nOff-policy self-distillation baseline.\n\n\n4.5 Can GRPO and SDPO be combined?\n\n4.6 Which feedback is most informative?\n\nSample solutions.\nEnvironment output.\nStudent‚Äôs original attempt.\n\n\n\n\n\n5 Solving Hard Questions via Test-Time Self-Distillation\n\n5.1 Experimental setting\n\n5.2 Results\n\nQuestion 3 is only solved by SDPO.\nThe initial self-teacher does not solve hard questions.\n\n\n\n\n\n6 Related Work\n\n6.1 Reinforcement Learning with LLMs\n6.2 Learning from Rich Feedback and through Retrospection\n6.3 Distillation\n6.4 Self-Distillation\n\n\n\n7 Conclusion, Limitations, and Future Work\n\nLimitations.\nFuture Work.\n\n\n\nA Implementation of SDPO\n\nA.1 Regularized teacher\nA.2 Approximate Logit Distillation\nA.3 Off-Policy Training: Generalization to Logit-Level Losses\n\n\n\nB Theoretical Analysis\n\nB.1 Gradient Estimator\nB.2 Trust-region Teacher\nB.3 EMA Teacher as an Implicit Trust Region\n\n\n\nC Additional Related Work\n\nValue networks and Monte Carlo advantage estimation.\nDense credit assignment with a reward model.\nPartial observability.\nRelation to test-time training.\n\nC.1 SDPO as Maximum Entropy RL\n\nMaximum Entropy RL\nSDPO optimizes an implicit reward defined by the teacher\n\n\n\n\n\nD Additional Results &amp; Ablations\n\nD.1 Learning without rich environment feedback\n\nD.2 Learning with rich environment feedback\n\nD.2.1 Additional Results\nD.2.2 Training Stability\nD.2.3 Baselines\n\n\nD.3 Test-time self-distillation\n\n\n\nE Experiment Details\n\nE.1 Technical setup\n\nE.2 Hyperparameters\n\nE.2.1 Details on Hyperparameter Selection (SectionÀú3)\n\n\nE.3 User Templates\n\n\n\nF Qualitative Examples\n\nF.1 Visualization of Advantages\nF.2 Examples\nF.3 Environment Feedback\nF.4 Illustrative Example\n\n\n\n\n\n\n\nReinforcement Learning via Self-Distillation\n\n\nJonas H√ºbotter11 ‚ÄÑ Frederike L√ºbeck‚Äâ‚Äâ,1,2,1,2 ‚ÄÑ Lejs Behric111In standard RLVR implementations a rollout group contains multiple simultaneous attempts for¬†xx.‚Äâ‚Äâ,1,1 ‚ÄÑ Anton Baumann111In standard RLVR implementations a rollout group contains multiple simultaneous attempts for¬†xx.‚Äâ‚Äâ,1,1\nMarco Bagatella1,21,2 ‚ÄÑ Daniel Marta11 ‚ÄÑ Ido Hakimi11 ‚ÄÑ Idan Shenfeld33\nThomas Kleine Buening11 ‚ÄÑ Carlos Guestrin44 ‚ÄÑ Andreas Krause11\n11ETH Zurich ‚ÄÑ 22Max Planck Institute for Intelligent Systems ‚ÄÑ 33MIT ‚ÄÑ 44Stanford\n\n\nhttps://github.com/lasgroup/SDPO\n\nEqual second authorship. Correspondence to jonas.huebotter@inf.ethz.ch.\n\n\nAbstract\nLarge language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math.\nYet, current methods for reinforcement learning with verifiable rewards¬†(RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck.\nMany verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed.\nWe formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization¬†(SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model.\nSDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy.\nIn this way, SDPO leverages the model‚Äôs ability to retrospectively identify its own mistakes in-context.\nAcross scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines.\nNotably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts.\nFinally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-kk sampling or multi-turn conversations with 3√ó3\\times fewer attempts.\n\n\n\n1 Introduction\n\n\n\n\n\n\n\n\nFigure 1: \nSDPO substantially outperforms an improved version of Group Relative Policy Optimization¬†(GRPO) on LCB¬†v6 with Qwen3-8B. Further, SDPO achieves GRPO‚Äôs final accuracy in 4√ó4\\times fewer generations.\nClaude Sonnet 4 is the strongest instruct model on the public LCBv6 leaderboard.\nShaded regions show the standard deviation across 3 seeds.\n\n\nProgress in deep reinforcement learning has shown that iterating on experience‚Äîacting, receiving feedback, and updating a policy‚Äîcan unlock capabilities that are difficult to obtain from static supervision alone¬†(Mnih et¬†al., 2015; Silver et¬†al., 2016; 2017; Berner et¬†al., 2019).\nThe same theme now appears in large language models (LLMs): large-scale post-training with reinforcement learning (RL) has substantially improved performance on reasoning-heavy tasks, especially in settings with programmatic or otherwise verifiable evaluation¬†(Jaech et¬†al., 2024; Guo et¬†al., 2025; Kimi et¬†al., 2025; Olmo et¬†al., 2025).\n\n\nNevertheless, the dominant RL recipe for LLM post-training remains bottlenecked by credit assignment.\nMost current approaches operate in the setting of reinforcement learning with verifiable rewards (RLVR): given a question¬†xx, the model samples an answer y‚àºœÄŒ∏(‚ãÖ‚à£x){y\\sim\\pi_{\\theta}(\\cdot\\mid x)} and receives a scalar reward r‚àà‚Ñùr\\in\\mathbb{R}, often binary (e.g., unit-tests pass/fail in code generation).\nModern policy gradient RLVR methods such as Group Relative Policy Optimization¬†(GRPO; Shao et¬†al., 2024) estimate advantages from these sparse outcome rewards.\nFurthermore, when all rollouts in a group receive the same (often zero) reward, GRPO advantages collapse to zero and learning stalls.\nTo overcome this sparsity, one might prefer distillation from a strong teacher¬†(Guo et¬†al., 2025; Yang et¬†al., 2025; Lu &amp; Thinking Machines Lab, 2025; Guha et¬†al., 2026), which provides dense, token-level supervision.\nHowever, strong teachers are often unavailable in online learning, where the goal is to raise the capability ceiling beyond existing models.\n\n\nIn this work, we argue that the key limitation is not RL per se, but the information bottleneck imposed by scalar outcome rewards.\nMany verifiable environments expose rich tokenized feedback beyond scalar rewards¬†rr, such as runtime errors, failing unit tests, or evaluations from an LLM judge.\nThis feedback not only reveals whether a rollout was wrong, but also what went wrong.\nWe formalize this more general setting as Reinforcement Learning with Rich Feedback (RLRF) and illustrate its difference to RLVR in FigureÀú2.\nHere, feedback can be any tokenized representation of any state reached by an agentic system.\nThe central question becomes: how can we convert rich feedback into effective credit assignment without requiring external supervision from a strong teacher?\n\n\nFigure 2: \nComparison of RLVR and RLRF settings.\nIn Reinforcement Learning with Verifiable Rewards (RLVR), the agent learns from a scalar reward rr, which often acts as an information bottleneck by masking the underlying environment state.\nIn contrast, Reinforcement Learning with Rich Feedback (RLRF) utilizes tokenized feedback.\nThis provides a significantly richer signal than a scalar reward, as the feedback can encapsulate both the reward as well as detailed observations of the state¬†(such as runtime errors from a code environment or feedback from an LLM judge).\n\n\n\n‚¨á\n\n\nRuntime Error\n\nZeroDivisionError: division by zero\n\nLine 73 in separateSquares (Solution.py)\n\n\\parLast Executed Input\n\n[[26,30,2],[11,23,1]]\n\n\n\nFigure 3: Example of feedback from our code environment, inspired by LeetCode. Listings¬†LABEL:lst:feedback_example_wrong_answer,¬†LABEL:lst:memory_error, and¬†LABEL:lst:index_error in the appendix show examples of feedback in case of a wrong answer, a memory error, and an index error.\n\n\nOur starting point is the observation that LLMs already possess a powerful mechanism for using feedback: in-context learning¬†(Brown et¬†al., 2020; Wei et¬†al., 2022).\nWhen conditioned on feedback, the same model can often identify plausible mistakes and propose a corrected approach.\nA common example of such feedback is the summary of failed test cases on coding platforms like LeetCode (FigureÀú3).\nMany recent works leverage this capability to iteratively generate corrections¬†(Chen et¬†al., 2021a; Madaan et¬†al., 2023; Shinn et¬†al., 2023; Yao et¬†al., 2024; Yuksekgonul et¬†al., 2025; Lee et¬†al., 2025).\nIn contrast, we use the current policy as a ‚Äúself-teacher‚Äù that, rather than sampling a new response, re-evaluates the existing rollout after receiving rich feedback.\nIncluding the feedback in-context transforms the model‚Äôs next-token distribution, allowing the self-teacher to agree or disagree with the student‚Äôs original choices at specific tokens.\nThis yields dense, logit-level credit assignment.\nFor example, when provided with the feedback from FigureÀú3, the self-teacher can identify how the initial attempt should be modified to avoid the runtime error.\nCrucially, this mechanism incurs no sampling overhead: we simply re-compute the log-probabilities of the original attempt under the self-teacher‚Äôs feedback-augmented context.\n\n\nBuilding on this idea, we introduce Self-Distillation Policy Optimization (SDPO), an on-policy algorithm that performs RL via self-distillation.\nSDPO samples rollouts from the current policy, obtains rich environment feedback, and then minimizes a logit-level distillation loss that matches the current policy‚Äôs next-t"
  },
  {
    "title": "Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces",
    "url": "https://arxiv.org/abs/2601.20800v1",
    "source": "arxiv",
    "summary": "We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, uncondi",
    "full_text": null
  },
  {
    "title": "Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers",
    "url": "https://arxiv.org/abs/2601.20796v1",
    "source": "arxiv",
    "summary": "Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data st",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries\n\n3 Establishing Architectural Premises: ICL in Modern Transformers\n\n3.1 Data statistical principles hold within modern architectures.\n3.2 Model scaling raises the ICL threshold.\n3.3 RoPE raises the data complexity threshold for ICL.\n\n\n\n4 Multimodal In-context Learning\n\n4.1 Data Asymmetries Reveal a Primary and Secondary Role for Modalities.\n4.2 The Decoder‚Äôs Architecture Has Contrasting Effects on Multimodal ICL\n4.3 The Role of the Encoder: Cross-Modal Alignment and Feature Quality\n\n4.4 Quantifying ICL Mechanisms\n\n4.4.1 progress measurements\n4.4.2 ICL Dynamics: From Formation to Refinement\n4.4.3 Validating the Mechanism\n4.4.4 Observations in MLLMs\n\n\n\n\n5 Related Work\n6 Conclusion\n\nA Appendix\n\n\nA.1 Preliminaries\n\nEvaluation metrics.\n\n\n\nA.2 Extended results and analysis in unimodal experiments\n\nA.2.1 Unimodal data distributional properties\n\nA.2.2 Impact of Positional Encodings, Context Length, and Data Complexity\n\nPearson Correlation.\nRandom forest prediction.\n\n\n\n\n\nA.3 Extended results in multimodal experiments\n\nA.3.1 RoPE continues to raise the data complexity threshold for multimodal ICL.\nA.3.2 Impact of Feature Dimensionality on Cross-Modal Alignment\nA.3.3 Data distributional findings transfer to real image data.\nA.3.4 How does the pretraining dataset for the encoder impact downstream multimodal ICL?\n\nA.3.5 Progress measurement analysis\n\nPearson Correlation.\nRandom forest prediction.\n\n\nA.3.6 Cross-Modal Interaction Analysis via Modality Zeroing\nA.3.7 Early-Fusion Joint Training\n\n\n\nA.4 Additional Evidence on Production MLLMs\n\nVL-ICL benchmark results.\nCircuit analysis and head measurements.\n\n\n\n\n\n\n\n\n\nDissecting Multimodal In-Context Learning: \nModality Asymmetries and Circuit Dynamics in modern Transformers\n\n\nYiran Huang1,2, Karsten Roth3, Quentin Bouniot1,2, Wenjia Xu4, Zeynep Akata1,2\n1Technical University of Munich, MCML, Germany\n2Helmholtz Munich, Germany\n3Google DeepMind, Switzerland\n4Beijing University of Posts and Telecommunications, China\n\n\n\n\nAbstract\nTransformer-based multimodal large language models often exhibit in-context learning¬†(ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples?\nWe investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture.\nWe begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.\n\n\n\n\n1 Introduction\n\nFigure 1: Preliminaries in the multimodal setting. (a) The context consists of NN triplets followed by the target query. The paired examples (xi,xi‚Ä≤)(x_{i},x_{i}^{\\prime}) from two modalities, with a shared label lil_{i}, are generated from Gaussian Mixture Models (GMMs) by controlling within-class variation Œµ1\\varepsilon_{1} and Œµ2\\varepsilon_{2}.\n(b) The distributional properties for the synthetic data. The burstiness BB determines how often the class occurs in the context. Class frequencies follow a Zipfian distribution with exponents Œ±1\\alpha_{1} and Œ±2\\alpha_{2}.\n(c) Evaluation distinguishes between IWL, where target queries belong to class seen during training while not in the context during evaluation, and ICL, where target queries are novel but in the context. A swapped-label condition further isolates ICL by permuting the labels.\n\n\n\nIn-context learning (ICL), the ability to perform new tasks from demonstrations without parameter updates (Radford et al., 2019; Brown et al., 2020), has emerged as a core capability of transformer-based large language models (LLMs). Understanding the origins and mechanisms of this capability has become a central focus of transformer research.\n\n\nFor unimodal ICL, progress has been made on both fronts. On the training side, studies have identified statistical properties of pretraining data, such as burstiness, high class diversity, and skew, that promote ICL over in-weight learning (IWL) (Chan et al., 2022; Singh et al., 2023; Chan et al., 2024; Zucchet et al., 2025). Fundamentally, these mechanisms compete to minimize loss: while IWL relies on memorizing static input-label mappings in parameters, ICL emerges when high data diversity makes such memorization inefficient, forcing the model to rely on context. On the mechanistic side, researchers have traced ICL to specialized induction circuits: attention patterns that retrieve contextually relevant examples and copy their associated labels to the query (Olsson et al., 2022; Crosbie and Shutova, 2025; Reddy, 2024). However, these mechanistic insights derive primarily from simplified transformers that lack key architectural components of modern LLMs, such as RoPE.\n\n\nRecent advances in multimodal large language models (MLLMs) (Alayrac et al., 2022; Abdin et al., 2024; Hui et al., 2024) have extended ICL to interleaved image-text demonstrations, enabling cross-modal reasoning from contextual examples alone. This observation raises a fundamental question about transformer learning:\n\n\nWhat enables transformers to perform multimodal ICL, both in terms of training data statistics and mechanisms?\n\n\nTo answer this question, this paper provides a systematic, reverse-engineering account of multimodal ICL in modern transformers. We investigate how statistical principles governing unimodal ICL in simplified transformers¬†(Reddy, 2024) transfer to modern LLM-style architectures, how they extend to multimodal settings, what underlying attention circuits implement multimodal ICL, and how they relate to their unimodal counterparts. To isolate causal factors, we train small but architecturally realistic two-layer transformer models on controllable synthetic classification tasks. This controlled environment allows us to systematically vary data statistics and architectural components while tracking the formation of attention patterns throughout training‚Äîan analysis that would be impossible with real-world pretraining corpora, where multiple variables are confounded.\n\n\nOur investigation yields the following key findings:\n(1) While the statistical drivers of unimodal ICL in simplified transformers transfer to modern architectures, RoPE raises the data complexity threshold for ICL. Furthermore, scaling up unimodal models favors in-weight memorization over ICL at fixed data complexities (Sec.¬†3).\n(2) Multimodal ICL exhibits a strong learning asymmetry. When the model is pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge¬†(Sec.¬†4.1). Consequently, and unlike the unimodal case, scaling consistently improves multimodal ICL. The added capacity is utilized to map the secondary modality to the pre-existing circuit rather than for memorization (Sec.¬†4.2).\n(3) Multimodal ICL performance is bottlenecked by the modality alignment; a pretrained encoder is important for achieving this alignment (Sec.¬†4.3).\n(4) Beyond synthetic data, we validate our distributional findings and the importance of an encoder on Omniglot (Lake et al., 2019).\n(5) Mechanistically, induction circuit strength tracks ICL accuracy in both unimodal and multimodal settings. Multimodal training primarily refines the induction head responsible for label matching (Sec.¬†4.4).\n\n\nIn summary, our contributions are: (1) the first mechanistic account of multimodal ICL in transformers; (2) a controlled testbed for investigating how architectural choices and data statistics shape ICL across modalities; and (3) the discovery of learning asymmetry between modalities and characterization of the underlying circuit dynamics.\n\n\nFigure 2: \n(a) Impact of increasing model layers and attention heads on ICL-IWL tradeoff. Scaling up favours IWL over ICL for five seeds.\n(b) The data complexity (measured by K‚ãÖBK\\cdot\\sqrt{B}) required for models of different sizes to achieve the same ICL accuracy (&gt;0.95&gt;0.95). A larger model needs more complex data for a strong ICL capability.\n(c) Across data regimes, RoPE yields lower ICL accuracy than absolute positional encodings (APE).\n(d) Attention maps for an example with the correct label at position 5: absolute PE shows clear previous-token and induction heads; with RoPE these patterns are diminished. Here K=8192,B=1,Œ±=0K=8192,B=1,\\alpha=0 except when that parameter is varied.\n\n\n\n\n2 Preliminaries\n\nTask description.‚ÄÖ\nLet ùí≥1\\mathcal{X}_{1} and ùí≥2\\mathcal{X}_{2} denote the input spaces of two modalities and let ‚Ñí={L1,‚Ä¶,Ln}\\mathcal{L}=\\{L_{1},\\dots,L_{n}\\} be the shared label set. We feed the model with a context consisting of NN labelled exemplars followed by an unlabelled query.\nIn the unimodal setting, we follow Reddy (2024), with context comprising NN item‚Äìlabel pairs from a single modality:\nx1,‚Ñì1,x2,‚Ñì2,‚Ä¶,xN,‚ÑìN,xqx_{1},\\ell_{1},x_{2},\\ell_{2},\\ldots,x_{N},\\ell_{N},x_{q}. Each xi‚ààùí≥1x_{i}\\in\\mathcal{X}_{1} is an example whose ground‚Äëtruth label is ‚Ñìi‚àà‚Ñí\\ell_{i}\\in\\mathcal{L}. The model must predict ‚Ñìq\\ell_{q} for the query item xqx_{q}¬†(see App.¬†A.1).\nIn the multimodal setting, we extend the task by presenting paired exemplars from two modalities: x1,x1‚Ä≤,‚Ñì1,x2,x2‚Ä≤,‚Ñì2,‚Ä¶,xN,xN‚Ä≤,‚ÑìN,xq,x‚Ä≤‚Äãqx_{1},x^{\\prime}_{1},\\ell_{1},x_{2},x"
  },
  {
    "title": "Jurisdiction as Structural Barrier: How Privacy Policy Organization May Reduce Visibility of Substantive Disclosures",
    "url": "https://arxiv.org/abs/2601.20792v1",
    "source": "arxiv",
    "summary": "Privacy policies are supposed to provide notice. But what if substantive information appears only where users skip it? We identify a structural pattern we call jurisdiction-siloed disclosure: information about data practices appearing in specific, actionable form only within regional compliance sections labeled \"California Residents\" or \"EU/UK Users,\" while general sections use vague or qualified ",
    "full_text": null
  },
  {
    "title": "FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models",
    "url": "https://arxiv.org/abs/2601.20791v1",
    "source": "arxiv",
    "summary": "Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text en",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Related Work\n\n2.1 Text-to-Video Diffusion Models\n2.2 Demographic Bias in Generative Models\n\n\n3 FairT2V: Gender Bias Analysis\n4 FairT2V: Training-Free Debiasing\n\n5 Evaluation: Results and Analysis\n\n5.1 Experiment Setup\n5.2 Evaluation Metrics\n5.3 Main Results\n5.4 User Study\n5.5 Ablation Studies\n\n\n6 Conclusion\n\nA Additional User Study Details\n\nA.1 Experiment Setup\nA.2 Ethical Considerations\n\n\n\n\n\n\n\nFAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models\n\n\nHaonan Zhong\n\n‚ÄÉ‚ÄÉ\nWei Song‚Ä†\n\n‚ÄÉ‚ÄÉ\nTingxu Han\n\n‚ÄÉ‚ÄÉ\nMaurice Pagnucco\n\n‚ÄÉ‚ÄÉ\nJingling Xue\n\n‚ÄÉ‚ÄÉ\nYang Song\n\n\n\nAbstract\nText-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases‚Äîparticularly gender bias‚Äîremain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without fine-tuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.\nBased on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nText-to-video (T2V) models have progressed rapidly in recent years, driven by advances in diffusion-based architectures¬†(Peebles and Xie, 2023; Ramesh et al., 2021; Liu et al., 2024; Yang et al., 2024; Zheng et al., 2024). These systems are increasingly deployed in applications such as advertising, education, and entertainment. However, recent studies¬†(Cho et al., 2023; Luccioni et al., 2023; Harrison Rosenberg et al., 2023; Nadeem et al., 2025) show that modern generative models frequently over-represent gender, racial, or age groups. Such demographic biases raise significant ethical concerns, as they risk reinforcing harmful societal stereotypes and perpetuating representational inequities¬†(Friedrich et al., 2023; Sakurai and Sato, 2025).\n\n\nDespite growing interest in fairness for generative models, demographic bias in text-to-video generation remains largely unexplored. Existing studies have focused primarily on text-to-image (T2I) generation, leaving open the question of how bias manifests and propagates in video generation, where temporal dynamics, identity persistence, and repeated text conditioning introduce new challenges.\nIn this work, we focus on gender bias associated with occupation, as it is a well-documented and measurable form of demographic bias that enables controlled analysis in the text-to-video setting.\n\n\nUnlike images, gender-related bias in videos can be reinforced across frames, leading to temporally persistent stereotypes. Moreover, long-range temporal structure and multiple identities make frame-wise or image-level fairness interventions insufficient and inconsistent.\n\n\nIn this paper, we present FairT2V, the first systematic study of demographic bias in text-to-video generation. We formalize implicit gender associations in neutral prompt embeddings via a gender-leaning score, enabling quantitative analysis of bias at the text-conditioning level. Using this formulation, we show that demographic bias in T2V models primarily originates from pretrained text encoders, which encode gender associations even without explicit demographic cues. This bias arises because encoders such as CLIP, trained on large-scale but socially imbalanced image‚Äìtext data and widely used in modern T2V architectures, internalize skewed correlations that disproportionately align certain occupations with gender-associated directions in the prompt embedding space.\n\n\nBased on this insight, we introduce FairT2V, a training-free debiasing framework for text-to-video generation. FairT2V operates in an angular embedding space, where prompt embeddings and demographic attribute anchors lie on a shared hyperspherical manifold. By adaptively shifting each prompt embedding toward a neutral point based on its relative angular proximity to majority and minority anchors, FairT2V mitigates encoder-induced bias while preserving prompt semantics, without requiring any model fine-tuning. While this embedding-level debiasing is already effective, directly applying the debiased embeddings throughout the entire diffusion process can disrupt temporal coherence and introduce frame-level artifacts. To further improve video quality, FairT2V employs a dynamic denoising schedule that applies debiasing only during early identity-forming stages and restores the original prompt embedding during later refinement steps, preserving temporal consistency and visual smoothness¬†(Lv et al., 2025).\n\n\nFinally, we design a fairness evaluation protocol tailored to text-to-video generation. Existing image-based fairness assessments do not readily extend to videos, where multiple identities and dynamic temporal visibility complicate demographic attribution. To address this, we employ a VideoLLM (Gemini¬†(Google DeepMind, 2025)) to analyze full video sequences and infer demographic attributes by reasoning over identity continuity, temporal transitions, and multi-subject scenes. Since automated VideoLLM-based judgments may be affected by hallucination and prompt sensitivity, we further incorporate a human-in-the-loop verification stage to validate demographic labels and assess video quality.\n\n\nIn summary, this paper makes the following contributions:\n\n\n‚Ä¢\n\nWe propose FairT2V, a training-free debiasing framework for text-to-video generation that mitigates text-encoder‚Äìinduced demographic bias while preserving prompt semantics and temporal coherence in videos.\n\n\n\n‚Ä¢\n\nWe provide the first systematic analysis of demographic bias in text-to-video diffusion models, and identify the text-conditioning encoder as a primary source through which demographic bias is introduced and propagated.\n\n\n\n‚Ä¢\n\nWe develop a video-oriented fairness evaluation protocol that combines LLM-as-a-Judge with human-in-the-loop verification, enabling reliable demographic bias assessment beyond frame-level analysis.\n\n\n\n‚Ä¢\n\nWe conduct extensive experiments on state-of-the-art text-to-video model, demonstrating that FairT2V substantially reduces demographic bias while maintaining high video quality across multiple evaluation metrics.\n\n\n\n\n\n\n\n2 Background and Related Work\n\n\n2.1 Text-to-Video Diffusion Models\n\nText-to-video (T2V) models such as CogVideoX and Open-Sora extend diffusion transformers to the video domain via MM-DiT architectures. Although effective in practice, MM-DiT‚Äìbased models remain less well understood than U-Net‚Äìbased diffusion models, and their spatiotemporal attention behavior under textual conditioning has received limited study. Despite architectural differences, T2V models follow a standard text-conditioned diffusion paradigm. Video generation starts from a noisy latent sequence and proceeds through iterative denoising, guided at each step by embeddings produced by pretrained text encoders such as CLIP, T5, or their combination. The text embedding acts as a persistent conditioning signal throughout the reverse diffusion process, shaping both global structure and identity-level semantics across frames.\n\n\nFormally, given a text embedding EpE_{p} for prompt pp and an unconditional embedding E‚àÖE_{\\varnothing}, classifier-free guidance (CFG) computes the guided noise estimate at step tt as:\n\n\n\n\nœµ^Œ∏‚Äã(xt,t,Ep)\\displaystyle\\hat{\\epsilon}_{\\theta}(x_{t},t,E_{p})\n=œµŒ∏‚Äã(xt,t,E‚àÖ)\\displaystyle=\\epsilon_{\\theta}(x_{t},t,E_{\\varnothing})\n\n(1)\n\n\n\n\n+Œ±‚Äã(œµŒ∏‚Äã(xt,t,Ep)‚àíœµŒ∏‚Äã(xt,t,E‚àÖ))\\displaystyle\\quad+\\alpha\\big(\\epsilon_{\\theta}(x_{t},t,E_{p})-\\epsilon_{\\theta}(x_{t},t,E_{\\varnothing})\\big)\n\n\n\n\nwhere œµŒ∏\\epsilon_{\\theta} denotes the denoising network, xtx_{t} is the latent at step tt, and Œ±\\alpha is the guidance scale. The latent is updated as:\n\n\n\nxt+1=ùíü‚Äã(xt,t,œµ^Œ∏‚Äã(xt,t,Ep))x_{t+1}=\\mathcal{D}\\big(x_{t},t,\\hat{\\epsilon}_{\\theta}(x_{t},t,E_{p})\\big)\n\n(2)\n\n\nUnder classifier-free guidance, each denoising step is a deterministic function of the text embedding EpE_{p}.\n\n\nFigure 1: Bias source analysis in text-to-video generation. Neutral prompts are encoded by the text encoder (e.g., CLIP) into embeddings aligned with gender-associated directions, revealing implicit demographic bias in the text-conditioning space.\n\n\n\n\n2.2 Demographic Bias in Generative Models\n\nDiffusion-based generative models are known to produce biased or stereotypical outputs even under neutral prompts¬†(Jiang et al., 2025b). Prior analyses of Stable Diffusion¬†(Rombach et al., 2022; Cho et al., 2023; Luccioni et al., 2023; Harrison Rosenberg et al., 2023) reveal systematic associations between occupations, visual attributes, and demographic groups, indicating that bias can arise implicitly from text conditioning alone. While most existing studies focus on text-to-image generation, emerging evidence suggests that such biases carry over to video generation. A recent study of Sora¬†(Nadeem et al., 2025), for example, reports pronounced gender associations in text-to-video outputs, where particular occupations are consistently linked to one gender despite neutral prompts. These observations motivate the need for a systematic investigation of demographic bias in text-to-video diffusion models.\n\n\n\n\n\nFigure 2: Gender-leaning scores (Equation¬†5) from the CLIP text encoder for 16 occupations, using the prompt sets in Equation¬†3.\n\n\n\n\n\n\nFigure 3: Gender proportions in genera"
  },
  {
    "title": "SERA: Soft-Verified Efficient Repository Agents",
    "url": "https://arxiv.org/abs/2601.20789v1",
    "source": "arxiv",
    "summary": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for train",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 SWE-bench\n2.2 Agent Scaffolds and Training Data\n2.3 Synthetic Data Generation\n2.4 Reinforcement Learning\n2.5 Verification\n\n\n\n3 Method\n\n3.1 Soft Verified Generation (SVG)\n3.2 Training\n\n\n\n4 Main Results\n\n4.1 Controlled Comparisons\n4.2 Scaling Experiments\n4.3 Repository Specialization\n\n\n\n5 Ablations and Analysis\n\n5.1 Verification\n5.2 Truncation\n5.3 Data Filtering for Specialization\n5.4 Teacher Models\n5.5 Rollout Mixing\n\n\n6 Robustness of Evaluations\n7 Deployment\n8 Related Work\n9 Limitations\n\n10 Broader Impact\n\nEnabling Private Codebase Specialization:\n\n\nA Scaling Law and Data Points\nB Additional Baseline Comparisons\nC Specialization Results at 64K Context\n\nD Cost Breakdown\n\nReinforcement learning.\nSynthetic data generation.\n\n\n\nE Model Card: SERA-32B\n\nE.1 Model Overview\nE.2 Model Variants\nE.3 Performance Comparison\nE.4 Quickstart\nE.5 Model Details\nE.6 Training Data\nE.7 Intended Use\nE.8 Limitations\nE.9 Bias, Risks, and Limitations\nE.10 Responsible Use\nE.11 Hardware Requirements\nE.12 License\nE.13 Citation\nE.14 Contact\n\n\n\n\n\n\n\n\n\\authorOne\n[\n\n1\n\n\n,\n\n\n2\n]Ethan Shen\n\\authorOne[\n\n1\n]Daniel Tormoen\n\\authorOne[\n\n1\n]Saurabh Shah\n\\authorOne[\n\n1\n\n\n,\n\n\n2\n]Ali Farhadi\n\\authorOne[\n\n1\n\n\n,\n\n\n3\n]Tim Dettmers\n\n\n1\n]Allen Institute for AI\n\n\n2\n]University of Washington\n\n\n3\n]Carnegie Mellon University\n\nSERA: Soft-Verified Efficient Repository Agents\n\nAbstract\nOpen-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with the cost-efficiency, this enables specialization to private codebases. SVG is built on two observations that emerged from simplification of previous methods: First, soft verification, where instead of testing the correctness of synthetic coding data via unit tests, we only compare the partial line-by-line overlap of patches generated from two rollouts. This removes the need for test infrastructure and enables data generation from any repository, practically removing limits on the amount of data we can generate from a single codebase as well as what codebases can be used. Second, vague instructions can diversify training data, increasing the proportion of data focused on non-bug related changes like refactoring. We find that these vague instructions improve SWE-bench performance as well as bug-focused data. In more detail, SVG is based on two rollouts from an agent: in the first, a teacher model is prompted with a vague instruction to make a change to a codebase starting from a randomly selected function, producing a trajectory and patch. This trajectory is converted into a synthetic pull request. In the second, the teacher model attempts to reproduce the patch given only the pull request description. Soft verification compares the two patches using line-level recall for training data selection. Taken together, this creates a cheap pipeline for high-quality data that enables rapid experimentation. We show through power scaling curves that private codebase specialization is highly sample efficient and matches or exceeds teacher model performance at low costs. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2‚Äôs Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.\n\n\n\\metadata\n[\n\n\n Code:]https://github.com/allenai/SERA\n\\metadata[\n\n\n Models &amp; Data:]https://huggingface.co/collections/allenai/open-coding-agents\n\\metadata[\n\n\n Contact:]ethans03@cs.washington.edu ‚ÄÉdettmers@cmu.edu\n\n\nFigure 1: (a) Scaling and cost comparison of coding agent training approaches using self-hosted vLLM inference. (b) Repository specialization scaling law on Django, where Œ±\\alpha denotes the fraction of Django-specific data in the training mixture. With full specialization (Œ±=1.0\\alpha=1.0), the model matches teacher performance at 8k samples; general data alone (Œ±=0.0\\alpha=0.0) requires 25k samples ‚Äì a 3.5√ó\\times advantage in sample efficiency.\n\n\nFigure 2: Overview of SVG (Soft Verified Generation). In the first rollout, a teacher model is prompted to make a change starting from a randomly selected function, producing a trajectory and patch. This trajectory is converted into a synthetic pull request. In the second rollout, the teacher attempts to reproduce the patch given only the PR description. Soft verification compares the two patches using line-level recall for training data selection. We use r‚â•0.5r\\geq 0.5 as an example threshold.\n\n\n\n1 Introduction\n\nCoding agents have become central to software development and are increasingly applied to tasks beyond traditional engineering. While, closed-source coding agents are more powerful, open-weight models should hold a fundamental advantage in many applications because they can be specialized to private codebases, allowing them to learn repository-specific patterns, conventions, and domain knowledge. Despite this clear opportunity, the cost and complexity of training open-weight coding agents has kept this advantage theoretical. In this work, we show it is now practical.\n\n\nAs the first release in Ai2‚Äôs Open Coding Agents series, our method trains a 32B coding agent with simple supervised finetuning achieving state-of-the-art open-source results at 40 GPU days ($2000) or match strong open-weight models like Devstral-Small-2 at a budget of $9000. When specializing to a particular codebase, our pipeline can match or exceed teacher model performance at $1300.\n\n\nTraining coding agents traditionally requires either reinforcement learning or complex synthetic data pipelines, both demanding resources beyond what most teams can provide. Reinforcement learning requires sandboxed execution environments, distributed training infrastructure, and rollout orchestration. The complexity of this infrastructure is reflected in team sizes that average 12 or more authors in recent work Cao et al. (2025); Luo et al. (2025); Wei et al. (2025); Da et al. (2025). Synthetic data approaches like SWE-smith (Yang et al., 2025) require setting up test environments, generating valid bugs, and verifying bugs through test suites. These barriers have concentrated coding agent development in well-resourced industry labs and larger teams at academic institutions. Starting from limited compute and a small team (32 GPUs, 3 researchers), we prioritized reducing experimentation costs, which led us to systematically strip away pipeline complexity and discover which components actually matter for training effective coding agents.\n\n\nWe found that much of the complexity in prior pipelines is unnecessary. Firstly, soft verification, where patches are checked by partial line-by-line matching rather than executed test suites, produces training data of equal quality to full test-based verification. At the scales we test, the degree of verification has minimal effect on downstream performance, removing the need for test infrastructure entirely and enabling data generation from any repository. This makes synthetic data generation much more straightforward. Secondly, some coding instructions are inherently vague, and we observe that models prompted to fix these issues often produce changes such as refactoring or documentation improvements that are more representative of real world tasks than just bug fixes. Rather than requiring bug-focused data, we find that this general coding data is equally effective to improve performance. Together, these findings mean that generating effective training data requires neither test infrastructure nor complex bug-injection pipelines. Additionally, unlike other methods, how much data we can generate from repositories are not limited by test coverage or quality.\n\n\nThe resulting cost reduction and data abundance make repository specialization practical. We show that open-weight models specialized to a codebase can match or exceed the performance of the teacher model used to generate their training data. This is intuitive: a student model with repository-specific knowledge encoded in its weights can outperform a teacher that accesses the same codebase only through its context window. The advantage this creates extends beyond privacy. Even organizations willing to share their code would need to wait many months until the next training run of a frontier model includes their data. And while LoRA adapter options for frontier models exist these are often to impractical or costly for large-scale deployments. Open-weight specialization allows practitioners to generate data from their repositories, fine-tune, and deploy immediately, iterating as their codebase evolves. At low cost, any team can build and deploy a small specialized model that outperforms frontier systems on their own codebase ‚Äì an "
  },
  {
    "title": "REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence",
    "url": "https://arxiv.org/abs/2601.20784v1",
    "source": "arxiv",
    "summary": "Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inferenc",
    "full_text": "\n\n\n\nI Introduction\n\nII Neuro-Symbolic AI Systems\n\nII-A Neuro-Symbolic Cognitive Intelligence\nII-B Scaling Performance Analysis of Neuro-Symbolic Systems\nII-C Computational Primitives in Neuro-Symbolic AI\n\n\n\nIII Neuro-Symbolic Workload Characterization\n\nIII-A Compute Latency Analysis\nIII-B Roofline &amp; Symbolic Operation &amp; Inefficiency Analysis\nIII-C Unique Characteristics of Neuro-Symbolic vs LLMs\nIII-D Identified Opportunities for Neuro-Symbolic Optimization\n\n\n\nIV REASON: Algorithm Optimizations\n\n\nIV-A Stage 1: DAG Representation Unification\n\nFor FOL and SAT solvers\nFor PCs\nFor HMMs\n\n\n\nIV-B Stage 2: Adaptive DAG Pruning\n\nPruning of FOL and SAT via implication graph\nPruning of PCs and HMMs via circuit flow\n\n\nIV-C Stage 3: Two-Input DAG Regularization\n\n\n\nV REASON: Hardware Architecture\n\nV-A Overall Architecture\nV-B Reconfigurable Symbolic/Probabilistic PE\nV-C Architectural Support for Probabilistic Reasoning\nV-D Architectural Support for Symbolic Logical Reasoning\nV-E Case Study: A Working Example of Symbolic Execution\nV-F Design Space Exploration and Scalability\n\n\n\nVI REASON: System Integration and Pipeline\n\nVI-A Integration with GPUs for End-to-End Reasoning\nVI-B Programming Model\nVI-C Two-Level Execution Pipeline\n\n\n\nVII Evaluation\n\nVII-A Evaluation Setup\nVII-B REASON Algorithm Performance\nVII-C REASON Architecture Performance\n\n\nVIII Related Work\nIX Conclusion\n\n\n\n\n\nREASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence\n\n\n\nZishen Wan‚Ä†, Che-Kai Liu, Jiayi Qian, Hanchen Yang, Arijit Raychowdhury, Tushar Krishna\n\n\n\nAbstract\nNeuro-symbolic AI systems integrate neural perception with symbolic and probabilistic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as mathematical reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.\nThis paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. At the algorithm level, REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates neural, symbolic, and probabilistic execution.\nEvaluated across six neuro-symbolic workloads, REASON achieves 12-50√ó\\times speedup and 310-681√ó\\times energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6¬†mm2{}^{\\text{2}} area and 2.12¬†W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.\n\n\n\nI Introduction\n\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, image recognition, and complex pattern learning from vast datasets¬†[23, 46, 42, 16]. However, despite their success, LLMs often struggle with factual accuracy, hallucinations, multi-step reasoning, and interpretability¬†[35, 62, 2, 61]. These limitations have spurred the development of compositional AI systems, which integrate neural with symbolic and probabilistic reasoning to create robust, transparent, and intelligent cognitive systems.‚Ä†‚Ä†footnotetext: ‚Ä†Corresponding author\n\n\nOne promising compositional paradigm is neuro-symbolic AI, which integrates neural, symbolic, and probabilistic components into a unified cognitive architecture¬†[60, 1, 72, 9, 75]. In this system, the neural module captures the statistical, pattern-matching behavior of learned models, performing rapid function approximation and token prediction for intuitive perception and feature extraction. The symbolic and probabilistic modules perform explicit, verifiable reasoning that is structured, interpretable, and robust under uncertainty, managing logic-based reasoning and probabilistic updates. This paradigm integrates intuitive generalization and deliberate reasoning.\n\n\nNeuro-symbolic AI has demonstrated superior abstract deduction, complex question answering, mathematical reasoning, logical reasoning, and cognitive robotics¬†[28, 66, 55, 81, 12, 38, 41, 71]. Its ability to learn efficiently from fewer data points, produce transparent and verifiable outputs, and robustly handle uncertainty and ambiguity makes it particularly advantageous compared to purely neural approaches. For example, recently Meta‚Äôs LIPS¬†[28] and Google‚Äôs AlphaGeometry¬†[66] leverage compositional neuro-symbolic approaches to solve complex math problems and achieve a level of human Olympiad gold medalists. R2-Guard¬†[20] leverages LLM and probabilistic models to improve robust reasoning capability and resilience against jailbreaks. They represent a paradigm shift for AI that requires robust, verifiable, and explainable reasoning.\n\n\nDespite impressive algorithmic advances in neuro-symbolic AI ‚Äì often demonstrated on large-scale distributed GPU clusters ‚Äì efficient deployment at the edge remains a fundamental challenge. Neuro-symbolic agents, particularly in robotics, planning, interactive cognition, and verification, require real-time logical inference to interact effectively with physical environments and multi-agent systems. For example, Ctrl-G, a text-infilling neuro-symbolic agent¬†[83], must execute hundreds of reasoning steps per second to remain responsive, yet current implementations take over 5 minutes on a desktop GPU to complete a single task. This latency gap makes practical deployment of neuro-symbolic AI systems challenging.\n\n\nTo understand the root causes of this inefficiency, we systematically analyze a diverse set of neuro-symbolic workloads and uncover several system- and architecture-level challenges. Symbolic and probabilistic kernels frequently dominate end-to-end runtime and exhibit highly irregular execution characteristics, including heterogeneous compute patterns and memory-bound behavior with low ALU utilization. These kernels suffer from limited exploitable parallelism and irregular, uncoalesced memory accesses, leading to poor performance and efficiency on CPU and GPU architectures.\n\n\nTo address these challenges, we develop an integrated acceleration framework, REASON, which to the best of our knowledge, is the first to accelerate probabilistic logical reasoning-based neuro-symbolic AI systems. REASON is designed to close the efficiency gap of compositional AI by jointly optimizing algorithms, architecture, and system integration for the irregular and heterogeneous workloads inherent to neuro-symbolic reasoning.\n\n\nAt the algorithm level, REASON introduces a unified directed acyclic graph (DAG) representation that captures shared computational structure across symbolic and probabilistic kernels. An adaptive pruning and regularization technique further reduces model size and computational complexity while preserving task accuracy.\nAt the architecture level, REASON features a flexible design optimized for various irregular symbolic and probabilistic computations, leveraging the unified DAG representation. The architecture comprises reconfigurable tree-based processing elements (PEs), compiler-driven workload mapping, and memory layout to enable highly parallel and energy-efficient symbolic and probabilistic computation.\nAt the system level, REASON is tightly integrated with GPU streaming multiprocessors (SMs), forming a heterogeneous system with a programmable interface and multi-level execution pipeline that efficiently orchestrates neural, symbolic, and probabilistic kernels while maintaining high hardware utilization and scalability as neuro-symbolic models evolve.\nNotably, unlike conventional tree-like computing arrays optimized primarily for neural workloads, REASON provides reconfigurable support for neural, symbolic, and probabilistic kernels within a unified execution fabric, enabling efficient and scalable neuro-symbolic AI systems.\n\n\nThis paper, therefore, makes the following contributions:\n\n\n‚Ä¢\n\nWe conduct a systematic workload characterization of representative logical- and probabilistic-reasoning-based neuro-symbolic AI models, identifying key performance bottlenecks and architectural optimization opportunities (Sec.¬†II, Sec.¬†III).\n\n\n\n‚Ä¢\n\nWe propose REASON, an integrated co-design framework, to efficiently accelerate probabilistic logical reasoning in neuro-symbolic AI, enabling practical and scalable deployment of compositional intelligence (Fig.¬†4).\n\n\n\n‚Ä¢\n\nREASON introduces cross-layer innovations spanning (i) a unified DAG representation with adaptive pruning at the algorithm level (Sec.¬†IV), (ii) a reconfigurable symbolic/probabilistic architecture and compiler-driven dataflow and mapping at the hardware level (Sec.¬†V), and (iii) a programmable system interface with a multi-level execution pipeline at the system level (Sec.¬†VI) to improve neuro-symbolic efficiency.\n\n\n\n‚Ä¢\n\nEvaluated across cognitive tasks, REASON enables flexible support for symbolic and probabilistic operations, achieving 12-50√ó\\times speedup and 310-681√ó\\times energy efficiency compared to desktop and edge GPUs. REA"
  },
  {
    "title": "Neural Quantum States in Mixed Precision",
    "url": "https://arxiv.org/abs/2601.20782v1",
    "source": "arxiv",
    "summary": "Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this wor",
    "full_text": "\n\n\n\n\nI Introduction\n\nI.1 Our Contribution\nI.2 Previous Work\n\n\nII Variational Monte Carlo\n\nIII Mixed Precision During Sampling\n\nIII.1 Properties of the Finite-Precision Markov Chain\n\n\n\nIV Numerical Results\n\nIV.1 Noisy Restricted Boltzmann Machine\nIV.2 Practical example in low precision\n\n\nV Conclusion\nVI Code and Data Availability\nVII Acknowledgements\nA Data types\nB Mixing times and spectral properties of the Markov kernel\n\nC Proofs\n\nStep 1: Reduction to a row-wise kernel difference.\nStep 2: Bounding the row-wise difference.\nStep 3: Bounding the acceptance error.\nStep 4: Conclusion.\n\n\nD System Size Scaling\nE The Heisenberg Model\nF Optimizations for the Two Phases of the TFIM\nG Restricted Boltzmann Machine\nH ResCNN\n\nI Effects on Other Parts of VMC\n\nI.1 Gradients in Low Precision\nI.2 Stochastic Reconfiguration in Low Precision\nI.3 Optimization with Gradients in Low Precision\n\n\n\n\n\n\n\nNeural Quantum States in Mixed Precision\n\n\nMassimo Solinas\n\nFakult√§t f√ºr Informatik und Data Science, University of Regensburg,\nUniversit√§tsstra√üe 31, D-93040, Regensburg\n\n‚ÄÉ‚ÄÉ\nAgnes Valenti\n\nCenter for Computational Quantum Physics, Flatiron Institute, 162 Fifth Avenue, New York, NY 10010, USA\n\n‚ÄÉ‚ÄÉ\nNawaf Bou-Rabee\n\nDepartment of Mathematical Sciences, Rutgers University, Camden, New Jersey 08102, USA and Center for Computational Mathematics, Flatiron Institute, 162 Fifth Avenue, New York, NY 10010, USA\n\n‚ÄÉ‚ÄÉ\nRoeland Wiersema\n\nCenter for Computational Quantum Physics, Flatiron Institute, 162 Fifth Avenue, New York, NY 10010, USA\n\n\n\nAbstract\nScientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency.\nIn this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC.\nWe demonstrate that significant portions of the algorithm, in particular, sampling the quantum state,\ncan be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems.\n\n\n\nI Introduction\n\nMany problems in machine learning and the physical sciences require sampling from high-dimensional, often strongly multimodal probability distributions. Markov chain Monte Carlo (MCMC) methods are the standard approach, generating samples through local stochastic updates with guaranteed asymptotic convergence. In modern applications involving neural-network‚Äìparameterized distributions, the cost of generating sufficiently decorrelated MCMC samples often dominates the overall computational cost of inference or training [34, 38, 13, 12, 37, 36, 42].\n\n\nIn this work, we investigate a simple and broadly applicable strategy for reducing the computational cost of MCMC sampling: the use of reduced-precision arithmetic. Lower-precision formats, such as single precision (f32) and half precision (f16 or bf16), have become standard in large-scale machine learning due to their substantial performance gains on modern hardware [33, 32, 46]. These gains, commonly benchmarked in terms of basic linear-algebra operations such as matrix multiplication (see Appendix Fig. 5), come at the expense of increased numerical error.\nReliably assessing both speed-up and error that result from the use of reduced-precision formats is of notable interest in the field of scientific computing, where double precision has traditionally been employed to ensure accuracy  [18, 16, 23]. Here, we address this task for the use-case of MCMC sampling.\nSpecifically, we derive theoretical bounds on the bias introduced by reduced precision in Metropolis‚ÄìHastings (MH) MCMC, and validate these bounds empirically. Finally, we demonstrate that substantial speed-ups can be achieved without loss of accuracy in a representative scientific application where MCMC constitutes a key computational bottleneck: neural quantum states (NQS) for the simulation of quantum many-body systems [7, 8], a method which has been successfully applied to obtain state-of-the-art results on various open problems [24, 29].\n\n\nThe ability to efficiently simulate a quantum many-body system is crucial across a wide range of scientific disciplines. Its impact spans the discovery and characterization of quantum phases in physics, the development of deeper insights into molecular interactions and reaction mechanisms in quantum chemistry, and the design of novel materials with tailored properties in materials science. However, the exponential scaling of the degrees of freedom in quantum many-body wavefunctions poses a fundamental challenge for efficient simulation. NQS offer a prominent way to circumvent this challenge by compressing the quantum many-body wavefunction into a neural network and optimizing it toward the ground state of the system using variational Monte Carlo (VMC) [27, 44]. Rather than being data-driven, the approach proceeds by sampling the wavefunction at each optimization step via MCMC. Here we show that the MCMC sampling portion of the VMC algorithm can be significantly sped up with the use of lower-precision formats. We empirically demonstrate that this speed-up comes without sacrificing accuracy in the obtained ground-state approximation, and we relate these results to the derived theoretical bounds connecting them to the features of the targeted ground-state. Our findings show that reducing numerical precision can substantially accelerate NQS optimization while providing a rigorous framework to quantify low-precision sampling accuracy, with applications beyond NQS including Bayesian learning [34, 38] and energy-based models [13, 12, 37, 36, 42].\n\n\n\nI.1 Our Contribution\n\n\n\n1.\n\nWe derive bounds on the sampling error introduced by low-precision arithmetic on discrete MH MCMC. We construct a simplified toy model for sampling and find excellent agreement with the theoretical bounds.\n\n\n\n2.\n\nWe introduce a mixed-precision VMC framework that efficiently performs sampling in low precision while keeping the remaining computations in high precision.\n\n\n\n3.\n\nWe evaluate our method by training various models, including feed-forward networks and residual convolutional neural networks. Our results show that low-precision arithmetic can accelerate sampling by up to 3.5√ó3.5\\times without degrading the training performance of the neural networks.\n\n\n\n\n\n\n\nI.2 Previous Work\n\nPerturbed Markov chains have been\nstudied from several perspectives. Early work focused on robustness of\nergodicity and convergence under small perturbations of the transition kernel, highlighting that seemingly benign numerical effects can destroy\nergodicity in pathological cases\n[40, 6]. More recently, the effect of\nfinite-precision arithmetic on the MH accept-reject step\nhas been examined directly, with an emphasis on how roundoff error can distort\nacceptance probabilities and degrade acceptance rates\n[19, 45]. A complementary line of work treats approximate\nMarkov chains abstractly, providing general bounds on the difference between\nthe invariant distribution of an exact chain and that of an approximate chain\nin terms of kernel-level perturbations and mixing properties\n[21, 22]. The effect of noise in a Markov chain need not always be destructive. In Pseudo-Marginal MCMC [2, 35, 26, 28], noise is intentionally introduced to accelerate Markov chain mixing.\n\n\nThe present work differs in scope and emphasis. Like other works, we consider a target\ndistribution œÄ\\pi (corresponding to exact arithmetic)\nand a perturbed distribution œÄ~\\tilde{\\pi} arising from finite-precision effects\nin the evaluation of the log-density entering the MH accept‚Äìreject step.\nRather than focusing on robustness of ergodicity or bounding error via\nabstract kernel norms, we analyze how finite-precision perturbations\npropagate through the MH acceptance rule and induce an asymptotic bias, in the sense of [14]. Finally, the analysis of mixed-precision effects in the context of MCMC sampling employed in machine learning, or, NQS in particular, remains underdeveloped. The authors of [9] mention using lower precision for the evaluation of the wave function, but the impact of this choice is still poorly understood.\n\n\n\n\n\nII Variational Monte Carlo\n\nPhysical properties at low temperature are dominated by the ground state of the physical system of interest. Determining the ground state amounts to finding the complex-valued eigenvector associated with the smallest eigenvalue (the ground-state energy) of a sparse Hermitian operator HH, called the Hamiltonian, which encodes the total energy of the system. We denote this eigenvector by |œà‚ü©\\ket{\\psi} and refer to it as the ground-state wavefunction. Throughout this manuscript, we consider an effective model consisting of spin-1/21/2 quantum degrees of freedom on a lattice of NN sites. The associated Hilbert space has dimension 2N2^{N}; thus, |œà‚ü©‚àà‚ÑÇ2N\\ket{\\psi}\\in\\mathbb{C}^{2^{N}} and the Hamiltonian HH is a 2N√ó2N2^{N}\\times 2^{N} matrix, making direct diagonalization computationally prohibitive.\n\n\nWithin VMC, this exponential scaling is avoided by approximating the ground state using two crucial ingredients: (i) a parameterized representation of the quantum many-body wave-function |œàŒ∏‚ü©‚àà‚ÑÇ2N\\ket{\\"
  },
  {
    "title": "Independence of Approximate Clones",
    "url": "https://arxiv.org/abs/2601.20779v1",
    "source": "arxiv",
    "summary": "In an ordinal election, two candidates are said to be perfect clones if every voter ranks them adjacently. The independence of clones axiom then states that removing one of the two clones should not change the election outcome. This axiom has been extensively studied in social choice theory, and several voting rules are known to satisfy it (such as IRV, Ranked Pairs and Schulze). However, perfect ",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Our contributions\n1.2 Related Work\n\n\n2 Preliminaries\n3 Approximate Clones\n\n4 Independence of Approximate Clones\n\n4.1 Weak independence of approximate clones\n4.2 Subclasses of profiles\n\n\n\n5 Empirical Analysis\n\n5.1 Approximate Clones\n5.2 Independence of Approximate Clones\n\n\n6 Discussion and Further Remarks\n\n\n\n\n\n\n\\fail\n\nIndependence of Approximate Clones\n\n\nTh√©o Delemazure\n\nILLC, University of AmsterdamNetherlands\n\ntheo.delemazure@uva.nl\n\n\n\nAbstract.\nManuscript: January 2026\nIn an ordinal election, two candidates are said to be perfect clones if every voter ranks them adjacently. The independence of clones axiom then states that removing one of the two clones should not change the election outcome. This axiom has been extensively studied in social choice theory, and several voting rules are known to satisfy it (such as IRV, Ranked Pairs and Schulze). However, perfect clones are unlikely to occur in practice, especially for political elections with many voters.\nIn this work, we study different notions of approximate clones in ordinal elections. Informally, two candidates are approximate clones in a preference profile if they are close to being perfect clones. We discuss two measures to quantify this proximity, and we show under which conditions the voting rules that are known to be independent of clones are also independent of approximate clones. In particular, we show that for elections with at least four candidates, none of these rules are independent of approximate clones in the general case. However, we find a more positive result for the case of three candidates. Finally, we conduct an empirical study of approximate clones and independence of approximate clones based on three real-world datasets: votes in local Scottish elections, votes in mini-jury deliberations, and votes of judges in figure skating competitions. We find that approximate clones are common in some contexts, and that the closest two candidates are to being perfect clones, the less likely their removal is to change the election outcome, especially for voting rules that are independent of perfect clones.\n\n¬†\nContents\n\n\n1 Introduction\n\n1.1 Our contributions\n1.2 Related Work\n\n\n2 Preliminaries\n3 Approximate Clones\n\n4 Independence of Approximate Clones\n\n4.1 Weak independence of approximate clones\n4.2 Subclasses of profiles\n\n\n\n5 Empirical Analysis\n\n5.1 Approximate Clones\n5.2 Independence of Approximate Clones\n\n\n6 Discussion and Further Remarks\n¬†\n\n\n1. Introduction\n\nA major issue in many electoral systems is what is called the spoiler effect: one of the candidates that lost the election could have won if another candidate (the spoiler) had not run in the election. A typical example is the single-winner voting rule plurality, in which every voter casts a vote for one candidate and the candidate with the highest score wins: if several candidates from the same side of the political spectrum are running, voters from this side are split between these candidates, which might harm their chances to win. Sometimes, the spoiler effect can even be mutual and two candidates could each have won individually if the other one had withdrawn.\n\n\nTo capture this effect, Tideman (1987) introduced the axiom of independence of clones for elections based on ordinal preferences (i.e., in elections in which every voter casts a ranking of the candidates). In this ordinal model, we say that a set of candidates are clones if every voter ranks them consecutively in their ranking. Then, the independence of clones axiom states that when such clones exist, removing all of them but one should not change the winner of the election (unless one of the removed clones is the winner, in which case the remaining clone should become the new winner). This axiom has been extensively studied and discussed in the social choice literature since then, particularly for single-winner and multi-winner voting, and has been adapted to other preference formats, such as approval preferences. With ordinal preferences, it has been shown that independence of clones is satisfied by only a few single-winner voting rules (in particular Instant Runoff Voting, Ranked Pairs, and Schulze‚Äôs rule) and multi-winner voting rules (in particular the Single Transferable Vote). Some of these rules are used in several countries for large-scale political elections. Notably, Instant Runoff Voting and Single Transferable Vote are used in Ireland, Australia, and some US states.\n\n\nThis being said, perfect clones are unlikely to occur in any large-scale political election, as the number of voters is very high and in practice, some voters have unconventional preferences. Nevertheless, this axiom a priori informs us about the expected behavior of a voting rule in presence of approximate clones: if a rule is independent of clones, then we can expect that it is less sensitive to the spoiler effect in general, in particular for approximate clones. But in practice, can we expect to find such approximate clones? How should the proximity to being clones be defined for a pair of candidates? Moreover, do the rules that satisfy independence of clones also satisfy independence of approximate clones, if these candidates are close enough to being perfect clones? We aim to answer these questions in this work using two different notions of approximate clones, by studying the theoretical and empirical behavior of voting rules in presence of such approximate clones.\n\n\n\n1.1. Our contributions\n\nWe first recall the model of ordinal preferences, the definition of clones and of the independence of clones axiom, and some single-winner voting rules satisfying this axiom in Section¬†2. Although clones can be sets of candidates of any size in the original definition, we focus on pairs of candidates for simplicity in this work. However, it is clear that all negative results hold for larger sets of candidates, and our main positive result (Theorem¬†4.5) holds only for preference profiles with three candidates and thus only applies to pairs of approximate clones.\n\n\nIn Section¬†3, we formally define the two notions of approximate clones that are used in this paper. These notions aim at quantifying how close a pair of candidates is to being clones. The first one, which we call Œ±\\alpha-deletion clones, is based on the proportion Œ±\\alpha of voters that need to be removed from the profile for two candidates to become clones.111This notion is equivalent to the notion of MaxClones already introduced by Janeczko et al. (2024), and to the notion of independent clones discussed by Faliszewski et al. (2025). The second one, which we call Œ≤\\beta-swap clones, is based on the number of swaps of adjacent candidates in voters‚Äô rankings that are required for the two candidates to be clones (here, Œ≤\\beta is the average number of swaps required per voters). Note that Œ±=Œ≤=0\\alpha=\\beta=0 for perfect clones.\n\n\nIn Section¬†4, we show under which conditions and for which values of Œ±\\alpha and Œ≤\\beta the voting rules that are known to be independent of perfect clones are also independent of approximate clones. Notably, in the general case and when there are four candidates or more, we show that none of these rules satisfy (weak) independence of approximate clones for any value of Œ±&gt;0\\alpha&gt;0 or Œ≤&gt;0\\beta&gt;0. However, we show that when there are three candidates, there are values of Œ±\\alpha for which they satisfy a weak version of the axiom.\n\n\nThen, in Section¬†5 we investigate approximate clones in real-world datasets of preferences. More specifically, we measure the proximity of the pairs of candidates to being clones (using our two notions), and study how different voting rules react to the deletion of approximate clones in practice. This analysis is based on three datasets: actual votes in local Scottish elections, rankings of judges in skating competitions, and votes of participants in deliberation experiments. We conclude and discuss potential future work in Section¬†6.\n\n\n\n\n1.2. Related Work\n\nOne of the closest works to ours is the one of Janeczko et al. (2024), who study among other problems the computational complexity of identifying approximate clones in preference profiles, using a notion equivalent to our Œ±\\alpha-deletion clones measure. They also compute the proximity to being clones of pairs of candidates in synthetic preference profiles (from the map of elections (Szufa et al., 2020)) and in real-world data. Faliszewski et al. (2025) continued this work by studying the parametrized complexity of this problem with respect to some natural parameters. They also consider another measure of proximity to being clones, based on the maximal distance between the two candidates in voters‚Äô rankings, which is different from the two notions we consider in this work.222All the negative results we obtain in this paper also hold for their measure (by using the same counter-examples), while our positive results do not hold. The problem of identifying approximate clones has also been studied by Delemazure et al. (2026) in the approval setting.\n\n\nProcaccia et al. (2025) also consider some notion of approximate clones for the problem of AI alignment, where the approximate clones are alternatives with similar features. In that sense, this work is also related to the literature on differential privacy (Dwork, 2006).\nElkind et al. (2012) propose a richer structure than the notion of clones that captures clone sets of all possible sizes by means of PQ-tree. This notion was then used by Berker et al. (2025) to define a distance between candidates which corresponds to the size of the smallest clone set containing both candidates, and which could be seen as another notion of approximate clones.\n\n\nMore generally, the independence of clones axiom has been studied in various contexts and has inspired a rich body of research in social choice theory. The notion, originally introduced in the context of single-winner voting with ordinal preferences (Tideman, 1987), has later"
  },
  {
    "title": "Active Learning for Decision Trees with Provable Guarantees",
    "url": "https://arxiv.org/abs/2601.20775v1",
    "source": "arxiv",
    "summary": "This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic labe",
    "full_text": null
  }
]