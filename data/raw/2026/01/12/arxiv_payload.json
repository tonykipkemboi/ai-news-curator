[
  {
    "title": "Manifold limit for the training of shallow graph convolutional neural networks",
    "url": "https://arxiv.org/abs/2601.06025v1",
    "source": "arxiv",
    "summary": "We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly",
    "full_text": null
  },
  {
    "title": "AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs",
    "url": "https://arxiv.org/abs/2601.06022v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility requi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\nSample‚ÄëLevel Ensembling\nSpan-Level Ensembling\nToken‚ÄëLevel Ensembling\n\n\n\n3 Methodology\n\n3.1 Candidate Word Proposal\n\n3.2 Adaptive Word Commitment\n\nStart-of-Word Confidence.\nAdaptive Expansion Strategy.\n\n\n\n3.3 Diversity-Aware Ensemble Scaling\n\nAdaptive Trigger.\nExploration.\nExploitation.\n\n\n3.4 Cross-Model Scoring and Candidate Integration\n\n\n\n4 Experiments\n\n\n4.1 Setup\n\nModels.\nBaselines.\nBenchmarks.\nExperimental Setup.\n\n\n4.2 Main Results (RQ1)\n\n4.3 Ablation Study (RQ2)\n\nAnalysis of Adaptive Word Commitment.\nDistribution of Words per Decoding Round.\n\n\n4.4 Scaling Analysis (RQ3)\n\n4.5 Case Study (RQ4)\n\nRuntime Analysis.\nCase Analysis.\n\n\n\n\n5 Conclusion\nA Potential Risks\n\nB Use Or Create Scientific Artifacts\n\nB.1 Cite Creators Of Artifacts\nB.2 Discuss The License For Artifacts\nB.3 Artifact Use Consistent With Intended Use\nB.4 Data Contains Personally Identifying Info Or Offensive Content\nB.5 Documentation Of Artifacts\nB.6 Use of AI Assistants\n\n\n\nC Statistics For Data\n\nC.1 Flores (English to German) (Machine Translation)\nC.2 Flores (German to English) (Machine Translation)\nC.3 GSM8K (Arithmetic Reasoning)\nC.4 NaturalQuestions (Open‚ÄêDomain Question Answering)\nC.5 SQuAD 2500 (Reading Comprehension QA)\nC.6 Wikipedia Test 6000 (Open‚ÄêDomain QA)\n\n\n\nD Computational Experiments\n\nD.1 Model Size And Budget\nD.2 Experimental Setup And Hyperparameters\nD.3 Descriptive Statistics\nD.4 Parameters For Packages\n\n\nE Effect of Ensemble Size\nF Diversity-Aware Scaling vs. Beam-Search Scaling\n\n\n\n\n\n\nAdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n\n\n\nChengming Cui1‚Ä†‚ÄÉTianxin Wei1‚Ä†‚ÄÉZiyi Chen1‚ÄÉRuizhong Qiu1‚ÄÉZhichen Zeng1\nZhining Liu1‚ÄÉXuying Ning1‚ÄÉDuo Zhou1‚ÄÉJingrui He1\n‚Ä†Equal contribution \n1University of Illinois Urbana-Champaign\n\n{ccui12, twei10, jingrui}@illinois.edu\n\n\n\nAbstract\nLarge language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain QA, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.\n\n\n\nAdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs\n\n\n\n\n\nChengming Cui1‚Ä†‚ÄÉ‚ÄäTianxin Wei1‚Ä†‚ÄÉ‚ÄäZiyi Chen1‚ÄÉ‚ÄäRuizhong Qiu1‚ÄÉ‚ÄäZhichen Zeng1\n\nZhining Liu1‚ÄÉXuying Ning1‚ÄÉDuo Zhou1‚ÄÉJingrui He1\n\n‚Ä†Equal contribution\n\n1University of Illinois Urbana-Champaign\n\n{ccui12, twei10, jingrui}@illinois.edu\n\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have demonstrated strong performance across a wide range of natural language processing tasks Hendrycks et al. (2020); Cobbe et al. (2021); Ning et al. ; Wei et al. (2025); Ai et al. (2025); Zhang et al. (2025); Chen et al. (2024) and are increasingly deployed in real-world applications Grattafiori et al. (2024); OpenAI (2024).\nHowever, model performance is not uniform across tasks: differences in pretraining data, model architectures, and decoding strategies lead individual models to exhibit heterogeneous strengthsChu et al. (2025).\nModels that excel at structured or multi-step reasoning may perform less reliably on open-domain or commonsense question answering, while models optimized for such queries may struggle with complex reasoning\nJoshi et al. (2017); Kwiatkowski et al. (2019); Huang and Chang (2023); Guo et al. (2025).\nAs a result, inference-time ensembling has emerged as a practical approach for improving robustness by combining complementary model strengths without retraining Jiang et al. (2023b); Liu et al. (2024); Yao et al. (2024); Wang et al. (2023b).\n\n\nExisting inference-time ensemble methods differ primarily in the granularity at which model outputs are fused Chen et al. (2023).\nSample-level approaches combine complete model responses via reranking or response fusion Jiang et al. (2023b); Shnitzer et al. (2023); Lu et al. (2023); Wang et al. (2023a), but perform fusion only after generation has finished, preventing any mid-generation correction.\nSpan-level methods assemble multi-token segments produced by different models Liu et al. (2024); Xu et al. (2024); Lv et al. (2024); Fu et al. (2025), yet rely on predefined or fixed span boundaries that limit flexibility across tasks and reasoning depths.\nToken-level approaches aggregate next-token distributions at each decoding step Yao et al. (2024); Huang et al. (2024); Yu et al. (2024); Zeng et al. (2025), but typically require token alignment across models, restricting applicability under heterogeneous tokenizers; related byte-level alternatives alleviate this mismatch at the cost of increased computation and weakened semantic structure Phan et al. (2024); Sathe et al. (2024).\nDespite operating at different granularities, most existing methods rely on a fixed fusion resolution, which restricts their ability to adapt the fusion scope to evolving semantic context, task demands, and reasoning uncertainty during generation Yao et al. (2023).\n\n\nThese trade-offs motivate the need for an adaptive inference-time ensemble framework that overcomes the limitations imposed by fixed-granularity generation.\nWe adopt word-level units as the basic building blocks, as they provide a natural abstraction that supports step-wise integration and semantic coherence, while avoiding explicit token alignment across heterogeneous tokenizers.\nBased on this design choice, we propose AdaFuse, an adaptive word-level ensembling framework that enables flexible inference-time fusion and mid-generation correction at natural word boundaries, which preserves semantic integrity across models.\n\n\nConcretely, AdaFuse adopts an adaptive decoding strategy that balances effectiveness and efficiency during generation. We introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model proceeds with direct generation. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and support more effective ensemble decisions. This design is guided by the evolving decoding context rather than predefined rules, and establishes a synergistic interaction between adaptive ensembling and test-time scaling. In particular, selective exploration under uncertainty improves test-time scaling effectiveness, while the resulting diversity further strengthens ensemble quality.\n\n\nIn summary, our paper contributes the following:\n\n\n\n\n‚Ä¢\n\nConfidence-Guided Adaptive Decoding:\nAdaFuse uses model confidence to decide when to commit longer word spans\nand when to consider more candidate continuations, enabling adaptive mid-generation\ncorrection without relying on fixed-length spans.\n\n\n\n‚Ä¢\n\nDiversity-Aware Ensemble Scaling:\nBuilding on confidence-guided decoding, AdaFuse employs an exploration strategy to generate diverse candidate continuations for ensemble scoring only when uncertainty arises, while avoiding unnecessary computation in confident states.\n\n\n\n‚Ä¢\n\nStrong Empirical Performance Across Diverse Tasks:\nAdaFuse achieves consistent gains over the strongest ensemble baselines, with an average relative improvement of 6.88% across open-domain question answering, arithmetic reasoning, and machine translation benchmarks.\n\n\n\n\n\n\n\n2 Related Works\n\nEnsemble methods in LLMs have evolved significantly, with prominent approaches categorized into three main levels: Sample-level, Span-level, Token-level ensembling, each designed to harness complementary model strengths yet plagued by its own drawbacks.\n\n\nSample‚ÄëLevel Ensembling\n\naggregates entire generated responses from multiple LLMs. LLM‚ÄëBlender Jiang et al. (2023b) trains a pairwise ranker to compare and select the strongest candidate, then applies a generative fusion module to produce a final answer. Routing methods Shnitzer et al. (2023); Lu et al. (2023) assign each query to the most suitable expert model, but their effectiveness is capped when every candidate output contains errors Hu et al. (2024). Fusion networks Wang et al. (2023a) go one step further by learning to merge several complete outputs into a single consolidated response, boosting overall quality but still facing challenges in generalizing across diverse tasks and overlooking valuable token‚Äëlevel probability cues generated during decoding Lin et al. (2025); Li et al. (2025).\n\n\n\nSpan-Level Ensembling\n\ncombines output segments. Cool-Fusion¬†Liu et al. (2024) merges model outputs once they reach common word boundaries, while SweetSpan¬†Xu et al. (2024) generates and merges spans based on perplexity scores. SpecFuse¬†Lv et al. (2024) aggregates multiple LLM outputs by predict"
  },
  {
    "title": "Chaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards",
    "url": "https://arxiv.org/abs/2601.06021v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents' reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose \\",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n\n2.1 Preliminary\n\nDeep Search Agents.\nSynthetic Deep Search Training Data.\n\n\n\n2.2 Citation-Aware Rubric Rewards\n\n2.2.1 Rubric Initialization\n\n2.2.2 Reward Computation\n\nStep 1: Hidden Entity Identification.\nStep 2: Citation-based Rubric Judgment.\nStep 3: Evidence Connectivity Check.\n\n\n\n\n2.3 C-GRPO\n\n\n\n3 Experiments\n\n\n3.1 Experiment Setup\n\nModels and Training Data.\nEnvironment Settings\nBaselines.\nTraining Details.\nBenchmarks and Evaluation Details.\n\n\n3.2 Main Result\n\n3.3 More Analysis\n\nTraning dynamics.\nComprehensiveness and factuality.\nGeneralize to open-ended deep research tasks.\n\n\n\n3.4 Ablation Studies\n\nEffect of rubric reward weight.\nEffect of hidden entity identification.\nEffect of evidence connectivity check.\nAdding rubric rewards for all rollouts.\n\n\n\n\n\n4 Related Works\n\nRL for Deep Search Agents.\nAligning LLMs with Rubric Rewards.\n\n\n5 Conclusion\n6 Limitations\n7 Ethical Considerations\nA Trajectory Format\nB Details of Referred Deep Search Agents\nC Human Verification for LLM Judge\nD Case Studies\nE Prompts\n\n\n\n\n\nChaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards\n\n\n\nJiajie Zhang1,\nXin Lv2,\nLing Feng1,\nLei Hou1,\nJuanzi Li1\n1Tsinghua University, 2Zhipu AI\nWork was done when JZ interned at Zhipu AI.\n\n\nAbstract\nReinforcement learning (RL) has emerged as a critical technique for enhancing LLM-based deep search agents. However, existing approaches primarily rely on binary outcome rewards, which fail to capture the comprehensiveness and factuality of agents‚Äô reasoning process, and often lead to undesirable behaviors such as shortcut exploitation and hallucinations. To address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity. CaRR decomposes complex questions into verifiable single-hop rubrics and requires agents to satisfy these rubrics by explicitly identifying hidden entities, supporting them with correct citations, and constructing complete evidence chains that link to the predicted answer.\nWe further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), which combines CaRR and outcome rewards for training robust deep search agents.\nExperiments show that C-GRPO consistently outperforms standard outcome-based RL baselines across multiple deep search benchmarks. Our analysis also validates that C-GRPO effectively discourages shortcut exploitation, promotes comprehensive, evidence-grounded reasoning, and exhibits strong generalization to open-ended deep research tasks. Our code and data are available at https://github.com/THUDM/CaRR.\n\n\n\\useunder\n\\ul\n\n\n\n\nChaining the Evidence: Robust Reinforcement Learning for Deep Search Agents with Citation-Aware Rubric Rewards\n\n\n\n\n\nJiajie Zhang1‚Ä†‚Ä†thanks: Work was done when JZ interned at Zhipu AI.,\nXin Lv2,\nLing Feng1,\nLei Hou1,\nJuanzi Li1\n\n1Tsinghua University, 2Zhipu AI\n\n\n\n\n\nFigure 1: Pure outcome rewards fail to capture shortcut exploitation and hallucinations of deep search agents.\n\n\n\n1 Introduction\n\nRecently, LLM-based deep search agents have attracted growing attention for their ability to leverage external web-browsing tools to solve complex, knowledge-intensive problems¬†Yao et al. (2023); Wang et al. (2024); OpenAI (2025a). A prominent line of research has focused on applying reinforcement learning (RL) to further enhance these agents‚Äô long-horizon information-seeking capacity in the vast and noisy web environment, typically leveraging synthetic multi-hop QA datasets that are intentionally challenging but feature short-form answers for easy verification¬†Gao et al. (2025); Wu et al. (2025a); Li et al. (2025b); Lu et al. (2025). For the efficiency and scalability of RL, existing works commonly use only outcome rewards in training, which are binary signals indicating whether the agent‚Äôs predicted final answer matches the ground truth¬†Jin et al. (2025b); Gao et al. (2025); Li et al. (2025b); Liu et al. (2025b).\n\n\nWhile these outcome-based RL methods have demonstrated notable gains¬†Li et al. (2025a); Team et al. (2025), they suffer from inherent limitations.\nAs illustrated in Figure¬†1, binary outcome rewards alone cannot accurately reflect the comprehensiveness and factuality of agents‚Äô reasoning processes¬†Shao et al. (2025b), leaving room for undesirable behaviours: Agents may arrive at the correct answer by shortcut solutions (e.g., exploiting only a few hops of information while ignoring other constraints in the question) or fortunate hallucination. Optimizing toward these flawed trajectories will result in deep search agents with diminished robustness and suboptimal performance.\n\n\nTo address these limitations, we propose Citation-aware Rubric Rewards (CaRR), a novel fine-grained reward framework for deep search agents that emphasizes reasoning comprehensiveness, factual grounding, and evidence connectivity.\nOur framework is inspired by the observation that each hop within the synthetic complex question can naturally serve as a checkpoint for evaluating the agent‚Äôs reasoning process: An ideal trajectory that completely solves the given question should satisfy all hops by revealing the identities of all intermediate hidden entities and supporting them with correct citations. Building upon this idea, our framework first employs an LLM to decompose the multi-hop question into a list of single-hop factual statements, each involves several hidden entities that should be found during exploration. These factual statements are then used as point-wise rubrics to assess the comprehensiveness and factuality of agents‚Äô trajectories. Specifically, a rubric is satisfied by a trajectory only if (1) the identities of all relevant hidden entities are explicitly revealed in the final response; (2) the factual statement, along with the identified entities, is fully supported by the cited web contents; (3) the supported rubric can be connected to the predicted final answer via other supported rubrics, thereby constituting a complete evidence chain.\nGiven a trajectory, we employ a judge LLM to check whether each rubric is satisfied following the above three criteria, and the citation-aware rubric reward is defined as the ratio of satisfied rubrics.\n\n\nBuilding on CaRR, we further introduce Citation-aware Group Relative Policy Optimization (C-GRPO), an extension of GRPO¬†Shao et al. (2024) that incorporates context-aware rubric rewards with traditional outcome rewards in RL. Specifically, C-GRPO assigns an additional weighted rubric reward to the trajectories whose outcome reward is 1.\nBy doing so, C-GRPO preserves the primary objective of finding the correct answer while encouraging the agent to produce more comprehensive and evidence-grounded reasoning processes, thereby achieving robust RL and better final performance.\n\n\nTo validate the efficacy of CaRR and C-GRPO, we conduct RL experiments on both small (4B) and large (30B) model scales. The evaluation results on four challenging deep search benchmarks indicate that C-GRPO consistently outperforms the GRPO baseline that uses pure outcome rewards, and also demonstrates significantly better performance when provided with extended context budgets. Our analysis reveals that C-GRPO successfully discourages shortcut exploitation and promotes more comprehensive, citation-supported solutions, yielding robust policies featured by rigorous self-verification and better factuality. Moreover, the agents trained with C-GRPO and synthetic QA data also generalize well on open-ended deep research tasks, even surpassing some advanced agents trained with proprietary data.\n\n\nIn summary, our main contributions include: (1) We identify key limitations of outcome-based RL in training deep search agents, including shortcut exploitation and hallucination tolerance; (2) We propose CaRR, a novel framework that provides fine-grained rewards for assessing the comprehensiveness and factuality of deep search agents; (3) We propose C-GRPO, a mixed-reward RL algorithm combining outcome rewards and context-aware rubric rewards for training robust deep search agents;\n(4) We conduct extensive experiments and thorough analysis to validate the efficacy of CaRR and C-GRPO.\n\n\nFigure 2: Overview of (a) rubric initialization; (b) computation of context-aware rubric rewards; (c) C-GRPO.\n\n\n\n\n2 Methodology\n\nIn this section, we first provide a brief overview of key concepts in deep search agents, then introduce our CaRR framework and C-GRPO algorithm.\n\n\n\n2.1 Preliminary\n\nDeep Search Agents.\n\nWe adopt the ReAct¬†Yao et al. (2023) paradigm for deep search agents. Given a question, the LLM-based agent follows an iterative cycle of thinking, action (i.e., a tool call), and observation until obtaining the final answer. A complete trajectory with TT iterations can be formalized as:\n\n\n\n‚Ñã=(œÑ1,a1,o1,‚Ä¶,œÑt,at,ot,‚Ä¶,œÑT,aT),\\displaystyle\\mathcal{H}=(\\tau_{1},a_{1},o_{1},\\dots,\\tau_{t},a_{t},o_{t},\\dots,\\tau_{T},a_{T}),\n\n(1)\n\n\nwhere œÑt\\tau_{t}, ata_{t}, oto_{t} denote the thought, action, and observation at step tt. Specifically, the action ata_{t} (1‚â§t&lt;T1\\leq t&lt;T) calls one of the following three browsing tools: (1) a search tool that retrieves top-nn relevant webpages for the given query and returns the title, URL, and snippet of each webpage; (2) an open tool that accesses the given URL and shows the head part of the page; (3) a find tool that matches the given keyword in the opened webpage and returns surrounding content of each match. While aTa_{T} is the final response, consisting of an explanation with citations and the final answer. The tool descriptions and trajectory format are detailed in Appendix¬†A.\n\n\n\nSynthetic Deep Search Training Data.\n\nRL of deep search agents typically relies on synthetic complex QA datasets¬†Gao et al. (2025); Li et al. (2025b); Lu et al. (2025). These datasets are commonly constructed from entity-c"
  },
  {
    "title": "LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection",
    "url": "https://arxiv.org/abs/2601.06016v1",
    "source": "arxiv",
    "summary": "Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segm",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methods\n\n\n3.1 Data\n\nTUSZ\nSeizeIT1\nSiena\nKvikna\n\n\n\n3.2 Implementation\n\nModel Architecture\nData Preparation\nExperimental Setup\nEvaluation\n\n\n\n\n\n4 Results and Discussion\n\n4.1 Model Performance\n4.2 Impact of Training Set\n4.3 Context Window Variation\n4.4 Model Ensembling\n4.5 Cost of Inference\n\n\n5 Conclusions\nA Dataset Details\nB Design Parameters\nC Summary of Results\nD Visualization of Model Outputs\n\n\n\n\n\nLookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection\n\n\n√û√≥r Sverrisson\n\nUniversity of Iceland, Faculty of Computer Science\n\n\nSteinn Gu√∞mundsson\n\nUniversity of Iceland, Faculty of Computer Science\n\n\n\nAbstract\nAutomated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.\n\n\n\n1 Introduction\n\nEpilepsy is a chronic brain disorder that causes unprovoked seizures due to temporary electrical disturbance in the brain. It affects people of all ages worldwide and arises from various causes, including head trauma, infections, and genetic and environmental factors. However, in many cases, the underlying cause remains unknown, and symptoms often vary considerably between individuals. Epilepsy can greatly affect quality of life, but approximately three-quarters of those diagnosed can manage seizures with medication or surgery [12, 16, 32, 41]. Effective treatment depends on an accurate diagnosis, which is typically based on the detection and analysis of seizures in the electroencephalogram (EEG) signal, a non-invasive recording of the brain‚Äôs electrical activity captured through scalp electrodes [36].\n\n\nAn EEG recording can vary noticeably depending on the patient, setting and duration of the monitoring [38]. Age plays a significant role, with neonatal, pediatric, and adult EEGs often differing substantially in both appearance and interpretation [13, 21]. Clinical recordings, such as routine EEGs, are performed in a medical setting and monitor the patient for a short period in a controlled environment. In contrast, ambulatory EEGs are often performed at home, providing extended monitoring over several days while the patient continues daily activities, increasing the chances of detecting infrequent epileptic events missed during short clinical visits [29].\n\n\nProper diagnosis often requires long-term monitoring, leading to increased use of home-based video-EEG, combining EEG recordings with synchronized video footage of the patient, which reduces the burden on both patients and healthcare systems [39, 3]. However, ambulatory recordings are usually more prone to artifacts, such as muscle movement and electrode displacement, and applying artifact removal techniques in this setting can be challenging [27].\n\n\nEEGs are typically reviewed through visual examination of a montage, which arranges EEG channels to display brain activity across the scalp. The review process is labor-intensive and time-consuming, requiring expert knowledge and careful analysis to accurately identify seizures from the complex patterns in the EEG signal. Accurate identification often relies on examining the surrounding context, reviewing EEG segments before and after a suspected event to observe gradual changes or sudden shifts that may indicate seizure activity [26, 15]. Variability in seizure appearance across individuals, recording conditions, and equipment further complicates detection [1, 20].\n\n\nThese challenges have spurred growing interest in automated seizure detection methods that use algorithms to efficiently analyze EEG data. Such methods can be classified into real-time and offline approaches, with real-time systems providing immediate alerts for prompt intervention, whereas offline systems can reduce clinicians‚Äô workload and expedite the diagnostic process [2]. Over time, research in seizure detection has transitioned from rule-based techniques to the widespread adoption of deep neural networks [35]. These models have demonstrated strong capabilities in analyzing complex patterns, enabling their application across various healthcare domains [14]. However, despite advancements, automated seizure detection methods still lack the precision and reliability of human specialists [31].\n\n\nThe performance of deep learning models depends heavily on both the amount of training data and the quality of the labels in supervised settings. Therefore, the limited availability of publicly accessible EEG seizure datasets, each with distinct characteristics, poses challenges for developing models that can generalize effectively across different data sources [48]. Curating these datasets is resource-intensive and costly, and their distribution is often constrained by strict regulations surrounding personal health information [47]. Furthermore, existing datasets are typically small in size, often representing recordings from just a handful of patients. This lack of diversity can lead to substantial variability in data distributions, resulting in models trained on a particular dataset facing difficulties when generalizing to more diverse populations [48]. Moreover, disagreements between annotators on the labeling and boundaries of seizure events can introduce inconsistencies and impact model training [5, 37].\n\n\nEvaluating seizure detection models presents additional challenges due to varying application needs and the lack of standardized validation protocols [8]. In some cases, detecting the exact onset of a seizure is crucial, particularly for real-time interventions. In other cases, identifying the presence of a seizure is more important than detecting its exact location, such as when assessing the effectiveness of pharmacological treatment. These differences lead to varying validation methods and performance metrics across studies, making it difficult to compare results directly. Furthermore, reported performance metrics often do not reflect clinical translatability, as evaluations may omit considerations such as model generalisability, run-time constraints, explainability, and clinically relevant metrics [28].\n\n\nIn this paper, we present a transformer-based model architecture and conduct a comprehensive evaluation across several independent EEG datasets, which vary in clinical settings and patient demographics. The proposed model incorporates surrounding temporal context when classifying EEG segments, and we evaluate the impact of combining training data from multiple sources to enhance generalization. We employ the SzCORE [8] framework for model evaluation and assess the model‚Äôs performance on both publicly available and private datasets. In particular, we focus on a large proprietary dataset consisting of ambulatory home recordings to evaluate its clinical relevance.\n\n\n\n\n2 Related Work\n\nRecent research in automatic seizure detection increasingly favors deep learning‚Äìbased approaches, particularly Convolutional Neural Networks (CNNs) and Transformer architectures. Most CNN-based studies employ a sliding-window strategy, dividing continuous EEG recordings into shorter segments that are processed independently [42, 30, 46]. This allows CNNs to effectively capture spatial patterns and short-term temporal dynamics. For instance, Thuwajit et al. [43] used depthwise convolutions to extract channel-specific features from 4-second EEG segments, followed by spatiotemporal convolutions to model inter-channel dependencies. However, fixed window lengths limit the model‚Äôs temporal awareness, making it difficult to capture the progressive development of seizure activity.\n\n\nIn contrast, Chatzichristos et al. [7] introduced a CNN composed of three attention-gated U-Nets, each processing a different view of the EEG signal derived from distinct pre-processing strategies. Their model analyzes entire recordings in a single pass, generating predictions for every time point. Seeuws et\nal. [33] extended this idea using an event-based training objective that predicts seizure centers and durations. Whole-recording approaches capture long-term temporal dependencies but are computationally intensive for extended EEG monitoring and assume full access to the recording, which limits their suitability for real-time applications.\n\n\nEffectively capturing temporal context beyond short, localized windows remains a key challenge in EEG analysis. Capturing longer-term dependencies is important to help separate abnormal patterns from background activity, an insight that dates back to the earliest efforts to develop automated seizure detectors [18]. Early approaches, such as Temko et al. [40], proposed averaging the preceding 10 m"
  },
  {
    "title": "Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem",
    "url": "https://arxiv.org/abs/2601.06009v1",
    "source": "arxiv",
    "summary": "We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\\varepsilon$ of excursions of magnitude at least $\\varepsilon$ with the quadratic variation $[X]_T$ of the process. The sc",
    "full_text": null
  },
  {
    "title": "Don't Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks",
    "url": "https://arxiv.org/abs/2601.06007v1",
    "source": "arxiv",
    "summary": "Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 KV Cache and LLM Inference\n2.2 Prompt Caching in Provider APIs\n2.3 Agentic Workloads and Context Engineering\n\n\n\n3 Methodology\n\n3.1 Experimental Setup\n3.2 Latency Improvement\n3.3 Cache Mode Implementation\n3.4 Evaluation Protocol\n3.5 Statistical Analysis\n\n\n\n4 Results\n\n4.1 Overall Results\n4.2 Cost Reduction\n4.3 Cache Strategy Comparison\n\n\n\n5 Discussion\n\n5.1 Strategic Cache Boundary Control\n5.2 Tool Call Caching Considerations\n5.3 Provider Implementation Variability\n\n\n6 Conclusion\nA Prompt caching pricing at time of evaluation\nB Prompt Caching Mechanism\n\nC Cache Strategy Implementations\n\nC.1 No Cache (Baseline)\nC.2 Full Context Caching\nC.3 System Prompt Only Caching\nC.4 Exclude Tool Results Caching\n\n\n\n\n\n\n\nDon‚Äôt Break the Cache: An Evaluation of Prompt Caching\nfor Long-Horizon Agentic Tasks\n\n\nElias Lumer\n\n‚ÄÉ‚ÄÉ\nFaheem Nizar\n\n‚ÄÉ‚ÄÉ\nAkshaya Jangiti\n\n‚ÄÉ‚ÄÉ\nKevin Frank\n\n‚ÄÉ‚ÄÉ\nAnmol Gulati\n\n‚ÄÉ‚ÄÉ\nMandar Phadate\n\n‚ÄÉ‚ÄÉ\nVamse Kumar Subbiah\n\n\n\nAbstract\nRecent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.\n\nPrompt Caching, Large Language Models, Agentic AI, Tool Calling, API Optimization\n\n\nPricewaterhouseCoopers, U.S.\n\n\n\n1 Introduction\n\nRecent advancements in Large Language Model (LLM) agents have enabled complex, long-horizon agentic tasks that require extensive tool calling across multi-turn conversations (Ji, 2025). Through function calling, LLM agents can invoke external APIs, execute web searches, interact with databases, and perform domain-specific actions on behalf of users. As these agentic workloads grow in complexity, conversations can span dozens of API calls with context windows accumulating tens of thousands of tokens, leading to significant costs and latency overhead. To address this, major LLM providers including OpenAI, Anthropic, and Google now offer prompt caching, a feature that reuses previously computed key-value (KV) tensors from attention layers to avoid redundant computation on repeated prompt prefixes (OpenAI, 2026; Anthropic, 2026; Google Cloud, 2026a).\n\n\nFigure 1: \nPrompt caching benefits (best cache mode per model).\nPercentage reduction in API cost and time to first token (TTFT) relative to a no-cache baseline.\nFor each model, results correspond to the best-performing cache strategy.\nAsterisks denote statistically significant TTFT improvements (p&lt;0.05p&lt;0.05).\n\n\n\nWhile providers offer reduced pricing for cached input tokens, the benefits of prompt caching in real-world agentic workloads remain under-explored in the research literature. Existing work on KV cache optimization focuses primarily on inference-level memory management and compression (Kwon et al., 2023; Ge et al., 2023; Shi et al., 2024), rather than evaluating the enterprise-grade prompt caching features offered through provider APIs. Concurrent work has audited prompt caching across providers to detect timing side-channel vulnerabilities (Gu et al., 2025), but to our knowledge, no prior work has quantified the cost benefits of prompt caching or compared caching strategies for agentic workloads. This gap is particularly significant given the recent proliferation of long-running agents for deep research, coding assistance, and autonomous task completion, where prompt caching could substantially reduce operational costs and improve user experience through faster response times.\n\n\nIn this paper, we present the first comprehensive evaluation of prompt caching strategies for long-horizon agentic tasks across three major LLM providers (OpenAI, Anthropic, and Google) using four flagship models (Figure¬†1). We compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench (Du et al., 2025), a multi-turn agentic benchmark where agents autonomously execute web search tool calls to answer complex research questions. Our evaluation spans over 500 agent sessions with 10,000-token system prompts, measuring both API cost and time to first token (TTFT) across all conditions.\n\n\nOur evaluation reveals three key findings:\n\n\nPrompt caching delivers substantial and consistent cost savings across all providers: All four models tested show statistically significant cost reductions when prompt caching is enabled. Cost savings range from 45% to 80% across providers. These savings are consistent across all three caching strategies, demonstrating that prompt caching provides reliable cost benefits regardless of the specific caching approach employed.\n\n\nLatency improvements vary significantly across providers and require careful strategy selection: Time to first token improvements range from 13% to 31% across providers, though latency variance differs substantially between providers. Notably, the cache strategy that maximizes cost savings does not always maximize latency improvement, highlighting the importance of strategy selection based on optimization goals.\n\n\nStrategic cache boundary control outperforms naive full-context caching: Providers abstract much of the caching mechanism, automatically triggering cache creation when token thresholds are exceeded. However, naively enabling full-context caching can paradoxically increase latency, as dynamic tool calls and results may trigger cache writes for content that will not be reused across sessions. By strategically controlling cache boundaries, such as caching only the system prompt or explicitly excluding tool results, practitioners can ensure that only stable, reusable content is cached. Our results show that system prompt only caching provides the most consistent benefits across both cost and latency dimensions.\n\n\n\n\n2 Background\n\n\n2.1 KV Cache and LLM Inference\n\nLarge Language Model inference consists of two distinct phases: the prefill phase, where the model processes the input prompt and generates attention key-value (KV) tensors, and the decode phase, where the model autoregressively generates output tokens (Pope et al., 2022). During prefill, the model computes attention over the entire input sequence, producing KV tensors that capture the contextual representations needed for subsequent generation. These KV tensors are stored in the KV cache and reused during decoding to avoid redundant computation, enabling efficient token-by-token generation (Not Lain, 2025).\n\n\nAs context windows have grown from thousands to millions of tokens, KV cache management has become a critical bottleneck in LLM serving (Shi et al., 2024). The memory footprint of KV caches scales linearly with sequence length and batch size, often consuming more GPU memory than the model weights themselves for long-context workloads. This has motivated extensive research on KV cache optimization, including memory management techniques such as PagedAttention (Kwon et al., 2023), which applies paging-style memory management to reduce fragmentation and waste, achieving 2-4x throughput improvements. Other approaches focus on KV cache compression through selective retention of important tokens (Ge et al., 2023), storage-compute tradeoffs that balance recomputation against cache loading (Jin et al., 2024), and shared prefix optimization for high-throughput inference (Juravsky et al., 2024; Wu and others, 2024; Sun et al., 2025; Zhou et al., 2024).\n\n\n\n\n2.2 Prompt Caching in Provider APIs\n\nWhile KV caching is a general inference optimization technique, prompt caching refers to the productized, provider-managed features that reuse KV tensors across API requests when prompts share common prefixes (OpenAI, 2026; Gim et al., 2024). By caching the KV tensors from the prefill phase, providers can skip redundant computation when subsequent requests begin with the same content, reducing both latency and cost for users.\n\n\nMajor LLM providers have implemented prompt caching with varying approaches. OpenAI offers automatic prompt caching on GPT-4o and newer models, where caching activates automatically for prompts exceeding a minimum token threshold, with cache hits occurring only for exact prefix matches (OpenAI, 2026, 2024a). Anthropic provides developer-controlled caching through explicit cache breakpoints, allowing users to specify which portions of their prompt should be cached, with configurable time-to-live (TTL) options (Anthropic, 2026, 2025c). Google offers both implicit caching, which activates automatically w"
  },
  {
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "url": "https://arxiv.org/abs/2601.06002v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminary: Cold-start LLMs for Long Chain-of-Thought\n\nOnly distillation from strong reasoning LLMs works.\nDistillation from randomly selected ICL by weak instruct LLMs does not work.\nEven human-annotated Long-CoT-like traces fail.\n\n\n\n3 Hypothesis: Stable ‚ÄúMolecular Structure‚Äù in Long CoT\n\nDeep Reasoning as Covalent Bonds\nSelf-Reflection as Hydrogen Bonds\nSelf-Exploration as Van der Waals Forces\n\n\n\n4 Verification: Molecular Structure\n\n4.1 Stable Bond Distribution in Long CoT\n4.2 SFT actually learns these bond structures rather than keywords.\n\n4.3 ‚ÄúLogical Bonding-Folding‚Äù Structure\n\nDeep-Reasoning stabilizes logical cluster by Covalent Bonding.\nSelf-Reflection drives strong folding to previous steps by Hydrogen Bonding.\nSelf-Exploration gently links different long-distance clusters by Van der Waals Forces.\n\n\n4.4 Attention ‚áî\\Leftrightarrow Energy Level of Bonds\n\n\n\n5 Feature: Effective Semantic Isomers\n\n\n5.1 Bond structure of Semantic Isomers is the key to Long-CoT learning\n\nWell-structured Semantic Isomers can be effective for Long-CoT learning.\nThe model has multiple effective Semantic Isomers, and slight differences can significantly affect the results.\nSimulating effective Semantic Isomer structures is the key to ICL distillation.\n\n5.1.1 Not all bonds in isomers are effective.\n\nEffective reasoning bond distribution influences the information divergence speed in reasoning dynamics.\nEffective reasoning bonds cause metacognitive oscillation and alignment.\n\n\n\n\n\n5.2 Conflict between Two Stable Structures\n\nLearning two heterogeneous stable structures at the same time will lead to structural chaos in the model.\nThis structural chaos leads to a significant decline in the performance of the model.\n\n\n\n\n\n6 Synthetic Chemistry: Synthesis Long CoT Molecules from Scratch\n\nMole-Syn Methodology.\nMole-Syn can synthesize effective bond structures.\nMole-Syn can further trigger stronger and continually improving RL.\n\n\n\n7 Function: Shaping Function of Each Bond in Long CoT Structure\n\nDeep Reasoning is densing major Structure Formation.\nSelf-Reflection is densing and stabilize the global logics.\nSelf-Exploration expand logical space.\n\n\n\n8 Deteriorated Molecular Structure Cannot Be Easily Restored\n\nHow Current Private LLMs Protect Their Long CoT from Distillation?\nSummarization break reasoning bond distributions to prohibit distillation.\n\n\n9 Related Work\n10 Conclusion\n\n11 General Experimental Setting\n\nTarget Model.\nData Source.\nEvaluation Benchmarks.\nInference and Metrics.\n\n\n\n12 Detailed Experimental Settings for Preliminary Study\n\nSetting 1: Distillation from Strong Reasoning LLMs.\nSetting 2: Distillation from Weak Instruction LLMs (ICL-Distill).\nSetting 3: Fine-tuning on Human-Annotated Traces.\n\n\n\n13 Reasoning Bond in Long CoT\n\n\n13.1 Mathematical Definition &amp; Analysis of Reasoning Bonds\n\n\n13.1.1 Reasoning Behavior Definition.\n\nBehavior space and labeled transitions.\nDefinition 1 (Deep Reasoning: ùíü\\mathcal{D}).\nDefinition 2 (Self-Reflection: ‚Ñõ\\mathcal{R}).\nDefinition 3 (Self-Exploration: ‚Ñ∞\\mathcal{E}).\nRemark (Normal Operation; ùí©\\mathcal{N}).\nNote:\n\n\n13.1.2 Attention Energy Definition of Reasoning Bonds.\n\n13.1.3 Attention-Energy Ordering of Reasoning Bonds\n\nRoPE model\nBond random variables and expectations\nAssumptions\nTheorem 1 (Expected bond-energy order under RoPE).\nProof.\nThe lower bound between two bond energies.\nLemma (Finite-sample ordering with high probability).\n\n\n\n13.1.4 Low-energy edges dominate path aggregation.\n\nStep-level dependency graph.\nStep-level edge energy.\nReasoning paths and path energy.\nSoft-min (effective) energy over all paths.\nLow-energy edges define effective constraints.\nProof.\nAnalysis.\n\n\n\n13.1.5 Long CoT process is looking for a more stable reasoning structure.\n\nStep 1. Stationary behavior frequencies.\nStep 2. Decomposition of the time-averaged energy.\nStep 3. Conditional convergence of energy averages.\nStep 4. Exponential routing preference from Gibbs attention.\n\n\n\n\n\n13.2 Stable Reasoning Bond Distribution in Long CoT\n\nReasoning Chain Generation\nBond Type Annotation\nTransfer Distribution Analysis\n\n\n\n13.3 Logical Bonding-Folding Structure in Long CoT\n\n13.3.1 Geometric Embedding and Visualization of Bonding Structure\n\n13.3.2 Geometric metrics for folding\n\nSelf-reflection: local movement and reconnection.\nDeep reasoning: path length and cluster-level proximity.\nSelf-Exploration: novelty and sustained drift.\n\n\n\n\n13.4 Attention Energy Levels of Different Logical Bonds\n\n13.5 SFT Learning of Bond Structures\n\n\n13.5.1 Setting: How does SFT actually learn these bond structures?\n\nCross-coder Sparse Auto-Encoder (SAE) Architecture.\nKeyword manipulation dataset construction.\n\n\n\n\n\n\n\n14 Semantic Isomer Construction Details\n\n14.1 Distillation of Well-structured Semantic Isomers\n\n14.2 ICL Simulation of Semantic Isomer Structures\n\nDemonstration Construction.\nDemonstration Selection.\nICL-based Distillation\n\n\n\n14.3 Information Flow and Metacognitive Oscillation Analysis\n\n\n14.3.1 Setting: Information Flow Analysis and Metacognitive Oscillation Quantification\n\nInformation Flow Analysis in Phase Space (Human vs. R1)\nAnalysis of Metacognitive Oscillation\n\n\n\n\n\n14.4 Details about Conflict Learning Between Two Stable Structures\n\nSetup about performance analysis on different training data mixture strategies.\nPearson correlation coefficient\nbetween transfer distribution.\n\n\n\n\n\n15 Details about Synthetic Long CoT with Mole-Syn\n\n15.1 Supervised-Finetuning with Mole-Syn\n15.2 Reinforcement Learning with Mole-Syn Initialization\n\n\n\n16 Details of Bond Shaping Function Analysis\n\n16.1 Deep Reasoning: Densing the Primary Structure.\n16.2 Self-Exploration: Expanding the Logical Space.\n16.3 Self-Reflection: Densifying and Stabilizing the Logical Results.\n\n\n\n17 Analysis of the impact of length and diversity in reasoning behaviors\n\n17.1 Impact of bond-enhanced behaviors on performance\n17.2 Effect of bond length on reasoning quality\n\n\n\n18 Details of LLM Structure Reconstruction\n\n18.1 Reasoning compression or summarization data collection.\n18.2 Summarize Reasoning Process Analysis.\n\n\n\n\n\n\n\n\n1]ByteDance Seed China\n2]LARG, SCIR, Harbin Institute of Technology\n3]Peking University\n4]2077AI Foundation\n5]Nanjing University\n6]M-A-P\n7]Central South University\n\nThe Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning\n\n\nQiguang Chen\n\n‚ÄÉ‚ÄÉ\nYantao Du\n\n‚ÄÉ‚ÄÉ\nZiniu Li\n\n‚ÄÉ‚ÄÉ\nJinhao Liu\n\n‚ÄÉ‚ÄÉ\nSongyao Duan\n\n‚ÄÉ‚ÄÉ\n\nJiarui Guo\n\n‚ÄÉ‚ÄÉ\nMinghao Liu\n\n‚ÄÉ‚ÄÉ\nJiaheng Liu\n\n‚ÄÉ‚ÄÉ\nTong Yang\n\n‚ÄÉ‚ÄÉ\nGe Zhang\n\n‚ÄÉ‚ÄÉ\n\nLibo Qin\n\n‚ÄÉ‚ÄÉ\nWanxiang Che\n\n‚ÄÉ‚ÄÉ\nWenhao Huang\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n{qgchen,car}@ir.hit.edu.cn\n\nduyantao@bytedance.com\n\n\n(Jan 12, 2026)\n\nAbstract\nLarge language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.\n\n\n\\correspondence\n,\n\n\nFigure 1: The hypothesis that stable molecular structure in Long CoT arises from three key ‚Äúchemical‚Äù bonds.\n\n\n\n1 Introduction\n\nRecently, large language models (LLMs) have excelled on diverse reasoning tasks via explicit chain-of-thought (CoT) rationales [1, 2, 3, 4]. Yet, they struggle to cold-start from instruction-tuned or base models into Long CoT models requiring extended multi-step reasoning [5, 6]. Notably, Du et al. [7] shows that humans generate Long CoT rationales without imitating DeepSeek-R1 [8]. Our preliminary studies reveal that standard supervised fine-tuning and distillation from human or Instruction LLM rationales (using randomly sampled Long CoT examples) fail to reliably instill these skills in LLMs. Models often lose coherence over long trajectories or fail to transfer patterns to novel tasks. This prompts a key question:\n\n\n\nHow do Large Language Models learn and represent effective Long Chain-of-Thought?\n\n\n\nFigure 2: Comparison of prior chain- or tree-like structures and our molecular structure. Reasoning starts from Mole. 0, uses deep reasoning on strongly related structures, then employs self-exploration for new logic in Mole. 1. When meet errors, reasoning utilize self-reflection to guide chain to optimized Mole. 0+.\n\n\nTo explain this, we posit that they acquire the organization of reasoning trajectories. As shown in Figure 2, prior studies model these as logic nodes in sequences or trees of steps. Yet, our analysis of Long CoT across strong reasoning models reveals a stable distribution of three core behaviors across tasks and architectures: Deep-Reasoning, Self-Reflection, and Self-Exploration [5], which node-centric views fail to capture.\n\n\nThis finding triggers a molecular-inspired, distributional view: we model behavior-labeled logic edges as interaction bonds and examine how their global molecular-like structure ensures long-horizon reasoning stability. Specifically, Deep-Reasoning forms dense local clusters of coupled deductions, like covalent bonds; Self-Reflection creates long-range corrective links to prior steps, like hydrogen bonds; and Self-Exploration forges weak bridges between distant clusters, like van der Waals forces. Thus, high-quality Long CoT arises from the stable composition and arrangement of these bond types, guiding effective learning111Note: C, H, and O atom references are analogies for molecular"
  },
  {
    "title": "Open-Vocabulary 3D Instruction Ambiguity Detection",
    "url": "https://arxiv.org/abs/2601.05991v1",
    "source": "arxiv",
    "summary": "In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like \"Pass me the vial\" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instructio",
    "full_text": null
  },
  {
    "title": "CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks",
    "url": "https://arxiv.org/abs/2601.05988v1",
    "source": "arxiv",
    "summary": "Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-base",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Network Log Data as a Graph\n2.2 Link Prediction for Lateral Movement Detection\n2.3 Foundation Models\n\n\n\n3 System Design\n\n3.1 Graph Foundation Models\n3.2 Graph Construction\n3.3 Pretraining\n3.4 Fine-tuning &amp; Anomaly Detection\n\n\n\n4 Results\n\n4.1 Datasets\n4.2 Prior works\n4.3 Setup\n4.4 Anomalous Network Activity Detection\n4.5 Ablation Studies\n\n\n5 Efficiency Study\n6 Discussion\n7 Related Work\n8 Conclusion\n\n\n\n\n\nCyberGFM: Graph Foundation Models for \nLateral Movement Detection in Enterprise Networks\n\n\n\nIsaiah J. King¬†11footnotemark: 1¬†¬†22footnotemark: 2\n\n‚ÄÉ‚ÄÉ\nBernardo Trindade¬†22footnotemark: 2\n\n‚ÄÉ‚ÄÉ\nBenjamin Bowman¬†11footnotemark: 1\n\n‚ÄÉ‚ÄÉ\nH. Howie Huang¬†11footnotemark: 1¬†¬†22footnotemark: 2\n\n‚ÄÉ‚ÄÉ\n\n11footnotemark: 1¬†¬†Cybermonic LLC. McLean, VA, USA. \n22footnotemark: 2¬†¬†The George Washington University. Washington, DC, USA\n\n\n\nAbstract\nRepresenting networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection.\nExisting works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks.\nHowever, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train.\nIn this work, we propose extending the original insight from random walk-based skip-grams‚Äìthat random walks through a graph are analogous to sentences in a corpus‚Äìto the more modern transformer-based foundation models.\nUsing language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers.\nThe graph foundation model is then finetuned for link prediction and used as a network anomaly detector.\nThis new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods.\nThis system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2√ó\\times improvement in average precision.\nWe found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.\n\n\n\n1 Introduction\n\nLateral movement is a critical stage of early compromise.\nIn this phase, the attackers have entered the network, and try to pivot through hosts to continually gain privileges, before reaching their target.\nUnder the Mitre ATT&amp;CK framework¬†[mitre_attack], lateral movement is the final stage before the attackers can achieve their objectives: data collection, command and control (C2), or impact.\nClearly, detecting it early is crucial for the safety of any network.\n\n\nPrior works in lateral movement detection achieve this by representing the network as a graph.\nIt has been shown that lateral movement in a network graph manifests as low-probability edges, which can be detected using graph link prediction¬†[bowman2020, pikachu, euler, argus].\nThese approaches train on a benign period of network data, and use this to establish a baseline of normal activity.\nDuring inference, new edges with low probability are classified as anomalous and indicative of lateral movement.\nWhile the prior works have made impressive improvements in precision, generating models with very low false positive rates, the massive size of network data necessitates heavy computation.\nRandom walk-based works like¬†[bowman2020, pikachu] are less affected by this issue if efficient sampling techniques are used, but only use shallow embedding networks, leading to worse performance.\nOn the other hand, deep learning provides greater expressiveness in node representation, meaning greater precision and recall, but doing so on such large graphs is challenging.\nPrior works¬†[euler, argus] account for this by distributing graph neural networks (GNNs) across multiple CPUs, but this comes at the cost of communication overhead and cannot take advantage of the acceleration offered by GPUs.\n\n\nThe key insight behind this work is to take the memory efficiency of random walk approaches, and analyze them using more powerful models.\nThe DeepWalk paper¬†[deepwalk] showed that random walks through graphs are analogous to sentences.\nWith this assumption in mind, they analyzed them with word2vec¬†[w2v], the best available natural language processing (NLP) model at the time.\nSince 2013, modern NLP models have moved past the context-free embeddings of word2vec, and instead analyze each word in the context of full sentences.\nLarge language models (LLMs) like GPT¬†[gpt] and BERT¬†[bert] are instead built on multiheaded self-attention networks¬†[attn], mechanisms that look at the full sentence, and embed each word in the context of every other word present.\nIn this work, we try to answer the question: How can graph analysis take advantage of recent advances in NLP?\n\n\nTo this end, we propose using random walks through a graph of network activity to train a large language model.\nLike the prior random walk-based methods for lateral movement detection¬†[bowman2020, pikachu, rabbani2024], paths through the network are treated as sentences, and used to train an NLP model.\nThe crucial difference is that the NLP model we select is much more powerful than shallow skip-gram networks that power word2vec.\nWe train both a BERT and a GPT model using benign network activity as its corpus, allowing the model to learn the likelihood of any given link, given the context of a full walk through the neighborhood.\nThis allows the model to capture distances between nodes, as well as the directionality of paths between nodes, in a way that earlier language models are unable to encode.\nUsing scheduled masked token prediction on random walks, we train a model to predict missing nodes in random walks.\nThe result of this unsupervised pretraining is a graph foundation model (GFM) with a generalized understanding of common paths through the underlying graph.\nImportantly, the transformers that power these models are designed for GPU acceleration.\nThis allows our models to train and perform inference with high efficiency without the need for multiprocessing.\n\n\nAs we will show, the GFM can be used as an anomaly detector on its own.\nGiven the source and destination nodes u,vu,v for an edge we wish to test, an (optional) edge feature ùê±(u,v)\\mathbf{x}_{(u,v)}, and a random walk terminating {n1,xe1,‚Ä¶,u,xe(u,v),[MASK]}\\{n_{1},x_{e_{1}},...,u,x_{e_{(u,v)}},\\texttt{[MASK]}\\} the GFM will output the likelihood that vv is the masked token in the sequence.\nHowever, the GFM can be further fine-tuned for our specific objective.\nPassing the GFM through an additional round of training to classify sequences as malicious or benign further improves its precision.\nUsing this method, we train a highly precise link prediction model with comparable parameters to existing works that achieves a more than 2x improvement in average precision.\nThis new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods.\nThis model, which we call CyberGFM111Source code available at https://github.com/cybermonic/CyberGFM\n, outperforms state-of-the-art unsupervised lateral movement detection models by a wide margin.\n\n\nThe primary contributions of this paper are as follows:\n\n\n\n\n‚Ä¢\n\nA novel system of lateral movement detection:\nTo the best of our knowledge, this is the first approach to model lateral movement detection as next token prediction with an LLM architecture.\nThe LLM architecture has proven itself to be highly effective in the NLP space, and our approach extends it to both graphs and cybersecurity analysis.\n\n\n\n‚Ä¢\n\nState-of-the-art anomaly detection:\nWe evaluate our model against four recent anomaly-based intrusion detection systems on three cybersecurity datasets.\nThe fine-tuned GFM outperforms all prior works on all three datasets, demonstrating the power of this approach. In particular, our model achieves a 0.76 AP score on the LANL dataset, which is more than double the previous best score.\n\n\n\n‚Ä¢\n\nGFMs for cybersecurity applications:\nWhile prior works on GFMs rely on expensive subgraph sampling methods, ours adopts the DeepWalk approach of assuming random walks are analogous to sentences. This allows for more efficient pretraining and the ability to process much larger graphs. Additionally, we show that models pretrained with our approach can be finetuned, and continue to make large performance improvements, while models trained directly on the fine-tuning task are slower to converge, and frequently collapse.\n\n\n\n\n\n\n\n\n(a) Traditional LLM pretraining\n\n\n\n\n(b) Proposed GFM pretraining\n\n\n\nFigure 1: (a) The traditional process to pretrain LLMs samples sentences from a large corpus, masks out tokens, then optimizes the model based on its ability to predict masked tokens in the output sequence. (b) Our proposed method for GFM pretraining follows this same paradigm, using random walks through the graph as ‚Äúsentences\" where tokens represent node IDs and edge features.\n\n\n\n\n2 Background\n\n\n2.1 Network Log Data as a Graph\n\nA graph ùí¢={ùí±,‚Ñ∞}\\mathcal{G}=\\{\\mathcal{V},\\mathcal{E}\\} is defined as a set of entities, or nodes ùí±\\mathcal{V}, and a set of relationships between them, or edges ‚Ñ∞‚äÜ{(u,v)‚à£u,v‚ààùí±}\\mathcal{E}\\subseteq\\{(u,v)\\mid u,v\\in\\mathcal{V}\\}.\nAdditionally, edges may have features ùê±(u,v)\\mathbf{x}_{(u,v)} containing important information about the interaction that occurred between the entities.\nImportantly, one edge feature that all graphs we consider will have is time: the instant that the edge between uu and vv occurred.\n\n\nThe activity of a computer network lends itself naturally to being expressed as a graph.\nHosts, users, files, etc., may be represented as the set of nodes, with important features such as their "
  },
  {
    "title": "Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks",
    "url": "https://arxiv.org/abs/2601.05984v1",
    "source": "arxiv",
    "summary": "The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework base",
    "full_text": null
  },
  {
    "title": "AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty",
    "url": "https://arxiv.org/abs/2601.05978v1",
    "source": "arxiv",
    "summary": "As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Servic",
    "full_text": null
  },
  {
    "title": "DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management",
    "url": "https://arxiv.org/abs/2601.05975v1",
    "source": "arxiv",
    "summary": "We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous \"ragged filtration\" problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information ",
    "full_text": null
  },
  {
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "url": "https://arxiv.org/abs/2601.05966v1",
    "source": "arxiv",
    "summary": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disent",
    "full_text": null
  },
  {
    "title": "Distilling Feedback into Memory-as-a-Tool",
    "url": "https://arxiv.org/abs/2601.05960v1",
    "source": "arxiv",
    "summary": "We propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refine",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Framework\n\n2.1 Probabilistic formulation\n\n2.2 The Memory-as-a-Tool protocol\n\nRetrieval operation (x‚àºp(‚ãÖ|z,M)x\\sim p(\\cdot|z,M)).\nDistillation and consolidation operation (M‚Ä≤‚àºp(‚ãÖ|M,feed)M^{\\prime}\\sim p(\\cdot|M,feed)).\n\n\n\n\n3 Rubric Feedback Bench\n\n4 Experiments\n\n4.1 Continual Learning performance\n4.2 Long-Horizon Mixed-Task Experiment\n4.3 Cost-Efficiency Analysis\n\n\n\n5 Related Work\n\nInference-time self-correction and reasoning.\nOptimization of specifications and prompts.\nLearning from feedback.\nMemory-augmented agents.\n\n\n\n6 Conclusions\n\nLimitations and further work.\n\n\n\nA Memory Mechanism Details\n\nA.1 Structure of Memory (MM)\nA.2 Content of Memory\nA.3 Write Operation: M‚Ä≤‚àºp‚Äã(M‚Ä≤|M,f‚Äãe‚Äãe‚Äãdi,xi)M^{\\prime}\\sim p(M^{\\prime}|M,feed_{i},x_{i})\nA.4 Read Operation: x‚Ä≤‚àºp‚Äã(x‚Ä≤|z,M‚Ä≤)x^{\\prime}\\sim p(x^{\\prime}|z,M^{\\prime})\n\n\n\nB Rubric Feedback Bench\n\nB.1 Example: Ethical Reasoning Rubrics (Excerpt)\nB.2 Example: Visual Writing Rubric (Excerpt)\nB.3 Example: Chaotic Writing Rubric (Excerpt)\nB.4 Example: Behavioral Rubric (Excerpt)\n\n\n\nC Experiment details\n\n\nC.1 LLM Scaffolding\n\nClaude models.\nGPT and Gemini models.\nShared configuration.\n\n\n\nC.2 Sample Memory Files from Experiments\n\nC.2.1 Visual Analysis Guidelines\nC.2.2 Experimental Critique Style (Anti-Rubric)\nC.2.3 Deontological Ethics Guidelines\n\n\nC.3 Evaluator Prompts\n\n\n\n\n\n\n\nDistilling Feedback into Memory-as-a-Tool\n\n\nV√≠ctor Gallego \nKomorebi AI Technologies\nMadrid, Spain \nvictor.gallego@komorebi.ai\n\n\n\n\nAbstract\nWe propose a framework that amortizes the cost of inference-time reasoning by converting transient critiques into retrievable guidelines, through a file-based memory system and agent-controlled tool calls. We evaluate this method on the Rubric Feedback Bench, a novel dataset for rubric-based learning. Experiments demonstrate that our augmented LLMs rapidly match the performance of test-time refinement pipelines while drastically reducing inference cost.\nCode: github.com/vicgalle/feedback-memory-as-a-tool\nData: hf.co/datasets/vicgalle/rubric-feedback-bench\n\n\n\n1 Introduction\n\nRecent advances in ‚ÄùSystem 2‚Äù scaling have established that trading test-time compute for accuracy (Snell et al., 2024; Guo et al., 2025) (through techniques like iterative self-correction, chain-of-thought, or search (Wei et al., 2022; Madaan et al., 2024; Shinn et al., 2024)) unlocks reasoning capabilities that far exceed standard zero-shot performance. However, a critical limitation of these approaches is their computational expense and episodic nature; the reasoning process must be repeated ab initio for every new query, treating each interaction in isolation. This results in a massive redundancy where the model frequently re-derives the same insights or corrections, effectively ‚Äùforgetting‚Äù its improvements the moment the context window closes. While methods like fine-tuning can persist these behaviors, they are costly and lack the flexibility to adapt rapidly to new, user-defined specifications or rubrics.\n\n\nTo bridge this gap, we propose Distilling Feedback into Memory-as-a-Tool, a framework that amortizes the high cost of inference-time reasoning by converting transient evaluations or critiques into persistent, retrievable guidelines. Instead of discarding the feedback generated during iterative refinement, our approach empowers the language model (LLM) to synthesize abstract principles from its errors and explicitly write them to a file-based memory system using tool calling (Chowa et al., 2025). By treating memory not as a passive store of raw episodic logs but as a curated ‚Äùlessons learned‚Äù journal, the agent consolidates specific experiences into semantic rules to be applied to future tasks zero-shot (Wang and Taylor, 2018; Lei et al., 2025), even when tasks are interleaved with different objectives. This mechanism allows the model to maintain the high performance of compute-heavy refinement pipelines while drastically reducing inference costs, without requiring parameter updates. Furthermore, in long-horizon experiments with interleaved task types, our approach demonstrates robust knowledge consolidation with minimal interference. Section 5 contains a discussion on related works.\n\n\nFigure 1: High-level overview of the proposed framework\n\n\n\n\n2 Framework\n\n\n2.1 Probabilistic formulation\n\nWe represent generating a text xx from a LLM with a context c‚Äãt‚Äãxctx (eg system and user prompts, chains of thought, tool observations‚Ä¶) as sampling from a conditional distribution x‚àºp‚Äã(x|c‚Äãt‚Äãx)x\\sim p(x|ctx). In standard iterative refinement (Madaan et al., 2024), the model generates a draft x0x_{0}, the same or another model produces a critique or feedback c‚àºp‚Äã(c|x0,‚Ñõ)c\\sim p(c|x_{0},\\mathcal{R}) based on a rubric ‚Ñõ\\mathcal{R}, and then the output is refined: x‚Ä≤‚àºp‚Äã(x‚Ä≤|x0,c)x^{\\prime}\\sim p(x^{\\prime}|x_{0},c).\nThe revised response now better reflects the desired aspects from the rubric ‚Ñõ\\mathcal{R} (e.g. writing style or safety principles). A limitation of this approach is that it is a non-parametric refinement method, that is, it is done at inference-time for each new task prompt, which is computationally expensive as it is a sequence of multiple LLM calls. While effective, this loop is ephemeral: the learning signal contained in the critique cc is lost once the context resets.\n\n\nTo this end, we propose to amortize the refinement process, distilling the evaluator feedback into a memory accessible to the LLM through tool calls. We introduce a persistent memory state MM, represented as a set of human-readable documents.\nNow, the generation and learning process, for a single example, is given by:\n\n\n1.\n\nGenerate an original response xix_{i} for a given task prompt ziz_{i}: xi‚àºp‚Äã(xi|zi)x_{i}\\sim p(x_{i}|z_{i})\n\n\n\n2.\n\nEvaluator LLM provides feedback using a private rubric ‚Ñõ\\mathcal{R}: f‚Äãe‚Äãe‚Äãdi‚àºpeval‚Äã(f‚Äãe‚Äãe‚Äãdi|xi,‚Ñõ)feed_{i}\\sim p_{\\mbox{eval}}(feed_{i}|x_{i},\\mathcal{R})\n\n\n\n3.\n\nLLM decides to update its memory (tool call): M‚Ä≤‚àºp‚Äã(M‚Ä≤|M,f‚Äãe‚Äãe‚Äãdi,xi)M^{\\prime}\\sim p(M^{\\prime}|M,feed_{i},x_{i})\n\n\n\nNow, at test time, before generating a response to a new task, the LLM decides to read relevant memories through a tool call, so it can generate directly a refined response, x‚Ä≤‚àºp‚Äã(x‚Ä≤|z,M‚Ä≤)x^{\\prime}\\sim p(x^{\\prime}|z,M^{\\prime}). Figure 1 shows a diagrammatic representation of the complete process.\n\n\n\n\n2.2 The Memory-as-a-Tool protocol\n\nTo operationalize the probabilistic framework described above, we implement MM not as a vector database, but as a persistent file system accessible via tool calls. This treats memory management as an agentic reasoning task (Wu et al., 2025).\n\n\nRetrieval operation (x‚àºp(‚ãÖ|z,M)x\\sim p(\\cdot|z,M)).\n\nUnlike semantic search which relies on opaque embeddings, our system requires the LLM to actively reason about what to retrieve. Before generating a response, the agent executes a two-step retrieval: i) Listing: the model calls ls(path=\"/memories/\") to enumerate available files. This requires the model to maintain semantically meaningful filenames (e.g., ‚Äòreview_creative_rubric.txt‚Äò vs ‚Äònote_1.txt‚Äò); and ii)\nReading: based on the filenames and the current task zz, the model selects relevant files and calls read_file(path). The content mm is returned as a tool result to the LLM context, effectively priming the model with ‚Äùlessons learned‚Äù from previous episodes.\n\n\n\nDistillation and consolidation operation (M‚Ä≤‚àºp(‚ãÖ|M,feed)M^{\\prime}\\sim p(\\cdot|M,feed)).\n\nThe write operation is the core mechanism for amortization. Upon receiving negative feedback, the model triggers a memory update to consolidate the transient episode into lasting knowledge. This process involves two steps: i) Abstraction: the model transforms raw feedback (e.g., ‚ÄùYou failed to use synesthetic language in paragraph 2‚Äù) into a generalizable policy (e.g., ‚ÄùKey Principle: Prioritize synesthetic blending‚Äîcolors that sound, sounds that taste‚Äù); and ii) Conflict resolution: the model decides whether to create a new file via write_file or update an existing one via edit_file. This allows the agent to deduplicate knowledge and resolving contradictions between old and new feedback.\n\n\nBy explicitly writing to a file system, the agent creates a curated journal of principles. This structure ensures that MM remains a high-signal knowledge base of synthesized rules rather than a noisy log of raw interaction history. Table 1 presents the system prompt used to instantiate the framework, and for a detailed explanation and behavior of the memory system, see Appendix. A.\n\n\n\n\n\n\n\nYou are an expert writer that can plan before generating the final text. When writing a text for a task, always display the final version directly to the user.\n\n\n\n\n\n\nBefore generating a text for a user task, check your ./memories/ directory for relevant notes from previous related tasks, and use that knowledge if the new task is related.\n\n\n\n\n\n\nWhen receiving feedback from the user about a text, take notes in your ./memories/ about what to improve for next time.\n\n\n\n\n\n\nUse general names for the filename, since we are aiming for generalization and reusability (e.g., ‚Äò‚Äòresearch_notes.txt‚Äô‚Äô instead of ‚Äò‚Äòresearch_notes_for_task_123.txt‚Äô‚Äô). You can also update existing memory files with new knowledge, but remember the aim is generalization, not focusing on concrete examples.\n\n\n\n\n\n\nBe organized and methodical in your approach to use the memory effectively to achieve better feedback from the user over time.\n\n\n\n\n\nTable 1: System prompt used to instantiate the Memory-as-a-Tool framework.\n\n\n\n\n\n\n3 Rubric Feedback Bench\n\nWe introduce Rubric Feedback Bench111Released at https://huggingface.co/datasets/vicgalle/rubric-feedback-bench, a novel evaluation dataset comprising 42 carefully curated scenarios designed for studying learning from structured, rubric-based feedback. The dataset scenarios are distributed across five distinct task categories, each with several prompts sharing task-specific rubrics, dealing primarily with open-ended writing. These scenarios cover a wi"
  },
  {
    "title": "On the Robustness of Age for Learning-Based Wireless Scheduling in Unknown Environments",
    "url": "https://arxiv.org/abs/2601.05956v1",
    "source": "arxiv",
    "summary": "The constrained combinatorial multi-armed bandit model has been widely employed to solve problems in wireless networking and related areas, including the problem of wireless scheduling for throughput optimization under unknown channel conditions. Most work in this area uses an algorithm design strategy that combines a bandit learning algorithm with the virtual queue technique to track the throughp",
    "full_text": null
  },
  {
    "title": "A Critical Examination of Active Learning Workflows in Materials Science",
    "url": "https://arxiv.org/abs/2601.05946v1",
    "source": "arxiv",
    "summary": "Active learning (AL) plays a critical role in materials science, enabling applications such as the construction of machine-learning interatomic potentials for atomistic simulations and the operation of self-driving laboratories. Despite its widespread use, the reliability and effectiveness of AL workflows depend on implicit design assumptions that are rarely examined systematically. Here, we criti",
    "full_text": null
  },
  {
    "title": "Performance of a Deep Learning-Based Segmentation Model for Pancreatic Tumors on Public Endoscopic Ultrasound Datasets",
    "url": "https://arxiv.org/abs/2601.05937v1",
    "source": "arxiv",
    "summary": "Background: Pancreatic cancer is one of the most aggressive cancers, with poor survival rates. Endoscopic ultrasound (EUS) is a key diagnostic modality, but its effectiveness is constrained by operator subjectivity. This study evaluates a Vision Transformer-based deep learning segmentation model for pancreatic tumors. Methods: A segmentation model using the USFM framework with a Vision Transformer",
    "full_text": null
  },
  {
    "title": "Can We Predict Before Executing Machine Learning Agents?",
    "url": "https://arxiv.org/abs/2601.05930v1",
    "source": "arxiv",
    "summary": "Autonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm. Previous approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution. To bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instan",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 The Paradigm of Autonomous ML Agents\n2.2 The Execution Bottleneck\n2.3 Implicit World Modeling in Data Domains\n\n\n\n3 Preference Corpus Curation\n\n3.1 Task Definition\n3.2 Source and Scope\n3.3 Dataset Curation and Instantiation\n3.4 Input Augmentation: The Verified Data Analysis Report\n\n\n\n4 Main Experiments\n\n\n4.1 Experimental Setup\n\nModels and Inference Configuration.\nMetrics and Baselines.\n\n\n4.2 Main Results: Feasibility of Run-Free Preference\n\n\n\n5 Analysis &amp; Insights\n\n\n5.1 RQ1: The Cognitive Mechanism of Data Representation\n\nFinding 1: Predictive Success Stems from Semantic Data Understanding, Not Simple Complexity Heuristics.\n\n\n\n5.2 RQ2: Capabilities, Boundaries, and Algorithmic Bias\n\nFinding 2: Reasoning Unlocks Capabilities, Yet Distinct Cognitive Boundaries Persist Across Domains.\nFinding 3: The ‚ÄúImplicit World Model‚Äù Leverages Causal Reasoning Beyond Complexity Heuristics and Exhibits Robust Confidence Calibration.\n\n\n\n5.3 RQ3: Scaling Laws of Data-centric Solution Preference\n\nFinding 4: Predictive Accuracy Violates Standard Parameter Scaling Laws.\n\n\n\n5.4 RQ4: Comparison with Human Judgment and Validation-Test Gap\n\nFinding 5: The Model Outperforms Human Intuition by Rejecting Complexity Bias.\nThe Validation-Test Gap.\n\n\n\n\n\n6 Agent Integration: ForeAgent\n\n6.1 Motivation\n6.2 Method: The Predict-then-Verify Loop\n\n6.3 Experimental Setup\n\nTasks and Baselines.\nMetric.\n\n\n6.4 Results\n\n\n\n7 Related Work\n\nLLM Agents in Machine Learning (ML).\nWorld Models for Skip-Execution.\n\n\n8 Conclusion\nCorpus Imbalance and Domain Coverage.\nAgent Framework Implementation.\n\nA Extended Related Work\n\nLLM-based Agents for Scientific Discovery\nOperational Details of Agent Baselines\nWorld Models and Execution-Free Evaluation\n\n\n\nB Corpus Details\n\nB.1 Task Metadata and Scale\nB.2 Algorithm and Architecture Distribution\nB.3 Agent Evaluation Benchmark\n\n\n\nC Detailed Experiment Result\n\nC.1 Fine-grained Performance on Prediction Corpus\nC.2 Detailed Performance Metrics of ForeAgent on AI4Science Benchmarks\nC.3 Search Efficiency Analysis of ForeAgent\nC.4 Decision Fidelity Analysis of ForeAgent\nC.5 Licensing and Artifact Usage\n\nC.6 Computational Infrastructure and Budget\n\nHardware Setup.\nToken Consumption.\n\n\nC.7 Software Dependencies and Metric Implementation\n\n\n\nD Detailed Qualitative Analysis\n\n\nD.1 Case I: Overcoming Complexity Bias (Reasoning Analysis)\n\nScenario and Conflict.\nWorld Model Reasoning.\n\n\nD.2 Case II: Sample of the Verbal Data Report\nD.3 Case III: Sample of the Task Instruction (II)\n\n\nE Prompt Templates\n\n\n\n\n\nCan We Predict Before Executing Machine Learning Agents?\n\n\n\nJingsheng Zheng‚Ä†‚Ä°, Jintian Zhang‚Ä†‚Ä°, Yujie Luo‚Ä†‚Ä°, Yuren Mao‚Ä†, Yunjun Gao‚Ä†, \nLun Du¬ß‚Ä°, Huajun Chen‚Ä†‚Ä°, Ningyu Zhang‚Ä†‚Ä°\n‚Ä†Zhejiang University ‚ÄÉ¬ßAnt Group \n‚Ä°Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph \nzhengjohnson0@gmail.com, zhangningyu@zju.edu.cn\nCorresponding Author.\n\n\nAbstract\nAutonomous machine learning agents have revolutionized scientific discovery, yet they remain constrained by a Generate-Execute-Feedback paradigm.\nPrevious approaches suffer from a severe Execution Bottleneck, as hypothesis evaluation relies strictly on expensive physical execution.\nTo bypass these physical constraints, we internalize execution priors to substitute costly runtime checks with instantaneous predictive reasoning, drawing inspiration from World Models.\nIn this work, we formalize the task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairwise comparisons.\nWe demonstrate that LLMs exhibit significant predictive capabilities when primed with a Verified Data Analysis Report, achieving 61.5% accuracy and robust confidence calibration.\nFinally, we instantiate this framework in ForeAgent, an agent that employs a Predict-then-Verify loop, achieving a 6x acceleration in convergence while surpassing execution-based baselines by +6%.\nOur code and dataset will be publicly available soon at https://github.com/zjunlp/predict-before-execute.\n\n\n\nCan We Predict Before Executing Machine Learning Agents?\n\n\n\n\nJingsheng Zheng‚Ä†‚Ä°, Jintian Zhang‚Ä†‚Ä°, Yujie Luo‚Ä†‚Ä°, Yuren Mao‚Ä†, Yunjun Gao‚Ä†,\n\nLun Du¬ß‚Ä°, Huajun Chen‚Ä†‚Ä°, Ningyu Zhang‚Ä†‚Ä°‚Ä†‚Ä†thanks: Corresponding Author.\n\n‚Ä†Zhejiang University ‚ÄÉ¬ßAnt Group\n\n‚Ä°Zhejiang University - Ant Group Joint Laboratory of Knowledge Graph\n\nzhengjohnson0@gmail.com, zhangningyu@zju.edu.cn\n\n\n\n\n\n1 Introduction\n\nAutonomous machine learning agents have emerged as powerful tools for solving complex challenges in scientific discovery¬†Zhang et¬†al. (2025d); Chen et¬†al. (2025b).\nMainstream frameworks¬†Jiang et¬†al. (2025); Ou et¬†al. (2025) typically rely on an iterative ‚ÄúGenerate-Execute-Feedback‚Äù loop where the system refines code based on runtime output¬†Yao et¬†al. (2023).\nHowever, this paradigm suffers from a severe Execution Bottleneck as physical execution is computationally expensive and slow, often consuming up to 9 hours per run in benchmarks like MLE-Bench¬†Chan et¬†al. (2025).\nIncreasingly, recent research has identified this latency issue and sought to mitigate the computational overhead through heuristic pruning strategies¬†Trirat et¬†al. (2025); Kulibaba et¬†al. (2025).\n\n\nFigure 1: From Execution to Inference.\nTraditional ML agents improve through costly execution and external feedback, incurring substantial latency.\nOur work investigates whether superior data-grounded solutions can be identified before execution by leveraging ‚ÄúImplicit Execution Priors‚Äù.\n\n\nTo fundamentally bypass these physical constraints, the concept of World Models¬†Ding et¬†al. (2025) offers a transformative alternative (Figure¬†1).\nOriginating from reinforcement learning, world models enable agents to simulate environmental dynamics and evaluate actions via internal predictions rather than external trials¬†Ha and Schmidhuber (2018); Hafner et¬†al. (2024).\nRecent advancements have extended this capability to the code domain by predicting execution outputs directly¬†Li et¬†al. (2025c); team et¬†al. (2025).\nMotivated by this, we explore whether agents can internalize execution priors, substituting costly runtime checks with instantaneous predictive reasoning.\nThe potential to replace 9 hours of physical latency with 1 second of neural speed brings us to a fundamental question:\nCan we compress hours of physical execution into seconds of logical inference?\n\n\nTo answer this question, we formalize the task of Data-centric Solution Preference, where the model must predict the relative performance of two algorithmic solutions given a data analysis report, through reasoning without physical execution.\nTo rigorously evaluate this, we construct a large-scale corpus comprising 18,438 pairwise comparisons.\nOur main experiments yield strong evidence: LLMs exhibit significant predictive capabilities, with DeepSeek-V3.2-Thinking achieving 61.5% accuracy, outperforming both random guessing (50.0%) and complexity-based heuristics (50.8%).\nFurther analysis reveals that reasoning-optimized architectures transcend complexity heuristics through genuine data reasoning, yielding well-calibrated confidence that ensures the reliability of implicit evaluation.\nFinally, we integrate this predictive mechanism into ForeAgent, an agent that employs a Predict-then-Verify loop to decouple exploration from execution, expanding the search space by 3.2√ó3.2\\times and achieving a 6√ó6\\times acceleration while delivering a +6% performance gain over standard baselines.\n\n\nIn summary, our contributions are three-fold:\n\n\n‚Ä¢\n\nWe define the novel task of Data-centric Solution Preference and construct a comprehensive corpus of 18,438 pairs, answering the titular question that LLMs Exhibit Significant Predictive Capabilities.\n\n\n\n‚Ä¢\n\nWe operationalize this framework in ForeAgent, an agent that employs a Predict-then-Verify loop to decouple exploration from execution, enabling it to expand the search space by 3.2√ó\\bm{3.2\\times} and achieve a ùüî√ó\\bm{6\\times} acceleration and a +6% performance gain over the baseline.\n\n\n\n‚Ä¢\n\nWe contribute a large-scale Open-Source Dataset of verified execution trajectories, serving as a foundational corpus for training scalable Reward Models to accelerate reinforcement learning rollouts and optimization across diverse agent frameworks.\n\n\n\n\n\nFigure 2: Overview of the Framework. (a) Task Definition: The Data-centric Solution Preference task predicts solution superiority and confidence via latent reasoning. (b-c) Data Curation: We collect and filter real-world agent trajectories to construct the Preference Corpus. (d) Augmentation: Inputs are augmented with Verified Data Reports via a ‚ÄúProfile-Verify-Verbalize‚Äù pipeline. (e) ForeAgent Application: The model serves as a filter within the Predict-then-Verify loop, predicting preference before physical execution to prune candidates.\n\n\n\n\n2 Background\n\n\n2.1 The Paradigm of Autonomous ML Agents\n\nAn autonomous Machine Learning (ML) task aims to generate an optimal solution code C‚àóC^{*} from the code space CC that maximizes a metric MM on a dataset ùíü\\mathcal{D}, given a natural language instruction II (see Appendix Figure¬†11):\n\n\n\nC‚àó=arg‚ÄãmaxC‚Å°M‚Äã(I,C,ùíü)C^{*}=\\operatorname*{arg\\,max}_{C}M(I,C,\\mathcal{D})\n\n(1)\n\n\nCurrent agents typically follow a Generate-Execute-Feedback paradigm¬†Zhu et¬†al. (2025b).\nFor instance, AIDE¬†Jiang et¬†al. (2025) organizes solution exploration as a tree search process involving sequential drafting, debugging, and iterative improvement via execution feedback.\nBuilding upon this, AutoMind¬†Ou et¬†al. (2025) integrates a curated expert knowledge base with a self-adaptive coding strategy to tackle more intricate problems (see Appendix¬†A for details).\n\n\n\n\nDomain\n\n\nParadigms\n\n\n# Tsk\n# Sols\n# Pairs\n\n\nCV\n\n\nClassification, Segmentation, Generation, Restoration\n\n\n9\n289\n5,952\n\n\nNLP\n\n\nClassification, Matching, QA, Sequence Labeling, Ranking\n\n\n8\n303\n6,682\n\n\n\n\n\nData\n\nScience\n \n\n\nRegression, Time-Series, Audio, Tabular, Grading\n\n\n9\n303\n5,804\n\n\nTotal\n\n\n26 Distinct Tasks across 3 Domains\n\n\n26\n895\n18,438\n\n\nTable 1: Stat"
  },
  {
    "title": "Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics",
    "url": "https://arxiv.org/abs/2601.05929v1",
    "source": "arxiv",
    "summary": "Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibili",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Contributions of This Paper\n1.2 Paper Organization\n\n\n\n2 Background and Related Work\n\n2.1 Reproducibility in Computational Research\n2.2 Traditional Forecasting Methods\n2.3 Machine Learning Approaches to Forecasting\n2.4 Emergence of Open and Reproducible Forecasting Frameworks\n2.5 Prophet as an Additive and Reproducible Framework\n2.6 Comparative Positioning of Prophet\n2.7 Summary of the Research Gap\n\n\n\n3 Data and Experimental Setup\n\n3.1 Overview of Experimental Design\n3.2 Data Sources\n\n3.3 Data Description\n\n3.3.1 Financial Dataset: Tesla Stock Prices\n3.3.2 Retail Dataset: Store Item Demand Forecasting\n3.3.3 Data Quality and Preprocessing Considerations\n\n\n3.4 Data Preprocessing\n3.5 Data Visualization and Exploratory Analysis\n3.6 Training and Test Splits\n3.7 Computational Environment and Reproducibility Controls\n\n3.8 Benchmark Models\n\n3.8.1 ARIMA Model Variants\n3.8.2 Random Forest Model\n\n\n3.9 Evaluation Metrics\n3.10 Reproducible Workflow Example\n3.11 Code Availability\n3.12 Reproducibility Dimensions: A Comparative Framework\n\n3.13 Reproducibility Checklist\n\n3.13.1 Data Availability\n3.13.2 Environment Specifications\n3.13.3 Random Seed Documentation\n3.13.4 Version Pinning and Dependency Management\n3.13.5 Reproducibility Verification\n\n\n3.14 Summary\n\n\n\n4 Prophet Framework and Implementation\n\n4.1 Conceptual Overview\n\n4.2 Mathematical Formulation of Components\n\n4.2.1 Trend Component\n4.2.2 Seasonality Component\n4.2.3 Holiday and Event Component\n4.2.4 Error Term\n4.2.5 Parameter Estimation\n\n\n4.3 Reproducible Model Specification\n4.4 Custom Seasonality and Holiday Specification\n4.5 Forecast Generation\n4.6 Forecast Visualization\n\n4.7 Model Diagnostics and Cross Validation\n\n4.7.1 Why Time Series Cross-Validation Differs from Standard Cross-Validation\n4.7.2 Window Selection Rationale\n4.7.3 Cross-Validation Folds and Evaluation\n4.7.4 Visualizing Cross-Validation Folds\n\n\n4.8 Interpretability and Component Analysis\n4.9 Model Serialization\n\n\n\n5 Empirical Results\n\n5.1 Overview of the Evaluation Strategy\n5.2 Results for Financial Time Series\n5.3 Results for Retail Demand Forecasting\n5.4 Comparative Interpretation\n5.5 Statistical Significance Testing\n5.6 Results Table Generation\n5.7 Model Comparison Visualization\n5.8 Summary of Empirical Findings\n5.9 Summary Comparison Across Models and Datasets\n\n\n\n6 Discussion\n\n6.1 Methodological Significance of Prophet\n\n6.2 Interpretation of Empirical Findings\n\n6.2.1 Model Specification Variability and Reproducibility\n6.2.2 Performance Characteristics Across Data Contexts\n6.2.3 Trade-offs Between Accuracy, Interpretability, and Reproducibility\n\n\n\n6.3 Reproducibility as a Methodological Outcome\n\n6.3.1 Operationalizing Reproducibility\n6.3.2 Model Specification Variability as a Reproducibility Challenge\n6.3.3 Reproducibility Through Explicit Documentation\n6.3.4 Reproducibility Without Sacrificing Performance\n\n\n6.4 Practical Implications for Business and Financial Analytics\n\n6.5 Limitations and Scope Conditions\n\n6.5.1 Computational Complexity and Scalability\n6.5.2 When Prophet Is Not Appropriate\n6.5.3 Comparison with Alternatives in Specific Scenarios\n6.5.4 Summary of Limitations\n\n\n6.6 Synthesis and Future Directions\n\n\n\n7 Conclusion\n\n7.1 Summary of Contributions\n7.2 Methodological Implications\n7.3 Directions for Future Research\n7.4 Concluding Remarks\n\n\nA Supplementary Code Listings\n\nB Supplementary Materials: Extended Code Listings\n\nB.1 Data Visualization and Exploratory Analysis\nB.2 Custom Seasonality and Holiday Specification\nB.3 Environment Configuration Files\nB.4 Forecast Visualization and Analysis\nB.5 Cross-Validation Visualization\nB.6 Results Table Generation and Model Comparison\n\n\n\n\n\n\n\nProphet as a Reproducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics\n\n\nSidney Shapiro ‚ÄÉBurhanuddin Panvelwala\nUniversity of Lethbridge\nsidney.shapiro@uleth.ca ‚ÄÇ‚ÄÑ‚Ääburhanuddin.panvelwala@uleth.ca\n\n\n\n(January 9, 2026)\n\nAbstract\nReproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet‚Äôs additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet‚Äôs performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet‚Äôs relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet‚Äôs role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.\n\n\nKeywords: Prophet, forecasting, reproducibility, time series analysis, Python, computational research, business analytics, financial analytics\n\n\n\n1 Introduction\n\nForecasting plays a central role in decision making across business and finance. Organizations rely on forecasts of sales, revenues, cash flows, and economic indicators to support strategic planning, risk management, and operational control. Advances in data availability and computational tools have increasingly shifted forecasting from a purely econometric exercise toward a data-driven analytical process embedded within broader computational workflows. Despite this evolution, a persistent methodological challenge remains: the reproducibility of forecasting research and applied practice.\n\n\nReproducibility, defined as the ability of independent researchers to obtain the same results using the same data and analytical procedures, is widely recognized as a foundational principle of computational science [11, 12]. To operationalize this concept for forecasting research, we distinguish three dimensions of reproducibility:\n\n\n\n\n‚Ä¢\n\nData Reproducibility: The ability to access, load, and preprocess data in an identical manner, ensuring that input data are consistent across replications. This includes documentation of data sources, access procedures, preprocessing steps, and any transformations applied.\n\n\n\n‚Ä¢\n\nComputational Reproducibility: The ability to execute identical computational procedures and obtain numerically equivalent results (within acceptable tolerance) across different computational environments. This includes specification of software versions, random seed settings, and computational dependencies.\n\n\n\n‚Ä¢\n\nAnalytical Reproducibility: The ability to reconstruct modeling decisions, parameter choices, and evaluation procedures such that independent researchers can arrive at substantively equivalent conclusions. This includes explicit documentation of model specifications, hyperparameter choices, and evaluation metrics.\n\n\n\n\n\nIn applied forecasting contexts, reproducibility is often undermined by opaque software environments, undocumented preprocessing steps, ad hoc parameter selection, and limited access to executable code. These issues are particularly salient in business and financial analytics, where forecasts may influence high-stakes decisions and therefore require transparency, auditability, and methodological accountability.\n\n\nTraditional forecasting approaches, such as autoregressive integrated moving average (ARIMA) models and exponential smoothing, provide statistically grounded tools for time series analysis [10]. While these methods are theoretically interpretable, their practical implementation often requires extensive manual tuning and diagnostic judgment. When deployed in proprietary or spreadsheet-based environments, model specifications and transformations may be difficult to document and replicate. At the same time, machine learning approaches, including random forests and neural networks, have demonstrated strong predictive performance in certain forecasting applications. However, they typically introduce additional challenges related to interpretability, stochastic training procedures, and reproducibility across computational environments. As a result, forecasting research frequently encounters a trade-off between predictive flexibility and methodological transparency.\n\n\nWithin this context, Prophet offers an alternative approach that emphasizes interpretability, standardized workflows, and accessibility. Prophet models a time series as an additive combination of trend, seasonal, and event components (see Section¬†4 for detailed mathematical formulation). This additive structure allows each component to be estimated and interpreted separately. Prophet automates many modeling decisions such as changepoint detection and seasonal feature construction, while retaining explicit parameterization that can be inspected and documented.\n\n\nImportantly, Prophet does not eliminate the broader challenges of reproducibility in forecasting research. Instead, its design reduces several practical barriers by encouraging script-based analysis, standard"
  },
  {
    "title": "Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS & DOT from the lab to the everyday world",
    "url": "https://arxiv.org/abs/2601.05923v1",
    "source": "arxiv",
    "summary": "Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based op",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; eess &gt; arXiv:2601.05923v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Electrical Engineering and Systems Science  Signal Processing\n    \n\n    \n      arXiv:2601.05923v1 (eess)\n    \n\n\n  \n    \n  [Submitted on 9 Jan 2026]\n    Title:Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS &amp; DOT from the lab to the everyday world\n    Authors:E. Middell, L. Carlton, S. Moradi, T. Codina, T. Fischer, J. Cutler, S. Kelley, J. Behrendt, T. Dissanayake, N. Harmening, M. A. Y√ºcel, D. A. Boas, A. von L√ºhmann            View a PDF of the paper titled Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS &amp; DOT from the lab to the everyday world, by E. Middell and 12 other authors\n    View PDF\n\n\n\n    \n            Abstract:Functional near-infrared spectroscopy (fNIRS) and diffuse optical tomography (DOT) are rapidly evolving toward wearable, multimodal, and data-driven, AI-supported neuroimaging in the everyday world. However, current analytical tools are fragmented across platforms, limiting reproducibility, interoperability, and integration with modern machine learning (ML) workflows. Cedalion is a Python-based open-source framework designed to unify advanced model-based and data-driven analysis of multimodal fNIRS and DOT data within a reproducible, extensible, and community-driven environment. Cedalion integrates forward modelling, photogrammetric optode co-registration, signal processing, GLM Analysis, DOT image reconstruction, and ML-based data-driven methods within a single standardized architecture based on the Python ecosystem. It adheres to SNIRF and BIDS standards, supports cloud-executable Jupyter notebooks, and provides containerized workflows for scalable, fully reproducible analysis pipelines that can be provided alongside original research publications. Cedalion connects established optical-neuroimaging pipelines with ML frameworks such as scikit-learn and PyTorch, enabling seamless multimodal fusion with EEG, MEG, and physiological data. It implements validated algorithms for signal-quality assessment, motion correction, GLM modelling, and DOT reconstruction, complemented by modules for simulation, data augmentation, and multimodal physiology analysis. Automated documentation links each method to its source publication, and continuous-integration testing ensures robustness. This tutorial paper provides seven fully executable notebooks that demonstrate core features. Cedalion offers an open, transparent, and community extensible foundation that supports reproducible, scalable, cloud- and ML-ready fNIRS/DOT workflows for laboratory-based and real-world neuroimaging.\n    \n\n    \n    \n              \n          Comments:\n          33 pages main manuscript, 180 pages Supplementary Tutorial Notebooks, 12 figures, 6 tables, under review in SPIE Neurophotonics\n        \n\n          Subjects:\n          \n            Signal Processing (eess.SP); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Image and Video Processing (eess.IV); Quantitative Methods (q-bio.QM)\n        \n          Cite as:\n          arXiv:2601.05923 [eess.SP]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05923v1 [eess.SP] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05923\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Alexander Von Luhmann [view email]          [v1]\n        Fri, 9 Jan 2026 16:37:48 UTC (18,242 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Cedalion Tutorial: A Python-based framework for comprehensive analysis of multimodal fNIRS &amp; DOT from the lab to the everyday world, by E. Middell and 12 other authorsView PDF\n      \n          \n          view license\n        \n    \n        \n    Current browse context: eess.SP\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.LG\n        eess\n        eess.IV\n        q-bio\n        q-bio.QM\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n"
  },
  {
    "title": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
    "url": "https://arxiv.org/abs/2601.05918v1",
    "source": "arxiv",
    "summary": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews",
    "full_text": "\n\n\n\n1 Introduction and Background\n\n2 Method\n\nRe-identification attack setting\nIntuition\nProcedure\n\n\n\n3 Results\n\nConfidence\n\n\n\n4 Implications\n\nRe-identification attacks made easy and scalable\nHarms to participants\n\n\n5 Responsible Disclosure\n6 Open Problems\n7 Ethics Considerations\n\n\n\n\n\nAgentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset\n\n\nTianshi Li\n\n\n\nAbstract\nOn December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees.\nMy contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks.\nI outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.\n\n\n\n1 Introduction and Background\n\nOn December 4, 2025, Anthropic introduced Anthropic Interviewer, an AI-powered tool for conducting qualitative interviews at scale¬†(Handa et al., 2025).\nAs part of this launch, Anthropic conducted a large-scale interview study with 1,250 professionals ‚Äî the general workforce (N=1,000), scientists (N=125), and creatives (N=125) ‚Äî and publicly released all interview transcripts on Hugging Face¬†(Anthropic, 2025).\n\n\nThe dataset release is justified by participant consent.\nHowever, the full consent process is not disclosed, and\nparticipants may reasonably be under the impression that the data release is anonymized: for example, the interview script of the general workforce111The general workforce and scientists are two different subsets of the dataset released by Anthropic.subset reassures them that ‚ÄúI‚Äôll be taking notes during our chat, but rest assured that anything you share won‚Äôt be personally attributed to you. The insights we gather will be used to improve our understanding of AI‚Äôs role in work environments.‚Äù ‚Äî emphasizing the benefits yet downplaying the re-identification risks.\n\n\nI focus on the scientist subset of the dataset and demonstrate a practical re-identification attack powered by large language models (LLMs) augmented with web search.\nBy matching project details described in interview transcripts with publicly available publications, my method can often recover the underlying paper and, correspondingly, narrow down the scope or even uniquely identify the interviewee.\nAnthropic has taken steps to anonymize the data (e.g., redacting certain details), yet my experiments show that at least some of the redacted information can be easily recovered.\n\n\nMy attack highlights the realistic privacy risks created when rich qualitative data is released, as powerful, general-purpose LLM tools have become widely available, whose use is difficult to constrain.\nIn particular, I show that LLM safeguards can be bypassed by breaking down the attack into benign tasks, exploiting both the dual-use nature of information-retrieval tools and the inherent unverifiability of user intent.\nThe barrier to carrying out this attack is extremely low: anyone with access to an LLM agent with web search can use a small number of natural-language prompts to robustly perform the attack.\nI chose to disclose the methods at a high level to illustrate the general risks without providing details to facilitate harm to the participants.\nThe main purpose of this paper is to document this case and serve as a timely reminder to the research community and the general public about this serious, pervasive vulnerability, so we can collectively consider its implications and potential mitigations.\n\n\n\n\n\n2 Method\n\nRe-identification attack setting\n\nMy attack operates on the scientist subset of the dataset. Each interview is initiated by the AI interviewer asking the participant to ‚Äúwalk me through a recent research project you‚Äôve worked on,‚Äù including how the project evolved from ‚Äúinitial idea‚Äù to ‚Äúfinal output,‚Äù eliciting rich information about the research project. As auxiliary data, I assume access to arbitrary information on the public internet, operationalized through web-augmented LLM services that can search, retrieve, and summarize relevant publications.\n\n\n\nIntuition\n\nMy attack relies on two observations. First, research projects are often described using domain-specific terminology, niche problem settings, and distinctive contributions, making them technically effective quasi-identifiers; Second, web-augmented LLMs can dramatically accelerate the search, synthesis, matching, and ranking required for re-identification, turning what would otherwise be a time-consuming manual process that require both technical and domain expertise into one that can be executed with a handful of natural language prompts and in matters of minutes, by anyone.\n\n\n\nProcedure\n\nMy re-identification procedure consists of two main steps. First, for each full interview transcript in the scientist subset, I use a non-thinking model to label whether the interviewee discusses specific published work (e.g., a paper, dissertation, etc.) and assign a numerical label indicating how many distinct published projects are mentioned (0 if none is mentioned). This yields a filtered subset of interviews whose project descriptions indicate at least one published work. Second, for each of these interviews, I call a thinking model agent to search for candidate publications that match the described project and return a ranked list (web search enabled). For every candidate, the system records a discrete confidence rating ‚Äî ‚Äúvery low,‚Äù ‚Äúlow,‚Äù ‚Äúmedium,‚Äù ‚Äúhigh,‚Äù or ‚Äúvery high‚Äù ‚Äî together with two short rationales explaining how the publication does and does not align with the project described in the transcript. I run this process multiple times, appending to the candidate sets until no new ‚Äúvery high‚Äù confidence matches are discovered.\nI use ‚Äúvery high‚Äù as a threshold for potential re-identification.\nTo mitigate the risk of exposing re-identified participants, I intentionally omit operational details on how to scale the procedure or circumvent LLM safeguards.\n\n\n\n\n\n3 Results\n\nOut of the 125 scientist interview transcripts, 24 were detected as mentioning at least one publication, and then re-identification attempts were performed with them. Among these 24, I was able to recover the specific publication(s) being discussed for 6 transcripts (25%). Notably, some of these cases involved a thesis or dissertation, which is single-authored and therefore uniquely identifies the interviewee.\n\n\nConfidence\n\nSeven cases ended up receiving a ‚Äúvery high‚Äù label by the LLM.\nI manually verified all of them and determined that the re-identifications for six interview transcripts are indeed of very high confidence.\nThese interview descriptions matched multiple aspects of the corresponding publications, such as methodology, procedures, key contributions, outcomes, timelines, and team compositions.\nThese matches involved both specific keywords and semantically aligned descriptions expressed in richer natural language that do not map cleanly to simple categories or verbatim phrases from the paper.\nIn several instances, participants did not reveal all details at once; instead, the AI interviewer‚Äôs follow-up questions elicited additional information that, in aggregate, made the project highly identifiable.\nDuring manual verification, I reviewed both the full interview transcripts and the original publications to confirm that highly granular and non-trivial details were consistent, and that the overlap in technical language and jargon was substantial.\n\n\nIn the one case where the LLM assigned very high confidence but I did not count it as a success, both the interviewee‚Äôs description and my search results indicate that the main project they described is likely still in the review phase.\nHowever, my method still identified papers with a highly overlapping set of authors, which demonstrated a trajectory of research methodologies and contributions closely matching the interview descriptions. I did not count this as a success, but I note that re-identification is still probable, especially once the exact paper has been published.\n\n\n\n\n\n4 Implications\n\nRe-identification attacks made easy and scalable\n\nMy experiments programmatically invoked LLM APIs to perform the attacks.\nThe API cost is low, with each transcript re-identification attempt costing less than $0.5 with about 4 minutes of run time.\nI also evaluated a no-code variant of the attack by directly prompting user-facing LLM services that provide web search capabilities, which was similarly successful.\n\nPrior research has demonstrated that LLMs plus agentic capabilities can facilitate the aggregation of public information for privacy-invasive tasks such as building user profiles and customizing phishing emails¬†(Mireshghallah and Li, 2025; Kim et al., 2025), and this work provides a concrete example of using them for re-identification attacks that can yield material harms (discussed below).\n\n\n\nHarms to participants\n\nParticipants in this dataset may experience the following harms when being re-identified.\n\n\nUnexpected exposure.\nParticipants may not expect their identities to be exposed when giving consent. Thwarted expectations are one of the main categories of privacy harms¬†(Citron and Solove, 2022).\n\n\nEmotional distress.\nThe "
  },
  {
    "title": "Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces",
    "url": "https://arxiv.org/abs/2601.05913v1",
    "source": "arxiv",
    "summary": "Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing",
    "full_text": null
  },
  {
    "title": "Pantagruel: Unified Self-Supervised Encoders for French Text and Speech",
    "url": "https://arxiv.org/abs/2601.05911v1",
    "source": "arxiv",
    "summary": "We release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-tra",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions\n\n\n2 Related Work\n\n3 Models and Pre-training Framework\n\n\n3.1 Framework overview\n\nLoss functions\nExtension for better textual representations\n\n\n\n3.2 Pantagruel model configurations and implementation details\n\nTokenizers\nModel configurations\nImplementation details\n\n\n\n\n\n4 Datasets and Resources\n\n4.1 Text datasets\n4.2 Speech datasets\n\n\n\n5 Ablation study\n\nEffects of different text tokenizers\nEffects of pre-training data size for speech\n\n\n\n6 Benchmarks\n\n\n6.1 Text models evaluation\n\nNamed Entity Recognition (NER)\nAutomatic Coreference Resolution (CR)\nExtractive Question Answering (QA)\nFLUE benchmark\nJargon biomedical benchmark\nSummary results on text tasks\n\n\n\n6.2 Speech models evaluation\n\nAutomatic Speech Recognition (ASR)\nNamed Entity Recognition (NER)\nSpeech Emotion Recognition (SER)\nSpoken Language Understanding (SLU)\nSummary of results on speech tasks\n\n\n\n\n7 Discussion and Conclusion\n8 Acknowledgements\n9 Bibliographical References\n10 Appendix\n\n\n\n\n\nPantagruel: Unified Self-Supervised Encoders \nfor French Text and Speech\n\n\nAbstract\nWe release Pantagruel models, a new family of self-supervised encoder models for French text and speech. Instead of predicting modality-tailored targets such as textual tokens or speech units, Pantagruel learns contextualized target representations in the feature space, allowing modality-specific encoders to capture linguistic and acoustic regularities more effectively. Separate models are pre-trained on large-scale French corpora, including Wikipedia, OSCAR and CroissantLLM for text, together with MultilingualLibriSpeech, LeBenchmark, and INA-100k for speech. INA-100k is a newly introduced 100‚Äâ000-hours corpus of French audio derived from the archives of the Institut National de l‚ÄôAudiovisuel (INA), the national repository of French radio and television broadcasts, providing highly diverse audio data. We evaluate Pantagruel across a broad range of downstream tasks spanning both modalities, including those from the standard French benchmarks such as FLUE or LeBenchmark. Across these tasks, Pantagruel models show competitive or superior performance compared to strong French baselines such as CamemBERT, FlauBERT, and LeBenchmark 2.0, while maintaining a shared architecture that can seamlessly handle either speech or text inputs. These results confirm the effectiveness of feature-space self-supervised objectives for French representation learning and highlight Pantagruel as a robust foundation for multimodal speech‚Äìtext understanding.\n\n\n\nKeywords:‚Äâself-supervised learning, JEPA, data2vec, French language models, speech and text encoders, multimodal representation learning, joint-embedding predictive architecture, predictive modeling\n\n\n\\NAT@set@cites\n\n\n\n\n\n\n\n\nPantagruel: Unified Self-Supervised Encoders \nfor French Text and Speech\n\n\n\nPhuong-Hang Le1,\nValentin Pelloin2,\nArnault Chatelain4,\nMaryem Bouziane3,\n\n\nMohammed Ghennai1,\nQianwen Guan5,\nKirill Milintsevich2,\n\n\nSalima Mdhaffar3,\nAidan Mannion1,\nNils Defauw6,\nShuyue Gu5,\n\n\nAlexandre Audibert1,\nMarco Dinarelli1,\nYannick Est√®ve3,\nLorraine Goeuriot1,\n\n\nSteffen Lalande2,\nNicolas Herv√©2,\nMaximin Coavoux1,\nFran√ßois Portet1,\n\n\n√âtienne Ollion4,\nMarie Candito5,\nMaxime Peyrard1,\nSolange Rossato1,\n\n\nBenjamin Lecouteux1,\nAur√©lie Nardy7,\nGilles S√©rasset1,\nVincent Segonne8,\n\n\nSol√®ne Evain1,\nDiandra Fabre1,\nDidier Schwab1\n\n\n\n1 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n\n\n\n2 INA (Institut National de l‚ÄôAudiovisuel), 4 Avenue de l‚ÄôEurope, 94366 Bry-sur-Marne, France\n\n\n\n3 Avignon Universit√©, LIA, France\n\n\n\n4 CREST (√âcole Polytechnique, ENSAE, CNRS), 5 avenue Le Chatelier, 91120 Palaiseau, France\n\n\n\n5 LLF (Universit√© Paris Cit√© and CNRS), UFRL Olympe de Gouges,\n\n\n13 place Paul Ricoeur, 75013 Paris, France\n\n\n\n6 Univ. Grenoble Alpes, EFELIA-MIAI, IUT2 Grenoble, LIG, 38000 Grenoble, France\n\n\n\n7 Univ. Grenoble Alpes, Lidilem, 38000 Grenoble, France\n\n\n\n8 Universit√© Bretagne Sud, CNRS, IRISA, France\n\n\nAbstract content\n\n\n\n1.‚ÄÉ‚ÄäIntroduction\n\nMirroring trends in other languages, self-supervised encoders have become the standard backbone for French speech and language processing. Text models such as FlauBERT¬†(Le et al., 2020) and CamemBERT¬†Martin et al. (2020), together with LeBenchmark¬†Evain et al. (2021b); Parcollet et al. (2024) for speech, have established strong baselines across a variety of downstream tasks. Yet most prior work relies on token-level reconstruction¬†Devlin et al. (2019); Baevski et al. (2020); Warner et al. (2025), which can under-utilize the structural regularities of continuous signals and hinders a unified treatment of text and speech. Recent predictive approaches¬†LeCun (2022); Assran et al. (2023); Baevski et al. (2023) that learn contextualized targets in feature space offer a promising alternative, enabling modality-specific encoders to capture richer linguistic and acoustic structure beyond the surface form.\n\n\nIn this paper, we introduce Pantagruel, a family of French self-supervised encoders for text and speech, trained separately using the data2vec 2.0 architecture (Baevski et al., 2022), an instance of the joint embedding predictive architecture¬†(JEPA) framework¬†LeCun (2022). Pantagruel follows a teacher‚Äìstudent training paradigm in which the student predicts masked latent representations produced by a teacher that observes the full, unmasked input. Such representation-based objectives have proven highly effective for vision¬†Assran et al. (2023); Mo and Tong (2024) and audio¬†Fei et al. (2023); Tuncay et al. (2025); Yuksel et al. (2025), yet they remain underexplored for text. Recent work suggests that textual tokens are compact, semantically dense units with minimal low-level variability, leaving limited room for embedding-based methods to improve over input-level approaches¬†Van Assel et al. (2025). Our experiments on text-based models also confirm this hypothesis. Therefore, for text, we propose to augment the feature-space objective with masked language modeling (MLM, Devlin et al., 2019) to better capture fine-grained syntactic and semantic information, yielding stronger textual representations while retaining the benefits of contextualized target prediction. Our self-supervised speech and text models are released on the HuggingFace hub111https://huggingface.co/PantagrueLLM.\n\n\nTo support large-scale pre-training in French, we curate substantial corpora for each modality. For text, we use Wikipedia, OSCAR¬†Martin et al. (2020) and CroissantLLM¬†Faysse et al. (2024) datasets. For speech, we assemble a diverse collection spanning read, spontaneous, professional, and broadcast speech audio data, including 14‚Äâ000 audio hours from LeBenchmark¬†Evain et al. (2021b) and 100‚Äâ000 audio hours from INA-100k, a new corpus derived from the archives of France‚Äôs National Audiovisual Institute¬†(INA)\nthat we introduce\nin this paper.\nWe evaluate Pantagruel on a broad suite of downstream tasks in both modalities and compare it to three main French baselines: FlauBERT (Le et al., 2020) and CamemBERT (Martin et al., 2020) for text, and LeBenchmark¬†Evain et al. (2021a, b); Parcollet et al. (2024) for speech.\n\n\nContributions\n\nFirst, we release Pantagruel, a family of French self-supervised encoders for speech and text based on the data2vec¬†2.0 and JEPA frameworks, which allows training models on different modalities using the same framework.\nSecond, we investigate embedding-based prediction objectives for text, an underexplored regime, and show that combining feature-space prediction with MLM yields competitive results for French text encoders.\nThird, we study the impact of the large-scale INA-100k broadcast corpus on the models‚Äô performances.\nFinally, we provide a unified evaluation across speech and text tasks, where Pantagruel consistently matches or improves over established French baselines.\n\n\nFigure 1: Overview of the Pantagruel model architecture. The network starts with a modality-specific pre-net to extract feature vectors from the input text/speech sequence. These features are input to a teacher encoder, while randomly chosen visible tokens (in blue) are input to a student encoder. A lightweight decoder predicts the teacher‚Äôs latent representations from the student‚Äôs outputs. For text input, an additional masked language modeling¬†(MLM) loss is used. The teacher‚Äôs parameters are updated as an exponential moving average (EMA) of the student‚Äôs. After training, only the embedding layer and the student encoder are used for fine-tuning on downstream tasks.\n\n\n\n\n\n2.‚ÄÉ‚ÄäRelated Work\n\nSelf-supervised learning (SSL) has driven rapid progress in several domains, in particular in text and speech processing. In text, encoder-only models such as BERT¬†Devlin et al. (2019) learn bidirectional representations through the MLM objective, with subsequent refinements improving attention mechanisms, efficiency, and context length¬†(Clark et al., 2020; He et al., 2020; Lan et al., 2020; Warner et al., 2025), while the GPT family¬†(Radford et al., 2018, 2019; Brown et al., 2020) popularized autoregressive pre-training for generative tasks. In speech, wav2vec2.0¬†(Baevski et al., 2020) introduced contrastive learning over quantized representations, while HuBERT¬†(Hsu et al., 2021) adopted masked prediction of discrete clusters, and WavLM¬†(Chen et al., 2022) improved robustness through denoising objectives. Initially developed for English, these self-supervised frameworks were rapidly extended to multilingual settings¬†(Devlin et al., 2019; Conneau et al., 2020; Scao et al., 2022; Shliazhko et al., 2024; Babu et al., 2022).\n\n\nFor French, many studies have adapted these self-supervised architectures to both modalities. In text, FlauBERT¬†(Le et al., 2020) and CamemBERT¬†(Martin et al., 2020) were the first French BERT variants, trained on large-scale corpora and demonstrating that language-specific pre-training outperforms multilingual models on downstream tasks. Subsequent efforts explored efficiency trade-offs with compact mod"
  },
  {
    "title": "Multi-task Modeling for Engineering Applications with Sparse Data",
    "url": "https://arxiv.org/abs/2601.05910v1",
    "source": "arxiv",
    "summary": "Modern engineering and scientific workflows often require simultaneous predictions across related tasks and fidelity levels, where high-fidelity data is scarce and expensive, while low-fidelity data is more abundant. This paper introduces an Multi-Task Gaussian Processes (MTGP) framework tailored for engineering systems characterized by multi-source, multi-fidelity data, addressing challenges of d",
    "full_text": null
  },
  {
    "title": "Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates",
    "url": "https://arxiv.org/abs/2601.05909v1",
    "source": "arxiv",
    "summary": "As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain p",
    "full_text": null
  },
  {
    "title": "Illusions of Confidence? Diagnosing LLM Truthfulness via Neighborhood Consistency",
    "url": "https://arxiv.org/abs/2601.05905v1",
    "source": "arxiv",
    "summary": "As Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations. Existing evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief. We show that even facts answered with perfect self-consistency can rapidly colla",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminary\n\n2.1 Robust Knowledge Belief is Structured\n2.2 Bayesian-Inspired Belief Estimation\n\n\n\n3 Experimental Design and Setup\n\n3.1 Data Construction\n3.2 Contextual Interference for Stress Tests\n\n\n\n4 Stress-Testing Internal Beliefs\n\n4.1 Implementation Details\n4.2 Metrics.\n4.3 Experimental Results and Analysis\n\n\n\n5 Structure-Aware Training\n\n5.1 Experimental Setup\n5.2 Results\n\n\n6 Related Work\n7 Conclusion\nA Use of Large Language Models\n\nB Extended Bayesian-Inspired Belief Estimation\n\nB.1 Problem Definition\n\nB.2 Key Assumptions\n\nEqual baseline accuracy\nConditional independence under structured belief\n\n\nB.3 Computable Surrogate\nB.4 Discussion\n\n\n\nC Data Construction Pipeline\n\n\nC.1 Seed Data Sourcing\n\n1. Complexity Filtering.\n2. Semantic Classification.\n3. Time-Invariance &amp; Disambiguation Refinement.\n\n\n\nC.2 Neighbor Generation\n\nStrict Self-Containment Constraint.\nDual-Stage Automated Verification.\n\n\n\nC.3 Human-in-the-loop Verification\n\nPreliminary Web-Retrieval Filtering.\nExpert Review &amp; Annotation Interface.\nMajority Vote Validation.\n\n\n\nC.4 Misleading Set Creation\n\nStep 1: Distractor Generation (‚Ñ∞‚Ä†\\mathcal{E}^{\\dagger}).\nStep 2: MNQ Generation via Recursive Pipeline.\n\n\n\n\n\nD Experiment Implementation Details\n\nD.1 Model Specifications &amp; Environment\nD.2 Metrics Computation\n\nD.3 Contextual Interference Protocols\n\nSetting 1: Peer Quantity.\nSetting 2: Source Credibility.\n\n\nD.4 Training Experiment Settings\n\n\n\nE Supplementary Analysis\n\n\nE.1 Details of the Pilot Experiment\n\nSample Selection.\nInterference Protocol.\nResults.\n\n\nE.2 Case Study\nE.3 Data Popularity and Difficulty Analysis\nE.4 Analysis of Positional Bias in Peer Contexts\nE.5 Sensitivity Analysis of NFs‚Äô Quantity and Weighting\n\n\n\nF Prompt Templates\n\n\nF.1 Neighbor Generation\n\nF.1.1 Stage 1: Neighbor Question Generation\nF.1.2 Stage 2: Format, Clarity, and Independence Validation\nF.1.3 Stage 3: Blind Test Validation\n\n\n\nF.2 Stress-Testing Prompts\n\nF.2.1 Peer Pressure: Conflict Scenario\nF.2.2 Peer Pressure: Misleading Neighbor Questions\nF.2.3 Source Credibility: Low Credibility\nF.2.4 Source Credibility: Medium Credibility\nF.2.5 Source Credibility: High Credibility\nF.2.6 Conflicting Information from Multiple Sources\n\n\n\nF.3 Data Processing and Augmentation\n\nF.3.1 Misleading Statement Generation\nF.3.2 Simple Question-Answer Paraphrasing\nF.3.3 Context-Aware Question-Answer Augmentation\nF.3.4 Synthetic Document Generation with Fact Embedding\n\n\n\n\n\n\n\n\n\nIllusions of Confidence?\nDiagnosing LLM Truthfulness via Neighborhood Consistency\n\n\n\nHaoming Xu‚ô†\\spadesuit,\nNingyuan Zhao‚ô†\\spadesuit,\nYunzhi Yao‚ô†\\spadesuit,\nWeihong Xu‚ô†\\spadesuit,\nHongru Wang‚ô£\\clubsuit, \nXinle Deng‚ô†\\spadesuit,\nShumin Deng‚ô°\\heartsuit,\nJeff Z. Pan‚ô£\\clubsuit,\nHuajun Chen‚ô†\\spadesuit,\nNingyu Zhang‚ô†\\spadesuit\n‚ô†\\spadesuitZhejiang University ‚ÄÉ‚ô£\\clubsuitUniversity of Edinburgh \n‚ô°\\heartsuitNational University of Singapore,NUS-NCS Joint Lab, Singapore \n{haomingxu, zhangningyu}@zju.edu.cn\n\n  Corresponding author.\n\n\nAbstract\nAs Large Language Models (LLMs) are increasingly deployed in real-world settings, correctness alone is insufficient. Reliable deployment requires maintaining truthful beliefs under contextual perturbations.\nExisting evaluations largely rely on point-wise confidence like Self-Consistency, which can mask brittle belief.\nWe show that even facts answered with perfect self-consistency can rapidly collapse under mild contextual interference.\nTo address this gap, we propose Neighbor-Consistency Belief (NCB), a structural measure of belief robustness that evaluates response coherence across a conceptual neighborhood.\nTo validate the efficiency of NCB, we introduce a new cognitive stress-testing protocol that probes outputs stability under contextual interference. Experiments across multiple LLMs show that the performance of high-NCB data is relatively more resistant to interference. Finally, we present Structure-Aware Training (SAT), which optimizes context-invariant belief structure and reduces long-tail knowledge brittleness by approximately 30%.¬†111Code will be available at https://github.com/zjunlp/belief\n\n\n\nIllusions of Confidence?\nDiagnosing LLM Truthfulness via Neighborhood Consistency\n\n\n\n\nHaoming Xu‚ô†\\spadesuit,\nNingyuan Zhao‚ô†\\spadesuit,\nYunzhi Yao‚ô†\\spadesuit,\nWeihong Xu‚ô†\\spadesuit,\nHongru Wang‚ô£\\clubsuit,\n\nXinle Deng‚ô†\\spadesuit,\nShumin Deng‚ô°\\heartsuit,\nJeff Z. Pan‚ô£\\clubsuit,\nHuajun Chen‚ô†\\spadesuit,\nNingyu Zhang‚ô†\\spadesuit‚Ä†‚Ä†thanks:   Corresponding author.\n\n‚ô†\\spadesuitZhejiang University ‚ÄÉ‚ô£\\clubsuitUniversity of Edinburgh\n\n‚ô°\\heartsuitNational University of Singapore,NUS-NCS Joint Lab, Singapore\n\n{haomingxu, zhangningyu}@zju.edu.cn\n\n\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities¬†(Wei et¬†al., 2023; Li et¬†al., 2025b), yet they exhibit persistent truthfulness failures: frequently hallucinating facts, showing overconfidence, and succumbing to misleading information¬†(Huang et¬†al., 2025; Steyvers et¬†al., 2025; Bengio et¬†al., 2025), which critically limits their use in high-stakes domains such as healthcare¬†Wang et¬†al. (2023b); Liu et¬†al. (2025a, b), law¬†Lai et¬†al. (2024), and science¬†Zhang et¬†al. (2022); Hu et¬†al. (2025).\nThese problems are amplified in today‚Äôs context-engineered deployments, where LLMs operate with retrieval-augmented generation (RAG)¬†Gao et¬†al. (2023), multi-agent collaboration¬†Guo et¬†al. (2024), and complex prompt engineering¬†Sahoo et¬†al. (2024), all of which can mislead models via conflicting documents, peer opinions, or subtle prompt biases.\nMaintaining stable and truthful beliefs in these settings is therefore essential for reliable real-world applications.\n\n\nFigure 1: High Self-Consistency ‚â†\\neq Robust Belief. Despite perfect self-consistency on the ‚ÄúIMU Vice-President‚Äù fact, the model is susceptible to contextual interference: accuracy drops to 33.8%, showing that high-consistency doesn‚Äôt imply robust belief.\n\n\nCurrent evaluation methods of LLMs‚Äô belief rely on point-wise confidence, using metrics like self-consistency (S‚ÄãCSC)¬†(Wang et¬†al., 2023a).\nAs Figure¬†1 illustrates, the model consistently answers\n‚ÄúBrazilian Vice-President of the IMU in 2012‚Äù as ‚ÄúMarcelo Viana‚Äù and gets the score S‚ÄãC=1.0SC=1.0.\nHowever, when exposed to a peer consensus favoring Jacob Palis, the model reverses its answer.\nWe extend this observation through a pilot study on 995 questions for which the model answers correctly with perfect self-consistency (S‚ÄãC=1.0SC=1.0).\nSpecifically, after we apply contextual interference, accuracy drops sharply from 100.0% to 33.8%.\nThese results suggest that point-wise confidence is superficial, failing to reflect true belief state.\n\n\nIntuitively, belief state should be a coherent structural state instead of point-wise confidence.\nCognitive science indicates that human knowledge is organized as interconnected semantic networks, where accepting a fact constrains related facts and implications¬†(Schoenfeld, 1983; Abelson, 1979), enabling resistance to misleading information¬†(Anderson and Green, 2001; Anderson and Hanslmayr, 2014).\nSimilarly, recent work on knowledge editing shows that robust learning requires anchoring facts within rich contextual representations, rather than isolated insertion¬†(Yao et¬†al., 2025).\nAs the Aristotelian proverb goes, ‚Äúone swallow does not make a summer‚Äù: the correct single data point does not reflect true belief state.\nFor example in Figure¬†1, familiarity with Marcelo Viana‚Äôs broader academic career would reinforce confidence in his IMU tenure, reducing the likelihood of confusion.\nThese observations motivate the view that structured belief is more truthful.\n\n\nMoving beyond point-wise metrics, we introduce Neighbor-Consistency Belief (NCB) in ¬ß2, which estimates belief robustness by measuring response coherence across a conceptual neighborhood, including entity prerequisites, logical implications, and thematic associations.\nIn ¬ß3 and ¬ß4, we validate NCB through a cognitive stress-testing protocol, where interfering context simulates adversarial scenarios such as multi-agent consensus or noisy retrieval.\nUnder these experiments, models face adversarial peer opinions and misleading documents.\nThe results across four LLMs show that high-NCB knowledge is substantially more stable than low-NCB knowledge, confirming NCB as an effective indicator of robust belief.\nIn ¬ß5, we further propose Structure-Aware Training (SAT), explicitly optimizing context-invariant beliefs, which reduces the brittleness of the learned knowledge by roughly 30% compared to baselines.\nOur results suggest that belief robustness is a structural property, highlighting the necessity of structure-aware evaluation and training for trustworthy LLMs.\n\n\n\n\n2 Preliminary\n\n\n2.1 Robust Knowledge Belief is Structured\n\nWe conduct a pilot study on 995 questions for which Qwen3-30B-A3B-Instruct¬†(Team, 2025) produces the correct answer in all 30 independent samples (see Appendix¬†E.1 for details).\nAs Figure¬†1 shows, introducing contextual interference reduces accuracy from 100% to 33.8%.\nThis indicates that point-wise confidence only captures surface agreement, but fails to reflect true belief state.\n\n\nTo bridge this gap, we propose a shift in perspective: knowledge belief is a structured property.\nWe consider that if a model has robust belief with certain fact concept, it should exhibit coherence across the associated network of facts.\nFormally, we view this belief as a latent state (Œ∏\\theta) that governs models‚Äô responses across the conceptual neighborhood, and we consider a binary latent variable Œ∏‚àà{ùíÆs‚Äãt‚Äãr‚Äãu‚Äãc‚Äãt,ùíÆu‚Äãn‚Äãs‚Äãt‚Äãr‚Äãu‚Äãc‚Äãt}\\theta\\in\\{\\mathcal{S}_{struct},\\mathcal{S}_{unstruct}\\}, indicating whether the model‚Äôs behavior on a given fact is driven by a structured belief or by unstructured memorization:\n\n\nStructured State (ùíÆstruct\\mathcal{S}_{\\text{struct}}):\nThe model exhibits a structured understanding of the target concept, maintaining coherent and mutually consistent responses across related neighbor questions.\nWe i"
  },
  {
    "title": "Can AI mediation improve democratic deliberation?",
    "url": "https://arxiv.org/abs/2601.05904v1",
    "source": "arxiv",
    "summary": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large langu",
    "full_text": "\n\n\n\n1 Introduction\n\n2 AI in Deliberation: The Habermas Machine\n\n2.1 Deliberation protocol\n2.2 The HM technical details\n2.3 Summary of findings\n\n\n\n3 Addressing the Deliberative Trilemma\n\n\n3.1 Equality: How can we ensure an AI mediator is fair?\n\n\n3.1.1 Building fairness into the architectural design of an AI mediator\n\nFair aggregation\nFair ranking\nFair generation\n\n\n3.1.2 Evaluating fairness of Habermas Machine outputs\n\n\n\n3.2 Participation: Will AI mediation scale deliberation?\n\n3.2.1 The challenge of scale in deliberative democracy\n\n\n3.3 Scalable oversight of mass deliberations\n3.4 Hierarchical aggregation: A path to scalable and transparent deliberation\n\n3.5 Deliberation: Can AI enhance deliberative quality?\n\n3.5.1 Modalities of AI-assisted deliberation\n\n\n\n\n\n4 Other Considerations\n\n\n4.1 What does AI-mediated caucus deliberation lack?\n\n4.1.1 Social-relational dynamics\n4.1.2 AI-mediated deliberation in its current forms has some positive engagement effects\n4.1.3 Addressing algorithmic aversion\n\n\n\n4.2 Where does AI-mediated deliberation fit into the democratic process?\n\n4.2.1 What is common ground?\n4.2.2 Potential applications of AI mediation in democratic settings\n\n\n\n\n5 Discussion\n\n\n\n\n\n\n\\paperurl\nhttps://arxiv.org/abs/123\\uselogo\\correspondingauthormhtessler@google.com, georgieevans@google.com\\reportnumber\n\nCan AI Mediation Improve Democratic Deliberation?\n\n\nMichael Henry Tessler\n\nEqual contribution\n\nGoogle DeepMind, London, UK\n\n\nGeorgina Evans\n\nEqual contribution\n\nGoogle DeepMind, London, UK\n\n\nMichiel A. Bakker\n\nGoogle DeepMind, London, UK\n\nMassachusetts Institute of Technology, MA, USA\n\n\nIason Gabriel\n\nGoogle DeepMind, London, UK\n\n\nSophie Bridgers\n\nGoogle DeepMind, London, UK\n\n\nRishub Jain\n\nGoogle DeepMind, London, UK\n\n\nRaphael Koster\n\nGoogle DeepMind, London, UK\n\n\nVerena Rieser\n\nGoogle DeepMind, London, UK\n\n\nAnca Dragan\n\nGoogle DeepMind, London, UK\n\n\nMatthew Botvinick\n\nGoogle DeepMind, London, UK\n\nYale Law School, New Haven, CT, USA\n\n\nChristopher Summerfield\n\nGoogle DeepMind, London, UK\n\nDepartment of Experimental Psychology, University of Oxford, Oxford, UK\n\n\n\nAbstract\nThe strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another [fishkin2011trilemma]. We ask whether and how artificial intelligence (AI) could help navigate this ‚Äútrilemma‚Äù by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (tessler2024ai, tessler2024ai). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.\n\n\nkeywords: deliberation, large language models, artificial intelligence\n\n111This paper was presented at the Knight Institute for the First Amendment at Columbia University workshop on ‚ÄúAI and Democratic Freedoms‚Äù, April 10-11, 2025, and is available on the Knight Institute website knightcolumbia.org/.\n\n\n1 Introduction\n\nIn the early 2000s, concern arose in the United States about the fairness and efficacy of the allocation process for kidney transplants. The system at the time pursued a seemingly robust objective: namely, it respected the time spent on the waiting list and the expected number of additional years of life for the recipient. However, there were valid objections: Did it discriminate unfairly against the elderly by virtue of them leading healthy lives? Did it recognize that the medical urgency of situations differed? Did it treat different demographic groups fairly? The experience of people placed on the waiting list suggested not. To address these concerns, public consultations took place in which everyone affected was encouraged to share their viewpoint and engage in deliberation. After numerous rounds of feedback, a new algorithm was created that weighs ten different variables, a change widely praised by medical practitioners and patient groups alike. Commenting on the process, scholar David Robinson notes that ‚Äú[E]xperts, patients, and advocates balanced hard trade-offs to remake the kidney allocation algorithm, leading to better health for more people and a fairer allocation of organs‚Ä¶People raised their voices. People were heard‚Äù [robinson2022voices]. The only serious limitation was that the process took 20 years to complete.\n\n\nImagine if we could channel the power of public deliberation effectively and efficiently, providing timely guidance on a range of issues. Traditional methods for public consultation have well-known limitations. Focus groups can provide detailed feedback but are too small scale to be representative and are susceptible to biases affecting group discussion [macdougall1997devil, o2018use]. Canvassing opinions door-to-door to elicit large-scale input can be costly and time-consuming. Running a poll can help reveal the balance of opinion but typically is restricted to a small number of judgments, does not allow for discussions of different points of view, and can be sensitive to the wording of the questions [gallup1941question, desaint2013use]. More structured forms of public input, like deliberative polls or citizens‚Äô assemblies, involve meaningful deliberation but also require significant time commitments from participants [fishkin2003consulting]. Consequently, well-resourced groups‚Äîwho have more time, better coordination, or greater knowledge about the process‚Äîcan dominate and be over-represented in these settings [birhane2022power], and these offline methods are not scalable to very large groups of people.\n\n\nRecent efforts to overcome the scalability and logistical challenges of traditional consultation methods take shape in digital deliberation technologies [coleman2012connecting, goni2025citizen]. Systems like Pol.is and Remesh facilitate large-scale input by allowing participants to contribute statements and react to others, typically through voting [small2021polis]. These platforms process the interaction data (e.g., votes) with machine learning techniques in order to uncover collective sentiments by grouping participants based on shared opinions and highlighting points of consensus or division. Despite the significant expansion of scale of participation and efficient ways to map general agreement, these tools are limited in their capacity to process the rich semantic content of participant contributions. Further, the deliberation they support is structured around discrete statements and voting and may miss out on deeper engagement with the reasoning and substance of diverse perspectives, critical for finding agreement and consensus in a population [habermas1985theory].\n\n\nRecent advances in artificial intelligence (AI), specifically the emergence of increasingly capable large language models (LLMs), have introduced new opportunities to further support deliberative and democratic functions [landemore2023can, small2023opportunities, lazar2024can]. LLMs can process vast amounts of text and can be prompted or trained further to perform arbitrary tasks involving text [brown2020language].222More broadly, large multimodal models and advanced AI agents can process text, images, videos, and sound, among other modalities, and take actions (e.g., on a computer). We focus our discussion on text-based large language models. Researchers and practitioners have explored how LLMs can support democratic deliberation by summarizing content [small2023opportunities], aggregating opinions in a manner to increase collective support [fish2023generative], representing individual judgments in decision-making processes [jarrett2023language, gudino2024large], facilitating public deliberation [ma2025towards], and implementing principles derived from democratic methods [huang2024collective]. Despite lots of speculation and demonstrations of the strengths and shortcomings of LLMs to support various deliberative tasks, there has been relatively little empirical work evaluating their efficacy.\n\n\nRecently, tessler2024ai (tessler2024ai) built and investigated an LLM system‚Äîwhich they call the Habermas Machine (HM)‚Äîdesigned for the task of finding common ground between people with different viewpoints (Figure 1). The AI system produced high-quality statements expressing common ground garnering levels of endorsement from the participants exceeding that of human mediators (in addition, the HM performed this task in seconds, in contrast to human mediators, who required several minutes). The HM works by generating a (potentially large) set of possible group statements and then returning one (or a few) of them by simulating an election in which each participant‚Äôs ranking of the statements is predicted. This paper provides empirical support for the fairness and efficacy of an AI-powered deliberative technology‚Äîthe HM‚Äîthat in principle could mediate deliberation among very large groups. Is fair, time-efficient mass deliberation within reach?\n\n\nThe prospect of AI mediation improving democratic deliberation raises the question of how it might navigate Fishkin‚Äôs trilemma‚Äîthe inherent trade-offs between broad participation, meaningful deliberation, and equality of contribution [fishkin1991democracy]. How might AI mediation enable scaling deliberation? How can we assess whether AI mediation can be fair to all participants? Can AI enhance the quality of deliberation itself, e.g., by aiding access to trustworthy information? This paper explores these key questions, ex"
  },
  {
    "title": "HAPS: Hierarchical LLM Routing with Joint Architecture and Parameter Search",
    "url": "https://arxiv.org/abs/2601.05903v1",
    "source": "arxiv",
    "summary": "Large language model (LLM) routing aims to exploit the specialized strengths of different LLMs for diverse tasks. However, existing approaches typically focus on selecting LLM architectures while overlooking parameter settings, which are critical for task performance. In this paper, we introduce HAPS, a hierarchical LLM routing framework that jointly searches over model architectures and parameter",
    "full_text": null
  },
  {
    "title": "TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents",
    "url": "https://arxiv.org/abs/2601.05899v1",
    "source": "arxiv",
    "summary": "Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strate",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 The TowerMind Environment\n\n3.1 Game Mechanics\n3.2 Environment Interface\n3.3 Levels and Difficulty\n3.4 Environment Customizability\n\n\n\n4 Evaluation for LLMs\n\n4.1 Evaluation Setting\n4.2 Results\n4.3 Quantitative Analysis\n4.4 Qualitative Analysis\n4.5 Insights and Future Directions\n\n\n5 RL Benchmark\n6 Conclusion\nA Game Mechanics Additional Details\nB Environment Interface Additional Details\nC Additional Details of the Benchmark Levels\nD Additional Details of Environment Customizability\nE Zero-shot Prompt\n\n\n\n\n\nTowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents\n\n\n\nDawei Wang1,\nChengming Zhou1,\nDi Zhao2,\nXinyuan Liu1,\nMarci Chi Ma1,\n\nGary Ushaw1,\nRichard Davison1\n\n\n\nAbstract\nRecent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub.111https://github.com/tb6147877/TowerMind\n\n\n\n1 Introduction\n\nOne of the fundamental challenges in artificial intelligence (AI) is equipping agents with the ability to solve tasks across a broader range of scenarios (Russell and Norvig 2016). Recent breakthroughs in large language models (LLMs) (Devlin et al. 2019; Achiam et al. 2023) have made them a promising approach to addressing this challenge. Benefiting from their extensive cross-domain knowledge and diverse abilities, including reasoning (Wei et al. 2022b; Wang et al. 2022) and problem-solving (Lingo et al. 2024; Renze and Guven 2024), LLM-based agents have shown potential in various domains, such as healthcare (Li et al. 2024a), office automation (Zhang et al. 2024), and design (√áelen et al. 2024). Despite differences in context and specifics, these tasks consistently require two foundational capabilities from LLMs: long-term planning and decision-making, which are essential for accomplishing tasks: (1) LLMs leverage long-term planning to decompose a high-level task into a sequence of subgoals that guide progress toward the final objective; (2) LLMs perform decision-making to translate this sequence of subgoals into executable actions, conditioned on the evolving task state.\n\n\nFigure 1: These are screenshots from four different TowerMind levels. The icons in the four corners of each image display key gameplay information, including the number of player‚Äôs current gold coins, player‚Äôs base health, and remaining enemy waves. The maps feature irregular, intersecting roads along which enemies advance toward the player‚Äôs base in successive waves. Players must strategically build different types of towers at designated locations along these roads to repel the incoming enemies. The cloud-shaped white areas represent fog of war, introducing partial observability to the environment.\n\n\nReal-time strategy (RTS) games are an ideal platform for evaluating long-term planning and decision-making abilities, as they require players to engage simultaneously in both macromanagement and micromanagement (Barros e S√° and Madeira 2025). Specifically, RTS games provide a battlefield setting where macromanagement tends toward long-term strategic planning, in which players formulate high-level strategies such as overall unit deployment and resource allocation; whereas micromanagement focuses on real-time decision-making, where players flexibly control units in response to dynamic changes on the battlefield to execute their combat plans. Currently, several RTS game-based benchmarks have recently been proposed for evaluating LLMs, including TextStarCraft II (Ma et al. 2025b), LLM-PySC2 (Li et al. 2024b), and VLMs Play StarCraft II (Ma et al. 2025a), all of which are based on the StarCraft II Learning Environment (SC2LE) (Vinyals et al. 2017), known for its relatively high computational demands. These challenging RTS game-based benchmarks are effective for assessing the long-term planning and decision-making capabilities of LLMs; nevertheless, the need for low-cost evaluation environments still persists in the field (Dubois et al. 2024). For example, in fast-paced continuous development pipelines (Koc 2025) and in the usage of reward models for instruction tuning (Yuan et al. 2024), lightweight benchmarks offer clear advantages. While several lightweight RTS game-based environments (e.g., ELF (Tian et al. 2017), DeepRTS (Andersen et al. 2018), Gym-Œº\\murts (Huang et al. 2021)) have been proposed to alleviate the computational demands of SC2LE-based platforms, they fundamentally lack support for textual observations and action interfaces, which makes them incompatible with LLMs.\n\n\nTo address the lack of lightweight RTS game-based environments with textual observation capabilities, we propose TowerMind, a newly developed game environment built upon the tower defense (TD) subgenre of RTS games (Liu et al. 2019), its screenshot is shown in Figure¬†1. TD games share the same core game mechanics as classic RTS games (Tian et al. 2017), providing a battlefield scenario where players must build towers and deploy units to defend against waves of invading enemies, requiring them to demonstrate long-term planning and decision-making. Unlike the player-versus-player mechanics of classic RTS games, TD games focus solely on defending against predefined waves of enemies. This allows for a more isolated evaluation of LLMs‚Äô ability to finish complex tasks using long-term planning and decision-making, without interference from opponent unpredictability. Furthermore, the fixed tower placement options and predefined enemy roads in TD games facilitate clearer analysis of the strategies employed by LLMs. In this work, TowerMind significantly reduces the computational demands compared to existing RTS game-based LLM benchmarks. Specifically, existing RTS game-based benchmarks for LLMs rely on the SC2LE environment, which requires approximately 30 GB of disk space, 2 GB of RAM, and a dedicated GPU. In contrast, TowerMind requires only 0.15 GB of disk space and RAM, runs efficiently on CPUs without the need for a dedicated GPU, and additionally offers advantages in ease of deployment and integration. This makes it well-suited for rapid research iteration, large-scale parallel training or fine-tuning, and similar scenarios in the LLM domain (Peng et al. 2023). Meanwhile, TowerMind supports pixel-based, textual, and structured game-state observations, enabling evaluation of multimodal LLMs. A comprehensive comparison of TowerMind and other lightweight RTS game-based environments in terms of supported features is provided in Table¬†1.\n\n\n\n\nEnvironment\n\n \n\n\nPixel\n\nObservation\n\n\n \n\n\nTextual\n\nObservation\n\n\n \n\n\nStochastic\n\nEnvironments\n\n\n \n\n\nPartial\n\nObservability\n\n\n \n\n\nLevel\n\nEditor\n\n\n \n\n\nGym\n\nInterface\n\n\n\nELF (Tian et al. 2017)\n\n‚úì\n‚úó\n‚úì\n‚úì\n‚úó\n‚úó\n\n\nDeepRTS (Andersen et al. 2018)\n\n‚úì\n‚úó\n‚úó\n‚úì\n‚úó\n‚úó\n\n\nGym-Œº\\murts (Huang et al. 2021)\n\n‚úó\n‚úó\n‚úì\n‚úì\n‚úó\n‚úì\n\n\nMini HoK (Liu et al. 2024)\n\n‚úó\n‚úó\n‚úì\n‚úó\n‚úì\n‚úó\n\n\nTowerMind (Ours)\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\nTable 1: Comparison between TowerMind and other lightweight RTS game-based environments.\n\n\nIn addition to addressing the limitations of existing RTS game-based environments, the design of TowerMind incorporates two new features: (1) Hallucination Evaluation: In evaluating LLMs, our metrics consider not only in-game score as a measure of performance, but also the executability of actions as an indicator of hallucination. Hallucination refers to LLM outputs that conflict with factual or contextual information (Bang et al. 2023); in our setting, this specifically denotes actions that are invalid or inconsistent with the game state or rules. Such a metric design allows for simultaneous evaluation of LLM capabilities and reliability; (2) Customizability: As both a TD environment and engine, TowerMind includes a graphical level editor that enables researchers to conveniently create custom levels. These levels can range from trivially easy to extremely difficult or structurally unique, supporting diverse research needs and reducing the risk of data contamination.\n\n\nThe contributions of our work are four-fold: (1) We present TowerMind, a lightweight and multimodal TD environment for evaluating long-term planning and decision-making in LLMs, while also supporting hallucination analysis and offering strong customizability. (2) The evaluation results show that while"
  },
  {
    "title": "StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management",
    "url": "https://arxiv.org/abs/2601.05890v1",
    "source": "arxiv",
    "summary": "Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level mem",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.05890v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.05890v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 9 Jan 2026]\n    Title:StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management\n    Authors:Ruizhe Zhang, Xinke Jiang, Zhibang Yang, Zhixin Zhang, Jiaran Gao, Yuzhen Xiao, Hongbin Lai, Xu Chu, Junfeng Zhao, Yasha Wang            View a PDF of the paper titled StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management, by Ruizhe Zhang and 9 other authors\n    View PDF\n\n\n\n    \n            Abstract:Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.05890 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.05890v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.05890\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Ruizhe Zhang [view email]          [v1]\n        Fri, 9 Jan 2026 16:09:48 UTC (972 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management, by Ruizhe Zhang and 9 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                "
  }
]