[
  {
    "title": "AnyView: Synthesizing Any Novel View in Dynamic Scenes",
    "url": "https://arxiv.org/abs/2601.16982v1",
    "source": "arxiv",
    "summary": "Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \\textbf{AnyView}, a diffusion-based video generation framework for \\emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data ",
    "full_text": null
  },
  {
    "title": "A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs",
    "url": "https://arxiv.org/abs/2601.16979v1",
    "source": "arxiv",
    "summary": "Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($Œª_{\\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, di",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Contributions\n\n\n\n2 Characterizing Critical Sharpness: Theory and Empirics\n\n2.1 Setup\n\n2.2 Critical Sharpness: A Scalable Measure of Curvature\n\nGeometric Interpretation:\nEfficient estimation of critical learning rate:\n\n\n2.3 The Relationship between Critical Sharpness and Hessian Sharpness\n\n\n3 Critical Sharpness Dynamics at Scale\n4 How much Pre-training data is needed to avoid Catastrophic forgetting?\n5 Related Works\n6 Discussion and Conclusion\n\n7 Estimating Critical Learning Rate Using Forward Passes\n\nExponential Search:\n\n\n8 Experimental Details\n\n9 Additional Results\n\n9.1 GPT Pre-training\n9.2 OLMo checkpoint analysis\n9.3 OLMo mid-training on a mixture consisting of DCLM and Math\n9.4 OLMo mid-training\n\n\n\n10 Theoretical Results\n\n10.1 The Relationship between Directional and Hessian sharpness\n10.2 Stability Threshold for Optimizers with Weight Decay\n\n\n\n\n\n\n\n\n1]Meta Superintelligence Labs\n2]University of Maryland, College Park\n\\contribution[*]work done during an internship at Meta\n\nA Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs\n\n\nDayal Singh Kalra\n\n‚ÄÉ‚ÄÉ\nJean-Christophe Gagnon-Audet\n\n‚ÄÉ‚ÄÉ\nAndrey Gromov\n\n‚ÄÉ‚ÄÉ\nIshita Mediratta\n\n‚ÄÉ‚ÄÉ\nKelvin Niu\n\n‚ÄÉ‚ÄÉ\nAlexander H Miller\n\n‚ÄÉ‚ÄÉ\nMichael Shvartsman\n\n[\n\n[\n\ndayal@umd.edu\n\n\n(January 23, 2026)\n\nAbstract\nUnderstanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks.\nThe most commonly studied measure, Hessian sharpness (ŒªmaxH\\lambda_{\\max}^{H}) ‚Äîthe largest eigenvalue of the loss Hessian ‚Äîdetermines local training stability and interacts with the learning rate throughout training.\nDespite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost.\nWe analyze critical sharpness (Œªc\\lambda_{c}), a computationally efficient measure requiring fewer than 1010 forward passes given the update direction Œî‚ÄãùúΩ\\Delta\\bm{\\theta}.\nCritically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability.\nUsing this measure, we provide the first demonstration of these sharpness phenomena at scale, up to 77B parameters, spanning both pre-training and mid-training of OLMo-2 models.\nWe further introduce relative critical sharpness (Œªc1‚Üí2\\lambda_{c}^{1\\to 2}), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies.\nCritical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.\n\n\n\\correspondence\n\n\n\n1 Introduction\n\nUnderstanding the evolution of the loss landscape L‚Äã(ùúΩ)L(\\bm{\\theta}) over the high-dimensional parameter space ùúΩ\\bm{\\theta} is fundamental to analyzing the training dynamics of neural networks. The loss landscape describes how the objective function changes with model parameters, and its geometry directly influences optimization, generalization and stability throughout training (Gilmer et al., 2022; Kalra and Barkeshli, 2024). Intuitively, gradient-based optimization methods can swiftly navigate to the minima in smooth landscapes, while for rough landscapes, such methods can get trapped, hindering convergence or potentially converging to suboptimal solutions (Li et al., 2018).\n\n\nThe local geometry of the loss landscape is commonly examined through the eigenvalues and eigenvectors of its Hessian matrix H‚Äã(ùúΩ):=‚àáŒ∏2L‚Äã(ùúΩ)H(\\bm{\\theta}):=\\nabla_{\\theta}^{2}L(\\bm{\\theta}) (Wu et al., 2018; Lewkowycz et al., 2020; Cohen et al., 2021). These eigenvalues provide insights into the local curvature of the loss landscape ‚Äîlarge eigenvalues correspond to sharp, steep directions, while small eigenvalues indicate flat, smooth regions.\nThe largest eigenvalue of the Hessian, which is often referred to as Hessian sharpness ŒªmaxH\\lambda^{H}_{\\max}, quantifies the worst-case curvature of the landscape and is a fundamental metric in optimization. Its reciprocal, flatness 1/ŒªmaxH1/\\lambda_{\\max}^{H}, is a complementary measure used to describe the curvature.\nIn neural network optimization, Hessian sharpness relates to the local training stability (Wu et al., 2018; Lewkowycz et al., 2020). For vanilla Gradient Descent (GD), if the learning rate exceeds the threshold ‚àº2/ŒªmaxH\\sim 2/{\\lambda_{\\max}^{H}}, loss increases as training ‚Äòcatapults‚Äô out of the local basin and eventually converges to a flatter region where the stability conditions are satisfied (Lewkowycz et al., 2020).\nThis observation generalizes to more complex optimization problems, including mini-batch settings and adaptive optimizers, albeit governed by different notions of sharpness (Wu et al., 2018; Agarwala and\nPennington, 2025; Cohen et al., 2024; Kalra and Barkeshli, 2024).\n\n\nFigure 1: Hessian sharpness exhibits progressive sharpening and Edge of Stability (EoS) under constant learning rate. The dashed lines corresponding to the learning rate mark the EoS threshold.\n\n\nAs illustrated in FigureÀú1, Hessian sharpness exhibits several robust trends throughout neural network training, particularly with constant learning rates.\nTraining typically begins with an early reduction in Hessian sharpness (Kalra and Barkeshli, 2023; Kalra et al., 2025) followed by a continuous increase until it reaches the stability threshold (Jastrzebski et al., 2020; Cohen et al., 2021).\nOnce this threshold is reached, the training stabilizes through a self-stabilization mechanism (Damian et al., 2023; Cohen et al., 2025) ‚Äîinstead of diverging as classical optimization would predict, Hessian sharpness begins to oscillate around this critical value. The continual increase in Hessian sharpness is referred to as progressive sharpening while the subsequent oscillations around the critical threshold is termed the Edge of Stability (EoS) (Cohen et al., 2021).\nFor the more realistic setting of learning rate schedules involving warmup and decay, sharpness closely follows the learning rate schedule (Gilmer et al., 2022; Kalra and Barkeshli, 2023; Cohen et al., 2021). Due to its particularly close relationship with learning rate, sharpness can be used as a diagnostic tool for identifying training instabilities.\n\n\nBeyond training dynamics, sharpness of the final solutions is linked to generalization properties, motivated by the intuition that flatter minima generalize better (Hochreiter and Schmidhuber, 1997). However, this relationship has been called into question (Kaur et al., 2023), as empirical analyses show mixed results ‚Äîwhile flatter solutions generalize better in some settings, they can hurt performance in others.\nSharpness also influences the early phase of training, with flatter initializations typically achieving better performance (Dauphin and Schoenholz, 2019; Kalra and Barkeshli, 2024).\n\n\nDespite its value for examining model initialization, training dynamics, and generalization properties, analyzing sharpness at scale is challenging.\nComputing Hessian sharpness relies on iterative eigenvalue solvers (e.g., Power iteration, Lanczos), which require repeated Hessian-vector products (HVPs). However, HVP computation via automatic differentiation is often incompatible with modern training efficiency tools; kernels like Flash Attention (Dao et al., 2022) typically lack second-derivative implementations required for double backpropagation. Furthermore, iterative solvers can require hundreds of iterations with potential convergence failures.\nAs a result, most existing studies are restricted to small-scale experiments (typically ‚àº10\\sim 10M parameters), leaving open questions about how sharpness evolves in Large Language Models (LLMs) at scale, and how it relates to optimization and downstream performance. In this work, we address this challenge by analyzing critical sharpness, which leverages the relationship between curvature and training instability to serve as a computationally efficient proxy for the loss landscape curvature of LLMs.\n\n\n\n1.1 Contributions\n\nOur contributions are as follows:\n\n\n‚Ä¢\n\nWe analyze critical sharpness, defined as Œªc=2/Œ∑c\\lambda_{c}=2/\\eta_{c}, where Œ∑c\\eta_{c} is the critical learning rate‚Äîthe smallest learning rate that causes the training loss to increase in the next training step. To estimate the critical learning rate Œ∑c\\eta_{c}, we perform an efficient line search along the update direction Œî‚ÄãùúΩ\\Delta\\bm{\\theta} from training. This procedure only requires forward passes, making it fully compatible with modern large-scale distributed training infrastructure, while avoiding the convergence issues of iterative eigenvalue solvers.\nIn practice, we find that this procedure reliably estimates critical sharpness in only 55-66 forward passes111except for the first iteration, which depends on the initial guess Œ∑0\\eta_{0} for line search..\n\n\n\n‚Ä¢\n\nWe then examine the relationship between critical sharpness and Hessian sharpness. Under the quadratic loss approximation, we show that critical sharpness can be written as a weighted sum of Hessian eigenvalues and the two measures coincide when the gradient aligns with the largest eigenvector of the Hessian. We also generalize this result to adaptive optimizers.\n\n\n\n‚Ä¢\n\nWe demonstrate that critical sharpness reliably captures well-documented Hessian sharpness phenomena, such as progressive sharpening and the Edge of Stability (Cohen et al., 2021). This makes critical sharpness a practical and computationally efficient proxy for sharpness. Leveraging OLMo-2 checkpoints (Walsh et al., 2025) across pre-training and mid-training, we demonstrate progressive sharpening persists at realistic scales throughout training, including models with up to 7 billion parameters.\n\n\n\n‚Ä¢\n\nWe introduce a relative measure of critical sha"
  },
  {
    "title": "Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection",
    "url": "https://arxiv.org/abs/2601.16976v1",
    "source": "arxiv",
    "summary": "Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques",
    "full_text": null
  },
  {
    "title": "Auto-Regressive Masked Diffusion Models",
    "url": "https://arxiv.org/abs/2601.16971v1",
    "source": "arxiv",
    "summary": "Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel ",
    "full_text": null
  },
  {
    "title": "BONO-Bench: A Comprehensive Test Suite for Bi-objective Numerical Optimization with Traceable Pareto Sets",
    "url": "https://arxiv.org/abs/2601.16970v1",
    "source": "arxiv",
    "summary": "The evaluation of heuristic optimizers on test problems, better known as \\emph{benchmarking}, is a cornerstone of research in multi-objective optimization.\n  However, most test problems used in benchmarking numerical multi-objective black-box optimizers come from one of two flawed approaches: On the one hand, problems are constructed manually, which result in problems with well-understood optimal ",
    "full_text": null
  },
  {
    "title": "Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians",
    "url": "https://arxiv.org/abs/2601.16967v1",
    "source": "arxiv",
    "summary": "In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delaye",
    "full_text": "\n\n\n\n1 Introduction and Background\n2 Related Work and Differentiation\n3 Scope of the Study\n\n4 Methods\n\n\n4.1 Research Design\n\n4.1.1 Phase 0: Proof of Concept (Completed)\n4.1.2 Phase 1: Forum Integration &amp; Feedback Loop\n4.1.3 Phase 2: API &amp; IoT Connectivity for Device Integration\n4.1.4 Phase 3: Model Optimization &amp; Continuous Learning\n4.1.5 Phase 4: Pilot Deployment &amp; Evaluation\n4.1.6 Phase 5: Multi-Device Expansion\n\n\n4.2 Phase 0 Dataset\n4.3 LLM Model\n\n\n\n5 Preliminary Results of Phase 0\n\n5.1 Error Code Interpretation Accuracy\n5.2 Instructional Query Evaluation\n5.3 Implications\n\n\n6 Discussion\n7 Significance of the Study\n8 Conclusion\n\n\n\n\n11institutetext: Carnegie Mellon University Africa, Kigali, Rwanda 22institutetext: Medical Artificial Intelligence Laboratory, Lagos, Nigeria 33institutetext: University of Pennsylvania 44institutetext: McGill University, Montr√©al, Canada\n44email: batabonf@andrew.cmu.edu,\n44email: aissah@andrew.cmu.edu,\n44email: mabdulba@andrew.cmu.edu,\n44email: cingabir@andrew.cmu.edu,\n44email: thetolusuyi@gmail.com,\n44email: marufadewole@outlook.com,\n44email: udunna.anazodo@mcgill.ca,\n44email: timxb@cmu.edu\nEmpowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians\n\n\n\nBernes Lorier Atabonfack\n\n‚ÄÉ‚ÄÉ\nAhmed Tahiru Issah\n\n‚ÄÉ‚ÄÉ\nMohammed Hardi Abdul Baaki\n\n‚ÄÉ‚ÄÉ\nClemence INGABIRE\n\n‚ÄÉ‚ÄÉ\nTolulope Olusuyi\n\n‚ÄÉ‚ÄÉ\nMaruf Adewole\n\n‚ÄÉ‚ÄÉ\n Dr\n\n‚ÄÉ‚ÄÉ\nUdunna Anazodo\n\n‚ÄÉ‚ÄÉ\n Dr\n\n‚ÄÉ‚ÄÉ\nTimothy Brown\n\n\n\nAbstract\nIn low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.\n\n\n\n1 Introduction and Background\n\nMedical devices are indispensable for delivering quality healthcare services, encompassing diagnosis, treatment, and patient monitoring. In high-resource settings, these devices benefit from regular maintenance, trained operators, and manufacturer support. Conversely, in low- and middle-income countries (LMICs), the situation is markedly different.\nStudies estimate that 40% to 70% of medical equipment in LMICs is non-functional or underutilized at any given time due to factors like poor maintenance systems, lack of spare parts, and insufficiently trained personnel [2] [1] [3] [4]. For instance, a detailed case study in Tanzania found that 30‚Äì50% of medical equipment in sub-Saharan Africa experiences downtime because of unstructured maintenance practices and limited technician capacity[5]. In Uganda, research conducted across nine tertiary hospitals and five research institutions revealed that 34% of medical devices were faulty, and 85.6% lacked operational manuals, making even minor issues difficult to resolve [8]. The lifespan of usable devices is often reduced by as much as 80%, primarily due to inexperienced operators and a lack of preventive maintenance protocols [11].\nThe World Health Organization estimates that up to 70% of donated equipment fails to function as intended due to mismatches with infrastructure or lack of user training [6]. Moreover, 40% of devices donated by Medical Surplus Recovery Organizations (MSROs) are inoperable due to missing documentation or limited manufacturer support[12] [10].\n\n\nThe shortage of trained biomedical engineering technicians (BMETs) further aggravates the issue. With only a few organizations globally offering formal BMET training (e.g., Engineering World Health, Medisend) [12], many LMICs lack the workforce needed to keep devices operational. Additionally, equipment donated or procured through third-party vendors often comes without access to manufacturer service forums or troubleshooting support, leaving technicians isolated and ill-equipped.\nIn many African hospitals and clinics, even minor technical issues such as unfamiliar error codes or calibration warnings can result in devices being taken offline for extended periods. This leads to significant delays in diagnostics, unnecessary patient referrals, and, in some cases, avoidable mortality.\nWhile digital health innovations have improved access to care and information, very few have directly addressed the operational gap in maintaining physical medical infrastructure. This study aims to bridge that gap through the design and implementation of an AI-powered medical device support system (INGENZI Tech) for diagnostic imaging devices. By integrating advanced natural language processing with structured manuals, log data, and peer-generated insights, the system empowers biomedical technicians to act with confidence. The goal of this project is to reduce downtime, improve technician efficiency, and enhance healthcare system reliability, starting with a validated use case on the Philips HDI 5000 ultrasound machine in East Africa.\n\n\n\n\n2 Related Work and Differentiation\n\nWhile a number of AI-driven platforms exist globally for medical device management, most are tied to proprietary hardware, operate in online-only contexts, or target high-resource health systems. Table 1 provides a comparative overview of notable commercial solutions and positions INGENZI Tech within this landscape.\n\n\nIn parallel, academic research has made significant progress in predictive maintenance for medical equipment. For instance, Shamayleh et al.¬†[13] deployed an IoT-based maintenance system across over 8,000 medical devices in Malaysia, achieving a 25% cost reduction using SVM-based failure classification. Similarly, Zamzam et al.¬†[14] achieved 99.4% prediction accuracy using sensor fusion methods applied to over 13,000 medical assets. Mohamed et al.¬†[15] proposed a digital twin approach for MRI equipment, reducing machine downtime by 20%. Deep learning approaches, such as CNN-LSTM hybrid models, have shown 92‚Äì99% accuracy in remaining useful life estimation across several medical device categories¬†[16, 17]. Luschi et al.¬†[18] further demonstrated how predictive analytics integrated with IoT positioning systems can support hospital maintenance workflows in structured environments.\n\n\nDespite this progress, such systems typically require continuous data streams, sensor infrastructure, and robust connectivity, conditions that are often absent in LMIC settings. Research by Anazodo et al.¬†[4] and Diaconu et al.¬†[2] highlights the infrastructural barriers to medical equipment usage in LMICs, including poor internet access, inconsistent power supply, and lack of OEM support, particularly for donated devices.\n\n\nIn terms of knowledge management, Abidi¬†[19] and Tabrizi and Morgan¬†[20] emphasize the need for collaborative technical knowledge platforms in healthcare. While prior work has demonstrated the effectiveness of training platforms¬†[21], few systems combine this with real-time, context-aware AI guidance for medical equipment.\n\n\nINGENZI Tech differentiates itself through its integrated design tailored for LMICs. Unlike systems that focus solely on predictive analytics¬†[14, 15] or manufacturer-side service\nworkflows¬†[22], INGENZI Tech offers an end-user diagnostic platform combining:\n\n\n‚Ä¢\n\nA multilingual, LLM-powered chatbot for step-by-step device guidance.\n\n\n\n‚Ä¢\n\nA RAG-based architecture with segmented vector stores to reduce hallucinations and enhance contextual precision¬†[23, 24].\n\n\n\n‚Ä¢\n\nOffline-first deployment suitable for low-bandwidth and rural clinics.\n\n\n\n‚Ä¢\n\nA peer-to-peer technician forum for continuous feedback, local adaptation, and model improvement¬†[19, 20].\n\n\n\n\n\nThe Phase 0 proof-of-concept targets the Philips HDI 5000 ultrasound, a device commonly found in African clinics but frequently unsupported, addressing the challenges raised in recent surveys on equipment non-functionality¬†[8, 1]. Future phases include scaling to CT, MRI, and X-ray machines.\n\n\nTable 1: Comparative Landscape of Existing AI-Powered Medical Maintenance Solutions.\n\n\n\n\n\nAI Solution\n\n\n\n\nStrengths\n\n\n\n\nLimitations in LMIC Context / Differentiation vs. INGENZI Tech\n\n\n\n\n\n\n\n\nBruviti\n\n\n\n\nLLM-powered diagnostics; preserves institutional knowledge; supports global manufacturers\n\n\n\n\nNo public deployments in LMICs; not optimized for offline use or multilingual environments; high dependency on OEM integrations\n\n\n\n\n\n\nHadleigh Health, Vestfrost EMS, Nexleaf Analytics\n\n\n\n\nIoT-based predictive maintenance; real-time monitoring; some LMIC deployments\n\n\n\n\nPrimarily hardware-embedded systems; limited end-user diagnostic interaction; lacks LLM or multilingual troubleshooting support\n\n\n\n\n\n\nBosnia AI-Metrology Project\n\n\n\n\nApplies AI to device calibration and performance in public hospitals; shows feasibility in low-resource settings\n\n\n\n\nRegional, narrow scope; focused on measurement and standards, not interactive technician support or documenta"
  },
  {
    "title": "Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts",
    "url": "https://arxiv.org/abs/2601.16965v1",
    "source": "arxiv",
    "summary": "Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial informat",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nGeospatial Question Answering.\nLLM-based Agents and Tool Use.\nSpatial Core Concepts and Workflow Composition.\n\n\n\n3 Spatial-Agents\n\n\n3.1 Problem formulation of Agentic Geo-Spatial Reasoning\n\nSystem Overview.\n\n\n3.2 Geospatial Concept Grounding\n\n3.3 Spatial Orchestration\n\nGeoflow graph factorization.\nGeo-agentic workflow Constraints.\n\n\n\n3.4 Retrieval-augmented Orchestration\n\nTemplate Library.\n\n\n3.5 Learning with Geographic Constraints\n3.6 Execution and Response Generation\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\nDatasets.\nBaselines.\n\n\n\n4.2 Main Results\n\nResults on MapEval-API.\nResults on MapQA.\n\n\n\n4.3 Analysis\n\nError Analysis.\nLatency Analysis.\nCost Analysis.\n\n\n\n4.4 Ablation Study\n\nEffect of Fine-tuning.\nEffect of Template Composition.\n\n\n\n\n5 Conclusion\n\nA Core Spatial Concepts\n\nA.1 Definition\nA.2 Concept Semantics\nA.3 Examples\n\n\n\nB Functional Roles in Geo-Analytical Questions\n\nB.1 Definition\n\nB.2 Contextual vs. Procedural Roles\n\nContextual Roles\nProcedural Roles\n\n\nB.3 Procedural Precedence\nB.4 Examples\n\n\n\nC Operator Library\n\n\nC.1 Geocoding and Location Resolution\n\ngeocode(tt, aa, rr)\nbatch_geocode(TT, aa)\nreverse_geocode(œï\\phi, Œª\\lambda)\n\n\n\nC.2 Place Search and Retrieval\n\nplace_search(cc, œÅ\\rho, œÑ\\tau, Œ∫\\kappa, rminr_{\\min})\nplace_details(pp)\nbatch_place_details(PP)\n\n\n\nC.3 Routing and Navigation\n\ndirections(oo, dd, mm, WW)\ndistance_matrix(OO, DD, mm)\ncompare_routes(RR, Œº\\mu)\nfilter_routes(RR, Œ∫\\kappa)\nextract_distance(rr) / extract_duration(rr)\n\n\n\nC.4 Geometric Computation\n\nhaversine(œï1\\phi_{1}, Œª1\\lambda_{1}, œï2\\phi_{2}, Œª2\\lambda_{2})\nbearing(œï1\\phi_{1}, Œª1\\lambda_{1}, œï2\\phi_{2}, Œª2\\lambda_{2})\nbearing_to_direction(Œ∏\\theta)\n\n\n\nC.5 Spatial Analysis\n\nnearest(aa, CC, Œº\\mu)\nwithin_radius(cc, œÅ\\rho, CC)\npairwise_extremes(LL)\nfilter_places(PP, Œ∏\\theta)\n\n\n\nC.6 Temporal Reasoning\n\nopen_at_time(pp, tt)\ntimezone(œï\\phi, Œª\\lambda, œÑ\\tau)\ncalculate_finish_time(t0t_{0}, LL, SS, mm)\n\n\n\nC.7 Trip Optimization\n\ntsp_tw(DD, LL, SS, WW, t0t_{0}, TT)\nsteps_analysis(rr, ‚Ñì\\ell)\n\n\nC.8 Local Context Operators\n\n\n\nD Fine-tuning Details\n\nD.1 Stage 1: Supervised Fine-Tuning\nD.2 Stage 2: Direct Preference Optimization\n\n\n\nE Template Library\n\nFilter-Aggregate-Measure\nObject-Field-Measure\nRoute-Optimize\nGeocode-Batch-Compare\nLocation-Bearing-Classify\nRoute-Step-Extract\nMulti-Route-Compare\nPlace-Attribute-Query\nMulti-Segment-Aggregate\nTime-Window-Reverse\nE.1 Example Retrieval Mechanism\n\n\n\nF Execution and Response Generation Details\n\nF.1 Execution Semantics\nF.2 Grounded Response Generation\n\n\n\n\n\n\n\nSpatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts\n\n\n\nRiyang Bao1*,\nCheng Yang2*,\nDazhou Yu1,\nZhexiang Tang2,\nGengchen Mai3,\nLiang Zhao1\n\n1Emory University,\n2Rutgers University,\n3University of Texas at Austin\n\n*Equal contribution. ‚ÄÉCorrespondence: Liang Zhao\n\n\n\nAbstract\nGeospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs‚Äîdirected acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.\n\n\n\nSpatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts\n\n\n\n\n\nRiyang Bao1*,\nCheng Yang2*,\nDazhou Yu1,\nZhexiang Tang2,\nGengchen Mai3,\nLiang Zhao1\n\n\n\n1Emory University,\n2Rutgers University,\n3University of Texas at Austin\n\n*Equal contribution. ‚ÄÉCorrespondence: Liang Zhao\n\n\n\n\n\n\n1 Introduction\n\nGeospatial reasoning is essential for numerous real-world applications, including urban analytics, transportation planning, environmental monitoring, disaster response, and public health (Li et al., 2021, 2023; Mai et al., 2021, 2025). As geospatial datasets and modern GIS platforms\ncontinue to proliferate, users increasingly expect natural-language interfaces capable of handling complex geo-analytical questions Yu et al. (2025); Scheider et al. (2020, 2021). However, despite the impressive capabilities of contemporary large language models (LLMs), we observe that current agent-style systems, including function-calling LLMs and commercial AI agents, do not perform genuine geospatial reasoning.\n\n\nFigure 1: LLM-intuitive but incorrect workflow (left) vs. correct concept transformation (right). The incorrect applies spatial constraints after aggregation; the correct computes crime rates within spatial context first.\n\n\nPrior work shows that LLM-based agents lack inherent spatial awareness, relying on web search or textual pattern matching rather than computational spatial analysis¬†(Yan et al., 2023; Zhang et al., 2025a). As illustrated in Figure¬†1, they hallucinate spatial relationships, fail on geometric or topological predicates, and cannot construct valid workflows¬†(Wang et al., 2025; Ji et al., 2025).\n\n\nThis reflects a deeper limitation: geo-analytical questions require\nprocedural, multi-step reasoning over spatial data, which fundamentally differs from declarative QA. Classical GIScience research has long emphasized that geographical phenomena are inherently computational rather than purely linguistic Goodchild (1992); Miller and Goodchild (2015); Goodchild et al. (2007). Answering a geo-analytical question often involves: (i) identifying core spatial entities (objects, events, fields,\nnetworks), (ii) selecting appropriate spatial operators (buffering, overlay,\nrouting, aggregation), (iii) ordering operators into an executable workflow, and (iv) grounding abstract instructions into concrete GIS tools (e.g., PostGIS, ArcGIS, QGIS). These operations are geometric, topological, and sometimes spatiotemporal in nature, and thus beyond the representational power of language-only models.\n\n\nA promising direction in GIScience suggests that geo-analytical questions encode implicit procedural knowledge. Kuhn‚Äôs theory of\ncore concepts Kuhn (2012) and subsequent work by Scheider et al. Scheider et al. (2020) identifies foundational building blocks of spatial information, such as Object, Field, Event and Network. Complementing these, GeoAnQu research Xu et al. (2023) highlights functional roles (Measure, Condition, Subcondition, Support, Extent, etc.) that encode the procedural structure of geo-analytical questions. By combining core concepts and functional roles, GeoAnQu demonstrates that natural-language questions can be mapped into concept transformations, forming the basis of GIS workflows represented as directed acyclic graphs (DAGs). Yet this framework remains rule-based, offline, and detached from modern AI agent architectures.\n\n\nIn this paper, we ask: What would it take for an AI agent to truly understand and execute geo-analytical questions? Despite advances in tool-augmented LLMs, current agents remain limited in the geospatial domain. They often misinterpret spatial entities and relations, lack a procedural understanding of how geo-analytical tasks unfold, and cannot reliably map natural-language questions to coherent sequences of spatial operations. These systems also struggle to manage intermediate spatial states or ground their reasoning in computational GIS tools, which leads to answers that are largely descriptive rather than operational. As a result, existing agents fail to produce verifiable and executable geospatial analyses.\n\n\nTo address these challenges, we present Spatial-Agent, a geospatial AI agent grounded in foundational theories of spatial information. We formalize geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs. Drawing on core concepts and functional roles, Spatial-Agent establishes a principled intermediate representation that bridges language and computation. Specifically, the agent (i) extracts spatial concepts from questions and instantiates them as graph nodes, (ii) identifies functional roles that impose ordering constraints, (iii) composes transformation edges through a template-based approach that leverages recurring geo-analytical patterns, and (iv) executes the workflow via tool invocations, grounding its final response in verifiable computational results rather than parametric knowledge alone.\n\n\nOur contributions are summarized as follows:\n\n\n‚Ä¢\n\nWe present Spatial-Agent, which enables geospatial reasoning by uncovering the implicit structure of spatial questions and generating coherent, executable workflows.\n\n\n\n‚Ä¢\n\nWe propose a compositional GeoFlow Graph generation approach based on macro-templates, capturing recurring geo-analytical patterns and improving structural validity via template matching and IO-port composition.\n\n\n\n‚Ä¢\n\nExtensive evaluations show that SpatialAgent delivers significantly better correctness, interpretability, and executable workflow generation than existing agent baselines, bridging the gap between natural-language reasoning and computational GIS.\n\n\n\n\n\n\n\n2 Related Work\n\nGeospatial Question Answering.\n\nGeospatial question answering Mai et al. (2021) is a sub-domain of question answering focusing on questions that involve geographic entities, concepts, and/or require geospatial computation.\nEarly geospatial QA "
  },
  {
    "title": "AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems",
    "url": "https://arxiv.org/abs/2601.16964v1",
    "source": "arxiv",
    "summary": "The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark data",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-A LLM-Augmented Autonomous Driving Agents\nII-B LLM-Based Scenario and Code Generation\nII-C Benchmarks for Driving Scenario Reasoning\nII-D Driving Theory Tests for LLMs\nII-E AgentDrive in Context\n\n\n\nIII AgentDrive: Dataset Generation Methodology\n\nIII-A Scenario Space Definition\n\nIII-B Scenario Taxonomy and Diversity\n\nIII-B1 Scenario Types.\nIII-B2 Driver Behaviors.\nIII-B3 Environmental Conditions.\nIII-B4 Road Layouts.\nIII-B5 Objectives.\nIII-B6 Difficulty and Traffic Density.\n\n\nIII-C Scenario Specification via LLM\nIII-D Simulation Rollout Generation\nIII-E Surrogate Safety Metrics\nIII-F Rule-Based Labeling\nIII-G Dataset Construction\nIII-H End-to-End Scenario Generation Pipeline\n\n\n\nIV AgentDrive-MCQ Benchmark\n\nIV-A Motivation\nIV-B AgentDrive-MCQ: Dataset Generation Methodology\nIV-C Framework Illustration\nIV-D Prompt Design and Control\n\n\n\nV LLM Reasoning Performance on AgentDrive-MCQ\n\n\nV-A Performance Metrics Definition\n\nOverall Accuracy\nSafety Compliance Rate (SCR)\nSituational Awareness Score (SAS)\n\n\nV-B Overview of Model Performance\nV-C SAS‚ÄìSCR Correlation\nV-D Physics-Style Challenges\nV-E Policy-Style Challenges\nV-F Hybrid-Style Challenges\nV-G Comparative-Style Challenges\nV-H Scenario-Style Challenges\n\n\nVI Conclusion\n\n\n\n\n\n\nAgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems\n\n\n\nMohamed¬†Amine¬†Ferrag‚àó¬ß,¬†,\nAbderrahmane¬†Lakas‚àó,¬†,\nand¬†Merouane¬†Debbah1\n‚àóDepartment of Computer and Network Engineering, College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates.1Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates.¬ßCorresponding author: mohamed.ferrag@uaeu.ac.ae\n\n\nAbstract\nThe rapid advancement of Large Language Models (LLMs) has sparked growing interest in their integration within autonomous systems to enable reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for the training, fine-tuning, and evaluation of autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes‚Äîscenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density‚Äîand employs an LLM-driven prompt-to-JSON pipeline to produce semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question reasoning benchmark spanning five reasoning dimensions‚Äîphysics, policy, hybrid, scenario, and comparative‚Äîto systematically assess the cognitive and ethical reasoning of LLM-based agents. We conducted a large-scale evaluation of fifty leading LLMs on the AgentDrive-MCQ benchmark to measure their reasoning capabilities across these five dimensions, covering models such as GPT-5, ChatGPT 4o, Gemini 2.5 Flash, DeepSeek V3, Qwen3 235B, ERNIE 4.5 300B, Grok 4, Mistral Medium 3.1, and Phi 4 Reasoning Plus. Results reveal that while proprietary frontier models dominate in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. To support open science and reproducibility, we release the AgentDrive dataset (including labeled data), the AgentDrive-MCQ benchmark, evaluation scripts, and all related materials on GitHub: https://github.com/maferrag/AgentDrive.\n\n\nIndex Terms: \nAutonomous Driving, Large Language Models, Autonomous AI Agents, Benchmark Datasets, Reasoning.\n\n\n\n\nI Introduction\n\n\nThe rise of large language models (LLMs) has sparked broad interest in applying them to autonomous driving (AD) due to their strong reasoning and conversational abilities [22]. Researchers have introduced the concept of LLM4AD, designing AD systems that leverage LLMs for tasks ranging from perception and scene understanding to decision-making [7, 6, 9]. For example, Cui et al. [7] proposed an LLM4AD framework with a comprehensive simulation benchmark to evaluate how well LLMs follow driving instructions. Initial experiments in both simulation and real-world driving showed that LLMs can indeed enhance an autonomous vehicle‚Äôs understanding of complex environments and improve human‚Äìvehicle interactions. These findings, echoed by recent surveys [22, 6], suggest that combining LLMs with vision models could enable more open-world perception, logical reasoning, and adaptive learning than traditional rule-based or end-to-end systems. At the same time, the LLM4AD concept paper and surveys emphasize that this nascent field faces significant challenges, such as ensuring real-time performance and safety, which must be addressed as development continues.\n\n\nIntegrating LLMs into an autonomous driving agent raises new complexities in how the vehicle interprets instructions, interacts with humans, and adheres to traffic rules. One approach to this is DriVLMe, an LLM-based driving agent augmented with both embodied experiences (learned via a driving simulator) and social experiences from real human dialogues [10]. The goal of DriVLMe is to facilitate natural communication between humans and self-driving cars and to handle long-horizon navigation tasks through free-form dialogue. DriVLMe demonstrated competitive performance in open-loop simulations and closed-loop user studies; however, it also revealed several limitations, including unacceptably high inference times, imbalanced training data, and difficulty handling sudden environmental changes. To ensure LLM-driven vehicles make decisions that are not only effective but also legally and ethically aligned, other works focus on injecting domain knowledge and safeguards into the decision process. For example, Cai et al. [2] present a retrieval-augmented reasoning framework where a Traffic Regulation Retrieval module automatically fetches relevant traffic laws and safety guidelines based on the vehicle‚Äôs situation, and an LLM then interprets these rules to assess each action‚Äôs legality and safety. This approach yields an interpretable decision-making pipeline that adapts to regional regulations and provides transparency into why the AI chooses a certain action. Similarly, Kong et al. [12] introduce a multi-agent superalignment framework to enforce data security and policy compliance in LLM-driven cars. Their system safeguards sensitive vehicle data (e.g., precise locations, camera feeds) from potential leaks and filters the LLM‚Äôs queries and outputs, ensuring that driving commands do not violate safety rules or human values. These efforts demonstrate how researchers are enhancing LLM-based driving agents with embodied learning and alignment mechanisms, enabling autonomous vehicles to interact naturally while adhering to legal, safety, and trust constraints.\n\n\nBeyond high-level dialogue and compliance, LLMs and multimodal models are being used to tackle core perception and planning challenges in autonomous driving. Bai et al. [1] argue that prior vision‚Äìlanguage planning approaches, which tokenize only 2D images, cannot reliably perceive a 3D world. They propose a planner that uses DETR-style 3D perceptrons as tokenizers, feeding the LLM with 3D object queries from multi-view camera images. This strategy provided rich geometric context, yielding superior performance in 3D object detection and end-to-end planning on the nuScenes dataset, suggesting that 3D-tokenized LLMs could be key to more reliable path planning. Sah et al. [17] integrate deep learning models for traffic sign recognition and lane detection with a lightweight multimodal LLM that can reason about the scene. This hybrid system achieved near-perfect accuracy in clear conditions and maintained robustness under adverse weather and occlusions, demonstrating the LLM‚Äôs ability to interpret contextual clues. Other studies leverage LLMs for specialized tasks such as parking in mixed traffic [11] and interpretable end-to-end control. Chen et al. [4] fuse object-level vector modalities with LLM reasoning to provide explainable driving decisions, while Xu et al. [21] propose DriveGPT4, which generates both low-level control signals and natural language explanations for each action. These advances highlight LLMs as powerful engines for enhancing perception, planning, and transparency in AD systems.\n\n\nFigure 1: Overview of the AgentDrive benchmark suite, which comprises three complementary datasets:\nAgentDrive-Gen for LLM-based driving scenario generation,\nAgentDrive-Sim for simulation and outcome labeling, and\nAgentDrive-MCQ for reasoning and decision-making evaluation. \n\n\nTo validate and further develop LLM-driven autonomous vehicles, the community is creating new benchmarks and evaluation tools to enhance their development. One notable effort is Bench2ADVLM, a closed-loop evaluation framework introduced by Zhang et al. [24] to test vision‚Äìlanguage AD models in interactive settings. Bench2ADVLM enables real-time closed-loop testing across simulators and physical vehicles, with a scenario generator that uncovers failure modes. Results indicate that current systems still exhibit limitations under interactive conditions. In parallel, MAPLM [3] provides a large-scale multimodal dataset combining maps, LiDAR, panoramic images, and Q&amp;A annotations, highlighting the importance of domain-specific data for grounding LLMs in driving contexts. Gao et al. [9] survey how foundation models can generate and analyze diverse scenarios, showing their potential for evaluating and handling safety-critical cases. Despite progress, challenges persist in latency, interpretability, p"
  },
  {
    "title": "3D Molecule Generation from Rigid Motifs via SE(3) Flows",
    "url": "https://arxiv.org/abs/2601.16955v1",
    "source": "arxiv",
    "summary": "Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation,",
    "full_text": null
  },
  {
    "title": "Strategies for Span Labeling with Large Language Models",
    "url": "https://arxiv.org/abs/2601.16946v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are increasingly used for text analysis tasks, such as named entity recognition or error detection. Unlike encoder-based models, however, generative architectures lack an explicit mechanism to refer to specific parts of their input. This leads to a variety of ad-hoc prompting strategies for span labeling, often with inconsistent results. In this paper, we categorize th",
    "full_text": null
  },
  {
    "title": "Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles",
    "url": "https://arxiv.org/abs/2601.16936v1",
    "source": "arxiv",
    "summary": "In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only unde",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Related Work\n\nDeep Ensemble.\nBatchEnsemble.\nEnsemble Diversity and EU.\nThis work.\n\n\n3 Multiplicative Rank-1 Perturbations Have Measure Zero\n4 BatchEnsemble Underperforms Deep Ensemble\n5 BatchEnsemble Is Not Diverse\n6 Conclusion\n7 Limitations &amp; Future Work\nA BatchEnsemble Vectorization\nB Proofs For Limited Expressivity Of BatchEnsemble\n\nC Metrics\n\nPredictive performance.\nCalibration.\nUncertainty proxies.\nOOD detection.\n\n\n\nD Experimental Setup\n\nGeneral procedure.\nImplementation details.\nD.1 CIFAR-10\nD.2 MNIST\n\n\nE Computational Cost and Training Time\nF Channel Rescaling vs. Rank-1 Kernel Perturbation\n\n\n\n\n\n\nIs BatchEnsemble a Single Model? \nOn Calibration and Diversity of Efficient\nEnsembles\n\n\n\n\nAnton Zamyatin \nTU Wien \nanton.zamyatin@tuwien.ac.at\n&amp;Patrick Indri \nTU Wien \npatrick.indri@tuwien.ac.at\n&amp;Sagar Malhotra \nTU Wien \nsagar.malhotra@tuwien.ac.at\n&amp;Thomas G√§rtner \nTU Wien \nthomas.gaertner@tuwien.ac.at\n\n\n\nAbstract\nIn resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained.\nDeep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models.\nBatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network.\nWe show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR-10/10C/SVHN.\nA controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes.\nThus, BatchEnsemble behaves more like a single model than a true ensemble.\n\n\n\n1 Introduction\n\nModern neural networks are increasingly used in safety-critical settings, where reliable uncertainty estimates are essential.\nEnsembles can be used to improve both accuracy and calibration Lakshminarayanan et al. (2017).\nEnsemble methods train multiple models independently and aggregate their predictions to capture epistemic uncertainty (EU), that is, the uncertainty in the model parameters that arises from training.\nTechniques such as Deep Ensembles (Lakshminarayanan et al., 2017) can be used to evaluate EU but require to train a neural network many times, which is often prohibitively expensive.\nBatchEnsemble Wen et al. (2020) address this by (i) applying rank-1 perturbations to a single shared weight matrix to form distinct members and (ii) vectorizing inference so all members run in a single forward pass.\nDespite their popularity, there is no systematic comparison of BatchEnsemble with standard Deep Ensembles; this paper fills that gap and investigates the shortcomings of BatchEnsemble.\nWe compare BatchEnsemble to Deep Ensembles as well as MC Dropout Gal and Ghahramani (2016), and structure the rest of the paper as follows.\nIn Section¬†2, we provide some preliminary background on Deep Ensembles, BatchEnsembles, notions of ensemble diversity and EU in ensemble methods.\nIn Section¬†3, we theoretically highlight that BatchEnsemble can only explore a significantly restricted portion of the parameter space, compared to Deep Ensemble.\nIn Section¬†4 we empirically show that BatchEnsemble underperform both Deep Ensemble and MC Dropout in terms of predictive performance across various metrics.\nIn Section¬†5 we show that BatchEnsembles learned on MNIST Mu and Gilmer (2019) data are essentially equivalent to a single model, further confirming that the theoretical observations of restricted parameter space also transfers to restricted solution space in real-world settings.\n\n\n\n\n2 Background and Related Work\n\nDeep Ensemble.\n\nDeep Ensembles (Lakshminarayanan et al., 2017) approximate the posterior distribution p‚Äã(y‚à£x,D)p(y\\mid x,D) of observing label yy for an unseen input xx given the training data DD by uniformly averaging the outputs of kk independently trained models:\n\n\n\np^‚Äã(y‚à£x,D)=1k‚Äã‚àëi=1kp‚Äã(y‚à£x,Œ∏i),\\hat{p}(y\\mid x,D)=\\frac{1}{k}\\sum^{k}_{i=1}p(y\\mid x,\\theta_{i}),\n\n(1)\n\n\nwhere Œ∏i\\theta_{i} are parameters obtained from different random initialization and training trajectories.\nDeep Ensembles provide robust epistemic uncertainty (EU) under distribution shift and outperform scalable Bayesian approximations such as stochastic variational inference (SVI) Ovadia et al. (2019).\nHowever, the computational and memory cost of Deep Ensembles scales linearly with kk, which makes them impractical for low-latency or resource constrained settings such as high-throughput screening, real-time systems, or large-scale models.\n\n\n\nBatchEnsemble.\n\nTo mitigate the linear scaling in computational/memory cost of Deep Ensembles,\nWen et al. (2020) introduced BatchEnsemble.\nBatchEnsemble promises to deliver ensemble-level EU at (nearly) single model cost.\nTo this end, BatchEnsemble embeds an ensemble within one network by applying learned low-rank, per-member perturbations to a shared weight matrix.\nLet W‚àà‚Ñùm√ónW\\in\\mathbb{R}^{m\\times n} denote a layer‚Äôs shared weight and ri‚àà‚Ñùmr_{i}\\!\\in\\!\\mathbb{R}^{m}, si‚àà‚Ñùns_{i}\\!\\in\\!\\mathbb{R}^{n} be member-specific learned vectors.\nThen, BatchEnsemble parametrizes the weight matrix WiW_{i} of each member i=1,‚ãØ,ki=1,\\cdots,k as\n\n\n\nWi=W‚àòri‚Äãsi‚ä§,W_{i}=W\\circ r_{i}s_{i}^{\\top},\n\n(2)\n\n\nwhere ‚àò\\circ denotes the Hadamard product and each ri‚Äãsi‚ä§r_{i}s_{i}^{\\top} is a rank-1 matrix.\nBatchEnsemble thus reduces the per-layer parameters from O‚Äã(k‚Äãm‚Äãn)O(kmn) for independent members obtained using Deep Ensembles, to O‚Äã(m‚Äãn+k‚Äã(m+n))O(mn+k(m+n)).\nThis leads to a substantial parameter reduction, especially for large matrices where k‚â™min‚Å°(m,n)k\\!\\ll\\!\\min(m,n).\nThe forward pass for a single input x‚àà‚Ñùnx\\in\\mathbb{R}^{n} can be written as\n\n\n\nWi‚Äãx=(W‚àòri‚Äãsi‚ä§)=W‚Äã(si‚àòx)‚àòri,W_{i}x=(W\\circ r_{i}s_{i}^{\\top})=W(s_{i}\\circ x)\\circ r_{i},\n\n(3)\n\n\nwith member-specific biases omitted for brevity.\nThis formulation admits a vectorized implementation (Appendix¬†A) that computes the prediction of all kk members in a single batched forward pass‚Äîhence ‚ÄúBatchEnsemble‚Äù‚Äîenabling within-device parallelism and high-throughput inference.\nBy contrast the popular ensemble alternative MC Dropout Gal and Ghahramani (2016) requires multiple stochastic forward passes to obtain an ensemble of predictions.\n\n\n\nEnsemble Diversity and EU.\n\nA leading explanation for the success of Deep Ensembles is that independently trained members explore distinct solutions not only in parameter space but also in function space Fort et al. (2019).\nIn contrast, scalable Bayesian methods such as mean-field VI Graves (2011), Bayes by Backprop Blundell et al. (2015), or MC Dropout Gal and Ghahramani (2016) approximate uncertainty by placing a simple distribution (e.g., diagonal Gaussian or dropout mask) centered around a single optimum.\nConsequently, efficient ensembling methods should be designed such that they are able to replicate this effect by capturing distinct predictive modes, not just local perturbations around a single optimum.\n\n\n\nThis work.\n\nWe compare BatchEnsemble to Deep Ensembles and MC Dropout on CIFAR-10 across in-distribution, distribution shift, and out-of-distribution (OOD) settings (Section¬†4; protocols and metrics in Appendix¬†D.1 and C).\nBatchEnsemble does not deliver ensemble-level gains, yielding low epistemic uncertainty and closely matching the single-model baseline in terms of predictive performance, calibration, and OOD detection.\nA diagnostic MNIST study further reveals BatchEnsemble members are nearly identical in parameter space and function space, implying that limited capacity to realize multiple predictive modes may underlie BatchEnsemble‚Äôs limited EU.\n\n\n\n\n\n3 Multiplicative Rank-1 Perturbations Have Measure Zero\n\nWe begin by making some simple but pertinent theoretical observations that show that the parameter space expressible by BatchEnsembles is vanishingly small compared to Deep Ensembles.\n\n\n\nObservation 1.\n\n\nConsider ensembles with kk members.\nFor BatchEnsemble, consider the members as parameterized by ri‚àà‚Ñùmr_{i}\\in\\mathbb{R}^{m}, si‚àà‚Ñùns_{i}\\in\\mathbb{R}^{n}, and the shared weight matrix WW.\nLet ùí≤BE={(W‚àòr1‚Äãs1‚ä§,‚Ä¶,W‚àòrk‚Äãsk‚ä§)‚à£ri‚àà‚Ñùm,si‚àà‚Ñùn,i‚àà[k],W‚àà‚Ñùm√ón}\\mathcal{W}_{\\text{BE}}=\\{\\left(W\\circ r_{1}s_{1}^{\\top},\\dots,W\\circ r_{k}s_{k}^{\\top}\\right)\\mid r_{i}\\in\\mathbb{R}^{m},s_{i}\\in\\mathbb{R}^{n},i\\in[k],W\\in\\mathbb{R}^{m\\times n}\\} be the set of weights achievable with BatchEnsemble.\nLet ùí≤DE={(W1,‚Ä¶,Wk)‚à£Wi‚àà‚Ñùm√ón,i‚àà[k]}\\mathcal{W}_{\\text{DE}}=\\{\\left(W_{1},\\dots,W_{k}\\right)\\mid W_{i}\\in\\mathbb{R}^{m\\times n},i\\in[k]\\} be the set of weights achievable with Deep Ensembles, where each ensemble member has an independent weight matrix layer WiW_{i} of size ‚Ñùm√ón\\mathbb{R}^{m\\times n}.\nThen, ùí≤BE‚ääùí≤DE\\mathcal{W}_{\\text{BE}}\\subsetneq\\mathcal{W}_{\\text{DE}}. Moreover, ùí≤BE\\mathcal{W}_{\\text{BE}} has measure zero in ùí≤DE\\mathcal{W}_{\\text{DE}}.\n\n\n\nA proof of Observation 1 and examples of weight matrices unrepresented weight matrices are provided in Appendix¬†B.\nThis result shows that the weights reachable by BatchEnsemble‚Äôs rank-1 multiplicative perturbations constitute a strict, measure-zero subset of those reachable by independently trained members, i.e., Deep Ensembles.\nThis limited expressivity suggests that BatchEnsemble cannot capture the same degree of diversity as a Deep Ensemble.\nIn the next sections, we test this hypothesis empirically on real data.\n\n\n\n\n4 BatchEnsemble Underperforms Deep Ensemble\n\nWe compare a single model, MC Dropout, Deep Ensembles, and BatchEnsemble on image classification, using the same architecture (ResNet-18 He et al. (2016)) trained for 75 epochs on CIFAR-10 Krizhevsky et al. (2009).\nFollowing the protocol of Ovadia et al. (2019), we evaluate the models on three benchmarks:\n\n\n‚Ä¢\n\nCIFAR-10 test set to measure in-distribution performance,\n\n\n\n‚Ä¢\n\nCIFAR-10C Hendrycks and Dietterich (2019) at corruption severity 5 to assess robustness to distributional shift,\n\n\n\n‚Ä¢\n\nSVHN Netzer e"
  },
  {
    "title": "Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias",
    "url": "https://arxiv.org/abs/2601.16934v1",
    "source": "arxiv",
    "summary": "To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segme",
    "full_text": null
  },
  {
    "title": "Reward-Forcing: Autoregressive Video Generation with Reward Feedback",
    "url": "https://arxiv.org/abs/2601.16933v1",
    "source": "arxiv",
    "summary": "While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically l",
    "full_text": null
  },
  {
    "title": "Nishpaksh: TEC Standard-Compliant Framework for Fairness Auditing and Certification of AI Models",
    "url": "https://arxiv.org/abs/2601.16926v1",
    "source": "arxiv",
    "summary": "The growing reliance on Artificial Intelligence (AI) models in high-stakes decision-making systems, particularly within emerging telecom and 6G applications, underscores the urgent need for transparent and standardized fairness assessment frameworks. While global toolkits such as IBM AI Fairness 360 and Microsoft Fairlearn have advanced bias detection, they often lack alignment with region-specifi",
    "full_text": "\n\n\n\nI Introduction\nII Related Works\n\nIII TEC Standard and Evaluation\n\nIII-A Lifecycle based assessment and risk quantification\n\nIII-B Contextual Threshold Determination\nIII-C Inference and Results Architecture\n\n\nIV Design and Architecture of Nishpaksh\n\nV Experiments and Results\n\nV-A Dataset Description\nV-B Experimental Setup\nV-C Fairness Metrics and Formulations\nV-D Fairness Metrics Results and Visualizations\n\n\nVI Implications for Standardization\nVII Conclusion\n\n\n\n\n\nNishpaksh: TEC Standard-Compliant Framework for Fairness Auditing and Certification of AI Models\n\n\n\nShashank Prakash\n\n‚ÄÉ‚ÄÉ\nRanjitha Prasad\n\n‚ÄÉ‚ÄÉ\nAvinash Agarwal\n\n\n\nAbstract\nThe growing reliance on Artificial Intelligence (AI) models in high-stakes decision-making systems, particularly within emerging telecom and 6G applications, underscores the urgent need for transparent and standardized fairness assessment frameworks. While global toolkits such as IBM AI Fairness 360 and Microsoft Fairlearn have advanced bias detection, they often lack alignment with region-specific regulatory requirements and national priorities. To address this gap, we propose Nishpaksh, an indigenous fairness evaluation tool that operationalizes the Telecommunication Engineering Centre (TEC) Standard for the Evaluation and Rating of Artificial Intelligence Systems. Nishpaksh integrates survey-based risk quantification, contextual threshold determination, and quantitative fairness evaluation into a unified, web-based dashboard. The tool employs vectorized computation, reactive state management, and certification-ready reporting to enable reproducible, audit-grade assessments, thereby addressing a critical post-standardization implementation need. Experimental validation on the COMPAS dataset demonstrates Nishpaksh‚Äôs effectiveness in identifying attribute-specific bias and generating standardized fairness scores compliant with the TEC framework. The system bridges the gap between research-oriented fairness methodologies and regulatory AI governance in India, marking a significant step toward responsible and auditable AI deployment within critical infrastructure like telecommunications.\n\n\n\nI Introduction\n\n\nThe growing integration of Artificial Intelligence (AI) models into high-stakes decision-making systems across diverse domains, from hiring and healthcare to finance and law enforcement, has amplified concerns around bias and accountability. This urgency is particularly acute in the context of telecommunications, where AI is increasingly critical for network management, personalized service delivery, and resource allocation in emerging 6G paradigms. Biased AI within telecom can lead to unequal access to services, discriminatory pricing, or unfair resource distribution, directly impacting societal well-being and national digital inclusion goals. Consequently, there is an urgent need for transparent and systematic tools to evaluate and ensure fairness in these models. Several open-source frameworks have emerged globally to address this issue, such as IBM‚Äôs AI Fairness 360 [1], Fairlearn by Microsoft [2] and What-If tool by Google [3]. These tools offer a range of fairness metrics, bias mitigation algorithms, and visualization features that enable users to detect and analyze unfair behavior in AI systems.\n\n\nIndia has a rapidly expanding AI ecosystem, which presents distinct challenges arising from its immense social, cultural, and demographic diversity. The data used to train AI models often reflects underlying social structures, where sensitive attributes such as caste, region, and economic background can unintentionally act as proxies for bias, making fairness evaluation particularly critical in the Indian context [4]. Recognizing this, and aligning with national research priorities and the Bharat 6G vision, the Telecommunication Engineering Center (TEC) published the ‚ÄôStandard for the Evaluation and Rating of Artificial Intelligence Systems‚Äô [5]. This national standard establishes a structured three-step procedure that involves (a) classification of the risk of bias through a standardized questionnaire, (b) selection of the fairness metrics and thresholds, and (c) analytical bias testing to produce a composite Bias Index (BI) and Fairness Score (FS) via fairness metrics computation [6]. Unlike general-purpose global frameworks, the TEC standard is explicitly designed to quantify fairness within India‚Äôs regulatory and demographic realities, making it a pivotal instrument for AI governance. However, while the TEC standard provides a well-defined framework for evaluating fairness, practical tools that operationalize this standard remain largely absent [5].\n\n\nExisting open-source libraries are designed for global contexts and do not align with the TEC‚Äôs procedures or India-specific data characteristics. Consequently, organizations seeking to comply with this national standard, especially within the telecom sector, often lack automated means to conduct bias assessments or generate standardized fairness reports. This gap highlights the pressing need for an indigenous, implementable auditing tool that translates the TEC guidelines into actionable, data-driven evaluations of AI fairness, thereby enhancing audit readiness and compliance for AI deployed in India [4].\n\n\nIn this work, we propose Nishpaksh, a fairness testing tool that implements the TEC standard by translating its guidelines into a web-based audit dashboard. The tool enables users to complete the three-step evaluation process and obtain fairness certification according to the national standard. This work is deeply rooted in the standardization lifecycle, both by addressing a critical post-standardization implementation gap of an existing national telecom standard and by proposing methodologies within the tool that could inform future pre-standardization efforts for AI risk grading and fairness assessment processes. To address potential gaps in user familiarity with the TEC standard, the tool includes a brief drop down explanation that outlines the key fairness requirements, ensuring that evaluators receive the necessary guidance before completing the audit process.\n\n\nContributions: Nishpaksh makes five key contributions towards operationalizing fairness assessment in India, aligning with national priorities for AI governance and standardization:\n\n\n1.\n\nIt delivers the country‚Äôs first open-source tool that complies with the TEC standard, automating its three-step fairness evaluation methodology and thereby bridging a critical gap in the practical implementation of national AI governance standards within the telecom domain and beyond.\n\n\n\n2.\n\nIt enhances transparency through structured logging, traceable metric selection and computation, and standardized report generation, directly supporting post-standardization evaluation and interoperability goals.\n\n\n\n3.\n\nIt introduces contextual sensitivity by incorporating India-specific demographic attributes and proxy bias detection mechanisms into the evaluation workflow.\n\n\n\n4.\n\nIt advances audit readiness by generating reports aligned with the TEC‚Äôs certification template, thereby enabling both self assessment and independent auditing of deployed AI models.\n\n\n\n5.\n\nIt proposes standardization-ready processes for AI fairness assessment, particularly through its structured risk classification questionnaire, which can inform the development of broader frameworks for AI application risk grading in future telecom and other industrial standards.\n\n\n\n\n\nCollectively, these contributions bridge the gap between research oriented fairness frameworks and standardized AI governance practices in India. In the sequel, we describe the design and methodology adopted towards the proposed fairness assessment tool, and provide empirical results validating its performance and applicability.\n\n\n\n\nII Related Works\n\n\nThe field of algorithmic fairness has advanced through a range of bias detection and mitigation frameworks, each addressing fairness from distinct architectural standpoints. IBM AI Fairness 360 (AIF360) is among the most comprehensive, offering over seventy bias metrics and nine mitigation algorithms across pre, in, and post-processing stages [1]. Its modular architecture, introducing standardized Metric and Explainer classes, set early benchmarks for fairness evaluation. However, AIF360‚Äôs reliance on the IBM Cloud and global orientation limit its adaptability to region-specific governance standards, reducing its suitability for localized compliance and audit requirements.\n\n\nMicrosoft‚Äôs Fairlearn emphasizes computational evaluation [2]. Its MetricFrame API enables disaggregated performance analysis to identify allocation harms, and its strong interoperability with Python‚Äôs ML ecosystem supports industrial deployment. Yet, it lacks alignment with formal certification or regulatory frameworks.\n\n\nGoogle‚Äôs What-If Tool (WIT) focuses on visualization-driven exploration and counterfactual analysis via TensorBoard and Jupyter integrations [3]. While accessible to non-technical users, its exploratory emphasis limits reproducibility and audit readiness.\n\n\nSeveral other toolkits address fairness from complementary perspectives: LiFT (LinkedIn Fairness Toolkit) enables large-scale fairness auditing in distributed machine learning systems [7], the Fairness R package provides statistical bias metrics for analytical workflows [8], and Aequitas offers visualization and reporting tools for intuitive analysis of group disparities [9].\n\n\nNovelty:¬†These frameworks share fundamental gaps: (1) absence of regulatory alignment with national certification standards, (2) limited lifecycle coverage beyond model level evaluation , (3) generic threshold determination without context specific calibration, and (4) lack of certification ready reporting mechanisms. While existing tools are effective for algorithmic evaluation, they fall short of connecting technical assessments with country-specific regulatory compliance requirement"
  },
  {
    "title": "Group-realizable multi-group learning by minimizing empirical risk",
    "url": "https://arxiv.org/abs/2601.16922v1",
    "source": "arxiv",
    "summary": "The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach ",
    "full_text": null
  },
  {
    "title": "LoL: Longer than Longer, Scaling Video Generation to Hour",
    "url": "https://arxiv.org/abs/2601.16914v1",
    "source": "arxiv",
    "summary": "Recent research in long-form video generation has shifted from bidirectional to autoregressive models, yet these methods commonly suffer from error accumulation and a loss of long-term coherence. While attention sink frames have been introduced to mitigate this performance decay, they often induce a critical failure mode we term sink-collapse: the generated content repeatedly reverts to the sink f",
    "full_text": null
  },
  {
    "title": "Preventing the Collapse of Peer Review Requires Verification-First AI",
    "url": "https://arxiv.org/abs/2601.16909v1",
    "source": "arxiv",
    "summary": "This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.16909v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.16909v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 23 Jan 2026]\n    Title:Preventing the Collapse of Peer Review Requires Verification-First AI\n    Authors:Lei You, Lele Cao, Iryna Gurevych            View a PDF of the paper titled Preventing the Collapse of Peer Review Requires Verification-First AI, by Lei You and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:This paper argues that AI-assisted peer review should be verification-first rather than review-mimicking. We propose truth-coupling, i.e. how tightly venue scores track latent scientific truth, as the right objective for review tools. We formalize two forces that drive a phase transition toward proxy-sovereign evaluation: verification pressure, when claims outpace verification capacity, and signal shrinkage, when real improvements become hard to separate from noise. In a minimal model that mixes occasional high-fidelity checks with frequent proxy judgment, we derive an explicit coupling law and an incentive-collapse condition under which rational effort shifts from truth-seeking to proxy optimization, even when current decisions still appear reliable. These results motivate actions for tool builders and program chairs: deploy AI as an adversarial auditor that generates auditable verification artifacts and expands effective verification bandwidth, rather than as a score predictor that amplifies claim inflation.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.16909 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.16909v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.16909\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Lei You PhD [view email]          [v1]\n        Fri, 23 Jan 2026 17:17:32 UTC (68 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Preventing the Collapse of Peer Review Requires Verification-First AI, by Lei You and 2 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscribe\n                \n              \n            \n          \n"
  },
  {
    "title": "Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces",
    "url": "https://arxiv.org/abs/2601.16907v1",
    "source": "arxiv",
    "summary": "While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, co",
    "full_text": null
  },
  {
    "title": "The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning",
    "url": "https://arxiv.org/abs/2601.16906v1",
    "source": "arxiv",
    "summary": "The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajector",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Background\n\n3.1 Reinforcement Learning\n3.2 Learning Reward Functions from Preferences\n3.3 Trajectory Alignment Coefficient (TAC)\n\n\n\n4 Can TAC help with manual reward tuning?\n\n\n4.1 User Study Design\n\n4.1.1 Test Domain\n4.1.2 Study Protocol\n4.1.3 Experimental Conditions and Research Questions\n4.1.4 Evaluation\n\n\n4.2 Results\n\n\n\n5 Can TAC be used to learn reward weights?\n\n5.1 Differentiable TAC\n\n5.2 Experiments and Results ‚Äì Lunar Lander\n\n5.2.1 Baselines\n5.2.2 Preference Data Collection\n5.2.3 Training and Evaluation\n5.2.4 Results\n\n\n\n5.3 Experiments and Results ‚Äì Gran Turismo 7\n\n5.3.1 Preference Data Collection\n5.3.2 Training and Evaluation\n5.3.3 Results: Controlling Fast Driving Styles\n5.3.4 Results: Controlling Aggressive and Timid Behavior\n\n\n\n\n6 Conclusion\n7 Ethical Statement\n\nA User Study Details\n\n\nA.1 Environment: Lunar Lander\n\nA.1.1 Reward Function Components\n\n\nA.2 User Study Interface\nA.3 Additional Results\n\n\n\nB Soft TAC Details\n\nB.1 Discussion on the Lack of Human Preferences used in Preference Based RL\nB.2 Proof for Proposition 1\nB.3 Proof for Theorem 1\nB.4 Proof for Theorem 2\nB.5 Understanding the Gradient Behavior of Soft-TAC versus Cross-Entropy\n\n\n\nC Environment Details\n\n\nC.1 Gran Turismo 7\n\nC.1.1 GT7 Cars used in Preference Dataset\nC.1.2 Reward Function Components\nC.1.3 Evaluation Metrics\n\n\nC.2 Preference Collection Details\n\n\n\nD Training and Evaluation Details\n\nD.1 Reward Learning\nD.2 Learned Reward Functions\nD.3 RL Training and Evaluation\nD.4 Policy Selection\n\n\n\nE Additional Results\n\nE.1 Generalization of Learned Reward Models\nE.2 Empirical Robustness of TAC\n\n\n\n\n\n\n\nThe Trajectory Alignment Coefficient in Two Acts: \nFrom Reward Tuning to Reward Learning\n\n\n\nCalarina Muslimani1,2\nWork done while interning at Sony AI. Corresponding author: musliman@ualberta.ca.\n‚ÄÉ‚ÄÉ\nYunshu Du1\n\n‚ÄÉ‚ÄÉ\nKenta Kawamoto1\n\n‚ÄÉ‚ÄÉ\nKaushik Subramanian1\n\n‚ÄÉ‚ÄÉ\nPeter Stone1,3\n\n‚ÄÉ‚ÄÉ\nPeter Wurman1\n1 Sony AI\n2 University of Alberta\n3 The University of Texas at Austin\n\n\n\nAbstract\nThe success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification.\nTo address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function.\nWe leverage the Trajectory Alignment Coefficient (TAC),\na metric that evaluates how closely a reward function‚Äôs induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC.\nHowever, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data.\nValidated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss.\nThis work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.\n\n\n\n1 Introduction\n\nThe reward function lies at the foundation of reinforcement learning (RL), as agents aim to maximize the expected cumulative reward (Sutton and Barto, 2018). In practice, however, solving a task with RL assumes access to a reward function that accurately captures the task objective. Without such a reward function, learning can fail entirely, leading agents to pursue undesired or unsafe behaviors (Amodei et al., 2016; Skalse et al., 2022).\nThis issue can be particularly pronounced in real-world domains such as autonomous driving (Knox et al., 2023), where reward functions must capture subjective, preference-specific objectives (e.g., driving style or adherence to etiquette).\nMost practitioners rely on a trial-and-error approach, iteratively adjusting reward functions based on the behavior of RL agents trained with those rewards. This process is costly, time-consuming, and often results in reward misspecification (Booth et al., 2023). Despite its importance, best practices for reward design have received relatively little attention, particularly regarding tools that help practitioners iteratively refine reward specifications.\n\n\nThe first goal of this work is to evaluate the utility of the Trajectory Alignment Coefficient (TAC) (Muslimani et al., 2025), a reward alignment metric, for supporting RL practitioners during manual reward design, specifically when tuning a reward function from scratch.\nWhile other reward evaluation metrics exist, we focus on TAC because alternative approaches either require access to a ground-truth reward (Gleave et al., 2021; Wulfe et al., 2022; Skalse et al., 2024), which we cannot assume, or fail to provide a continuous alignment signal (Brown et al., 2021; Knox and MacGlashan, 2024), which is needed for iterative reward refinement.\n\n\nWe investigate whether TAC can guide RL practitioners in specifying reward functions. To do so, we conducted an ethics-approved human-subject study in Lunar Lander, a continuous state and action domain with eight tunable reward weights (Brockman et al., 2016), making reward design difficult.\nPractitioners were provided with a set of reward features and tasked with determining appropriate weights under one of two conditions: with TAC feedback or without. Participants in the TAC condition received TAC scores for each candidate reward function as they iteratively adjusted the weights.\nThis guidance led to statistically significant improvements: the final reward functions produced policies with higher task success rates, and perceived cognitive workload decreased by ‚àº\\sim30% relative to reward tuning without TAC.\n\n\nWhile these results demonstrate that TAC can support effective reward design, the process still required substantial effort; with participants iterating over their designs ‚àº\\sim40 times.\nThis observation motivates the second goal of our work: to automatically learn a reward model that maximizes TAC. We introduce a differentiable approximation of TAC, Soft-TAC, which can be used as a loss function to train reward models directly from human preferences.\n\n\nWe evaluate Soft-TAC across two domains: Lunar Lander, to confirm its ability to learn effective reward functions where human practitioners struggled; and the high-fidelity driving simulator Gran Turismo 7 (GT7) (Wurman et al., 2022), to assess its effectiveness in producing qualitatively distinct behaviors given diverse human preferences. Our results show that reward models trained with Soft-TAC successfully captured human preferences in both domains. In Lunar Lander, these reward models enabled RL agents to achieve higher task success rates than both the standard Cross-Entropy loss (Christiano et al., 2017) and manually tuned rewards. In GT7, Soft-TAC trained reward models better captured preferences reflecting different driving objectives, leading to qualitatively more distinct racing behaviors than models trained using Cross-Entropy.\nTogether, our work highlights that TAC can serve as both a tool to support\nreward design and as an objective for learning human-aligned reward models.\n\n\n\n\n2 Related Work\n\nPreference-based RL (PbRL) learns reward functions from human preferences between trajectories. The standard formulation, introduced by Christiano et al. (2017), assumes that humans tend to prefer trajectories with higher total reward. This assumption supports a probabilistic preference model (e.g., Bradley and Terry (1952)), enabling reward learning via the Cross-Entropy loss on the negative log-likelihood of the observed preferences.\nA majority of PbRL methods closely follow this framework, adding only techniques to reduce the number of human queries or improve policy learning (Ibarz et al., 2018; Lee et al., 2021; Liang et al., 2022; Park et al., 2022; Hwang et al., 2023; Shin et al., 2023; Hejna III and Sadigh, 2023; Cheng et al., 2024; Hu et al., 2024; Verma and Metcalf, 2024; Choi et al., 2024; Muslimani and Taylor, 2025; Kang and Oh, 2025; Pace et al., 2025). Some approaches vary from this setup by modifying the preference model or loss function. However, these methods often come with limitations, such as: (1) requiring online, policy-coupled training (Bƒ±yƒ±k and Sadigh, 2018; Liu et al., 2022; Xie et al., 2025); (2) relying on fixed sets of policies for value estimation (Knox et al., 2024); (3) removing reward learning altogether (Hejna et al., 2024); or (4) treating preference annotations as independent ‚Äúwin‚Äù or ‚Äúloss‚Äù labels, which ignore that a trajectory‚Äôs label depends on what it is being compared to and can result in conflicting labels (Sun et al., 2025).\n\n\n\n\n3 Background\n\nThis section first provides background on RL, then describes the reward alignment metric, the Trajectory Alignment Coefficient. It concludes with a discussion of reward learning.\n\n\n\n3.1 Reinforcement Learning\n\nWe consider a Markov decision process defined by the tuple\n(ùíÆ,ùíú,r,p,Œº,Œ≥)(\\mathcal{S},\\mathcal{A},r,p,\\mu,\\gamma).\nHere, ùíÆ\\mathcal{S} and ùíú\\mathcal{A} denote the state and action spaces,\nr:ùíÆ√óùíú√óùíÆ‚Üí‚Ñùr:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow\\mathbb{R}\nspecifies the reward per transition and\np:ùíÆ√óùíú√óùíÆ‚Üí[0,1]p:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\rightarrow[0,1]\ndefines the transition dynamics. The initial-state distribution is given by Œº\\mu,\nand the discount factor Œ≥‚àà[0,1)\\gamma\\i"
  },
  {
    "title": "GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints",
    "url": "https://arxiv.org/abs/2601.16905v1",
    "source": "arxiv",
    "summary": "Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility an",
    "full_text": null
  },
  {
    "title": "Embedding -based Crop Type Classification in the Groundnut Basin of Senegal",
    "url": "https://arxiv.org/abs/2601.16900v1",
    "source": "arxiv",
    "summary": "Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) t",
    "full_text": "\n\n\n\n1 Introduction\n2 Recent work\n\n3 Methods\n\n3.1 Criteria\n3.2 Labeled data preparation\n3.3 Remote sensing data\n3.4 Model training and evaluation\n\n\n\n4 Results\n\n4.1 Ensemble land cover classification\n4.2 Wall-to-wall land cover maps\n4.3 Temporal transfer learning for land cover classification\n4.4 CPU use\n4.5 Crop land change across years\n4.6 Ensemble crop cover classification\n4.7 Wall-to-wall crop maps\n4.8 Temporal transfer learning for crop type classification\n4.9 Embedding clustering\n4.10 Summary of evaluation\n\n\n5 Discussion\n6 Acknowledgments\n\n\n\n\n\nEmbedding -based Crop Type Classification in the Groundnut Basin of Senegal\n\n\nMadeline C. Lisaius, Srinivasan Keshav, Andrew Blake\n\n\nClement Atzberger\n\n\n\nAbstract\nCrop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.\n\n\nkeywords: \nremote sensing , geospatial foundation model , crop type classification , agriculture , Senegal\n\n\n\n\\affiliation\norganization=The University of Cambridge Department of Computer Science and Technology,\n\n\n\\affiliation\norganization=dClimate Labs,\n\n\n\n1 Introduction\n\nReliable, wall-to-wall mapping of smallholder crops landscapes is an important tool for food security, climate adaptation, and sustainable land management globally. Approximately three-quarters of the world‚Äôs poor live in rural areas and rely primarily on small-scale agriculture [1]. These farms are critical not only for rural livelihoods but also for local and regional food security; globally, farms under 2 hectares (ha) contribute an estimated 30‚Äì34% of food supply, devote a greater proportion of their production to food, and maintain greater crop diversity than larger farms [2]. Knowing what crops are grown where allows for informed decision making at regional, national and global scales that can mean survival for vulnerable people.\n\n\nIn west Africa, much of the agricultural landscape is dominated by smallholder systems, where small plots support the livelihoods of most rural households, yet these agricultural systems remain understudied relative to large farms in the Global North, making crop mapping more difficult [3].\n\n\nSatellite remote sensing provides an opportunity to bridge research and mapping gaps by enabling consistent, large-scale, and low-cost monitoring of agricultural landscapes. Freely available sensor data such as from Sentinel-1 and Sentinel-2 offer regularly repeating observations that can be leveraged for crop type mapping [4]. However, most existing methods have been developed in contexts of industrial agriculture, where fields are large, monocropped, and well-documented, making them poorly suited for smallholder regions [5]. In West Africa, the challenges of frequent cloud cover, limited ground samples, small and irregular field sizes, and intercropping further complicate the creation of accurate, wall-to-wall crop type maps [6, 7].\n\n\nRecent advances in geospatial foundation models (FMs) offer a promising new approach. Embeddings derived from large-scale pre-trained models have shown strong performance on agricultural classification tasks, often matching or surpassing traditional machine learning approaches in accuracy and F1 score [8, 9, 10]. Yet, these methods have not been systematically evaluated under true smallholder conditions such as diverse management and intercropping which is common in West Africa.\n\n\nTo address this gap, we evaluate the potential of embedding-based approaches for wall-to-wall crop cover classification in the groundnut basin of Senegal, a region representative of many smallholder agricultural systems in west Africa. We compare multiple crop type classification approaches across several years and assess their performance and practical utility for real-world agricultural mapping applications.\n\n\n\n\n2 Recent work\n\nSome recent literature exists for wall-to-wall crop type mapping of agriculture in West Africa:\n\n\nIbrahim et al. [6] employ Sentinel-1, Sentinel-2 and SkySat imagery for land and crop cover mapping in a smallholder, intercropped region of Nigeria where average field size ranges between 0.3 ha and 0.5 ha. They first create a wall-to-wall classification map of land cover using spectral temporal metrics (STMs) from both Sentinel-1 and Sentinel-2, informed by cropping season. They then use the the land cover map to mask for agriculture regions and classify their five agricultural crops of interest using Random Forest on the STMs. They use high resolution images from SkySat to verify. This work requires extensive crop-specific feature engineering to prepare data for use by RF, therefore requiring customization to adapt to new regions.\n\n\nAzzari et al. [11] use harmonic regressions to classify maize from non-maize in Malawi and Ethiopia using Sentinel-1 and Sentinel-2 images. Field sizes are not reported but study areas are noted as smallholder regions. Pixel-wise harmonic regressions along with rasterized weather data and topographic data are used as inputs to Random Forest classifier to distinguish maize and non-maize. This method relies on a high quality cropland mask to be applied beforehand, and relies on generation of the harmonic regression features for each year and region, and addition of other data such as topography and climate data. Other researchers using a similar harmonics approach also use it for few-class classification, which is unsuitable for mapping large, diverse crop regions and minority crops (Luan Pott 2022).\n\n\nGumma et al. [12] classify nine classes of crops in Senegal for 2020 using Sentinel-2 data, across all field sizes. They compare Random Forest, CART, SVM, and spectral matching technique (SMT). They prepare NDVI for the time frame of interest, group the NDVI time series with k-means clustering, and use labeled data to create ‚Äúideal spectra‚Äù that are leveraged to classify the clusters by similarity to the ideal. They begin by classifying 10 land cover classes and 9 crop cover classes for all of Senegal. They find the spectral matching technique has the highest accuracy of the methods evaluated but requires extensive feature preparation.\n\n\nRustowicz et al. [13] approach crop type classification and segmentation in Ghana, South Sudan and Germany. They compare a 3D U-Net and the 2D convolutional neural network (CNN) with convolutional long short term memory model (CLSTM) that incorporates both CNNs and recurrent neural networks (RNNs) for semantic segmentation of multi-temporal, multi-spatial satellite images from Sentinel-2. They find that their proposed approach has the highest accuracy of tested methods in Germany. However for the smallholder regions they find that Random Forest often matches the accuracy of their approach. While this method is focused primarily on segmentation, it does consider the smallholder context. It is challenged by high cloud coverage as well as limited and imbalanced labels.\n\n\n\n\n3 Methods\n\n\n3.1 Criteria\n\nTo identify the best method for crop type mapping in a smallholder region like the groundnut basin of Senegal, we identify four qualities of a suitable wall-to-wall mapping approach:\n\n\n1.\n\nPerformance: A good approach must offer high accuracy and F1 scores in crop type classification. As high quality datasets are difficult to acquire and maintain in smallholder regions, we acknowledge that these metrics may be lower than those in regions with high data quality and availability.\n\n\n\n2.\n\nPlausibility: A good approach will show mapping results that are plausible for the region of study. Maps across years should show consistency, and additionally, changes to the extent of cropland should match trends in cropland statistics.\n\n\n\n3.\n\nTransferability: As many organizations concerned with smallholder agriculture in the Global South do not have the capacity or resources to collect ground truth labels every year, it‚Äôs important that a modeling approach trained on one year can be transferred to other years. The best approach will maintain accuracy when transferring between years, taking into account that limited labels and varying label quality will impact accuracy.\n\n\n\n4.\n\nAccessibility: Given that projects supporting smallholder and subsistence agriculture often have fewer resources than industrial agriculture, approaches that eliminate the need for hand-engineered features and have reduced computational resource requirements are preferred to make mapping more accessible.\n\n\n\n\n\n\n\n3.2 Labeled data preparation\n\nData are obtained from the Joint Experiment for Crop Assessment and Monitoring (JECAM) initiative for a region of Senegal spanning the Fatick and Niakhar departments [14]. Fatick and Niakhar lie in the southeast of Senegal‚Äôs groundnut basin and primarily produce groundnuts and millet in biennial rotation as well as livestock [14]. The region is challenged by water scarcity, exacerbated by climate change, food insecurity in over 80% of the population, and the combination of population increase and natural resource degradation [15].\n\n\nFigure 1: Visualized are a) the location of the region of interest within Senegal and b) Sentinel-2 satellite imagery of the region of interest. This region is domin"
  },
  {
    "title": "FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization",
    "url": "https://arxiv.org/abs/2601.16897v1",
    "source": "arxiv",
    "summary": "We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. ",
    "full_text": null
  },
  {
    "title": "Evaluating Large Vision-language Models for Surgical Tool Detection",
    "url": "https://arxiv.org/abs/2601.16895v1",
    "source": "arxiv",
    "summary": "Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the int",
    "full_text": "\n\n\n\n1 Introduction\n\n2 METHODOLOGY\n\n2.1 Problem Definition\n2.2 Dataset\n2.3 Experimental Setup\n2.4 Evaluation Metrics\n\n\n3 RESULTS\n4 DISCUSSION AND CONCLUSIONS\n\n\n\n\n\nEvaluating Large Vision‚Äìlanguage Models for Surgical Tool Detection\n\n\n\nNakul Poudel \nCenter for Imaging Science\nRochester Institute of Technology\nRochester, NY 14623 \nnp1140@rit.edu\n\n\n‚ÄÉ‚ÄÉ\nRichard Simon \nBiomedical Engineering\nRochester Institute of Technology\nRochester, NY 14623 \nrasbme@rit.edu\n\n\n\n‚ÄÉ‚ÄÉ\nCristian A. Linte \nCenter for Imaging Science and Biomedical Engineering\nRochester Institute of Technology\nRochester, NY 14623 \ncalbme@rit.edu\n\n\n\n\nAbstract\nSurgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision‚Äìlanguage models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.\nKeywords: Image-guided surgery, surgical robotics, human-robot collaboration, holistic scene understanding, vision-language model, future directions in surgical AI\n\n\n\n1 Introduction\n\nSurgical environments are inherently complex, high-risk, and cognitively demanding. As procedures increasingly shift toward minimally invasive and robot-assisted techniques, the integration of Artificial Intelligence (AI) has emerged as a transformative force. Surgical AI systems provide critical intraoperative guidance and decision support, enhancing both surgical precision and patient safety¬†[5]. However, most current surgical AI systems are unimodal and task-specific, with limited modeling of interactions among surgical tasks. Developing such capabilities is important for achieving comprehensive scene understanding, augmenting entire surgical workflows, and enabling future autonomous robotic assistants¬†[8, 12].\n\n\nRecent advances in vision-language models (VLMs) take multimodal input, i.e., images and rich textual descriptions, thereby enabling a more comprehensive understanding of the input data and modeling interactions among tasks. In the surgery domain, such models can encode information from surgical images and textual annotations, including the presence and type of instruments, anatomical structures, surgical actions, and their spatial relationships. This approach can enable a single model to perform multiple tasks, including instrument and tissue detection, surgical phase recognition, action recognition, step recognition, and critical view of safety. Moreover, the interdependencies among these tasks enable mutual reinforcement, leading to a more holistic understanding of surgical scenes.\n\n\nDespite their promise, the effectiveness of VLMs in surgical environments depends on their visual perception capability. Errors in early visual perception can propagate to higher-level reasoning modules, potentially leading to unsafe or misleading predictions. Among all surgical tasks, surgical tool detection and localization can be considered the fundamental component, as they directly support downstream tasks such as action recognition, phase recognition, and skill assessment. However, surgical scenes present substantial visual challenges, including occlusion, blood, smoke, motion blur, illumination variation, and poor contrast, which significantly degrade visual perception performance.\n\n\nThe application of VLMs in the surgical domain remains in its early stages. Several studies¬†[12, 15] have developed surgery-specific VLMs by constructing large-scale surgical knowledge datasets, demonstrating encouraging results in multimodal reasoning. Other works¬†[13, 6] have explored the use of general-purpose VLMs for surgical instrument classification, showing promising potential. However, to the best of our knowledge, there is no systematic study that evaluates how general-purpose VLMs perform on the fundamental visual perception task of surgical instrument detection, particularly under both zero-shot and fine-tuned settings.\n\n\nIn this study, we present the comprehensive evaluation of three state-of-the-art large general-purpose vision-language models for surgical instrument detection. We investigate two practical deployment scenarios: (1) a zero-shot setting, in which off-the-shelf VLMs are directly prompted to classify and localize surgical instruments without any task-specific training, and (2) a fine-tuned setting, in which models are adapted to the surgical tool detection task using parameter-efficient LoRA fine-tuning. Through extensive experiments, we analyze the strengths and limitations of general-purpose VLMs for surgical visual perception and examine their potential applicability to future surgical AI systems.\n\n\n\n\n2 METHODOLOGY\n\n\n2.1 Problem Definition\n\nLet I‚àà‚ÑùH√óW√ó3I\\in\\mathbb{R}^{H\\times W\\times 3} denote an input surgical image. The goal is to predict a set of surgical instrument instances\nùí™={(ci,bi)}i=1N\\mathcal{O}=\\{(c_{i},b_{i})\\}_{i=1}^{N}, where cic_{i} represents the instrument category, bib_{i} denotes the corresponding bounding box, and NN is the total number of instruments present in the image. We evaluate this task using three large VLMs, specifically Qwen2.5-7B¬†[2], LLaVA1.5-7B¬†[9], and InternVL3.5-8B¬†[14] across both zero-shot and fine-tuned settings.\n\n\n\n\n2.2 Dataset\n\nWe used frames from the GraSP dataset¬†[1], extracted at 11 fps from 1313 Robot-Assisted Radical Prostatectomy videos. Following the original dataset splits, we used frames extracted from five videos for testing, and the remaining videos for fine-tuning the models. The frames are annotated at 3535 frames interval, yielding 11251125 frames for testing and 23242324 frames for fine-tuning. There are a total of 77 instrument categories, with each frame containing as few as 11 and as many as 55 instances. The distribution of instrument instances per instrument category is summarized in Table¬†1.\n\n\nTable 1: Class-wise distribution of instrument instances in the training and test sets.\n\n\n\nInstrument Category\nTrain\nTest\nTotal\n\n\nBipolar Forcep (BF)\n1,694\n809\n2,503\n\n\nPrograsp Forcep (PF)\n741\n330\n1,071\n\n\nLarge Needle Driver (LND)\n896\n449\n1,345\n\n\nMonopolar Curved Scissor (MCS)\n1,765\n844\n2,609\n\n\nSuction Instrument (SI)\n816\n310\n1,126\n\n\nClip Applier (CA)\n64\n38\n102\n\n\nLaparoscopic Grasper (LG)\n194\n81\n275\n\n\nAll Instruments\n6,170\n2,861\n9,031\n\n\n\n\n\n\n\n2.3 Experimental Setup\n\nWe experimented with three VLMs for the surgical tool detection task on the GraSP dataset. We imposed two settings‚Äîzero-shot and fine-tuned. For zero-shot inference, each model was prompted as-is, using its original pretrained weights to predict the instrument category and its corresponding bounding box on the test set. For the fine-tuned setting, we first adapted each model using supervised finetuning\nwith Rank-8 LoRA adaptation¬†[7] for 55 epochs, a batch size of 44, a gradient accumulation step of 44, and a learning rate of 1√óe‚àí41\\times e-4 on the training set, and evaluated on the test set. The prompts used to query the model for fine-tuning and zero-shot inference are presented in Table¬†2. The prompt for zero-shot inference includes a list of potential instrument categories to guide the model. However, after fine-tuning, the model has already learned the instrument categories directly from the training data, and therefore, no such guidance is required. All the VLM experiments were performed using the Swift framework 444https://github.com/modelscope/ms-swift on an NVIDIA A100 (40GB) GPU¬†[11]. As a baseline, Grounding DINO¬†[10] was fine-tuned for 15 epochs with a batch size of 4 and a learning rate of 1√óe‚àí41\\times e-4 using the Open-GroundingDino framework555https://github.com/longzw1997/Open-GroundingDino. During inference, a bounding box confidence threshold of 0.28 was applied to filter predicted bounding boxes.\n\n\nTable 2: Prompts used for zero-shot and fine-tuned VLM inference.\n\n\n\nSetting\n\n\nPrompt\n\n\n\n\n\n\nZero-shot\n\n\n&lt;image&gt;From the following list of surgical instruments: [Bipolar Forcep, Prograsp Forcep, Large Needle Driver, Monopolar Curved Scissor, Suction Instrument, Clip Applier, Laparoscopic Grasper], identify which instruments are present in the given image. Also, give bounding box coordinates for them. Return your answer as a JSON object.\n\n\n\n\nFine-tuned\n\n\n&lt;image&gt;Detect surgical instruments in the image.\n\n\n\n\n\n\n\nFigure 1: Illustration of error types. Green boxes indicate ground truth, and orange boxes indicate predictions corresponding to a specific error category. Shorthand instrument names are shown, along with the Intersection over Union (IoU) between the prediction and the ground truth box.\n\n\n\n\n2.4 Evaluation Metrics\n\nThe standard evaluation metric for object detection is mean Average Precision (mAP), which requires a confidence score for each predicted bounding box. In our VLM-based detection setting, the models ge"
  },
  {
    "title": "LLM-Based Adversarial Persuasion Attacks on Fact-Checking Systems",
    "url": "https://arxiv.org/abs/2601.16890v1",
    "source": "arxiv",
    "summary": "Automated fact-checking (AFC) systems are susceptible to adversarial attacks, enabling false claims to evade detection. Existing adversarial frameworks typically rely on injecting noise or altering semantics, yet no existing framework exploits the adversarial potential of persuasion techniques, which are widely used in disinformation campaigns to manipulate audiences. In this paper, we introduce a",
    "full_text": null
  },
  {
    "title": "MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion",
    "url": "https://arxiv.org/abs/2601.16886v1",
    "source": "arxiv",
    "summary": "Knowledge Tracing (KT) aims to model a student's learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferre",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n\n2.1 Heterogeneous Graph Construction\n\n2.1.1 Multi-relational KC Graph.\n2.1.2 S‚ÄìQ Interaction Graph.\n\n\n\n2.2 Multi-view Fusion Prediction\n\n2.2.1 Subgraph Retrieval\n2.2.2 Asymmetric Cross-Attention Fusion\n2.2.3 Prediction and Loss Function\n\n\n\n\n\n3 Experiments and results\n\n3.1 Datasets\n3.2 Baselines\n3.3 Implementation Details\n3.4 Main Results\n\n3.5 Ablation Results\n\n3.5.1 Model Components.\n3.5.2 Prerequisite‚ÄìSuccessor Extraction (Junyi).\n\n\n\n\n4 Conclusion And Future Work\n5 Acknowledgments\n\n\n\n\n\nMAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing \nwith Subgraph Retrieval and Asymmetric Fusion\n\nAbstract\nKnowledge Tracing (KT) aims to model a student‚Äôs learning trajectory and predict performance on the next question.\nA key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs).\nRecently, graph-based KT paradigms have shown promise for this problem.\nHowever, existing methods have not sufficiently explored inter-concept relations, often inferred solely from interaction sequences.\nIn addition, the scale and heterogeneity of KT graphs make full-graph encoding both computationally both costly and noise-prone, causing attention to bleed into student-irrelevant regions and degrading the fidelity of inter-KC relations.\nTo address these issues, we propose a novel framework: Multi-Agent Graph-Enhanced Knowledge Tracing (MAGE-KT).\nIt constructs a multi-view heterogeneous graph by combining a multi-agent KC relation extractor and a student‚Äìquestion interaction graph, capturing complementary semantic and behavioral signals.\nConditioned on the target student‚Äôs history, it retrieves compact, high-value subgraphs and integrates them using an Asymmetric Cross-attention Fusion Module to enhance prediction while avoiding attention diffusion and irrelevant computation.\nExperiments on three widely used KT datasets show substantial improvements in KC-relation accuracy and clear gains in next-question prediction over existing methods.\n\n\nIndex Terms‚Äî‚Äâ\nKnowledge Tracing, Multi-Agent, Multi-View\n\n\n\n1 Introduction\n\nKnowledge tracing (KT) is a core task in educational data mining that infers a student‚Äôs evolving knowledge state from past responses to forecast future performance [29].\nAs a foundation of personalized learning, KT has been extensively explored, with substantial advances in modeling answer sequences [22].\n\n\nAs Deep Learning (DL) has advanced, KT methods such as DKT [20], DKT+ [28], and DKVMN [31] have sparked a new wave of research, yet they lack explicit modeling of knowledge structure.\nTo address this gap, graph-based approaches have been introduced to better capture relations among students, questions, and knowledge concepts (KCs).\nMore recently, researchers have focused on relations within the KC space itself, examining interdependencies among concepts to more faithfully model real learning processes, as such relations have been shown to significantly boost KT performance.\nExisting techniques for extracting KC‚ÄìKC relations fall into two categories:\n(i) inferring predecessor‚Äìsuccessor and associative links from the temporal order in which KCs appear in students‚Äô interaction histories [25, 17], and\n(ii) prompting Large Language Models (LLMs) to identify associations between concepts [26, 30].\nSKT [25] determines predecessor‚Äìsuccessor and associative relations between KCs by analyzing the temporal order of interaction sequences.\nNGFKT [13] calibrates the skill matrix and the Q-matrix via importance ranking of knowledge relations, accurately identifying KC relationships.\nSINKT [26] leverages LLMs‚Äô built-in knowledge and in-context learning to assist in constructing a KC relation graph that includes prerequisite concepts.\n\n\nDespite progress in recent years, current KC-relation modeling remains incomplete and brittle.\nFirst, methods that infer relationships from interaction sequences rely on shallow cues, yielding sparse structures that overlook relations such as equivalence and containment and introducing erroneous relationships that conflate associative links with prerequisite‚Äìsuccessor relations;\nprompt-based LLM approaches also suffer from generative uncertainty, leading to incorrect KC links.\nSuch errors propagate through message passing and distort estimates of students‚Äô knowledge states.\nSecond, many graph-based models encode the entire knowledge graph for each instance [14, 7], causing attention to diffuse into student-irrelevant regions, which amplifies noise and inflates computational cost.\nConsequently, graph-based KT methods take a double hit, noisy relations and inefficient computation, that undermines the reliability of the resulting knowledge graph.\nThis motivates approaches that improve relational accuracy while concentrating computation on higher-value information.\n\n\nFig. 1: The MAGE-KT framework.\n\n\nTo address these challenges, we propose MAGE-KT, a framework that couples relation quality with efficient computation through three mutually reinforcing designs.\nFirst, we construct a heterogeneous graph construction pipeline that integrates a multi-agent KC relation extraction module and a student‚Äìquestion interaction graph.\nThe KC graph is enriched through a collaborative multi-agent pipeline, where specialized agents generate, score, and adjudicate five types of inter-KC relations, ensuring both semantic validity and structural consistency.\nMeanwhile, the S-Q interaction graph incorporates IRT-derived abilities and difficulties to model personalized student‚Äìquestion dynamics. The two graphs capture complementary dimensions of knowledge and learning behavior.\nSecond, a student-conditioned subgraph retriever leverages the target student‚Äôs history to jointly select high-value subgraphs from both views, focusing computation where it matters and avoiding irrelevant attention diffusion.\nFinally, we introduce an Asymmetric Cross-attention Fusion Module to fully capture the roles and interdependencies among students, questions, and knowledge concepts during prediction.\nThe main contributions of this paper are:\n\n\n‚Ä¢\n\nWe propose MEGA-KT, a method that integrates a multi-agent‚Äìenhanced, multi-relational KC graph into multi-view KT for more accurate and efficient knowledge tracing.\n\n\n\n‚Ä¢\n\nWe design a unified architecture that combines multi-agent KC graph construction, student‚Äìquestion interaction modeling, and asymmetric multi-view fusion, enabling accurate relation representation and efficient subgraph-based reasoning.\n\n\n\n‚Ä¢\n\nWe conduct comprehensive experiments on three widely used KT datasets, demonstrating that MEGA-KT achieves superior predictive accuracy, and that the multi-agent setting significantly improves KC-relation extraction accuracy.\n\n\n\n\n\n\n\n2 Methodology\n\n\n2.1 Heterogeneous Graph Construction\n\nThis stage is as shown in the upper part of Fig.¬†1.\nThe goal of this stage is to construct two complementary graphs, a Multi-relational KC Graph ùí¢l‚Äãa‚Äãx‚ÄãK\\mathcal{G}_{laxK} and a Student‚ÄìQuestion (S‚ÄìQ) Interaction Graph ùí¢S‚ÄãQ\\mathcal{G}_{SQ}, that supply inter-KC relations and calibrated ability/difficulty signals for subsequent student-conditioned retrieval and fusion.\n\n\n\n2.1.1 Multi-relational KC Graph.\n\nLet ùí¶={k1,‚Ä¶,k|ùí¶|}\\mathcal{K}=\\{k_{1},\\ldots,k_{|\\mathcal{K}|}\\} be the set of KCs.\nWe construct Multi-relational KC Graph ùí¢K=(ùí±K,‚Ñ∞K,œÑ)\\mathcal{G}_{K}=(\\mathcal{V}_{K},\\mathcal{E}_{K},\\tau), where ùí±K=ùí¶\\mathcal{V}_{K}=\\mathcal{K}, ‚Ñ∞K‚äÜùí±K√óùí±K\\mathcal{E}_{K}\\subseteq\\mathcal{V}_{K}\\times\\mathcal{V}_{K} denotes the set of KC‚ÄìKC edges, and œÑ\\tau is the edge type.\nWe consider five typed relations:\n(i) Association [9]: semantically related KCs that often co-occur without a strict teaching order;\n(ii) Containment [16]: a part‚Äìwhole or subconcept relation where BB is a component of AA;\n(iii) Equivalence [11]: two surface forms for the same concept;\n(iv) Sibling [24]: parallel subtopics under a common parent with no direct dependency;\n(v) Predecessor‚ÄìSuccessor [18]: a prerequisite relation where AA must be mastered before BB.\n\n\nWe employ a role-structured, three-agent pipeline to automatically generate and adjudicate five types of KC‚ÄìKC relations. The collaborating agents include a Semantic Agent, a Scoring Agent, and an Arbitration Agent. The pipeline comprises five steps:\n\n\nStep 1: Knowledge Concept Completion.\nThe Semantic Agent first standardizes KC names and performs semantic expansion, turning short labels into normalized descriptions that include definitions. The agent is given a prompt such as:\n\n\nPlease provide a standard ‚Äôdefinition‚Äô for the following knowledge concepts and determine their respective ‚Äôcategory.\n\n\nStep 2: Preliminary Relationship Judgment.\nUsing the detailed KC descriptions together with evidence from the interaction history, the Semantic Agent selects the most plausible relation type from the five candidates for each KC pair, and outputs a structured record with evidence excerpts, source pointers, and key justifications.\nWe prompt the agent as follows:\n\n\nPlease analyze the structural relationship between these two math knowledge concepts based on the following content: definition, classification, precedence probability, and co-occurrence frequency, and judge their relationship type.\n\n\nStep 3: Relationship Score.\nIngesting the Semantic Agent‚Äôs structured output and interaction evidence, the Scoring Agent evaluates each candidate with type-specific criteria on a 0‚Äì5 scale.\nThe instruction provided to the agent can be exemplified as follows:\n\n\nPlease score the ‚ÄùPredecessor-Successor‚Äù relationship between the two knowledge concepts based on the following criteria: Predecessor Dependency, Correctness Dependency, Answer Order Sequence. Please score each criterion (0‚Äì5) and provide a brief explanation for your scoring.\n\n\nStep 4: Relationship Review.\nThe Arbitration Agent cross-checks semantic candidates, evidence, score vectors, and consistency tags, and validates them against type-level axioms and graph-topology constraints.\nTo guid"
  },
  {
    "title": "Multigrade Neural Network Approximation",
    "url": "https://arxiv.org/abs/2601.16884v1",
    "source": "arxiv",
    "summary": "We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-",
    "full_text": null
  },
  {
    "title": "Explaining Group Recommendations via Counterfactuals",
    "url": "https://arxiv.org/abs/2601.16882v1",
    "source": "arxiv",
    "summary": "Group recommender systems help users make collective choices but often lack transparency, leaving group members uncertain about why items are suggested. Existing explanation methods focus on individuals, offering limited support for groups where multiple preferences interact. In this paper, we propose a framework for group counterfactual explanations, which reveal how removing specific past intera",
    "full_text": null
  },
  {
    "title": "Theory of Minimal Weight Perturbations in Deep Networks and its Applications for Low-Rank Activated Backdoor Attacks",
    "url": "https://arxiv.org/abs/2601.16880v1",
    "source": "arxiv",
    "summary": "The minimal norm weight perturbations of DNNs required to achieve a specified change in output are derived and the factors determining its size are discussed. These single-layer exact formulae are contrasted with more generic multi-layer Lipschitz constant based robustness guarantees; both are observed to be of the same order which indicates similar efficacy in their guarantees. These results are ",
    "full_text": null
  },
  {
    "title": "No Validation, No Problem: Predicting Model Performance from a Single Gradient",
    "url": "https://arxiv.org/abs/2601.16874v1",
    "source": "arxiv",
    "summary": "We propose a validation-free checkpointing signal from a single forward-backward pass: the Frobenius norm of the classifier-head gradient on one detached-feature batch, ||g||_F = ||dL/dW||_F. Across ImageNet-1k CNNs and Transformers, this proxy is strongly negative with Top-1 and positive with loss. Selecting the checkpoint with the minimum head gradient in a short tail window closes most of the g",
    "full_text": null
  },
  {
    "title": "Provably Learning Attention with Queries",
    "url": "https://arxiv.org/abs/2601.16873v1",
    "source": "arxiv",
    "summary": "We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to le",
    "full_text": null
  }
]