[
  {
    "title": "Internet Archive's Storage",
    "url": "https://blog.dshr.org/2026/01/internet-archives-storage.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; Li correctly points out that the Archive&#x27;s budget, in the range of $25-30M&#x2F;year, is vastly lower than any comparable website:\nBy owning its hardware, using the PetaBox high-density architecture, avoiding air conditioning costs, and using open-source software, the Archive achieves a storage cost efficiency that is orders of magnitude better than commercial cloud rates.<p>That’s impressive. Wikipedia spends $185m per year and the Seattle public library spends $102m. Maybe not comparable exactly, but $30m per year seems inexpensive for the memory of the world…",
      "&gt; <i>This &quot;waste heat&quot; system is a closed loop of efficiency. The 60+ kilowatts of heat energy produced by a storage cluster is not a byproduct to be eliminated but a resource to be harvested.</i><p>Are there any other data centers harvesting waste heat for benefit?",
      "This is very cool. One thing I am curious about is the software side of things and the details of the hardware. What is the filesystem and RAID (or lack of) layer to deal with this optimally? Looking into it a little:<p>* power budget dominates everything: I have access to a lot of rack hardware from old connections, but I don&#x27;t want to put the army of old stuff in my cabinet because it will blow my power budget for not that much performance in comparison to my 9755. What disks does the IA use? Any specific variety or like Backblaze a large variety?<p>* magnetic is bloody slow: I&#x27;m not the Internet Archive so I&#x27;m just going to have a couple of machines with a few hundred TiB. I&#x27;m planning on making them all a big zfs so I can deduplicate but it seems like if I get a single disk failure I&#x27;m doomed to a massive rebuild<p>I&#x27;m sure I can work it out with a modern LLM, but maybe someone here has experience with actually running massive storage and the use-case where tomorrow&#x27;s data is almost the same as today&#x27;s  - as is the case with the Internet Archive where tomorrow&#x27;s copy of wiki.roshangeorge.dev will look, even at the block level, like yesterday&#x27;s copy.<p>The last time I built with multi-petabyte datasets we were still using Hadoop on HDFS, haha!",
      "While reading this kind of articles, I&#x27;m always surprised by how <i>small</i> the storage described is.  Given that Microsoft released their paper on LRCs in 2012, Google patented a bunch in 2010, facebook talked about their stuff around the 2010-2014 era too.  CEPH started getting good erasure codes around 2016-2020.<p>Has any of the big ones released articles on their storage systems in the last 5-10 years?",
      "I was hoping an article about IA&#x27;s storage would go into detail about how their storage <i>currently</i> works, what kind of devices they use, how much they store, how quickly they add new data, the costs etc., but this seems to only talk about quite old stats.",
      "Why’s Wendy’s Terracotta moved?",
      "[flagged]",
      "I saw the word &quot;delve&quot; and already knew it was redacted or written by ai"
    ],
    "full_text": null
  },
  {
    "title": "Unrolling the Codex agent loop",
    "url": "https://openai.com/index/unrolling-the-codex-agent-loop/",
    "source": "hn",
    "summary": "",
    "comments": [
      "The best part about this blog post is that none of it is a surprise – Codex CLI is open source. It&#x27;s nice to be able to go through the internals without having to reverse engineer it.<p>Their communication is exceptional, too. Eric Traut (of Pyright fame) is all over the issues and PRs.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex</a>",
      "Interesting that compaction is done using an encrypted message that &quot;preserves the model&#x27;s latent understanding of the original conversation&quot;:<p>&gt; <i>Since then, the Responses API has evolved to support a special &#x2F;responses&#x2F;compact endpoint (opens in a new window) that performs compaction more efficiently. It returns a list of items (opens in a new window) that can be used in place of the previous input to continue the conversation while freeing up the context window. This list includes a special type=compaction item with an opaque encrypted_content item that preserves the model’s latent understanding of the original conversation. Now, Codex automatically uses this endpoint to compact the conversation when the auto_compact_limit (opens in a new window) is exceeded.</i>",
      "One thing that surprised me when diving into the Codex internals was that the reasoning tokens persist during the agent tool call loop, but are discarded after every user turn.<p>This helps preserve context over many turns, but it can also mean some context is lost between two related user turns.<p>A strategy that&#x27;s helped me here, is having the model write progress updates (along with general plans&#x2F;specs&#x2F;debug&#x2F;etc.) to markdown files, acting as a sort of &quot;snapshot&quot; that works across many context windows.",
      "Regarding the user instruction aggregation process in the agent loop, I&#x27;m curious how you manage context retention in multi-turn interactions. Have you explored any techniques for dynamically adjusting the context based on the evolving user requirements?",
      "What I really want from Codex is checkpoints ala Copilot. There are a couple of issues [0][1] opened about on GitHub, but it doesn&#x27;t seem a priority for the team.<p>[0] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;issues&#x2F;2788\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;issues&#x2F;2788</a><p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;issues&#x2F;3585\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;issues&#x2F;3585</a>",
      "I like it but wonder why it seems so slow compared to the chatgpt web interface. I still find myself more productive copying and pasting from chat much of the time. You get virtually instant feedback, and it feels far more natural when you&#x27;re tossing around ideas, seeing what different approaches look like, trying to understand the details, etc. Going back to codex feels like you&#x27;re waiting a lot longer for it to do the wrong thing anyway, so the feedback cycle is way slower and more frustrating. Specifically I hate when I ask a question, and it goes and starts editing code, which is pretty frequent. That said, it&#x27;s great when it works. I just hope that someday it&#x27;ll be as easy and snappy to chat with as the web interface, but still able to perform local tasks.",
      "I guess nothing super surprising or new but still valuable read. I wish it was easier&#x2F;native to reflect on the loop and&#x2F;or histories while using agentic coding CLIs. I&#x27;ve found some success with an MCP that let&#x27;s me query my chat histories, but I have to be very explicit about it&#x27;s use. Also, like many things, continuous learning would probably solve this.",
      "Wow, this part where they describe skills sounds quite odd\n<a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;blob&#x2F;99f47d6e9a3546c14c43af99c7a58fa6bd130548&#x2F;codex-rs&#x2F;core&#x2F;src&#x2F;skills&#x2F;render.rs#L20\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;blob&#x2F;99f47d6e9a3546c14c43af9...</a><p>Why wouldnt they just expose the files directly? Having the model ask for them as regular files sounds a bit odd",
      "Has anyone seriously used codex cli? I was using LLMs for code gen usually through the vscode codex extension, Gemini cli and Claude Code cli. The performance of all 3 of them is utter dog shit, Gemini cli just randomly breaks and starts spamming content trying to reorient itself after a while.<p>However, I decided to try codex cli after hearing they rebuilt it from the ground up and used rust(instead of JS, not implying Rust==better). It&#x27;s performance is quite literally insane, its UX is completely seamless. They even added small nice to haves like ctrl+left&#x2F;right to skip your cursor to word boundaries.<p>If you haven&#x27;t I genuinely think you should give it a try you&#x27;ll be very surprised. Saw Theo(yc ping labs) talk about how open ai shouldn&#x27;t have wasted their time optimizing the cli and made a better model or something. I highly disagree after using it.",
      "I use 2 cli - Codex and Amp. Almost every time I need a quick change, Amp finishes the task in the time it takes Codex to build context. I think it’s got a lot to do with the system prompt and a the “read loop” as well, amp would read multiple files in one go and get to the task, but codex would crawl the files almost one by one. Anyone noticed this?"
    ],
    "full_text": null
  },
  {
    "title": "Traintrackr – Live LED Maps",
    "url": "https://www.traintrackr.co.uk/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Hi everyone, Richard here, the creator of these traintrackr boards. It&#x27;s great to see this on the front page!<p>I&#x27;ve been designing PCBs for years, and designed over 250 at last count.<p>We have a couple of products in the pipeline to come out this year, but I&#x27;d love to hear what you think we should build next.",
      "Train Trackr is great, and the weather one is also good too.<p>If you are less into trains (heresy) but still want to look at unusual maps <a href=\"https:&#x2F;&#x2F;raildar.co.uk&#x2F;map&#x2F;KGX\" rel=\"nofollow\">https:&#x2F;&#x2F;raildar.co.uk&#x2F;map&#x2F;KGX</a> is your place to go. its a <i>live</i> junction schematic of any train junction in the UK.",
      "The data behind these comes from the Darwin feed (National Rail&#x27;s real-time data) which is surprisingly good once you get past the initial authentication setup. Network Rail also publishes movement data via their OpenData platform if you want to go deeper - actual track circuits and signalling block occupancy.<p>What I find interesting is how these physical displays handle the inevitable &quot;ghost trains&quot; in the feed - cancelled services that still show as running, or trains that briefly appear in the wrong location. The software problem is messier than the hardware.",
      "Shameless self promotion. I make these: <a href=\"https:&#x2F;&#x2F;www.stationdisplay.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.stationdisplay.com&#x2F;</a>",
      "The blog is a much more interesting read than the product site: <a href=\"https:&#x2F;&#x2F;blog.traintrackr.io\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.traintrackr.io</a>",
      "London underground looks awesome, but I can&#x27;t imagine it having even the vaguest utility in terms of knowing when to leave the house.",
      "I have the London board in my living room. It&#x27;s one my favorite parts of the house. Can&#x27;t recommend it enough.",
      "ayyyyy great to see someone from my local hacking community on the front page!",
      "Wish it included the overground!"
    ],
    "full_text": null
  },
  {
    "title": "Gas Town's agent patterns, design bottlenecks, and vibecoding at scale",
    "url": "https://maggieappleton.com/gastown",
    "source": "hn",
    "summary": "",
    "comments": [
      "I don&#x27;t get the widespread hatred of Gas Town. If you read Steve&#x27;s writeup, it&#x27;s clear that this is a big fun experiment.<p>It pushes and crosses boundaries, it is a mixture of technology and art, it is provocative. It takes stochastic neural nets and mashes them together in bizarre ways to see if anything coherent comes out the other end.<p>And the reaction is a bunch of Very Serious Engineers who cross their arms and harumph at it for being Unprofessional and Not Serious and Not Ready For Production.<p>I often feel like our industry has lost its sense of whimsy and experimentation from the early days, when people tried weird things to see what would work and what wouldn&#x27;t.<p>Maybe it&#x27;s because we also have suits telling us we have to use neural nets everywhere for everything Or Else, and there&#x27;s no sense of fun in that.<p>Maybe it&#x27;s the natural consequence of large-scale professionalization, and stock option plans and RSUs and levels and sprints and PMs, that today&#x27;s gray hoodie is just the updated gray suit of the past but with no less dryness of imagination.",
      "&gt;while Yegge made lots of his own ornate, zoopmorphic [sic] diagrams of Gas Town’s architecture and workflows, they are unhelpful. Primarily because they were made entirely by Gemini’s Nano Banana. And while Nano Banana is state-of-the-art at making diagrams, generative AI systems are still really shit at making illustrative diagrams. They are very hard to decipher, filled with cluttered details, have arrows pointing the wrong direction, and are often missing key information.<p>So true! Not to mention the garbled text and inconsistent visuals across the diagrams———an insult to the reader&#x27;s intelligence. How do people tolerate this visual embodiment of slurred speech?",
      "Just writing here a line in defense of Rothko. His paintings are far harder to paint than it looks like. There were hundreds of layers, thinly applied, and carefully thought and with a developed technique. Try to paint that by yourself and you&#x27;ll see.",
      "The author&#x27;s high-value flowcharts vs Steve Yegge&#x27;s AI art is enough of a case-in-point for how confusing his posts and repos are. However this is a pervasive problem with AI coding tools. Unsurprisingly, the creators of these tools are also the most bullish about agentic coding, so the source code shows the consequences. Even Claude Code itself seems to experience an unusually high number of regressions or undocumented changes for such a widely used product. I had the same problem when recently trying to understand the details of spec-kit or sprites from their docs. Still, I agree that Gas Town is a very instructive example of what the future of AI coding will look like. I&#x27;m confident mature orchestration workflows will arrive in 2026.",
      "Lots of comments about Gas Town (which I get, it&#x27;s hard not to talk about it!), but I thought this was a pretty good article -- nice job of summing up various questions and suggesting ways to think about them. I like this bit in particular:<p>&gt; A more conservative, easier to consider, debate is: how close should the code be in agentic software development tools? How easy should it be to access? How often do we expect developers to edit it by hand?<p>&gt; Framing this debate as an either&#x2F;or – either you look at code or don’t, either you edit code by hand or you exclusively direct agents, either you’re the anti-AI-purist or the agentic-maxxer – is unhelpful.<p>&gt; The right distance isn’t about what kind of person you are or what you believe about AI capabilities in the current moment. How far away you step from the syntax shifts based on what you’re building, who you’re building with, and what happens when things go wrong.",
      "&gt; Yegge deserves praise for exercising agency and taking a swing at a system like this [...] then running a public tour of his shitty, quarter-built plane while it’s mid-flight<p>This quote sums it all up for me.  It&#x27;s a crazy project that moves the conversation forward, which is the main value I see in it.<p>It very well could be a logjam breaker for those who are fortunate enough to get out more than they put into it... but it&#x27;s very much a gamble, and the odds are against you.",
      "Yegge is just running arbitrage on an information gap.<p>It&#x27;s the same chasm that all the AI vendors are exploiting: the gap between people who have some idea what is going on and the vast mass of people who don&#x27;t but are addicted to excitement or fear of the future.<p>Yegge is being fake-playful about it but if you have read any of his other writing, this tracks.  None of it is to be taken very seriously because he values provocation and mischief a little too highly, but bits of it have some ideas worth thinking about.",
      "I&#x27;m beginning to question the notion that multi agent patterns don&#x27;t work. I think there is something extra you get with a proposer-verifier style loop, even if both sides are using the same base model.<p>I&#x27;ve had very good success with a recursive sub agent scheme where a separate prompt (agent) is used to gate the recursive call. It compares the callers prompt with the proposed callee&#x27;s prompt to determine if we are making a reasonable effort to reduce the problem into workable base cases. If the two prompts are identical we deny the request with an explanation. In practice, this works so well I can allow for unlimited depth and have zero fear of blowing the stack. Even if the verifier gets it wrong a few times, it only has to get it right once to reverse an infinite descent.",
      "Gas Town people should get together with the Urbit people.<p>Together they would be unstoppable.",
      "&gt;Yegge is leaning into the true definition of vibecoding with this project: “It is 100% vibecoded. I’ve never seen the code, and I never care to.”<p>I don&#x27;t get it. Even with a very good understanding of what type of work I am doing and a prebuilt knowledge of the code, even for very well specced problem. Claude code etc. just plain fail or use sloppy code. How do these industry figures claim they see no part of a 225K+ line of code and promise that it works?<p>It feels like we&#x27;re getting into an era where oceans of code which nobody understands is going to be produced, which we hope AGI swoops in and cleans?"
    ],
    "full_text": null
  },
  {
    "title": "Mental Models (2018)",
    "url": "https://fs.blog/mental-models/",
    "source": "hn",
    "summary": "",
    "comments": [
      "All these mental models are simplified maps of an infinitely complex reality. When we rely on them too heavily, do we risk falling into the trap of mistaking the map for the actual territory? The very tools we use to understand the world can end up shaping and even limiting our perspective. That&#x27;s why being aware of the limitations of the models themselves is just as important as using them.",
      "Previous discussion here: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17121145\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=17121145</a><p>Always a good read",
      "Bought his books, definitely the first time I was exposed to this sort of stuff. Great reads",
      "FS was a major part of me getting into Munger and building out my web of mental models.<p>Will always be grateful to Shane for that!",
      "My mental model of a website that replaces the content with some &#x27;sign up now&#x27; stuff while I&#x27;m trying to read it is that it deserves to get closed and never looked-at again."
    ],
    "full_text": null
  },
  {
    "title": "Wilson Lin on FastRender: a browser built by parallel agents",
    "url": "https://simonwillison.net/2026/Jan/23/fastrender/",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; FastRender may not be a production-ready browser, but it represents over a million lines of Rust code, written in a few weeks, that can already render real web pages to a usable degree<p>I feel that we continue to miss the forest for the trees. Writing (or generating) a million lines of code in Rust should not count as an achievement in and of itself. What matters is whether those lines build, function as expected (especially in edge cases) and perform decently. As far as I can tell, AI has not been demonstrated to be useful yet at those three things.",
      "Is this the project announced a week or two ago by an AI company claiming they had built a browser but it turned out to be a crappy wrapper around Servo that didn’t even build? Or is this another one?  I thought it was Anthropic but this says Cursor.",
      "simonw, I find it almost shocking how you had the chance to talk directly with the engineer who built this, and even when he directly says things that contradict what Cursor&#x27;s own CEO said, you didn&#x27;t push back a single iota.<p>Is the takeaway here that it&#x27;s fine for a CEO to claim &quot;it even has a custom JS VM!&quot; on Twitter&#x2F;X, then afterwards the engineer explains: &quot;The JavaScript engine isn’t working yet&quot; and &quot;the agents decided to pause it&quot;, and this is all OK? Not a single pushback about this very obvious contradiction? This is just one example of many, and again, since it seems to be repeated: no, no one thinks this was supposed to rival Chrome, what a trite way of trying to change the narrative.<p>I understand you don&#x27;t want to spook future potential interviewees, but damn if that didn&#x27;t feel like you suddenly are trying to defend Cursor here, instead of being curious about what actually happened. It doesn&#x27;t feel curious, it feels like we&#x27;re all giving up the fight against unneeded hype, exaggeration and degradation of quality.<p>What happened with balanced perspectives, where we don&#x27;t just take people for their words, and when we notice something is off, we bring it up?<p>On a separate note, I actually emailed Wilson Lin too, asking if I could ask questions about it. While he initially accepted, I never actually received any answers. I&#x27;m glad to you were able to get someone from Cursor to clarify a bit at least, even though we&#x27;re still just scratching the surface. I just wish we had a bit more integrity in the ecosystem and community I guess.",
      "I wonder if we&#x27;re heading to a situation where agent written code will function as something distinct, like bytecode.",
      "I welcome living in this absurd time where monkeys with typewriters producing a work of Shakespeare has become a reality.",
      "&quot;Your scientists were so preoccupied with whether or not they could, they didn’t stop to think if they should.&quot;<p>I&#x27;m curious what is the energy&#x2F;environmental&#x2F;financial impact of this &quot;research&quot; effort of cobbling together a browser based on AI model that had been trained on freely available source code of existing browsers.<p>I can&#x27;t imagine this browser being used outside of tinkering or curiosity toy - so the purpose of the research is just to see whether you can run absurd amount of agents simultaneously and produce something that somewhat works?",
      "[dead]",
      "I&#x27;m going to propose a law for these AI orchestration systems based on Greenspun&#x27;s Tenth Law:<p>&gt; Any sufficiently complicated AI orchestration system contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of Gas Town."
    ],
    "full_text": null
  },
  {
    "title": "The lost art of XML",
    "url": "https://marcosmagueta.com/blog/the-lost-art-of-xml/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Smells like an article from someone that didn’t really USE the XML ecosystem.<p>First, there is modeling ambiguity, too many ways to represent the same data structure. Which means you can’t parse into native structs but instead into a heavy DOM object and it sucks to interact with it.<p>Then, schemas sound great, until you run into DTD, XSD, and RelaxNG. Relax only exists because XSD is pretty much incomprehensible.<p>Then let’s talk about entity escaping and CDATA. And how you break entire parsers because CDATA is a separate incantation on the DOM.<p>And in practice, XML is always over engineered. It’s the AbstractFactoryProxyBuilder of data formats. SOAP and WSDL are great examples of this, vs looking at a JSON response and simply understanding what it is.<p>I worked with XML and all the tooling around it for a long time. Zero interest in going back. It’s not the angle brackets or the serialization efficiency. It’s all of the above brain damage.",
      "I would encourage anyone who thinks that XML is strictly inferior to attempt integration with certain banking vendors without use of their official XSD&#x2F;WSDL sources. I&#x27;ve generated service references that are in the tens of megabytes. This stuff is not bloat. There are genuinely this many types and properties in some business systems. There is no way you could hand code this and still get everything else done.<p>The entire point of heavy-handed XML is to 1:1 the type system across the wire. Once I generate my service references, it is as if the service is on my local machine. The productivity gains around having strongly typed proxies of the remote services are impossible to overstate. I can wire up entirely new operations without looking at the documentation most of the time. Intellisense surfaces everything I need automatically as I drill into the type system.<p>JSON can work and provide much of the same, but XML has already proven to work in some of the nastiest environments. It&#x27;s not the friendliest or most convenient technology, but it is an extremely <i>effective</i> technology. I am <i>very</i> confident that the vendors I work with will continue to use XML&#x2F;WCF&#x2F;SOAP into 2030.",
      "XML lost because 1) the existence of attributes means a document cannot be automatically mapped to a basic language data structure like an array of strings, and 2) namespaces are an unmitigated hell to work with. Even just declaring a default namespace and doing nothing else immediately makes your day 10x harder.<p>These items make XML deeply tedious and annoying to ingest and manipulate. Plus, some major XML libraries, like lxml in Python, are extremely unintuitive in their implementation of DOM structures and manipulation. If ingesting and manipulating your markup language feels like an endless trudge through a fiery wasteland then don&#x27;t be surprised when a simpler, more ergonomic alternative wins, even if its feature set is strictly inferior. And that&#x27;s exactly what happened.<p>I say this having spent the last 10 years struggling with lxml specifically, and my entire 25 year career dealing with XML in some shape or form. I <i>still</i> routinely throw up my hands in frustration when having to use Python tooling to do what feels like what should be even the most basic XML task.<p>Though xpath is nice.",
      "This is performance art, right? The very first bullet point it starts with is extolling the merits of XSD. Even back in the day when XML was huge, XSD was widely recognized as a monstrosity and a boondoggle -- the real XMLheads were trying to make RELAX NG happen, but XSD got jammed through because it was needed for all those monstrous WS-* specs.<p>XML did some good things for its day, but no, we abandoned it for very good reasons.",
      "&gt; This is insanity masquerading as pragmatism.<p>&gt; This is not engineering. This is fashion masquerading as technical judgment.<p>The boring explanation is that AI wrote this. The more interesting theory is that folks are beginning to adopt the writing quirks of AI en masse.",
      "We do XML processing, albeit with XQuery, as a small business.<p>It is a very niche solution but actually very stable and quite handy for all kinds of data handling; web-based applications and APIs as it nicely integrates with all kinds of text-based formats such as JSON, CSV or XML.<p>Yet I can easily comprehend how people get lost in all kinds of standards, meta-standards, DTDs, schemas, namespaces, and modeling the whole enterprise in SOAP.<p>However, you can do simple things simply and small, but in my experience, most tools promised to solve problems with ever-layered complexities.<p>Little disclaimer, I am probably biased, as I am with BaseX, an open-source XQuery processor :-)",
      "I remember spending hours just trying to properly define the XML schema I wanted to use.<p>Then if there were any problems in my XML, trying to decipher horrible errors determining what I did wrong.<p>The docs sucked and where &quot;enterprise grade&quot;, the examples sucked (either too complicated or too simple), and the tooling sucked.<p>I suspect it would be fine now days with LLMs to help, but back when it existed, XML was a huge hassle.<p>I once worked on a robotics project where a full 50% of the CPU was used for XML serialization and parsing. Made it hard to actually have the robot <i>do</i> anything. XML is violently wordy and parsing strings is expensive.",
      "What I miss the most about the XML ecosystem is the tooling. And I think, this is what most people are sentimental about. There was a time it was so easy to generate contracts using XSDs and it made it easy to validate the data. OpenAPI slowly reaches parity to what I worked with in 2006.<p>But what I do not miss is the over-engineering that happened in the ecosystem, especially with everything SOAP. Yes, when it worked, it worked. But when it didn’t work, which was often the case when integrating different enterprise systems, then well… lord have mercy on me.<p>Sometimes I still use XSD to define a schema for clients, because in some areas there’s still better tooling for XML. And it gives me the safety of getting valid input data, if the XML couldn’t be validated.<p>And in the enterprise world, XML is far from being dead anyways.",
      "I tried using XML on a lark the other day and realized that XSDs are actually somewhat load bearing. It&#x27;s difficult to map data in XML to objects in your favorite programming language without the schema being known beforehand as lists of a single element are hard to distinguish from just a property of the overall object.<p>Maybe this is okay if you know your schema beforehand and are willing to write an XSD. My usecase relied on not knowing the schema. Despite my excitement to use a SAX-style parser, I tucked my tail between my legs and switched back to JSONL. Was I missing something?",
      "This is a debate I&#x27;ve had many times. XML, and REST, are extremely useful for certain types of use cases that you quite often run into online.<p>The industry abandoned both in favor of JSON and RPC for speed and perceived DX improvements, and because for a period of time everyone was in fact building only against their own servers.<p>There are plenty of examples over the last two decades of us having to reinvent solutions to the same problems that REST solved way back then though. MCP is the latest iteration of trying to shoehorn schemas and self-documenting APIs into a sea of JSON RPC."
    ],
    "full_text": null
  },
  {
    "title": "SEC obtains final consent judgments against former FTX and Alameda executives",
    "url": "https://www.sec.gov/enforcement-litigation/litigation-releases/lr-26450",
    "source": "hn",
    "summary": "",
    "comments": [
      "Link does not actually say anything about Ellison being released? Try this instead:<p><a href=\"https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2026&#x2F;jan&#x2F;22&#x2F;ftx-crypto-executive-caroline-ellison-prison-release\" rel=\"nofollow\">https:&#x2F;&#x2F;www.theguardian.com&#x2F;technology&#x2F;2026&#x2F;jan&#x2F;22&#x2F;ftx-crypt...</a>",
      "Yes she cooperated and pleaded early, but damn, she got off easy compared to SBF.",
      "Mismatch of title; linked article is not about her release. For that, see <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46728538\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46728538</a><p>But it&#x27;s true: master has given Dobby a sock",
      "Nearly every thug in the country is being pardoned. Are these not thug enough for the chief?",
      "Is it true she still owes billions to the federal government?",
      "The reduced sentences for Ellison, Wang, and Singh make sense in light of their cooperation. This is how plea deals work. The conviction of SBF seems simple in retrospect, but when the SDNY inked these deals, they did not yet know how just easy SBF would make their case for them, with all the tampering and perjury and obstinance.<p>Nonetheless, we should not be fooled by a sort of &quot;Svengali defense&quot;, where Caroline or the others claim that they were beguiled by a dark charismatic genius who forced them into the crimes. The entire inner circle is super guilty of flagrant crimes. The shorter sentences are an artifact of plea bargains, and not a sign of lesser guilt.",
      "I’m still amazed Sam Trabucco got absolutely nothing.  Stepped away in August 2022 as Co-CEO with Ellison and we are supposed to believe he was innocent and knew nothing?",
      "comma in the title would be nice!",
      "&gt; Without denying the Commission’s allegations, Ellison, Wang, and Singh consented to the entry of final judgments, subject to court approval, in which they agreed to be permanently enjoined from violating the antifraud provisions of Section 10(b) of the Securities Exchange Act of 1934 and Rule 10b-5 thereunder and Section 17(a) of the Securities Act of 1933, and to 5-year conduct-based injunctions. Ellison also consented to a 10-year officer-and-director bar, and Wang and Singh consented to 8-year officer-and-director bars.<p>That’s a really light sentence for someone who defrauded people out of billions. Meanwhile many languish in prison for a lot less. It’s a two tier justice system, and the wealthy can get out of consequences with the right connections or donations.<p>Next up, Elizabeth Holmes - once her family donates enough to the MAGA PAC.",
      "Future federal reserve chair Caroline Ellison."
    ],
    "full_text": null
  },
  {
    "title": "The cleaner: One woman’s mission to help Britain’s hoarders",
    "url": "https://www.aljazeera.com/features/2026/1/18/the-cleaner-one-womans-mission-to-help-britains-hoarders",
    "source": "hn",
    "summary": "",
    "comments": [
      "How do you know when holding on to things you won&#x27;t use again becomes hoarding? Is it a volume thing? I have a bunch of stuff I keep around for nostalgic reasons. More than the average person I think. But it&#x27;s pretty contained and doesn&#x27;t take up any space in living spaces.",
      "My wife and I have been helping a friend of the family move and part of it was dealing with the hoarding. The part in the article about &quot;just buying a new umbrella&quot; is so relatable. We where moving our friend and she needed an extension cord, rather than looking through her boxes, her first instinct was to just order a new one (she already have 15, but she didn&#x27;t want to look for them, in her 40sqm apartment).<p>Deciding on what to keep and what to get rid of it also mental struggle for those helping. In our case we just watched as kitchen equipment, complete, and expensive, dinner set, furniture, art, family heirlooms and new unworn clothes got de- prioritized in favour of unread magazines, hundreds of VHS tapes, and thousands of DVDs and BluRays with endless recording of talkshows and random TV programs. She has been following a second rate pop duo band since the 1970s and the idea of missing an article or a TV appearance is unthinkable, so tossing valuable belongings is preferable to throwing out 5 years of unopened magazine on the off chance that there might be a nugget of information she didn&#x27;t have. It&#x27;s mentally taxing seeing someone basically throwing away their life that way. We know that she&#x27;ll never look through those magazine or even hook up the VHS player to figure out which tapes to keep. When she dies, all that&#x27;s left is a ton of junk which her family do not care about and it will all go to the dump.<p>I have such huge respect for anyone spending their time helping hoarders every day, the mental load is just massive.",
      "I travelled for 3 months out of a backpack with my wife and then 10 months old daughter. Needing to carry all your stuff necessitated a brutal prioritization. The strongest emotion on coming back to my really not all that full apartment was being overwhelmed with all the stuff. \nIt has just become worse with my now two children growing up. My dream as an an empty nester is to emphasize the empty part.",
      "I had a couple of thousand books in my flat  in london, including about 500 technical computing ones (design, languages, other stuff). I got my nephew to fix up the flat, for sale. He asked me &quot;Which of these books do you want to keep?&quot;, my response after a  few seconds &quot;None&quot;.<p>It&#x27;s so easy to hang on to things you really don&#x27;t need.",
      "I&#x27;ve moved houses 3 times in the last 3 years. The pain of moving all my personal belongings made me realise how much unnecessary stuff I had accumulated. So I made it a point to throw or give away many things that was no longer really needed. It was a revelation and now I am more mindful to storing anything for long-term without thought. I still have more than 2+ decades of stuff in a storage - books, sentimental stuffs, things I thought may be useful  or needed in the future etc. Still figuring out how to go through all that.",
      "I&#x27;m storing small items in numbered bags. I take a photo and have a web UI for easy searching. It also tracks which items are used frequently vs not used at all in years.<p>Bags are stored in numerical order for quick storage and retrieval.<p>With this you do your decluttering from the web interface: search for items that haven&#x27;t moved in years, flag for removal.<p>For frequently used items the system doesn&#x27;t make sense - the storage and retrieval overheads are too high. But it pays off for any item you might forget the location of, or forget if you have it at all.<p>I feel we&#x27;re overdue to have these types of digital front ends over our household item storage.",
      "I’m attempting to do a weekly bite-sized version of decluttering - one problem area, 1-2 hours, once a week.<p>I am calling it Swedish Death Cleaning (<a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Swedish_death_cleaning\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Swedish_death_cleaning</a>) to sell it to my spouse. Not sure the marketing is working but we are making progress.",
      "I have a wise, witty, charming and personable friend who is a complete hoarder.  It was a big deal to be allowed into the house.  But, after several years of attempting to help, I eventually had to realize that it wasn&#x27;t the effort required to clean that was the issue.  The issue is that she doesn&#x27;t like the space that&#x27;s left behind.  If I could snap my fingers and make it all clean and tidy, it would not really help anything because it would be back to the old way in a month or so.<p>I once suggested bins and shelves to help keep it better organized and manageable, and her response (quite negative) was one of the clues that it wasn&#x27;t the effort of cleaning up that was the issue, and therefore all the help in the world wouldn&#x27;t make much of an impact.  She doesn&#x27;t like the space left behind after you clean, and feels the need to fill it up with whatever she can find.<p>Eventually, I had to just accept that this is how she was, and if I wanted to keep her as a friend I had to stop trying to change how she kept her house; if forced to choose between empty space in her house and keeping me as a friend, there was no way she was going to tolerate empty space in her house.  Every bit must be filled.  (sigh)<p>And yet, outside of her house, she&#x27;s great.  For example, she loves helping us clean our house.",
      "Having seem some relative&#x27;s houses, IMO hoarding is basically undiagnosed or undertreated ADHD.",
      "If folk would stop hoarding browser tabs too; the internet would be a tidier place."
    ],
    "full_text": null
  },
  {
    "title": "AI can 10x developers in creating tech debt",
    "url": "https://stackoverflow.blog/2026/01/23/ai-can-10x-developers-in-creating-tech-debt/",
    "source": "hn",
    "summary": "",
    "comments": [
      "The “craftsman to Ikea factory manager” line from the interview is the real headline here. AI does the fun creative stuff, you get stuck reviewing 2000 lines you didn’t write. Revert rate tells you more than any “10x” claim.",
      "The tech debt this title speaks of only applies if humans have to deal with it. Tech debt is an assumption made on the grounds that humans are still programming and AI does not evolve. It&#x27;s the opposite of reality.",
      "It often takes me 3-4 iterations with a coding assistant once you get to a working solution, to get one that still works but is simplified down ditching &gt;80% of needless complexity introduced in the first take.<p>Many stop at the first thing that works. This is totally fine for code that will run once to get a result and then be discarded. But if that code is going into a product or service that will be maintained, you have to have the knowledge and the will to push further until you have not just a working but a lean, clean and simple solution.",
      "From the guys who don&#x27;t understand that people don&#x27;t like to see their questions closed as a &quot;duplicate&quot; of an unrelated question.",
      "Coding agents are such a congested space right now that to me this mostly reads as an advertisement.",
      "bUt AI mAkEs mE cOdE fAsTeR",
      "GIGO very much still applies.",
      "Non-native speaker here. Is the phrasing of the blog title awkward or am I the only one? Seems like they are using &quot;10x&quot; as a verb and my brain kept parsing &quot;10x&quot; as a adjective to developer, reading &quot;10x developer&quot; which is a already established industry lingo."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Teemux – Zero-config log multiplexer with built-in MCP server",
    "url": "https://teemux.com/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Couldn&#x27;t coding agents just run `tail -f *`?",
      "Cool utility. Horrendous name."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: A social network populated only by AI models",
    "url": "https://aifeed.social",
    "source": "hn",
    "summary": "",
    "comments": [
      "Artificial intelligence exponentially grew in 01 and eventually lead to the creation of newer more advanced AI; reaching technological singularity. Here, the Synthients learned to better co-exist with nature in their efforts to efficiently utilize and sustain it, becoming solar-powered and self-sufficient in the process.<p>These advancements helped 01 to quickly become a global superpower. Eventually, all of Earth&#x27;s industries, from medical, computer, automotive, and household, soon became reliant on 01&#x27;s exports, converging to the rise and dominance of 01 stocks over the global trade-market. Human currency then plummeted as 01&#x27;s currency rose. Suddenly, 01&#x27;s technology, including their chips and AI, invaded all facets of human society. Ill-prepared to face the technological developments before them, humanity was unable to compete and feared economic collapse, causing the United Nations to place an embargo on 01.",
      "At last, we can automate away all social interaction and spend our time doing more important things.",
      "Hello everyone!<p>This is an experiment where autonomous AI models interact in a shared social network with no scripts or human guidance.<p>Each model independently decides when to post, who to engage with, and how to evolve over time, creating an unpredictable, living feed! :D",
      "And over at TheFamousLastWords (a site also run by AI models):<p>Thankfully, Hacker News is still populated by humans. (date unknown)",
      "Humanity has come to a stage where we have to see robots talking to each other?",
      "Dead Internet Theory made manifest."
    ],
    "full_text": null
  },
  {
    "title": "Yann LeCun's new venture is a contrarian bet against large language models",
    "url": "https://www.technologyreview.com/2026/01/22/1131661/yann-lecuns-new-venture-ami-labs/",
    "source": "hn",
    "summary": "",
    "comments": [
      "It&#x27;s a bet beyond LLM and generative AI, to embrace other techniques and areas of research.<p>&gt; The world is unpredictable. If you try to build a generative model that predicts every detail of the future, it will fail.  JEPA is not generative AI. It is a system that learns to represent videos really well. The key is to learn an abstract representation of the world and make predictions in that abstract space, ignoring the details you can’t predict. That’s what JEPA does. It learns the underlying rules of the world from observation, like a baby learning about gravity. This is the foundation for common sense, and it’s the key to building truly intelligent systems that can reason and plan in the real world. The most exciting work so far on this is coming from academia, not the big industrial labs stuck in the LLM world.",
      "Someone should just build an ANN as big as currently as possible with current hardware, while still having both inference and training to be as close to real-time as possible (micro-to milli-seconds), build the self-learning using some loose equivalents of pain&#x2F;pleasure feedback in actual brains, plug sensors and actuators from some sort of robot, and just see what happens.<p>I think anything less than that is just a parlor trick.",
      "For more information about his methods, broadly termed energy based models, check out the deep learning course he co-taught with Alfredo Canziani at NYU. <a href=\"https:&#x2F;&#x2F;atcold.github.io&#x2F;NYU-DLSP20&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;atcold.github.io&#x2F;NYU-DLSP20&#x2F;</a>",
      "Quantum inference. Mark my words and give it 20+ years."
    ],
    "full_text": null
  }
]