[
  {
    "title": "How Long Is a Piece of String? A Brief Empirical Analysis of Tokenizers",
    "url": "https://arxiv.org/abs/2601.11518v1",
    "source": "arxiv",
    "summary": "Frontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and ",
    "full_text": "\n\n\n\n1 Introduction\n2 Tokenization\n\n3 Domains and Tokenizers\n\nDomains.\nTokenizers.\n\n\n\n4 Experiments\n\n4.1 Character compression\n4.2 Word compression\n4.3 Context limits\n4.4 Additional experiments\n\n\n5 Related Work\n6 Conclusions\nA Additional words per token results\nB Languages Compression Ratios\nC Geospatial\nD Character Poisoning\n\n\n\n\n\n\nhow long is a pie ce of string ?\na brief empirical analysis of tokenizers\n\n\n\n\nJonathan Robertsψ\\psi   Kai Hanπ\\pi   Samuel Albanie\nψ\\psiUniversity of Cambridge   π\\piThe University of Hong Kong\n\n\n\nAbstract\nFrontier LLMs are increasingly utilised across academia, society and industry. A commonly used unit for comparing models, their inputs and outputs, and estimating inference pricing is the token. In general, tokens are used as a stable currency, assumed to be broadly consistent across tokenizers and contexts, enabling direct comparisons. However, tokenization varies significantly across models and domains of text, making naïve interpretation of token counts problematic. We quantify this variation by providing a comprehensive empirical analysis of tokenization, exploring the compression of sequences to tokens across different distributions of textual data. Our analysis challenges commonly held heuristics about token lengths, finding them to be overly simplistic. We hope the insights of our study add clarity and intuition toward tokenization in contemporary LLMs.\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are ubiquitous across contemporary AI research. As model capabilities continue to improve, LLMs are capturing attention more broadly, in society and industry. Frontier models follow instructions with sufficient consistency to robustly use tools, enabling them to act as agents capable of performing longer horizon tasks [1] and economically valuable activity such as software engineering [2, 3], scientific research [4], and web-based economic tasks [5].\n\n\nFundamental to LLMs is an often overlooked process: tokenization. Tokenization describes the learned conversion between text characters and discrete “tokens” represented by unique numeric IDs.\nThese token IDs correspond to items in a vocabulary—a database of all possible tokens a model can interpret and generate. When autoregressively generating new tokens an LLM samples from a probability distribution over this vocabulary. Tokenization is necessary as it enables the transformation of human-readable text characters into a numeric model-readable format. Each token ID corresponds to an embedding—a high-dimensional numeric representation that aims to capture its semantic meaning.\n\n\n\n\n\n\n\nLlama 1,2\n   Ayant Ayid Ayis Ayest Ayab Aylish Ayment Ayarian Ayism  9\n\n\n\n\nMistral\n   Ayant Ayid Ayis Ayest Ayablish Ayment Ayarian Ayism  8\n\n\n\n\nClaude 4.5 (est.)\n   Ayant Ayid Ayis Ayestablishment Ayar Ayian Ayis Aym  8\n\n\n\n\nMistral Tekken\n   Ayant Ayidis Ayest Ayabl Ayishment Ayarian Ayism  7\n\n\n\n\nGPT 5\n   Ayant Ayidis Ayest Ayablishment Ayarian Ayism  6\n\n\n\n\nOLMo\n   Ayant Ayidis Ayestabl Ayishment Ayarian Ayism  6\n\n\n\n\nQwen\n   Ayant Ayidis Ayestablish Ayment Ayarian Ayism  6\n\n\n\n\nDeepSeek R1/V3\n   Ayant Ayidis Ayestablish Ayment Ayarianism  5\n\n\n\n\nGrok\n   Ayant Ayidis Ayestablishment Ayarian Ayism  5\n\n\n\n\nT5\n   Ayanti Ayd Ayis Ayest Ayabl Ayish Ayment Ayaria Aynism  9\n\n\n\n\nBERT\n   Ayanti Aydis Ayest Ayab Aylish Ayment Ayarian Ayism  8\n\n\n\n\nGemini\n   Ayanti Aydis Ayestablishment Ayarian Ayism  5\n\n\n\n\nFuyu\n   Ayanti Aydis Ayestablishment Ayarianism  4\n\n\n\n\n\n\n\n\n\nLlama 1,2\n   Ayhum Ayu Ayhum Ayun Ayuk Ayun Ayu Ayku Ayā Ayp Ayua Ay’ Aya  13\n\n\n\n\nT5\n   Ayhum Ayu Ayhum Ayun Ayuk Ayun Ayuk Ayu Ayā Aypu Aya Ay’ Aya  13\n\n\n\n\nDeepSeek V2\n   Ayhum Ayu Ayhum Ayun Ayuk Ayun Ayuku Ayā Ayp Ayua Ay’ Aya  12\n\n\n\n\nFuyu\n   Ayhum Ayu Ayhum Ayunu Aykun Ayuku Ayāp Ayua Ay’ Aya  10\n\n\n\n\nYi\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuk Ayu Ayā Ayp Ayua Ay’ Aya  13\n\n\n\n\nMistral\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuk Ayu Ayā Aypu Aya Ay’ Aya  13\n\n\n\n\nBERT\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuk Ayua Aypu Aya Ay’ Aya  12\n\n\n\n\nJamba\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuku Ayā Ayp Ayua Ay’ Aya  12\n\n\n\n\nQwen\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuku Ayā Ayp Ayua Ay’a  11\n\n\n\n\nKimi K2\n   Ayhum Ayuh Ayum Ayun Ayuk Ayun Ayuku Ayā Aypu Aya Ay’a  11\n\n\n\n\nGemini\n   Ayhum Ayuh Ayum Ayun Ayukun Ayuku Ayā Ayp Ayua Ay’ Aya  11\n\n\n\n\nDeepSeek R1/V3\n   Ayhum Ayuh Ayum Ayun Ayukun Ayuku Ayā Ayp Ayua Ay’a  10\n\n\n\n\nGrok\n   Ayhum Ayuh Ayum Ayun Ayukun Ayuku Ayāp Ayua Ay’a  9\n\n\n\n\n\n\n\n\n\nLlama 1,2\n   AyA Ay fo Ayx Ay knows Ay many Ay things Ay, Ay but Ay a Ay h Ayedge Ayh Ayog Ay knows Ay one Ay big Ay thing Ay.∗\n 18\n\n\n\nLlama 3\n   AyA Ay fox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hedge Ayhog Ay knows Ay one Ay big Ay thing Ay.  15\n\n\n\nLlama 4\n   AyA Ay fox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hed Aygeh Ayog Ay knows Ay one Ay big Ay thing Ay.  16\n\n\n\n\n\n\n\n\nMistral\n   AyA Ay f Ayox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hed Ayge Ayh Ayog Ay knows Ay one Ay big Ay thing Ay.  18\n\n\n\nMistral Tekken\n   AyA Ay fox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hed Aygeh Ayog Ay knows Ay one Ay big Ay thing Ay.  16\n\n\n\n\n\n\n\n\nGPT 3.5/4\n   AyA Ay fox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hedge Ayhog Ay knows Ay one Ay big Ay thing Ay.  15\n\n\n\nGPT 4o/4.1/4.5/5; oss; o-series\n   AyA Ay fox Ay knows Ay many Ay things Ay, Ay but Ay a Ay hed Aygeh Ayog Ay knows Ay one Ay big Ay thing Ay.  16\n\n\n\n\nFigure 1: Tokenization schemes vary significantly. Token boundaries are represented as shaded colours and token counts are shown in purple. Antidisestablishmentarianism is tokenized many different ways, using between 4 (Fuyu) and 9 tokens (early Llama models). Variation in tokenization is also observable for words containing fewer unique letters and more vowels, such as the Hawaiian word humuhumunukunukuāpua’a (Reef Triggerfish). Small changes to tokenizers within model families result in different tokenization, even in relatively simple sentences. ∗From [6].\n\n\nThe token is a near-universally used unit of text sequences for describing numerous language modelling metrics, including sequence lengths, context limits, pricing, latency, and sizes of training corpora. Token counts are utilised to directly compare models, inference costs and provider platforms. Although different model series are known to implement bespoke tokenizers, and therefore tokenize text differently, tokens are used as though they are a consistent unit with differences that are trivial or averaged out over sufficient samplings or length of text. This assumption is evident in the widespread use of general heuristics for token lengths (such as one token is roughly 4 characters or 0.75 words (e.g., [7, 8])); or the comparison of API endpoint performance using number of tokens per second or usage metered as number of dollars per token.\n\n\nHowever, as we demonstrate, token counts are not a stable unit of length: counts vary non-trivially for different domains of text or tokenizer. Therefore, when used naïvely, token counts provide an inadequate basis of comparison. In Fig. 1, we show clear variations in token boundaries and the number of tokens used by different tokenization schemes. Among frontier LLMs, a word like antidisestablishmentarianism is tokenized by Claude into nearly twice as many tokens (9) as Gemini or Grok (5). As our experimentation demonstrates, these differences in tokenization can result in significant differences in token counts over longer sequences.\n\n\nAs LLM-based systems become more prevalent, there is an emerging need for clarity around tokenization. Simply comparing sequence lengths and context limits based on model-specific tokenization is insufficient and misleading. Moreover, when accessed via an API (as is the case for most frontier LLMs), usage is metered by the token: accurate accounting requires a clear understanding of how a text sequence is mapped to tokens for a given text sequence.\n\n\nPrior studies have explored tokenization for specific domains [9], languages [10] or strategies [11], painting a valuable though incomplete picture. We extend these works and offer a comprehensive empirical analysis of tokenizer efficiencies across models and domains. We provide robust quantification of domain-specific tokenizer character compression ratios and estimates of word compression ratios as a function of word frequency and distribution. Building on these empirical findings, we examine model context limits, suggesting domain-specific character counts or model-agnostic token counts offer more directly comparable context limits. Our study is intentionally brief and focused, with clear goals of adding clarity to tokenization, quantifying differences in tokenization between models and domains, and imparting intuition on how to interpret token-based metrics.\n\n\n\n\n2 Tokenization\n\nDuring inference, an input text string is encoded into a sequence of token IDs by the tokenizer. These token IDs map to embeddings, which the LLM ingests along with positional information as input and iteratively generates a sequence of output token IDs. Finally, the tokenizer decodes this output into a human-readable text string.\nThe exact nature of the mapping between formats and the level of compression between text characters and tokens is a trade-off balancing efficiency and meaning. Tokenizing at the character-level ensures a relatively small vocabulary, constrained by the number of unique characters, resulting in smaller embedding/output matrices at the expense of longer sequences and semantic meaning (information density) per token (‘c’ carries less meaning than ‘car’). On the other hand, tokenizing at the word-level (or longer) results in fewer tokens that capture more semantic meaning at the expense of a larger vocabulary; this approach is subject to out-of-vocabulary issues. To compromise these conflicting objectives, tokenizers in LLMs are typically trained to tokenize at the subword level.\n\n\nTokenization typically consists of two phases: training and segmentation."
  },
  {
    "title": "Do explanations generalize across large reasoning models?",
    "url": "https://arxiv.org/abs/2601.11517v1",
    "source": "arxiv",
    "summary": "Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.11517v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.11517v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 16 Jan 2026]\n    Title:Do explanations generalize across large reasoning models?\n    Authors:Koyena Pal, David Bau, Chandan Singh            View a PDF of the paper titled Do explanations generalize across large reasoning models?, by Koyena Pal and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large reasoning models (LRMs) produce a textual chain of thought (CoT) in the process of solving a problem, which serves as a potentially powerful tool to understand the problem by surfacing a human-readable, natural-language explanation. However, it is unclear whether these explanations generalize, i.e. whether they capture general patterns about the underlying problem rather than patterns which are esoteric to the LRM. This is a crucial question in understanding or discovering new concepts, e.g. in AI for science. We study this generalization question by evaluating a specific notion of generalizability: whether explanations produced by one LRM induce the same behavior when given to other LRMs. We find that CoT explanations often exhibit this form of generalization (i.e. they increase consistency between LRMs) and that this increased generalization is correlated with human preference rankings and post-training with reinforcement learning. We further analyze the conditions under which explanations yield consistent answers and propose a straightforward, sentence-level ensembling strategy that improves consistency. Taken together, these results prescribe caution when using LRM explanations to yield new insights and outline a framework for characterizing LRM explanation generalization.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.11517 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.11517v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.11517\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Koyena Pal [view email]          [v1]\n        Fri, 16 Jan 2026 18:55:29 UTC (2,934 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Do explanations generalize across large reasoning models?, by Koyena Pal and 2 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n    "
  },
  {
    "title": "Building Production-Ready Probes For Gemini",
    "url": "https://arxiv.org/abs/2601.11516v1",
    "source": "arxiv",
    "summary": "Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift ",
    "full_text": "\n\n\n\n1 Introduction\n2 Setup and Notation\n\n3 Our Classifiers\n\n\n3.1 Baseline Probe Architectures\n\n3.1.1 Linear Probes\n3.1.2 Exponential Moving Average (EMA) Probes\n3.1.3 MLP Probes\n3.1.4 Attention Probes\n\n\n\n3.2 Our Probe Architectures\n\n3.2.1 MultiMax Probes\n3.2.2 Max of Rolling Means Attention Probe\n3.2.3 AlphaEvolve Architectures\n\n\n\n3.3 LLM-Based Classifiers\n\n3.3.1 Prompted Language Model\n3.3.2 Cascading Classifier\n\n\n\n\n\n4 Datasets and Metrics\n\n4.1 Metric\n4.2 Initialization Seeds\n\n\n\n5 Results\n\n\n5.1 Main Cyber Probing Results\n\n5.1.1 Statistical Significance of Probe Comparisons\n5.1.2 Training on Long-Context Data\n\n\n\n5.2 Cascading Classifiers\n\n5.2.1 Warmup: Heuristic Band Method\n5.2.2 Generalization: Threshold-Randomization-Optimal Cascading\n5.2.3 Cascading Results\n\n\n5.3 Automating Safety Research with AlphaEvolve\n\n\n\n6 Related Work\n\n6.1 Misuse Mitigation\n6.2 Activation Probing\n6.3 Cascading Classifiers\n6.4 Distribution Shifts\n6.5 Jailbreaking Models\n6.6 Automated Safety Research\n\n\n7 Conclusion\n8 Acknowledgements\n9 Author Contributions\n\nA Running our findings on other datasets and models\n\nArchitecture Ranking.\nSeed Variance.\nLinear Probe Baseline.\nComparison to Prior Work.\n\n\n\nB Dataset Statistics\n\nB.1 Evaluation and Validation Splits\nB.2 Training Data Configuration\nB.3 Long Context Data Filtering\n\n\nC General Training Details\n\nD Seed Selection Details\n\nD.1 High-Variance Architectures\nD.2 Raw Seed Selection Statistics\nD.3 Seed Selection vs Architecture Choice\n\n\n\nE Efficiently finding the Threshold-Randomization-Optimal cascading policy\n\nSetup.\nThe Greedy Savings Frame.\nThe Algorithm.\n\nE.1 Vertex Optimality: Why No Randomization is Needed At The Optimal Low Error Point\n\nGeometric Argument.\nApplication to Figure˜1.\n\n\n\n\n\nF AlphaEvolve\n\nF.1 Further Details on Setup\nF.2 Alpha Evolve Probe Pseudocode\nF.3 Training Curve\n\n\nG Prompt Optimization Experiments\n\nH Automated Red Teaming Analysis\n\nH.1 Definition of “Success” in Adaptive Red Teaming\n\n\nI Infrastructure Recommendations\n\nJ Error Bar Methodology for Figure˜1\n\nJ.1 Probe Methods (AlphaEvolve, Selected Probe, EMA Linear Probe)\nJ.2 Language Model Methods (Gemini 2.5 Flash)\nJ.3 Cascading Methods (Best Probe + Gemini 2.5 Flash)\nJ.4 Interpretation\n\n\nK Discusssion and results from weighting Equation˜12 differently\nL Attention Probe Inference\n\n\n\n\n\n\n\\pdftrailerid\nredacted\\correspondingauthorconmy@google.com, janosk@google.com, neelnanda@google.com\\reportnumber001\n\nBuilding Production-Ready Probes For Gemini\n\n\nJános Kramár\n\nEqual contributions to this work.\n\n\nJoshua Engels\n\n\n\n\nZheng Wang\n\n\n\n\nBilal Chughtai\n\n\n\n\nRohin Shah\n\n\n\n\nNeel Nanda\n\n\n\n\nArthur Conmy\n\nEqual contributions to this work.\n\n\n\nAbstract\nFrontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift.\nWe evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes.\nThese findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google’s frontier language model. Finally, we find early positive results using AlphaEvolve [novikov2025alphaevolve] to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.\n\n\nkeywords: Activation Probing, Interpretability, Language Models, Misuse Risk, AI Safety, Monitoring\n\n\n\n1 Introduction\n\nIn this paper, we describe our experience applying probes to detect cyber-offensive prompts given as input to Gemini 2.5 Flash [flash]. We describe the challenges we faced and the solutions we arrived at as a case study for other frontier language model developers wishing to deploy probes as a misuse mitigation in production.\n\n\nWhat do we mean by a misue mitigation? Misuse mitigations are techniques that prevent malicious users from performing cyber-offensive, CBRN111Chemical, Biological, Radiological, and Nuclear and other similar attacks using frontier large language models [googledeepmind_fsf_v3_2025, anthropic_rsp_v2_2_2025, openai_preparedness_framework_v2_2025]. This risk is not theoretical: frontier models can already significantly increase the abilities of malicious users to perform these attacks [Anthropic2025AIOrchestratedCyberEspionage], and misuse mitigations have already been deployed by frontier AI companies to decrease this risk [anthropic2025claude, deepthink].222Anthropic: “Our ASL-3 capability threshold for CBRN (Chemical, Biological, Radiological, and Nuclear)\nweapons measures the ability to significantly help individuals or groups with basic\ntechnical backgrounds (e.g. undergraduate STEM degrees) to create/obtain and deploy\nCBRN weapons[…] we were unable to\nrule out the need for ASL-3 safeguards.”. Google: “Gemini 2.5 Deep Think continues the trend of increased model capabilities — it generates detailed technical knowledge of CBRN domains. It provides uplift in some stages of some harm journeys.” One might hope that a sufficient mitigation would be to train LLMs to reject harmful queries, but unfortunately current training techniques are not robust enough [nasr2025attacker].\n\n\nWe thus focus on monitoring in this paper: additional deployment time systems that aim to detect harmful user requests. In this work, we only study input monitoring techniques, although we note that training probes on model outputs is an important future direction in Section˜7.\n\n\nIn the main text we focus entirely on detecting the harmful and critical domain of cyber-misuse [Anthropic2025AIOrchestratedCyberEspionage], although we expect many findings to transfer between mitigation domains. Cyber-offensive capabilities are a particularly worthwhile domain for studying defenses because harmful prompts are very similar to common and valuable defensive cybersecurity requests, and also somewhat similar to even more common coding requests. Therefore, it is particularly difficult to avoid over-triggering while also preventing harm in the cyber domain.\n\n\nRecent work shows that large language model (LLM) classifiers can be an effective monitor [sharma2025constitutional] for frontier models. But monitoring all interactions with another LLM can be extremely expensive, potentially doubling the cost if using a comparably capable model. A more cost-effective alternative is an activation probe: a small model trained on the internal hidden states of the model one plans to monitor. Activation probes are far cheaper than language model monitors because language model activations are already generated by the monitored model during a forward pass, so the only additional computational cost is running the probe itself. Indeed, [cunningham2025cheap, cunningham2026] investigate probes and find evidence showing that they are a promising and cost-effective technique for misuse and jailbreak detection.\n\n\nOverall, probes are indeed a cost-effective misuse mitigation, but there is a key problem: they are also fragile to distribution shifts. In particular, we find that probes trained on short-context data fail to generalize to important production distribution shifts, particularly long-context inputs. While training directly on long-context data is possible, we find it increases training costs by over an order of magnitude to achieve similar results due to memory bandwidth constraints (see Appendix˜I), which in turn would significantly increase the costs of AlphaEvolve (Section˜5.3), for example. Distribution shifts like jailbreaks and multi-turn data are not fully addressed by our work, and hence challenges building robust and cheap defenses remain.\n\n\nOur main technical contributionsare detailed investigation of four techniques that we investigate to address these distribution shift performance degradations. Below, we summarize these techniques and our main results from applying each one:\n\n\n\n\n1.\n\nImproving probe architecture: it is a difficult engineering challenge to train probes on long-context data, even with strong infrastructure (Appendix˜I). Yet it is highly important that probes are performant on long-context data. We introduce a new probe architecture family, MultiMax, that has higher classifier accuracy on long context prompts compared to other probe architectures. In Section˜5.1, we show that MultiMax improves long context generalization compared to existing baselines such as mean-aggregated MLP probes (Figure˜4).\n\n\n\n2.\n\nAutomated probe architecture search: In Section˜5.3, we also present results from an automated architecture search using AlphaEvolve [novikov2025alphaevolve], which discovers a probe that outperforms other baselines (bootstrap probability &gt;0.95&gt;0.95, Section˜5.1.1).\n\n\n\n3.\n\nUsing a cascading classifier: Large language models are inherently more robust to distribution shifts due to their generality, but are expensive to run. In Section˜5.2 we show that by only using the LLM when the probe is uncertain, we can create stronger classifiers than using either the probe or the LLM on its own, at a tiny fraction of the LLM’s inference cost. For example, a probe-Flash cascade achieves lower test "
  },
  {
    "title": "ShapeR: Robust Conditional 3D Shape Generation from Casual Captures",
    "url": "https://arxiv.org/abs/2601.11514v1",
    "source": "arxiv",
    "summary": "Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D de",
    "full_text": null
  },
  {
    "title": "MetaboNet: The Largest Publicly Available Consolidated Dataset for Type 1 Diabetes Management",
    "url": "https://arxiv.org/abs/2601.11505v1",
    "source": "arxiv",
    "summary": "Progress in Type 1 Diabetes (T1D) algorithm development is limited by the fragmentation and lack of standardization across existing T1D management datasets. Current datasets differ substantially in structure and are time-consuming to access and process, which impedes data integration and reduces the comparability and generalizability of algorithmic developments. This work aims to establish a unifi",
    "full_text": null
  },
  {
    "title": "QUPID: A Partitioned Quantum Neural Network for Anomaly Detection in Smart Grid",
    "url": "https://arxiv.org/abs/2601.11500v1",
    "source": "arxiv",
    "summary": "Smart grid infrastructures have revolutionized energy distribution, but their day-to-day operations require robust anomaly detection methods to counter risks associated with cyber-physical threats and system faults potentially caused by natural disasters, equipment malfunctions, and cyber attacks. Conventional machine learning (ML) models are effective in several domains, yet they struggle to repr",
    "full_text": null
  },
  {
    "title": "On the Probability of First Success in Differential Evolution: Hazard Identities and Tail Bounds",
    "url": "https://arxiv.org/abs/2601.11499v1",
    "source": "arxiv",
    "summary": "We study first-hitting times in Differential Evolution (DE) through a conditional hazard frame work. Instead of analyzing convergence via Markov-chain transition kernels or drift arguments, we ex press the survival probability of a measurable target set $A$ as a product of conditional first-hit probabilities (hazards) $p_t=\\Prob(E_t\\mid\\mathcal F_{t-1})$. This yields distribution-free identities f",
    "full_text": null
  },
  {
    "title": "The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents",
    "url": "https://arxiv.org/abs/2601.11496v1",
    "source": "arxiv",
    "summary": "The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increas",
    "full_text": "\n\n\n\n1 Introduction\n2 Study Design and Experimental Framework\n\n3 Results\n\nThe Poisoned Apple Effect\nSystemic Vulnerability\nRegulatory Objectives and Stability\n\n\n4 Discussion\n\nA The GLEE Framework\n\n\nA.1 Game Families: A Taxonomy of Core Economic Interactions\n\n\nA.1.1 Bargaining (Resource Division)\n\nStrategic Dynamics and Implications:\n\n\n\nA.1.2 Negotiation (Bilateral Trade)\n\nStrategic Dynamics and Implications:\n\n\n\nA.1.3 Persuasion (Strategic Information Transmission)\n\nStrategic Dynamics and Implications:\n\n\n\n\n\nA.2 Markets and Parameters: The Architecture of Interaction\n\nA.2.1 Parameters Defining Markets by Family\n\n\n\nA.3 Regulatory Metrics: Fairness and Efficiency\n\nEfficiency (Welfare):\nFairness (Equity):\n\n\nA.4 Language Models and Collected Data: The Strategy Space\n\nA.5 Payoff Estimation via Linear Regression\n\nModel Specification:\n\n\n\n\n\nB The Meta Game\n\nCalculating Game Matrices\nFinding Equilibrium\nRegulatory Optimization\n\n\n\nC The Poisoned Apple Effect\n\nPhase 1: The Status Quo (Pre-Release).\nPhase 2: The Toxic Release (Intermediate State).\nPhase 3: Regulatory Flight and Payoff Reversal (Post-Release).\n\n\n\n\n\n\n\nThe Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents\n\n\nEilam Shapira\n\n\n\n\nMoshe Tennenholtz\n\n\n\n\nRoi Reichart\n\n\n\n\n(January 2026)\n\nAbstract\nThe integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the ”Poisoned Apple” effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator’s choice of market design in their favor. This strategic release improves the releaser’s welfare at the expense of their opponent and the regulator’s fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.\n\n\n\n1 Introduction\n\nThe rapid integration of AI agents into the global economy is fundamentally altering the landscape of strategic interaction. In the near future, a substantial fraction of economic activities — ranging from real estate transactions to corporate partnerships — will be mediated by AI delegates acting on behalf of individuals and firms [2]. While current regulatory debates focus on model safety and bias, we identify a critical, overlooked economic vulnerability [12] arising from the mere availability of these technologies.\n\n\nWe investigate the strategic implications of expanding the set of available AI technologies within regulated markets. Specifically, we identify a phenomenon we term the Poisoned Apple effect. In this scenario, a strategic actor releases a new technology not to use it, but to manipulate the regulator’s calculations of market design. By introducing a ”poisoned” option, an agent can force a fairness-maximizing regulator to shift the market equilibrium in a way that benefits the releaser at the expense of their opponent.\n\n\nOur analysis models this interaction as a meta-game [4] involving two economic agents (”Alice” and ”Bob”) and a regulator. In this game, the regulator first determines the rules of interaction, while the participants choose among the available technologies. The utility of each participant is determined by the rules of interaction and the technologies selected by both participants. While the regulator intervenes to maximize social objectives like fairness, the agents strategically select AI representatives to maximize their own utility. We demonstrate that static regulatory frameworks are highly vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.\n\n\n\nFigure 1: Illustration of the “poisoned apple” example, in which Alice increases her payoff at Bob’s expense by releasing a new technology—without the players actually using that technology in practice. (1) The technologies available to Alice and Bob are language models A–D.\n(2a) For each possible market, the equilibrium in games between Alice and Bob under the market conditions is computed. For each equilibrium, the average fairness value that would be obtained if the equilibrium were played is calculated. (2b) The regulator, whose objective is to maximize fairness, decides that Market 4 will be the market in which Alice and Bob will play—the market that yields the maximum fairness value. Alice earns 0.49, Bob earns 0.50, and the fairness value is 1.00. (3) Technology E is released and is now available to both players. (4a) The process performed in 2a is repeated.\n(4b) In the new equilibrium in Market 4, the resulting fairness value is 0.976. In the new equilibrium in Market 8, the resulting fairness value is 0.99. The regulator decides that Market 8 will be the market in which Alice and Bob will play. Alice earns 0.52, Bob earns 0.46.\n\n\n\n\n2 Study Design and Experimental Framework\n\nTo analyze these dynamics empirically, we utilize the GLEE dataset [11].\nGLEE facilitates large-scale simulation of language-based economic environments across a combinatorial space of 1,320 distinct configurations. The dataset comprises over 580,000 strategic decisions generated by 13 state-of-the-art LLMs, treated as simulated economic agents [3, 10], across three canonical non-cooperative game families that capture fundamental economic interactions:\n(1) Bargaining: An alternating-offers game where players must agree on splitting a surplus or receive zero [9];\n(2) Negotiation: A bilateral trade setting involving private information between a buyer and seller [7];\nand (3) Persuasion: A sender-receiver game where a seller attempts to convince a buyer to purchase based on strategic information transmission [1, 5].\nWe classify these interactions into distinct Markets, where each market mm is defined by a specific structural configuration of three parameters: Information Structure, Communication Form, and Game Horizon. This classification yields a set of fundamental environments that span the critical dimensions of economic interaction.\n\n\nIn our meta-game model, agents (Alice and Bob) review the performance matrix of available AI delegates and simultaneously select the representative that maximizes their expected utility. We solve for the Nash Equilibrium [8] – a state where neither agent has an incentive to switch technologies solely given the other’s choice. The regulator then evaluates the outcome based on Efficiency (total social welfare) or Fairness (minimizing the disparity between agents’ payoffs) [6].\n\n\nThe core of our analysis involves a technology expansion. We first establish a baseline equilibrium with a subset of NN technologies (2≤N&lt;132\\leq N&lt;13) available in the GLEE dataset. The regulator first selects the market environment that maximizes its objective (e.g., fairness). We then expand the set of available technologies to the agents’ choice set by adding one of the other technologies from the GLEE dataset. In this newly generated meta-game, a new equilibrium may emerge for any given market selection; consequently, the regulator selects a potentially new market, optimizing its objective based on this new equilibrium. This allows us to isolate the specific economic impact of making a new technology available, regardless of whether it was ultimately adopted by the agents.\n\n\n\n\n3 Results\n\nThe Poisoned Apple Effect\n\nOur most striking finding is a strategic phenomenon we term the Poisoned Apple effect, where an agent releases a new technology not to use it, but to manipulate the regulator’s market design. Consider a representative Bargaining meta-game (Figure 1) where, initially, the agents have access to technologies A−DA-D. In this setting, the regulator selects the market that maximizes fairness (Market 4), yielding expected payoffs of 0.49 for Alice and 0.50 for Bob.\n\n\nAlice then releases a new technology (EE). Crucially, if the regulator were to maintain the original market design, Alice would adopt strategy EE in the new equilibrium, causing fairness to drop significantly (from 1.00 to 0.976). To minimize this harm, the regulator is forced to migrate to a new market environment (Market 8) where fairness is relatively higher (0.990).\n\n\nIn this new market, the equilibrium strategies do not involve using EE. However, the forced market shift dramatically alters the payoff distribution: Alice’s payoff jumps to 0.52 while Bob’s drops to 0.46. Thus, Alice successfully leverages the threat of using a ”poisoned” technology to coerce the regulator into a favorable market design, improving her welfare at Bob’s expense without ever actually deploying the model.\n\n\n\nSystemic Vulnerability\n\nThis manipulation is not an isolated anomaly. Across more than 50,000 simulated meta-games, we observe a recurrent pattern where expanding the choice set causes payoffs to move in opposite directions—one player benefits while the other is harmed (Fig. 2A). Strikingly, in approximately one-third of these zero-sum shifts, the outcome reversal occurs even though the new technology remains unused by either player in the final equilibrium (Fig. 2B). This confirms that open-weight releases or API availability can serve as strategic weapons for regulatory arbitrage.\n\n\n\nRegulatory Objectives and Stability\n\nBroader analysis reveals that the impact of such expansions depends heavily on the regulator’s goal. While technol"
  },
  {
    "title": "BoxMind: Closed-loop AI strategy optimization for elite boxing validated in the 2024 Olympics",
    "url": "https://arxiv.org/abs/2601.11492v1",
    "source": "arxiv",
    "summary": "Competitive sports require sophisticated tactical analysis, yet combat disciplines like boxing remain underdeveloped in AI-driven analytics due to the complexity of action dynamics and the lack of structured tactical representations. To address this, we present BoxMind, a closed-loop AI expert system validated in elite boxing competition. By defining atomic punch events with precise temporal bound",
    "full_text": null
  },
  {
    "title": "Extractive summarization on a CMOS Ising machine",
    "url": "https://arxiv.org/abs/2601.11491v1",
    "source": "arxiv",
    "summary": "Extractive summarization (ES) aims to generate a concise summary by selecting a subset of sentences from a document while maximizing relevance and minimizing redundancy. Although modern ES systems achieve high accuracy using powerful neural models, their deployment typically relies on CPU or GPU infrastructures that are energy-intensive and poorly suited for real-time inference in resource-constra",
    "full_text": null
  },
  {
    "title": "CTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation",
    "url": "https://arxiv.org/abs/2601.11488v1",
    "source": "arxiv",
    "summary": "In the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clin",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methodology\n\n3.1 Writing Style Generalizability Test (WSG)\n3.2 Synthetic Error Injection Test (SEI)\n3.3 Metrics-vs-Expert Correlation Test (MvE)\n\n\n\n4 Experiments and Results\n\n4.1 Dataset and Training Details\n4.2 Analyzing Writing Style Generalizability Test (WSG)\n4.3 Analyzing Synthetic Error Injection Test (SEI)\n4.4 Analyzing Metrics-vs-Expert Correlation Test (MvE)\n\n\n5 Discussion and Concluding Remarks\n6 Compliance with ethical standards\n\n\n\n\n\n\nCTest-Metric: A Unified Framework to Assess Clinical Validity of Metrics for CT Report Generation\n\nIn the generative AI era, where even critical medical tasks are increasingly automated, radiology report generation (RRG) continues to rely on suboptimal metrics for quality assessment. Developing domain-specific metrics has therefore been an active area of research, yet it remains challenging due to the lack of a unified, well-defined framework to assess their robustness and applicability in clinical contexts. To address this, we present CTest-Metric, a first unified metric assessment framework with three modules determining the clinical feasibility of metrics for CT RRG. The modules test: (i) Writing Style Generalizability (WSG) via LLM‑based rephrasing; (ii) Synthetic Error Injection (SEI) at graded severities; and (iii) Metrics‑vs‑Expert correlation (MvE) using clinician ratings on 175 “disagreement” cases. Eight widely used metrics (BLEU, ROUGE, METEOR, BERTScore‑F1, F1‑RadGraph, RaTEScore, GREEN Score, CRG) are studied across seven LLMs built on a CT‑CLIP encoder. Using our novel framework, we found that lexical NLG metrics are highly sensitive to stylistic variations; GREEN Score aligns best with expert judgments (Spearman 0.70), while CRG shows negative correlation; and BERTScore‑F1 is least sensitive to factual error injection.\nWe will release the framework, code, and allowable portion of the anonymized evaluation data (rephrased/error-injected CT reports), to facilitate reproducible benchmarking and future metric development.\n\n\n\n1 Introduction\n\nFig. 1: (a) The propsoed framework, CTest-Metric, comprises three modules: (i) Writing Style Generalizability Test (WSG); (ii) Synthetic Error Injection Test (SEI); and (iii) Metrics‑vs‑Expert Correlation Test (MvE). (b) Sample reports are given.\n\n\nWith recent breakthroughs in Large Language Models (LLMs) for automated radiology report generation (RRG) [xin2025med3dvlm, di2025ct], a crucial question arises: Do existing metrics capture what actually matters to trace the clinical efficacy and acceptability of LLM-generated reports? A clinically grounded metric must be insensitive to report writing styles and sensitive to subtle yet critical factual mismatches. It should also identify synonymous medical terminologies, and be robust enough to capture overfitting in AI-based generative models before deploying them for clinical use.\n\n\nDespite the availability of a large number of metrics, most are developed for general-domain text [papineni2002bleu, lin2004rouge, banerjee2005meteor], and even a few domain-specific metrics overlook deeper clinical relevance or ignore synonymous terminologies. Therefore, the existing RRG methods [di2025ct, shi2024med] continue to rely on metrics that are not reflective of clinical fidelity. Consequently, the designing of radiology-specific metrics has been an active area of research with the aim of evaluating both clinical aspects and linguistic similarity between generated radiology reports and the ground truth. However, due to the absence of any standardized tool and well-defined criteria to test these metrics, RRG tasks still rely on inconsistent metrics, resulting in misleading model selections. This underscores the need for a system that can serve as a well-defined framework for metrics developers to assess the clinical applicability of the metrics.\n\n\nFew prior studies [banerjee2024rexamine, yu2023evaluating] have addressed this problem; however, their focus has predominantly been on the X-ray RRG. Since X-ray images are 2D scans, the corresponding reports are limited to a specific anatomical context and span shorter sentences. In contrast, 3D CT scans capture volumetric multi-slice information, resulting in semantically denser narratives and a broader vocabulary that includes richer anatomical detail, diverse medical terminology, lesion descriptions, and measurements. Most existing clinical-efficacy (CE) metrics were originally tailored to X-ray-based vocabulary and thus struggle to capture complex and diverse terminologies present in CT reports. Despite this limitation, CT-based studies still use these metrics for reporting results and model comparison. Therefore, CT RRG requires special attention in terms of both designing appropriate metrics and developing a tool to assess their applicability and feasibility.\n\n\nIn this paper, we develop CTest-Metric, a framework for evaluating the extent to which a given metric satisfies the criteria for being clinically grounded. This assessment is conducted on eight benchmarking metrics that have been extensively used in recent CT report generation studies, ensuring consistency with established evaluation practices. The proposed framework includes three analytical modules: a) Writing Style Generalizability Test (WSG) examines the metrics’ generalizability across different writing styles, b) Synthetic Error Injection Test (SEI) introduces factual errors in the reports at three different levels and investigates their impact on the metrics’ outcomes, and c) Metrics-vs-Expert Correlation Test (MvE) obtains expert ratings for reports exhibiting disagreement among the metrics. Further, a correlation is established between the eight metrics and expert ratings for a comprehensive study. By leveraging reports generated using seven different LLMs in conjunction with expert assessment, the proposed framework presents a robust pathway that metrics developers can utilize when designing new metrics.\nThe paper’s contributions are summarized below:\n\n\n•\n\nFirst framework to assess metrics for CT RRG: We developed CTest-Metric, a novel unified framework to assess metrics for CT RRG. It investigates eight benchmarking metrics on CT reports generated by seven different LLMs and analyzes the behavior of both NLG (text-based) and CE metrics for CT report evaluation.\n\n\n\n•\n\nExpert assessment: Our study selected 175 cases across predictions from seven LLMs where the metrics showed conflicting assessments. These specific reports were then reviewed by clinical experts, and correlations were derived between expert ratings and the metrics’ scores.\n\n\n\n•\n\nAnalyzed the impact of stylistic variations and graded synthetic errors: We introduced stylistic variations and factual errors at three severity levels in the CT reports. We quantified the effect of these changes on metric sensitivity.\n\n\n\n\n\n\n\n2 Related Work\n\nThe RRG literature reveals that various NLG and CE metrics are commonly used to assess predicted CT reports. The earlier studies primarily relied on NLG metrics, including BLEU [papineni2002bleu], ROUGE [lin2004rouge], METEOR [banerjee2005meteor], and BERTScore-F1 [zhang2019bertscore]. These metrics quantify textual similarity, for example, BLEU measures n-gram overlap, ROUGE emphasizes sequence-level recall, and METEOR incorporates synonym matches between the generated and the ground truth report. Similarly, BERTScore-F1 computes similarity using contextual embeddings. To validate the clinical context, CE metrics including F1-RadGraph [jain2021radgraph] and CheXpert [irvin2019chexpert] were introduced. While the former extracts entities and relations from the given reports and measures graph-level scores, the latter adopts a rule-based labeler for 14 chest X-ray findings. More recently, RaTEScore [zhao2024ratescore], GREEN Score [ostmeier2024green], and CRG [hamamci2025crg] were introduced. The RaTEScore compares reports on the entity embedding level and the GREEN score uses regular expressions to parse error counts from their pre-trained model output. Unlike other score, the CRG balances penalities based on label distribution in the reports and ignores clinically irrelevant true negatives.\n\n\nFor the X-ray RRG tasks, prior works [wang2023r2gengpt, chen2025dia] followed a similar trend, relying on the same set of standard metrics including the four BLEU n-gram levels, METEOR, ROUGE, and CheXpert. A largely identical evaluation strategy is also observed in CT RRG, where most studies [xin2025med3dvlm, di2025ct] adopt a similar set of NLG and CE metrics, despite the latter being X-ray-oriented and thus, do not fully capture the semantic and anatomical complexity of CT reporting. Although the literature lacks any unified framework for evaluating CT RRG metrics, some X-ray-focused works such as Yu et al.[yu2023evaluating] and Banerjee et al. [banerjee2024rexamine] highlighted that even advanced metrics can be inconsistent and poorly correlated with expert ratings.\n\n\n\n\n3 Methodology\n\nOverview. Given a 3D CT scan i∈Ii\\in I with corresponding ground truth report r∈Rr\\in R, we employed seven report generation models to generate CT report pij∈Pp^{j}_{i}\\in P, where j=1,2,..,7j={1,2,..,7}. The seven deep learning models adopt a CT-CLIP [hamamci2024foundation] image encoder to extract image features combined with seven different LLMs, including variants of GPT (Distilgpt, GPT2, GPT2-Medium, LLaMA-3.2-1B) and LLaMA (LLaMA-3.2-1B, LLaMA-2-7b-chat-hf), with a biomedical-domain LLM (BioGPT-Large). These variants are paired with two configurations: LLM fine-tuning111https://github.com/fkodom/clip-text-decoder and a frozen-LLM setup (R2GenGPT’s shallow alignment [wang2023r2gengpt]).\n\n\nThe predictions obtained from each model are assessed using a set of eight metrics MM, where MM= {BLEU, ROUGE, METEOR, BERTScore-F1, F1-RadGraph, RaTEScore, GREEN Score, CRG}. The first four metrics are NLG-based whereas the last four are designed for clinical-efficacy check. Th"
  },
  {
    "title": "Health Facility Location in Ethiopia: Leveraging LLMs to Integrate Expert Knowledge into Algorithmic Planning",
    "url": "https://arxiv.org/abs/2601.11479v1",
    "source": "arxiv",
    "summary": "Ethiopia's Ministry of Health is upgrading health posts to improve access to essential services, particularly in rural areas. Limited resources, however, require careful prioritization of which facilities to upgrade to maximize population coverage while accounting for diverse expert and stakeholder preferences. In collaboration with the Ethiopian Public Health Institute and Ministry of Health, we ",
    "full_text": null
  },
  {
    "title": "A Probabilistic Approach to Trajectory-Based Optimal Experimental Design",
    "url": "https://arxiv.org/abs/2601.11473v1",
    "source": "arxiv",
    "summary": "We present a novel probabilistic approach for optimal path experimental design. In this approach a discrete path optimization problem is defined on a static navigation mesh, and trajectories are modeled as random variables governed by a parametric Markov policy. The discrete path optimization problem is then replaced with an equivalent stochastic optimization problem over the policy parameters, re",
    "full_text": null
  },
  {
    "title": "Low-Rank Key Value Attention",
    "url": "https://arxiv.org/abs/2601.11471v1",
    "source": "arxiv",
    "summary": "Transformer pretraining is increasingly constrained by memory and compute requirements, with the key-value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding. We propose \\textit{low-rank KV adaptation} (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-l",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nAnalyzing attention head diversity.\n\n\n3 Methodology\n\n4 Shared Subspaces, Spectral Bias, and Information Structure\n\nShared subspace and spectral structure.\nResidual rank as a control knob for diversity.\nDecomposition geometry and (non-)orthogonality.\nConnection to low-rank approximation.\nScaling, normalization, and stability.\n\n\n\n5 Experimental Setup\n\nPretraining Configuration.\nMid-training Configuration.\nAttention Mechanism Configurations.\n\n\n\n6 Results\n\n\n6.1 Pretraining Results\n\nFinal pretraining performance.\nTraining efficiency analysis.\nFLOPs-normalized pretraining curves.\n\n\n6.2 Downstream Task Performance\n\n6.3 Why Low-Rank Key-Value Attention Works\n\nAnalysis setup.\nRank selection determines capacity and performance.\nLRKV preserves functional head diversity.\nPCA-based analysis reveals compensation mechanisms.\nMagnitude scaling and post-projection normalization.\nSummary: capacity, optimality, and emergent geometry.\n\n\n\n\n\n7 Discussion\n\nLimitations.\nFuture work.\n\n\n8 Conclusion\n\nA Additional Experimental Details\n\nBatching and Optimization.\n\n\n\nB Memory-Computation Trade-offs in Attention Mechanisms\n\nThe Compression Paradigm: Lossy vs. Lossless.\nQuantitative Analysis: Memory vs. Latency.\n\n\n\nC Principled Head Diversity Analysis\nvia PCA in Bilinear Form Space\n\n\nC.1 Methodology: PCA in the Space of Bilinear Forms\n\nGauge invariance motivation.\nInner product on bilinear forms.\nCentering for proper PCA.\nInterpretation as variance decomposition.\nComparison to uncentered analysis.\n\n\nC.2 Complete Results Across Scales\nC.3 Effective Rank at 128M Scale\nC.4 PCA Eigenvalue Spectra\nC.5 Cumulative Variance Explained\n\nC.6 Comparison: Uncentered vs PCA-based Effective Rank\n\nThe MQA compensation effect (quantified).\nLRKV’s consistency across metrics.\nScale-dependent centering effects.\n\n\nC.7 Eigenvalue Spectra of Uncentered Similarity Matrices\nC.8 Head Similarity Heatmaps\n\nC.9 Key Findings from PCA Analysis\n\n1. LRKV preserves head independence within 1% of standard attention.\n2. The MQA compensation effect.\n3. MLA sits between GQA and MQA in diversity space.\n4. Scale-dependent behavior.\n5. Rank selection determines capacity.\n\n\n\nC.10 Comparison to Prior Diversity Metrics\n\nCKA/SVCCA (Kornblith et al., 2019; Raghu et al., 2017).\nAttention pattern similarity (Michel et al., 2019; Voita et al., 2019).\nRaw weight comparison.\nUncentered Gram matrices.\n\n\n\nC.11 Methodological Limitations and Extensions\n\nLimitations.\nFuture extensions.\n\n\n\n\n\n\n\n\n\nLow-Rank Key Value Attention\n\n\n\n\nJames O’Neill†  Robert Clancy  Mariia Matskevichus  Fergal Reid\n\n\n\nAI Group, Intercom\n\n\n\n124 St Stephen’s Green, Dublin 2, D02 C628, Ireland\n\n\n\n{james.oneill, rob.clancy, mariia.matskevichus, fergal.reid}@intercom.io\n\n\n\n\nAbstract\nTransformer pretraining is increasingly constrained by memory and compute requirements, with the key–value (KV) cache emerging as a dominant bottleneck during training and autoregressive decoding.\nWe propose low-rank KV adaptation (LRKV), a simple modification of multi-head attention that reduces KV cache memory by exploiting redundancy across attention heads while preserving full token-level resolution.\nEach layer uses a shared full-rank KV projection augmented with low-rank, head-specific residuals, yielding a continuous trade-off between complete sharing and fully independent attention.\nLRKV is a drop-in replacement for standard multi-head attention and directly subsumes query-sharing approaches such as multi-query and grouped-query attention, while remaining distinct from latent-compression methods such as multi-latent attention (MLA). Across large-scale pretraining experiments, LRKV consistently achieves faster loss reduction, lower validation perplexity, and stronger downstream task performance than standard attention, MQA/GQA, and MLA. At the 2.5B scale, LRKV outperforms standard attention while using roughly half the KV cache, and reaches equivalent model quality with up to 20–25% less training compute when measured in cumulative FLOPs. To explain these gains, we analyze attention head structure in operator space and show that LRKV preserves nearly all functional head diversity relative to standard attention, whereas more aggressive KV-sharing mechanisms rely on compensatory query specialization. Together, these results establish LRKV as a practical and effective attention mechanism for scaling Transformer pretraining under memory- and compute-constrained regimes.\n\n\n\n1 Introduction\n\nTransformers are the dominant architecture for large-scale sequence modeling in language, vision, and multimodal domains (Vaswani et al., 2017; OpenAI, 2023), but\nas their size, sequence length, and context window grow, so does, rapidly, their computational and memory costs.\nKV-caching, which stores the atttention key and value representations, is a primary contributor to this overhead, as it spans every attention layer, and scales linearly with sequence length and count of heads. The cumulative KV footprint of modern models with tens of billion parameters can exceed the parameter memory itself, especially for long-context inference (Dao et al., 2024; Child et al., 2019).\n\n\nA range of approaches have been proposed to alleviate the growing KV cache cost. Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023) reduce memory and latency by sharing key-value (KV) projections across heads or groups of heads, and are now standard in large-scale models such as PaLM and LLaMA (Touvron et al., 2023; 2024).\nHowever, aggressive KV sharing introduces a fundamental trade-off: while memory is reduced, head-level representational diversity is constrained, even though distinct attention heads are known to encode complementary syntactic and semantic patterns\n(Clark et al., 2019; Michel et al., 2019).\n\n\nAt the same time, empirical studies and recent spectral analyses show that attention heads are not fully independent: head-specific KV projections are highly correlated and occupy overlapping subspaces, indicating substantial redundancy (Yunis et al., 2024).\nCrucially, this redundancy is structured rather than uniform—small, head-specific variations remain important for capturing nuanced dependencies.\nThis raises a natural question: can KV memory be reduced by exploiting redundancy across heads, without collapsing the specialization that makes multi-head attention effective?\n\n\nFigure 1: Comparison of attention mechanisms and their KV cache costs. (a) Multi-Head Attention (MHA) uses independent K/V projections per head with cache cost 2​L​H​dh2LHd_{h}. (b) Multi-Latent Attention compresses inputs to latent dimension dcd_{c} before per-head projections (cache: L​dcLd_{c} where dc≪dd_{c}\\ll d). (c) Multi-Query Attention (MQA) shares a single K/V across all heads (cache: 2​L​dh2Ld_{h}) but suffers from low diversity. (d) Grouped-Query Attention (GQA) shares K/V within groups of heads (cache: 2​L​(H/G)​dh2L(H/G)d_{h}), balancing efficiency and diversity. (e) Our proposed LR-KV method maintains shared full-rank WK,WVW_{K},W_{V} projections while each head adds a low-rank residual Uh​(Bh)⊤U_{h}(B_{h})^{\\top} (rank r≪dhr\\ll d_{h}). We cache both the shared features and per-head latents Rh=X​UhR_{h}=XU_{h}, achieving cache cost 2​L​(dh+H​r)2L(d_{h}+Hr). Dashed boxes indicate cached components. LR-KV provides head diversity while maintaining cache efficiency comparable to MLA.\n\n\nBeyond KV sharing, efficiency research has pursued complementary directions such as sparse or kernelized attention(Wang et al., 2020; Beltagy et al., 2020; Child et al., 2019), architectural optimizations(Dao et al., 2024) , and latent compression methods including Multi-Latent Attention (MLA)(Liu et al., 2024a). However, none of these approaches explicitly resolve the duplication of per-head key and value representations that dominates the KV memory footprint in large models.\n\n\nIn this work, we propose Low-Rank KV Adaptation (LRKV), a simple and effective modification of multi-head attention that directly addresses this tension by exploiting structured redundancy across heads while preserving head specialization.\nEach Transformer layer maintains a single shared, full-rank key and value projection serving as a global basis across heads, while each head learns a compact, trainable low-rank residual that perturbs the shared representations.\nThe shared component encodes global relational structure, and the low-rank residuals restore the localized specialization that independent heads would otherwise provide. This factorization significantly reduces KV memory while retaining the head-level diversity critical for expressivity.\n\n\nUnlike prior KV-sharing mechanisms, LRKV preserves a full-rank shared base to maintain representational capacity and learns per-head residuals jointly during pretraining.\nThis provides a continuous spectrum between complete sharing (as in MQA) and full independence (as in standard attention), controlled by the residual rank parameter.\nOur experiments demonstrate that LRKV achieves 4040–50%50\\% reductions in KV cache size with negligible impact on convergence, perplexity, or downstream performance.\nBy aligning Transformer attention with its underlying spectral and informational structure, LRKV provides a practical foundation for scaling pretraining under memory-constrained regimes.\n\n\nThe design of LRKV differs from previous KV-sharing methods in two respects. First, it retains a full-rank shared component rather than constraining all heads to use the same low-rank representation, which maintains capacity for representing high-dimensional dependencies. Second, the low-rank residuals are trained jointly with the shared projection during pretraining, allowing the model to learn how much variation each head requires. This formulation provides a continuous trade-off between the efficiency of shared KVs and the flexibility of independent projections.\n\n\nWe evaluate LRKV on large-scale language model pretraining and downst"
  },
  {
    "title": "Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs",
    "url": "https://arxiv.org/abs/2601.11468v1",
    "source": "arxiv",
    "summary": "Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generalit",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.11468v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.11468v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 16 Jan 2026]\n    Title:Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs\n    Authors:Alessandro Padella, Massimiliano de Leoni, Marlon Dumas            View a PDF of the paper titled Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs, by Alessandro Padella and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Predictive Process Monitoring is a branch of process mining that aims to predict the outcome of an ongoing process. Recently, it leveraged machine-and-deep learning architectures. In this paper, we extend our prior LLM-based Predictive Process Monitoring framework, which was initially focused on total time prediction via prompting. The extension consists of comprehensively evaluating its generality, semantic leverage, and reasoning mechanisms, also across multiple Key Performance Indicators. Empirical evaluations conducted on three distinct event logs and across the Key Performance Indicators of Total Time and Activity Occurrence prediction indicate that, in data-scarce settings with only 100 traces, the LLM surpasses the benchmark methods. Furthermore, the experiments also show that the LLM exploits both its embodied prior knowledge and the internal correlations among training traces. Finally, we examine the reasoning strategies employed by the model, demonstrating that the LLM does not merely replicate existing predictive methods but performs higher-order reasoning to generate the predictions.\n    \n\n    \n    \n              \n          Comments:\n          19 pages, 4 figure, TMIS journal submission\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Information Theory (cs.IT)\n        \n          Cite as:\n          arXiv:2601.11468 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.11468v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.11468\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Alessandro Padella [view email]          [v1]\n        Fri, 16 Jan 2026 17:54:55 UTC (1,311 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Exploring LLM Features in Predictive Process Monitoring for Small-Scale Event-Logs, by Alessandro Padella and 2 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.IT\n        math\n        math.IT\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n       "
  },
  {
    "title": "MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models",
    "url": "https://arxiv.org/abs/2601.11464v1",
    "source": "arxiv",
    "summary": "As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains la",
    "full_text": "\n\n\n\n1 Introduction\n\n2 MHA2MLA-VLM\n\n\n2.1 Multimodal Partial-RoPE\n\nFull Vanilla RoPE\nFull Multimodal RoPE\nMultimodal Adaptive Partial-RoPE Strategies\n\n\n\n2.2 Modality-Decoupled SVD (MD-SVD)\n\nUnimodal SVD Baselines\nModality-Decoupled SVD\nMotivation\n\n\n\n\n\n3 Experiment\n\nSetups\n3.1 Main Results\n3.2 MHA2MLA, Cache Pruning and Compression\n\n3.3 Ablation Study\n\nEffect of Modality-Decoupled SVD\nEffect of Two Stage Training\nEffect of Partial-RoPE strategies\n\n\n\n\n\n4 Analysis\n\n4.1 Empirical Validation for MD-SVD\n4.2 Comparison of 𝒮2-norm\\mathcal{S}_{\\text{2-norm}} and 𝒮MKL\\mathcal{S}_{\\text{MKL}}\n\n\n\n5 Related Work\n\nVision-Language Models\nEfficient Architectures\n\n\n6 Conclusion\n\nA Appendix\n\n\nA.1 Experimental Setups\n\nModels\nDataset\nPEFT Training Strategy\nHyperParameters\n\nEvaluation Setups\n\nBenchmarks\nModel-specific availability\nMain Experiments and Ablation Studies\n\n\n\n\n\n\n\n\n\n\n\nMHA2MLA-VLM: Enabling DeepSeek’s Economical Multi-Head Latent Attention across Vision-Language Models\n\n\n\nXiaoran Fan1,\nZhichao Sun111footnotemark: 1,\nTao Ji111footnotemark: 1,\nLixing Shen2,\nTao Gui1,3,4\n\n Equal contribution.\n\n\nAbstract\nAs vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference.\nWhile Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored.\nIn this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA.\nOur approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces.\nFurthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss.\nExtensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.\n\n\nCode &amp; Appendix — https://github.com/JT-Ushio/MHA2MLA-VLM\n\n\n\n1 Introduction\n\nThe Key-Value (KV) cache stores the complete contextual information required by large language models (LLMs), enabling efficient and accurate decoding of the current token.\nAs the tasks handled by LLMs become increasingly complex (e.g. multimodal tasks (Bordes et al. 2024) and deep thinking (Pan et al. 2025)), the context length correspondingly increases.\nThis results in a rapid expansion of the KV cache, which not only occupies large GPU memory but also leads to severe memory access bottlenecks due to the quadratic complexity of the standard attention mechanism (Keles et al. 2023).\nConsequently, efficient inference in LLMs, especially in vision-language models (VLMs) with multimodal contexts, urgently requires cost-effective KV cache management and attention architectures.\n\n\nA series of studies have identified redundancies in the KV cache (Li et al. 2025).\nIn terms of sequence length (Zhang et al. 2023b; Oren et al. 2024), KV cache pruning removes irrelevant tokens from the cache.\nRegarding representation precision (Badri and Shaji 2023), KV cache quantization reduces the precision of vector representations.\nIn the vector dimension, modifications such as Grouped/Multi-Query Attention (GQA and MQA) restructure the attention mechanism by enabling a single KV pair to be shared among a group of queries (Ainslie et al. 2023b; Shazeer 2019).\n\n\nDeepSeek introduced Multi-Head Latent Attention (MLA), an advanced attention mechanism employing low-rank key-value joint compression (DeepSeek-AI et al. 2024).\nEmpirical results show that MLA outperforms standard Multi-Head Attention (MHA, 2017) and its variants, while significantly reducing the KV cache size during inference, thereby enhancing inference efficiency.\nJi et al. (2025) proposed MHA2MLA, demonstrating that LLMs originally trained with MHA/GQA can be adapted to leverage MLA during inference.\nHowever, whether VLMs can undergo a similar transition to the MLA architecture remains an open question.\n\n\nFigure 1: \nThe overview process of converting VLMs from MHA/GQA to MLA using MHA2MLA-VLM. Our method makes the attention inputs match MLA exactly, and low rank compression of the KV cache is consistent with MLA. The modality-decoupled design reduces truncation loss and maximizes the parameter reuse of pretrained weights.\n\n\n\nMHA2MLA involves two key steps: partial-rope conversion and KV joint low-rank approximation.\nFor partial-rope, text-only LLMs have demonstrated that significantly reducing (e.g., -87.5%) less important rotary frequencies requires only minimal fine-tuning to recover performance.\nIn the case of VLMs, it is necessary to verify whether the retained rotary frequencies are equally effective for both image and text tokens.\nWe address this question and further extend the method to multimodal rope (e.g., used in Qwen2-VL series).\n\n\nFor KV joint low-rank approximation, inspired by SVDLLM V2 (Wang et al. 2025a), we improve the approximation from being applied to parameters (min​‖W−W′‖F\\min||W-W^{\\prime}||_{F}, where W′W^{\\prime} is the low-rank approximation) to output activations (min​‖X​W−X​W′‖F\\min||XW-XW^{\\prime}||_{F}).\nThis enhancement significantly reduces performance degradation and the amount of fine-tuning data required.\nMoreover, we observe that the low-rank spaces of image and text tokens are orthogonal, necessitating separate low-rank approximations for each modality.\n\n\nTo reduce the cost of MHA2MLA-VLM adaptation, we introduce parameter-efficient fine-tuning (PEFT, 2023).\nDuring the partial-rope phase, only the two projection matrices for query and key are fine-tuned, while all other parameters are frozen.\nFor the low-rank approximation phase, only the parameters within MLA are fine-tuned.\nIt reduces the time required by 59% (e.g., the MHA2MLA-VLM of Qwen2.5-VL is shortened from 22 hours to 9 hours).\nWe validate the effectiveness of MHA2MLA-VLM on three representative models: LLaVA-1.5 (2024a), LLaVA-NeXT (2024b), and Qwen2.5-VL (2025).\nFurthermore, using LLaVA-NeXT, we demonstrate that MLA outperforms the KV cache pruning baseline and integrates seamlessly with KV quantization.\n\n\nOur main contributions are:\n\n\n•\n\nWe successfully extend the MHA2MLA adaptation from text-only LLMs to VLMs, designing multimodal partial-rope and low-rank approximation algorithms.\n\n\n\n•\n\nBy incorporating SVDLLM V2’s minimization of output activation error and introducing PEFT, we significantly reduce the performance degradation and fine-tuning cost.\n\n\n\n•\n\nWe demonstrate the effectiveness of MHA2MLA-VLM in three main VLMs with distinct architectures and demonstrate that it integrates seamlessly with KV quantization.\n\n\n\n\n\n\n\n2 MHA2MLA-VLM\n\nFigure 1 provides an overview of MHA2MLA-VLM, we will describe the details of the two main components of MHA2MLA-VLM: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces.\n\n\n\n2.1 Multimodal Partial-RoPE\n\nTo enable efficient migration of MHA/GQA-based VLMs to MLA, we introduce Multimodal Adaptive Partial-RoPE, which adaptively retains the most informative rotary dimensions according to the nature of the input from different modalities, achieving efficient architecture migration.\n\n\nCurrent research on partial-RoPE has been limited to unimodal settings. For example, studies such as (Black et al. 2021; Barbero et al. 2025) trained partial-RoPE models from scratch, achieving slightly better perplexity compared to full-RoPE.\nMore recent work (Ji et al. 2025) has explored adapting pre-trained full-RoPE models to partial-RoPE without costly retraining. However, their analyzes are strictly limited to LLM.\nIn multimodal scenarios, for the input, visual and textual information is interleaved; For the VLMs’ forward calculation, visual and text information are jointly entangled in the RoPE dimension. Simply applying the text-based partial RoPE strategy leads to suboptimal allocation of the retention frequency subspace, as visual and textual information exhibit distinct dimensional characteristics are ignored.\nTo overcome these limitations, retain the most informative rotation dimension based on modality-awareness, thereby enabling low-cost and efficient architectural transfer from MHA/GQA to MLA.\n\n\nFull Vanilla RoPE\n\nis a mechanism (Su et al. 2024) for encoding positional information into queries and keys through frequency-specific rotations. Formally, given a query 𝒒i∈ℝdh\\bm{q}_{i}\\in\\mathbb{R}^{d_{h}} and a key 𝒌i∈ℝdh\\bm{k}_{i}\\in\\mathbb{R}^{d_{h}}, we split them into 2D chunks:\n\n\n\n𝒒i,𝒌i=[𝒒i[2​k,2​k+1]]0≤k&lt;dh2,[𝒌i[2​k,2​k+1]]0≤k&lt;dh2.\\bm{q}_{i},\\bm{k}_{i}=\\left[\\bm{q}_{i}^{[2k,2k+1]}\\right]_{0\\leq k&lt;\\frac{d_{h}}{2}},\\left[\\bm{k}_{i}^{[2k,2k+1]}\\right]_{0\\leq k&lt;\\frac{d_{h}}{2}}.\n\n\n\nFormally, for each 2D chunk 𝒒i[2​k,2​k+1]\\bm{q}_{i}^{[2k,2k+1]} and 𝒌i[2​k,2​k+1]\\bm{k}_{i}^{[2k,2k+1]}, the rotation matrix at position ii is defined as:\n\n\n\n𝑹i[2​k,2​k+1]​(θk)=[cos⁡(i​θk)−sin⁡(i​θk)sin⁡(i​θk)cos⁡(i​θk)],\\bm{R}_{i}^{[2k,2k+1]}(\\theta_{k})=\\begin{bmatrix}\\cos(i\\theta_{k})&amp;-\\sin(i\\theta_{k})\\\\\n\\sin(i\\theta_{k})&amp;\\cos(i\\theta_{k})\\end{bmatrix},\n\n\n\nwhere θk=β−2​k/dh\\theta_{k}=\\beta^{-2k/{d_{h}}} is the frequency of rotation applied to a specific kk-th pair of 𝒢d∈[0,dh2)\\mathcal{G}_{d}\\in[0,\\tfrac{d_{h}}{2}), and β\\beta is the frequency base wavelength.\nThe vanilla RoPE defines a "
  },
  {
    "title": "Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations",
    "url": "https://arxiv.org/abs/2601.11460v1",
    "source": "arxiv",
    "summary": "Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric r",
    "full_text": "\n\n\n\nI Introduction\nII Related Work\nIII Foundations\n\nIV Learning Semantic-Geometric Task Graph-Representations\n\n\nIV-A Semantic-Geometric Graph Structure\n\nIV-A1 Node Features\nIV-A2 Edge Features\nIV-A3 Global Features\n\n\n\nIV-B Graph Neural Network Architecture\n\nIV-B1 MPNN Encoder\nIV-B2 Prediction Decoder Models\n\n\nIV-C Training Long-Horizon Task Predictions\n\n\n\nV Experiments and Results\n\nV-A Baseline Architectures\nV-B Prediction Results on Human Demonstrations\nV-C Transfer of Task Representations to Bimanual Robot\nV-D Discussion\n\n\nVI Conclusion and Outlook\n\n\n\n\n\nLearning Semantic-Geometric Task Graph-Representations from Human Demonstrations\n\n\n\nFranziska Herbert1,2, Vignesh Prasad1,2,3, Han Liu1,2, Dorothea Koert4,5 and Georgia Chalvatzaki1,2,3\n1 Interactive Robot Perception &amp; Learning (PEARL) Lab, Computer Science Dept., TU Darmstadt, Germany. 2 Hessian.AI, Darmstadt, Germany. 3 Robotics Institute Germany (RIG). 4 Interactive AI Algorithms &amp; Cognitive Models for Human-AI Interaction (IKIDA), Computer Science Dept., TU Darmstadt, Germany. 5 Center for Cognitive Science, TU Darmstadt, Germany. Contact: franziska.herbert@tu-darmstadt.de\n\n\nAbstract\nLearning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic–geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic–geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.\n\n\n\nI Introduction\n\n\nLearning successful robotic task executions from human demonstrations requires understanding four key elements: what actions to perform, how to execute them, which objects to manipulate, and in what sequence. Particularly in bimanual settings where action order and object interactions can vary, learning structured task representations from human demonstrations is crucial for understanding long-horizon manipulation behaviors.\n\n\nScene Graphs [12, 1, 6, 11, 21] provide a structured representation for capturing semantic relationships and geometric information.\nThis representation is well-suited for manipulation tasks, where understanding task progression requires reasoning both about discrete action sequences and continuous geometric evolution of the scene.\nBy modeling tasks as temporal scene graphs, we can explicitly capture object interactions and action dependencies, information that is only implicitly represented in raw human demonstration videos.\n\n\nGraph Neural Networks [8, 23, 3] are naturally suited to learn from structured representations, enabling message passing to aggregate information about object interactions and relations across the graph.\nThis makes them a natural choice for modeling manipulation tasks, where understanding task progression requires reasoning about interactions between objects, hands, and their evolving spatial relationships.\nHowever, existing scene-graph-based approaches to task understanding [13, 17, 5] typically emphasize either semantic structure (e.g., action–object relations) or geometric evolution (e.g., motion trajectories), but rarely integrate both in a unified representation. As a result, these methods struggle to capture task progression in tasks that exhibit variability in action ordering, object usage, or interaction geometry.\n\n\nWe hypothesize that jointly modeling semantic relations and geometric evolution within a single task graph-representation leads to richer object-centric task abstractions. Such representations should enable predicting how a task evolves given the current state and interaction history, and support improved generalization to unseen demonstrations—particularly for tasks characterized by high action and object variation.\nTo this end, we introduce a semantic–geometric task graph-representation and a GNN-based encoder that jointly models object identities, inter-object relations, and their temporal geometric evolution, enabling the learning of temporal scene representations that capture object-centric relational dynamics over extended horizons.\n\n\nUnlike typical action recognition methods that predict a single label per frame [5, 25, 4], our objective is to learn task-level representations that jointly capture semantic action structure and geometric scene evolution; we explicitly capture action-conditioned relational dynamics over time, enabling reasoning about task progression for extended temporal horizons. To this end, we predict future action sequences together with their associated objects and individual object motions. This joint learning encourages the emergence of more general task embeddings and enables forecasting the future evolution of the scene—an ability that is critical for downstream decision-making and planning for manipulation.\n\n\nOur key contributions are:\n(1) a semantic–geometric task graph-representation that jointly encodes object identities, inter-object relations, and their temporal geometric evolution to represent long-horizon manipulation tasks from human demonstrations;\n(2) a graph-encoder-transformer-decoder architecture that learns task-level graph representations, capturing action-conditioned relational dynamics under variable action and object orderings;\n(3) a joint learning framework that couples future action, object, and motion prediction to shape task-level representations capable of forecasting task progression over extended horizons; and\n(4) a demonstration of transferability, showing that task graph-representations learned from human demonstrations can be reused on a physical bimanual robot for online action selection.\n\n\nFigure 1: Graph model architecture: the graph encoder transforms features into embeddings, MPNN learns graph embeddings, and prediction heads forecast actions, objects, and motions.\n\n\n\n\nII Related Work\n\n\nLearning manipulation task progression from human demonstrations requires representations that capture both semantic task structure and geometric scene evolution.\nPrior work on graph-based action and motion prediction has primarily focused on whole-body skeleton-based representations [24, 27, 4], some of which also make use of spatial-temporal GNNs [15, 29, 19]. While such representations have shown promising results for human activity analysis, tabletop manipulation, especially in robotic scenarios, semantic relationships between humans and manipulated objects, as well as task-dependent object-to-object interactions, are more relevant than skeletal joint configurations. In contrast, object-centric representations using scene graphs [12, 1, 6, 11, 21] provide a natural framework for capturing these relationships in manipulation contexts.\nSuch scene graph representations find applications in Human-Object Interaction research [16, 26, 22], however, they mainly focus on frame-level action recognition and segmentation in human videos and do not consider the future progression of a task.\n\n\nAlong similar lines, Dreher et al. [5] proposed using a scene graph with semantic spatial relations of salient objects and the human in table-top manipulation scenarios to perform GNN-based action prediction. However, like previous works, they only look at instantaneous frame-level predictions. For learning long-horizon plans, it is essential to understand how an action affects the scene, what trajectory to take, and what objects are salient.\nTo this end, Razali et al. [17] explore a geometric scene graph representation for learning whole-body motion prediction from human demonstrations. They learn fine-grained motion progressions of the scene using the geometric relations between different objects in an action-conditioned manner. While they incorporate geometric reasoning, they just predict the evolution of the scene and the fine-grained motion in a zero-shot manner without adapting to the observed scene.\nLagamtzis et al. [14] take a step further and explore simultaneous graph-based action recognition, future action prediction, and motion forecasting from human demonstrations. Their approach incorporates geometric information (object positions) to learn node embeddings for downstream predictions. However, their method neglects semantic edge features and global task context, focusing on single-step unimanual predictions at a fixed horizon. This limits capturing rich semantic relationships and reason about bimanual task progression over extended temporal spans.\n\n\nIn contrast, we introduce a semantic-geometric task graph-representation that addresses these limitations by jointly modeling three key aspects within a unified framework: (1) object identities and inter-object semantic relations (via edge "
  },
  {
    "title": "Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking",
    "url": "https://arxiv.org/abs/2601.11459v1",
    "source": "arxiv",
    "summary": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures thro",
    "full_text": null
  },
  {
    "title": "PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs",
    "url": "https://arxiv.org/abs/2601.11451v1",
    "source": "arxiv",
    "summary": "Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and char",
    "full_text": null
  },
  {
    "title": "IMS: Intelligent Hardware Monitoring System for Secure SoCs",
    "url": "https://arxiv.org/abs/2601.11447v1",
    "source": "arxiv",
    "summary": "In the modern Systems-on-Chip (SoC), the Advanced eXtensible Interface (AXI) protocol exhibits security vulnerabilities, enabling partial or complete denial-of-service (DoS) through protocol-violation attacks. The recent countermeasures lack a dedicated real-time protocol semantic analysis and evade protocol compliance checks. This paper tackles this AXI vulnerability issue and presents an intelli",
    "full_text": "\n\n\n\n\nI Introduction\n\nI-A SoC Denial of Service\nI-B Paper Contributions\n\n\n\nII Background &amp; Motivation\n\nII-A Threat Model\nII-B Hardware-Based Security Monitoring\nII-C Research Gap and Motivation\n\n\n\nIII IMS Design Methodology and Realization\n\nIII-A IMS Design Steps\nIII-B Dataset construction and Attack Synthesis\nIII-C Pre-processing Pipeline and Feature Engineering\nIII-D ML Model Architecture and Training\n\n\n\nIV Hardware Implementation, Optimization and Discussion\n\nIV-A IMS Implementation and Optimization\n\nIV-B IMS Detectability Analysis\n\nIV-B1 IMS Evaluation\nIV-B2 Attack-Specific Detection Evaluation\n\n\n\n\nV Related Work\nVI Conclusion\n\n\n\n\n\nIMS: Intelligent Hardware Monitoring System for Secure SoCs\n††thanks: * These authors contributed equally\nThis work has been partially funded by the German Federal Ministry of Research, Technology and Space (BMFTR) through the project RILKOSAN.\n††thanks: This paper has been accepted for publication at the Design, Automation &amp; Test in Europe Conference (DATE) 2026.\n\n\n\nWadid Foudhaili1*, Aykut Rencber2*, Anouar Nechi1, Rainer Buchty1, Mladen Berekovic1,\nAndres Gomez2*, and Saleh Mulhem1*\n\n\n\nAbstract\nIn the modern Systems-on-Chip (SoC), the Advanced eXtensible Interface (AXI) protocol exhibits security vulnerabilities, enabling partial or complete denial-of-service (DoS) through protocol-violation attacks.\nThe recent countermeasures lack a dedicated real-time protocol semantic analysis and evade protocol compliance checks.\nThis paper tackles this AXI vulnerability issue and presents an intelligent hardware monitoring system (IMS) for real-time detection of AXI protocol violations.\nIMS is a hardware module leveraging neural networks to achieve high detection accuracy.\nFor model training, we perform DoS attacks through header-field manipulation and systematic malicious operations, while recording AXI transactions to build a training dataset.\nWe then deploy a quantization-optimized neural network, achieving 98.7% detection accuracy with &lt;=3% latency overhead, and throughput of &gt;2.5 million inferences/s.\nWe subsequently integrate this IMS into a RISC-V SoC as a memory-mapped IP core to monitor its AXI bus.\nFor demonstration and initial assessment for later ASIC integration, we implemented this IMS on an AMD Zynq UltraScale+ MPSoC ZCU104 board, showing an overall small hardware footprint (9.04% look-up-tables (LUTs), 0.23% DSP slices, and 0.70% flip-flops) and negligible impact on the overall design’s achievable frequency.\nThis demonstrates the feasibility of lightweight, security monitoring for resource-constrained edge environments.\n\n\n\nI Introduction\n\n\nContemporary electronic devices have become ubiquitous, from smartphones to automotive systems. These devices are mainly powered by system-on-chip (SoC) architectures. Within these SoCs, individual components – so-called Intellectual Property (IP) cores – are interconnected and communicate using on-chip buses. Modern SoC architectures rely on the Advanced eXtensible Interface (AXI) protocol for high-performance communication between IP cores [14]. However, AXI’s design prioritizes performance and flexibility over security, which inadvertently leads to security vulnerabilities and flaws.\n\n\nRecent security analysis has revealed implementation flaws in AXI interconnects that enable new attack vectors targeting protocol-level vulnerabilities [18, 19]. Analysis tools such as XRAY have identified 41 distinct implementation vulnerabilities in certain AXI interconnects [18, 17], demonstrating that even protocol-compliant traffic can be exploited to bypass conventional protection mechanisms [18]. These vulnerabilities come from implementation flaws rather than protocol specification issues, yet enable sophisticated attacks [19, 18].\n\n\n\nI-A SoC Denial of Service\n\n\nDenial-of-service (DoS) attacks represent a critical attack vector where malicious or compromised components exploit implementation weaknesses and protocol characteristics to disrupt communication among SoC components [4]. Here, partial blocking increases the unavailability of some IPs with a possibility of SoC malfunction, while complete blocking causes immediate SoC malfunction. Fig. 1 illustrates a typical SoC architecture where the host CPU connects to peripherals and IP cores via the AXI bus. Both malicious and buggy legitimate cores can trigger protocol violations, resulting in a partial or complete DoS.\nThis can be performed or happens through malformed header fields, such as illegal burst lengths, transaction ID reuse, or signal flooding. Such protocol-level malicious operations remain undetectable to external monitoring systems because they occur within the SoC’s internal communication fabric. Consequently, the SoC may continue operating while performance degrades, potentially violating service-level agreements or causing complete DoS.\n\n\nThe current security countermeasures and mechanisms focus primarily on access control, preventing unauthorized memory access through memory protection units [11, 6] and interconnect policies [4, 10]. However, these approaches cannot detect protocol violations where malicious components use legitimate access patterns to violate protocol semantics through header field manipulation. This highlights a new class of SoC security challenges and vulnerabilities:\n\n\nC1\n\nNew attack vectors that exploit protocol semantics of on-chip buses.\n\n\n\nC2\n\nThe inability of existing protection mechanisms to detect or mitigate such low-level real-time threats.\n\n\n\n\n\nFigure 1: AXI-Bus Hardware Monitoring System Concept\n\n\n\n\nI-B Paper Contributions\n\n\nThis paper presents a novel approach to detect and mitigate DoS attacks on SoC AXI buses using Machine Learning (ML).\nWe advance the state-of-the-art with the following key contributions:\n\n\n•\n\nWe introduce a new security countermeasure called the intelligent hardware monitoring system (IMS) against critical AXI protocol-violation attacks, causing partial or complete DoS of the SoC.\n\n\n\n•\n\nWe show how to build IMS as a machine learning model.\nTherefore, we start with the generation of a learning dataset, and we propose a thoroughly optimized hardware-friendly ML model by applying several ML optimization techniques.\n\n\n\n•\n\nOur proposed IMS achieves a highly accurate detection rate of 98.7%, making it suitable for real-world deployment.\nTo demonstrate, we implement IMS in an RISC-V SoC and show the required hardware overhead for IMS integration.\n\n\n\nOur dataset for attack detection in AXI bus headers is made available for the public on our repository to allow reproducibility of the presented results and to address the lack of public datasets and benchmarks for protocol-level security research in the SoC environment.\n\n\n\n\n\nII Background &amp; Motivation\n\n\nThis section introduces our threat model and highlights its related state-of-the-art countermeasures.\nThen, we motivate our proposed solution.\n\n\n\nII-A Threat Model\n\n\nSoC security validation addresses the following security principles: Confidentiality, Integrity, Availability, and Authenticity [4].\nThis results in three main threat categories[4]: (i) Availability Violations: Malicious or malfunctioning components/IP cores may make shared resources unavailable to legitimate users, (ii) Confidentiality Breaches: It mainly covers unauthorized access to sensitive data, and (iii) Integrity Compromises: Untrusted components try to modify a critical system state.\n\n\nIn this work, we focus on availability-violation threats, which represent a critical and immediate concern in SoC designs as they can cause immediate system-wide failures and are readily observable through performance degradation [18, 17]. While confidentiality and integrity violations are equally important in comprehensive security frameworks, availability attacks often serve as first indicators of system compromise and directly impact operational functionality.\n\n\nTherefore, the proposed threat model considers an adversary who can be a malicious IP core or can exploit a malfunctioning IP to disrupt the availability of shared resources on the AXI bus infrastructure.\nThis threat model is an extension of the established model in [10], which focuses on the transaction level of the AXI bus protocol [4, 10].\n\n\n\n\nII-B Hardware-Based Security Monitoring\n\n\nCurrent hardware security monitoring approaches for SoCs employ two primary strategies: Memory Protection Unit (MPU) and Access Control Policy (ACP).\nMPU-based monitoring approaches enforce access boundaries to specific memory regions, ensuring only authorized components can read or write to them [11],while ACP-based monitoring approaches define communication rules among components/IP blocks using primarily address-based filtering [4, 10].\nHowever, these solutions focus on preventing unauthorized access rather than detecting protocol-level semantic violations.\nTherefore, they exhibit fundamental limitations, as they rely on static threat models, cannot adapt to evolving attack patterns, and lack the intelligence to distinguish between legitimate transactions and protocol-compliant malicious behavior [2, 15].\n\n\n\n\nII-C Research Gap and Motivation\n\n\nExisting solutions focus on access control and memory protection but lack dedicated real-time protocol semantic analysis [19]. While static verification tools can identify design-time vulnerabilities, they cannot address runtime attacks that evade protocol compliance checks.\nThis indicates a critical security gap, which current countermeasures cannot overcome.\nTherefore, we introduce an intelligent hardware monitoring system (IMS), deploying an ML model to monitor the AXI.\nML has been intensively explored and investigated to monitor network transaction [9], device operation, and CPU execution [12].\nOur work addresses this gap by introducing protocol-aware ML models deployed as a lightweight hardware engine for continuous on-chip monitoring.\nTo our knowledge, no existing approach employs ML for real-time AXI protocol monitoring in"
  },
  {
    "title": "When Are Two Scores Better Than One? Investigating Ensembles of Diffusion Models",
    "url": "https://arxiv.org/abs/2601.11444v1",
    "source": "arxiv",
    "summary": "Diffusion models now generate high-quality, diverse samples, with an increasing focus on more powerful models. Although ensembling is a well-known way to improve supervised models, its application to unconditional score-based diffusion models remains largely unexplored. In this work we investigate whether it provides tangible benefits for generative modelling. We find that while ensembling the sco",
    "full_text": null
  },
  {
    "title": "Predict the Retrieval! Test time adaptation for Retrieval Augmented Generation",
    "url": "https://arxiv.org/abs/2601.11443v1",
    "source": "arxiv",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful approach for enhancing large language models' question-answering capabilities through the integration of external knowledge. However, when adapting RAG systems to specialized domains, challenges arise from distribution shifts, resulting in suboptimal generalization performance. In this work, we propose TTARAG, a test-time adaptation me",
    "full_text": null
  },
  {
    "title": "Map2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps",
    "url": "https://arxiv.org/abs/2601.11442v1",
    "source": "arxiv",
    "summary": "We propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for pr",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 The Proposed Framework: Map2Thought\n\n3.1 Architecture\n3.2 Design of Metric-CogMap\n3.3 Construction of Metric Cognitive Map\n\n3.4 Cog-CoT: Cognitive Chain-of-Thought\n\n\n4 Experiments\n\n4.1 Training Details\n4.2 Training Dataset Construction\n\n4.3 Evaluations\n\nComparison baselines\nComparison on VSI-Bench\n\n\n\n4.4 Ablation Study\n\nAblation on dataset proportion\n\nAblation on Metric-CogMap and Cog-CoT\n\n\n5 Conclusion\n\nA Additional Metric-CogMap examples for evaluation\n\n\n\n\n\n\n\n\n\nB Metric-CogMap construction for training\n\nMetadata extraction.\nLoad scene XY range.\nMake Metric-CogMap for each scene\nObject selection for each QA pair\nCreate Metric-CogMap for each QA pair.\n\nC Cog-CoT construction and Complete QA.\n\nGenerating Cog-CoT for each QA pair.\nProgrammatic CoT generation.\n\n\n\n\nStructured response format.\nD Zero-shot qualitative demonstrations\n\n\n\n\n\n\n\n\n\nMap2Thought: Explicit 3D Spatial Reasoning via Metric Cognitive Maps\n\n\n\nXiangjun Gao1\n Zhensong Zhang2\n Dave Zhenyu Chen2\n Songcen Xu2\n \nLong Quan1\n Eduardo Pérez-Pellitero2\n Youngkyoon Jang2\n1The Hong Kong University of Science and Technology\n 2Huawei Noah’s Ark Lab\n\n\n\n\n\n\nAbstract\nWe propose Map2Thought, a framework that enables explicit and interpretable spatial reasoning for 3D VLMs. The framework is grounded in two key components: Metric Cognitive Map (Metric-CogMap) and Cognitive Chain-of-Thought (Cog-CoT). Metric-CogMap provides a unified spatial representation by integrating a discrete grid for relational reasoning with a continuous, metric-scale representation for precise geometric understanding. Building upon the Metric-CogMap, Cog-CoT performs explicit geometric reasoning through deterministic operations (e.g., vector operations, bounding-box distances, and occlusion-aware appearance order cues)\nproducing interpretable inference traces grounded in 3D structure.\nExperimental results show that Map2Thought enables explainable 3D understanding, achieving 59.9%59.9\\% accuracy using only half the supervision—closely matching the 60.9%60.9\\% baseline trained with full dataset. It consistently outperforms state-of-the-art methods by 5.3%5.3\\%, 4.8%4.8\\%, and 4.0%4.0\\% under 10%10\\%, 25%25\\%, and 50%50\\% training subsets, respectively, on the VSI-Bench.\n\n\n\n\n\nFigure 1: \nComparison with prior approaches and efficiency gains. (a) Previous 3D VLMs fuse visual and geometric tokens but through implicit latent reasoning, limiting spatial interpretability. (b) Our approach introduces a metrically grounded cognitive map (Metric-CogMap) and a chain-of-thought-style reasoning process (Cog-CoT), enabling explicit and interpretable spatial reasoning from the same multimodal inputs. (c) This design yields substantial data efficiency: with just 10% or 25% of the training data, our model is comparable to or surpasses the performance of the baseline model trained with 50% of the data.\n\n\n\n\n1 Introduction\n\nRecent advances in 3D Vision-Language Models (3D-VLMs) have increasingly explored bridging multi-modal input signals to harness the rich knowledge priors of large language models (LLMs) for 3D spatial understanding. Beyond projecting 3D geometric tokens from point clouds [20, 49] or RGB-D sequences [56, 54] into the latent space of large language models, recent advances have leveraged pretrained 3D vision foundation models—such as VGGT [38] and CUT3R [39]—which extract 3D structure directly from monocular RGB videos to support more grounded and scalable spatial reasoning. This integration allows 3D-VLMs to access geometry-aware representations without relying on additional modalities. While these methods represent a step forward in generalizability compared to earlier spatial reasoning techniques [2, 46, 5, 10, 6, 11, 15], several core challenges remain unresolved:\n\n\n•\n\nImplicit fusion without geometric grounding: Multimodal representations are fused implicitly without enforcing alignment with physical constraints, limiting transparency and verifiability in spatial reasoning.\n\n\n\n•\n\nLack of accumulated spatial context: Temporal and scene-level spatial cues remain weakly supervised, limiting the model’s ability to reason about appearance order and global layout.\n\n\n\n•\n\nBiased supervision and poor generalization: Training on subsampled or biased visual inputs leads to overfitting, impairing reasoning about object scale, type, and spatial anchoring across diverse scene types.\n\n\n\n\n\nTo address\nthese challenges, we propose Map2Thought, an explicit 3D spatial reasoning framework grounded in Metric Cognitive Maps that enables reliable and interpretable 3D understanding, as shown in Fig. 2. Map2Thought is built upon two core components: 1) Metric-CogMap, a unified spatial representation that combines a discrete grid for symbolic relational reasoning with a continuous metric-scale grid for precise geometric perception; and 2) Cog-CoT, an explicit chain-of-thought reasoning module that performs interpretable geometric computations over the Metric-CogMap.\nUnlike prior cognitive maps used in [45, 48], our Metric-CogMap encodes richer spatial detail, including object occupancy, metric-scale positions, and real-world scale bounding boxes—enabling more fine-grained spatial inference. Our proposed Metric-CogMap is constructed through a robust video-to-map pipeline that extends state-of-the-art 2D object detection and segmentation models—originally designed for frame-based input and output—by integrating 3D vision foundation models and a novel covisibility map–based geometric validation step to ensure accurate and consistent 3D spatial grounding.\nCog-CoT complements this representation by performing transparent, modular geometric reasoning (e.g., distance, direction) through deterministic computations, enabling verifiable inference traces and making our framework easily extensible to other 3D-VLMs without retraining.\nWith this design, Map2Thought attains 59.9%59.9\\% accuracy on VSI-Bench using only 50%50\\% of the training data—nearly matching the 60.9%60.9\\% full-data baseline—while still exceeding the baseline by a consistent 4.0%4.0\\% margin under equal 50%50\\% supervision.\n\n\nThe main contributions of Map2Thought are threefold:\n\n\n•\n\nExplicit Metric-CogMap representation: We introduce Metric-CogMap, a unified spatial representation that integrates discrete symbolic grids with continuous metric-scale geometry, enabling LLMs to interpret 3D scenes with structured, interpretable reasoning.\n\n\n\n•\n\nCog-CoT reasoning paradigm: We introduce Cog-CoT, an interpretable CoT that operates over the accumulated spatial context of the Metric-CogMap to perform explicit and verifiable 3D spatial reasoning.\n\n\n\n•\n\nData-efficient 3D-VLMs: Our framework exhibits strong data efficiency, achieving better accuracy than comparable baselines under limited training data, while improving generalization by mitigating data-dependent overfitting.\n\n\n\n\n\n\n\n2 Related Work\n\nFigure 2: Overview of our method. Given an RGB video and a language query, we extract 2D image tokens and 3D geometry-aware tokens, fuse them into 3D-aware visual tokens, and input them to the VLM. The Metric-CogMap (orange blocks) encodes the scene using both a discrete grid and a metric-scale spatial representation. Cog-CoT (right, grey panel) then performs explicit and deterministic geometric reasoning over the map, yielding a transparent and interpretable answer.\n\n\nLarge Language Models (LLMs) have progressed rapidly in recent years, with active research expanding their capabilities across diverse domains such as code generation, commonsense reasoning, and mathematics [36]. A prominent direction in this evolution involves extending LLMs to handle multiple modalities—enabling them to develop visual [3] and spatial understanding [32]. Early Visual-Language Models (VLMs), such as CLIP [33] and ALIGN [24], learn joint image-text embeddings, while later works [1, 27] decouple vision and language modules to better support cross-modal reasoning. While these models perform well on single-image tasks, they remain limited in reasoning about 3D spatial context intuitive to humans—such as distance, relative positioning, and object counting.\n\n\nPoint cloud encoding in 3D VLMs.\nRecent 3D-VLMs integrate point clouds with RGB and text inputs to provide geometric grounding, but their fusion in latent space often limits spatial precision and interpretability [26]. Models such as  [20, 14] use spatial transformers [7] to encode object-centric 3D tokens and capture inter-object geometry, while  [18, 8] align 2D and 3D features by projecting them back to reconstructed points for enhanced multimodal understanding. However, both rely on latent feature-level fusion across modalities—point clouds, images, and text—leading to opaque reasoning pipelines that hinder interpretability and analysis. More explicit pipelines, introduced by [49, 42, 28], combine multi-view 2D semantics features with 3D geometric information for improved indoor scene understanding. However, they assume access to high-quality, densely reconstructed point clouds—an assumption that breaks down in real-world settings where input videos span several minutes, as is common in datasets like ScanNet[12], ScanNet++[47] or ARKitScenes[13]. In such cases, sparse sampling or partial reconstructions may miss key objects, making reliance on complete point cloud–based instance detection a major bottleneck.\n\n\nPositional encodings for 3D VLMs.\nTo improve spatial grounding, recent 3D VLMs introduce positional encodings derived from camera poses, depth maps, or back-projected coordinates. These encodings localize visual features within a 3D coordinate system, aiding alignment across views and with language [31]. For instance, LLaVA‑3D [56] and Video‑3D‑LLM [54] embed spatial tokens linking 2D appearances to 3D context. While this enhances object-level spatial sensitivity, they [56, 37, 54] often fail to capture broader geometric structure. Consequently, the resulting representations remain opaque and overfitti"
  },
  {
    "title": "Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models",
    "url": "https://arxiv.org/abs/2601.11441v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) exhibit exceptional performance across various domains, yet they face critical safety concerns. Model editing has emerged as an effective approach to mitigate these issues. Existing model editing methods often focus on optimizing an information matrix that blends new and old knowledge. While effective, these approaches can be computationally expensive and may cause con",
    "full_text": null
  },
  {
    "title": "GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance",
    "url": "https://arxiv.org/abs/2601.11440v1",
    "source": "arxiv",
    "summary": "Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture",
    "full_text": null
  },
  {
    "title": "Near-Optimal Decentralized Stochastic Nonconvex Optimization with Heavy-Tailed Noise",
    "url": "https://arxiv.org/abs/2601.11435v1",
    "source": "arxiv",
    "summary": "This paper studies decentralized stochastic nonconvex optimization problem over row-stochastic networks. We consider the heavy-tailed gradient noise which is empirically observed in many popular real-world applications. Specifically, we propose a decentralized normalized stochastic gradient descent with Pull-Diag gradient tracking, which achieves approximate stationary points with the optimal samp",
    "full_text": null
  },
  {
    "title": "Inter-patient ECG Arrhythmia Classification with LGNs and LUTNs",
    "url": "https://arxiv.org/abs/2601.11433v1",
    "source": "arxiv",
    "summary": "Deep Differentiable Logic Gate Networks (LGNs) and Lookup Table Networks (LUTNs) are demonstrated to be suitable for the automatic classification of electrocardiograms (ECGs) using the inter-patient paradigm. The methods are benchmarked using the MIT-BIH arrhythmia data set, achieving up to 94.28% accuracy and a $jκ$ index of 0.683 on a four-class classification problem. Our models use between 2.8",
    "full_text": null
  },
  {
    "title": "The unreasonable effectiveness of pattern matching",
    "url": "https://arxiv.org/abs/2601.11432v1",
    "source": "arxiv",
    "summary": "We report on an astonishing ability of large language models (LLMs) to make sense of \"Jabberwocky\" language in which most or all content words have been randomly replaced by nonsense strings, e.g., translating \"He dwushed a ghanc zawk\" to \"He dragged a spare chair\". This result addresses ongoing controversies regarding how to best think of what LLMs are doing: are they a language mimic, a database",
    "full_text": null
  },
  {
    "title": "Relational Linearity is a Predictor of Hallucinations",
    "url": "https://arxiv.org/abs/2601.11429v1",
    "source": "arxiv",
    "summary": "Hallucination is a central failure mode in large language models (LLMs). We focus on hallucinations of answers to questions like: \"Which instrument did Glenn Gould play?\", but we ask these questions for synthetic entities that are unknown to the model. Surprisingly, we find that medium-size models like Gemma-7B-IT frequently hallucinate, i.e., they have difficulty recognizing that the hallucinated",
    "full_text": null
  },
  {
    "title": "Forcing and Diagnosing Failure Modes of Fourier Neural Operators Across Diverse PDE Families",
    "url": "https://arxiv.org/abs/2601.11428v1",
    "source": "arxiv",
    "summary": "Fourier Neural Operators (FNOs) have shown strong performance in learning solution maps of partial differential equations (PDEs), but their robustness under distribution shifts, long-horizon rollouts, and structural perturbations remains poorly understood. We present a systematic stress-testing framework that probes failure modes of FNOs across five qualitatively different PDE families: dispersive",
    "full_text": null
  }
]