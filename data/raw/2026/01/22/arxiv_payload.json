[
  {
    "title": "Iterative Refinement Improves Compositional Image Generation",
    "url": "https://arxiv.org/abs/2601.15286v1",
    "source": "arxiv",
    "summary": "Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Method\n\n4 Experiments\n\n4.1 Compositional Image Generation\n4.2 Qualitative analysis and human evaluation\n4.3 Visual Jenga Scene Decomposition\n4.4 Ablations\n\n\n5 Conclusion\n6 Further qualitative examples.\n\n7 Further experiment details\n\n7.1 Human evaluation details\n7.2 Additional experiment specifications\n\n\n\n\n\n\n\nIterative Refinement Improves Compositional Image Generation\n\n\nShantanu Jaiswal1  Mihir Prabhudesai1  Nikash Bhardwaj1  Zheyang Qin1\nAmir Zadeh2  Chuan Li2  Katerina Fragkiadaki1  Deepak Pathak1\n\n1Carnegie Mellon University  2Lambda AI\n\n\n\nAbstract\nText-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9%16.9\\% improvement in all-correct rate on ConceptMix (k=7), a 13.8%13.8\\% improvement on T2I-CompBench (3D-Spatial category) and a 12.5%12.5\\% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation.\n\n\nhttps://iterative-img-gen.github.io/\n\n\n\n\n\n\nFigure 1: Iterative refinement during inference time enables high fidelity generation of complex prompts on which traditional inference-time scaling strategies such as parallel sampling can fail to generate a fully accurate image even at high num. of samples as shown above.\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have achieved remarkable progress in recent years, as a result of simply scaling test-time compute [30, 4, 26]. A particularly influential development has been the use of chain-of-thought (CoT) prompting, where models are instructed to “think step by step” [30, 15]. Despite its simplicity, this strategy enables models to exhibit sophisticated behaviors such as self-correction, error checking, and iterative refinement, ultimately leading to significant gains on reasoning-intensive benchmarks. These behaviors highlight the potential of LLMs not only as static predictors but as systems that can actively refine their outputs through structured intermediate reasoning.\n\n\nThe success of CoT reasoning in LLMs is closely tied to their pre-training data. During training, LLMs are exposed to large volumes of text that naturally contain traces of human step-by-step reasoning – mathematical derivations, logical arguments, and instructional writing. This supervision on the internet implicitly provides the prior that chain-of-thought prompting later exploits, enabling the model to perform multi-step reasoning. By contrast, text-to-image (T2I) models are trained on large-scale datasets of image–caption pairs that lack such structured reasoning traces. As a result, these models do not inherently develop capabilities like self-correction or iterative refinement, instead rely on one-shot generation strategies that limit their robustness in complex settings.\n\n\nFigure 2: Our iterative inference-time strategy achieves strong benefits over computation-matched parallel inference time scaling on multiple state-of-art image generation models.\n\n\nIn this work, we investigate how can we enable self-correction in T2I models. Our central idea is to leverage complementary modules that together mimic the iterative reasoning process observed in LLMs. Concretely, our framework integrates four components: (i) a text-to-image (T2I) model to generate an initial image, (ii) a vision-language model (VLM) critic to propose corrections by comparing the generated image with target prompt, (iii) an image editor to apply suggested edits, and (iv) a verifier to evaluate alignment between final image and target prompt. This pipeline allows the model to iteratively refine its outputs rather than relying solely on a single forward pass.\n\n\nWe compare our approach against the widely adopted strategy of parallel sampling [20, 38], where multiple images are generated independently and the best one is selected using a verifier. While parallel sampling increases diversity, it does not fundamentally change the underlying generation process, nor does it allow the model to revise or build upon earlier outputs. As a result, it struggles with complex compositional prompts. For example, consider a prompt requiring dozens of concept bindings: if the model’s attention heads cannot jointly resolve all bindings in a single forward pass, the pass@k will remain near zero regardless of how many samples are drawn.\n\n\nIn contrast, our approach explicitly reuses intermediate generations and progressively improves them through guided corrections. This factorization allows the model to handle only a subset of bindings at each step, compounding previously resolved components over time. Such sequential, step-by-step refinement—analogous to chain-of-thought reasoning— is crucial for reliably generating highly compositional images.\n\n\nFigure 1, highlights the capability of our approach to generate complex compositional prompts. Given the caption on top, parallel sampling simply is unable to build on top of the previous steps thus being unsuccessful even after 4 passes through the generative model. In contrast, iterative refinement successfully generates the final image, while using the same amount of compute. Quantitatively in Figure 2, we demonstrate that this leads to consistent performance improvements: our approach achieves a 16.9% higher all-correct rate on ConceptMix [34] (for concept binding=7) and a 13.8% gain on T2I-Bench 3D Spatial category [13] relative to compute-matched parallel sampling.\n\n\nAn alternate family of methods—such as GenArtist [28] and CompAgent [29]—also performs sequential sampling by building on top of previous generations. However, these approaches rely on a large toolbox of auxiliary modules (e.g., layout-to-image models, bounding-box detectors, dragging tools, and object-removal systems). Because these toolchains evolve at different rates and often lag behind foundation model capabilities, the overall pipeline becomes brittle: errors from individual tools can accumulate rather than help the generation process for complex prompts. Other methods such as RPG [35] similarly show gains via increased test-time compute, but still depend on complex region-wise priors and bespoke pipelines not readily applicable to black-box foundation models.\n\n\nIn contrast, with recent advances in VLMs and modern image-editing models, we find that many of these specialized tool-based pipelines are no longer necessary for effective test-time scaling. Across all benchmarks, simply combining a strong VLM critic feedback generator with a standard image-editing model is sufficient to achieve state-of-the-art compositional image generation – without relying on heavy tool stacks or model-specific training and engineering pipelines. As shown in Figure 5, methods such as GenArtist and RPG under-perform substantially in highly compositional settings, whereas our approach delivers a consistent ∼9+%\\sim 9{+}\\% point improvement under matched compute. Further, our framework naturally extends to the recent Visual Jenga scene decomposition task [2] as detailed in sec. 4.3.\n\n\nOur findings further suggest that self-correction—long recognized as a key ingredient in LLM reasoning—also serves as a powerful inductive principle for generative vision models. Introducing a simple and general refinement pipeline enables behaviors traditionally associated with language models to naturally transfer into image generation, yielding tangible performance gains. More broadly, this work points to the promise of designing generative systems that not only produce outputs but also critique and improve upon them, moving towards a more unified view of reasoning across modalities.\n\n\n\n\n2 Related Work\n\nText-to-Image Inference-Time Strategies.\nRecent advances in text-to-image (T2I) generation have demonstrated impressive capabilities in producing high-quality and diverse images from natural language prompts [10, 11, 1]. However, complex prompts with multiple objects, relations, and fine-grained attributes remain challenging. Inference-time strategies such as classifier-free guidance [12], parallel sampling [8, 5], and grounding-based methods [18, 19] improve prompt fidelity but often fail to scale to richly compositional prompts. Iterative refinement methods, including SDEdit [22], InstructPix2Pix [3], and IterComp[39] attempt to progressively improve image alignment with prompts by using multiple generation steps and feedback mechanisms. Human-preference-guided evaluation and optimization, as in [17, 33, 14], further highlight the importance of incorporating adaptive guidance at inference time. T2I models [24, 32, 25, 16] and compositional methods such as IterComp [39], RPG [35], GenArtist [28], PARM [37], LLM Diffusion [19] and CompAgent [29] are related to our method, but either make use of tool-calling, regional generation priors"
  },
  {
    "title": "Rethinking Video Generation Model for the Embodied World",
    "url": "https://arxiv.org/abs/2601.15282v1",
    "source": "arxiv",
    "summary": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Video World Modeling for Robotics\n2.2 Datasets for Robot Learning\n2.3 Benchmarks for Video Generation\n\n\n\n3 RBench\n\n3.1 Benchmark Construction\n\n3.2 Automatic Metrics\n\n3.2.1 Task Completion\n3.2.2 Visual Quality\n\n\n\n\n\n4 RoVid-X\n\n4.1 Dataset Construction\n4.2 Dataset Analysis\n\n\n\n5 Experiment\n\n5.1 Evaluation Setups\n\n5.2 Main Analysis\n\n5.2.1 Quantitative Results\n5.2.2 Qualitative Results\n\n\n5.3 Human Preference Study\n5.4 Validation of RoVid-X\n\n\n6 Conclusion\n\nA Evaluation Set Details\n\n\nA.1 Task-Oriented Evaluation Set\n\nA.1.1 Common Manipulation\nA.1.2 Long-Horizon Planning\nA.1.3 Multi-Entity Collaboration\nA.1.4 Spatial Relationship\nA.1.5 Visual Reasoning\n\n\nA.2 Embodiment-Specific Evaluation Set\n\n\n\nB Automatic Metrics Details\n\nB.1 Physical-Semantic Plausibility\nB.2 Task-Adherence Consistency\nB.3 Robot-Subject Stability\nB.4 Motion Amplitude\nB.5 Motion Smoothness\nB.6 Score Aggregation\n\n\n\nC Model Descriptions and Implementation Setups\n\nC.1 Commercial Models\nC.2 Open-source Models\nC.3 Robotics-specific Models\n\n\n\nD Human Preference Study Details\n\nBland–Altman basics.\nLinear LOO calibration.\n\n\n\nE Prompt Template\n\nQuestion-chain construction.\nVideo assessment prompt.\n\n\nF Additional Qualitative Comparisons\nG Comprehensive Quantitative Results\n\n\n\n\n\n\n1]Peking University\n2]ByteDance Seed\n\nRethinking Video Generation Model for the Embodied World\n\n\n\nYufan Deng1,2∗  \nZilin Pan1∗  \nHongyu Zhang1∗  \nXiaojie Li2  \nRuoqing Hu2   \nYufei Ding1  \nYiming Zou1  \nYan Zeng2  \nDaquan Zhou1\n\n[\n\n[\n\n\n\nAbstract\nVideo generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world.\nHowever, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress.\nTo address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments.\nIt assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness.\nEvaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness.\nWhile RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations.\nCollectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.\n\n\n\\checkdata\n[\n  Project Page]https://dagroup-pku.github.io/ReVidgen.github.io/\n\n\\checkdata[\n  GitHub Repo]https://github.com/DAGroup-PKU/ReVidgen/\n\n\\checkdata[\n  HuggingFace Dataset]https://huggingface.co/datasets/DAGroup-PKU/RoVid-X/\n\n\nFigure 1: Overview of the comprehensive robotics benchmark and dataset for video generation. Top: We present RBench that includes the embodiment-based evaluation set and automated evaluation metrics. Our evaluation results on 25 video models show a high level of agreement with subjective human assessments. Bottom: We introduce a large-scale high-quality robotic dataset (RoVid-X) specifically designed for training video generation models, with data sourced from internet videos and open-source embodied videos.\n\n\n\n1 Introduction\n\nRecent advancements in diffusion models [44, 84, 79] and video generation [83, 80, 35, 98, 92] have led to significant breakthroughs, enabling applications like video editing, multi-subject generation, and motion control [51, 53, 20, 21, 96]. These models have been extended to areas such as 3D scenes [57, 82], autonomous driving [28, 104], and world modeling [54, 5], showing strong generalization across tasks. A recent study [97] suggests that, similar to LLMs in natural language processing, video models are evolving into unified foundation models for machine vision. Additionally, video models are being increasingly used in robot learning and action prediction [37, 113, 45, 111, 38, 61], as well as controllable simulators for synthesizing robotic video trajectories, addressing the lack of large-scale human teleoperation data [47, 8, 86]. These advancements highlight the growing potential of video models in the perception-reasoning-action loop of embodied agents, paving the way for more generalizable intelligent systems in the physical world.\n\n\nDespite these strides, systematic evaluation for robotic video generation remains underdeveloped. Current practices rely mostly on perceptual metrics, focusing on visual quality [46, 66, 42], while existing physics-based benchmarks often lack task-specific datasets and criteria [72, 36].\nConsequently, evaluations frequently overlook critical aspects such as task completion, action-goal alignment, and physical feasibility. This leads to overly optimistic conclusions, where high scores are assigned even to videos containing unnatural movements or incomplete tasks. The core challenge lies in rigorously assessing whether generated videos faithfully reproduce robotic behaviors. This necessitates evaluation protocols that transcend perceptual metrics, incorporating both the physical plausibility of actions and their alignment with instructions to ensure discriminative and reproducible assessments.\n\n\nTo address this challenge, we propose RBench, a benchmark designed to evaluate the fidelity and utility of video-based world models in robotic video generation. To the best of our knowledge, it is the first comprehensive benchmark with fine-grained metrics for robotic video generation, consisting of 650 image–text pairs across five task categories and four robot types. Evaluations are based on two dimensions: task completion and visual quality, incorporating sub-metrics like structural consistency, physical plausibility, and execution completeness.\nBased on RBench, we conduct qualitative and quantitative assessments of 25 representative models. The results highlight that general video foundation models still have significant room for improvement in physical robot video generation, revealing a persistent gap between these models and the requirements of embodied robotic tasks. This underscores the need for systematic advancements in both robotic video data and training methodologies.\n\n\nAdvancing general robotic video generation with human-like capabilities and adaptability requires diverse, scalable, and comprehensive training data [78, 12]. However, unlike computer vision and natural language processing, which can leverage vast web-scale datasets, robotic interaction data has long been constrained by both scale and diversity [10, 110, 26]. Even the largest existing collections are smaller and less varied than those for vision or language. More critically, many datasets have narrow distributions along key axes such as environment, object set, task spectrum, and robot morphology [105, 95], often confined to specific robot types, low-resolution recordings, or limited task ranges, which hampers the generalization of video foundation models. To address these gaps, we integrate over 20 open-source datasets and multi-source video platforms, creating a four-stage end-to-end data pipeline. The stages include robot video collection, video quality filtering, task segmentation and captioning, and physical property annotation, resulting in RoVid-X, a large-scale, high-quality embodied robotic video dataset (see Table 1). To our knowledge, RoVid-X is currently the largest dataset specifically designed for embodied video generation models, covering a broad range of robot morphologies and task types. It aims to enhance video foundation models with physical interaction priors and task semantic diversity, driving further advancements in the field. Overall, the main contributions are summarized as follows:\n\n\n•\n\nA systematic benchmark tailored for robotic video generation. We propose RBench, which comprehensively evaluates the performance of video foundation models across five robotic tasks and four robot types with 650 meticulously curated evaluation samples, while introducing reproducible automated evaluation metrics.\n\n\n\n•\n\nKey insights into robotic video generation for embodied research. We conduct a systematic evaluation of 25 representative video models, including open-source, commercial, and robotics-specific ones, revealing the limitations of current video foundation models and potential directions for improvement, offering new perspectives for researchers exploring the embodied domain using video world models.\n\n\n\n•\n\nA large-scale, high-quality robotic video dataset. We construct RoVid-X, a dataset containing approximately 4 million curated robotic videos with standardized task descriptions and physical property annotations, providing essential support for the training and evaluation of embodied video models.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Video World Modeling for Robotics\n\nThe latest breakthroughs in video generation technology have led to the development of powerful models capable of generating high-quality videos from text or image prompts [83, 80, 75, 59, 92]. With the advancement of these technologies, an increasing number of studies have begun applying them to the field of embodied intelligence [25, 91, 2, 11]. Video prov"
  },
  {
    "title": "MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs",
    "url": "https://arxiv.org/abs/2601.15279v1",
    "source": "arxiv",
    "summary": "A molecule's properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or rea",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.15279v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2601.15279v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 21 Jan 2026]\n    Title:MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs\n    Authors:Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen            View a PDF of the paper titled MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs, by Christoph Bartmann and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:A molecule&#39;s properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Machine Learning (cs.LG); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2601.15279 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.15279v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.15279\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Johannes Schimunek [view email]          [v1]\n        Wed, 21 Jan 2026 18:58:01 UTC (1,764 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs, by Christoph Bartmann and 5 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    "
  },
  {
    "title": "Robust Fake News Detection using Large Language Models under Adversarial Sentiment Attacks",
    "url": "https://arxiv.org/abs/2601.15277v1",
    "source": "arxiv",
    "summary": "Misinformation and fake news have become a pressing societal challenge, driving the need for reliable automated detection methods. Prior research has highlighted sentiment as an important signal in fake news detection, either by analyzing which sentiments are associated with fake news or by using sentiment and emotion features for classification. However, this poses a vulnerability since adversari",
    "full_text": null
  },
  {
    "title": "RayRoPE: Projective Ray Positional Encoding for Multi-view Attention",
    "url": "https://arxiv.org/abs/2601.15275v1",
    "source": "arxiv",
    "summary": "We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above des",
    "full_text": null
  },
  {
    "title": "Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions",
    "url": "https://arxiv.org/abs/2601.15267v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoni",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Challenges: Risks and Complexity of LLM Applications in Law\n\n2.1 Applications to Judges\n2.2 Applications to Lawyers\n2.3 Applications to the General Public\n\n\n\n3 Methods: Evaluation Benchmarks for LLMs in the Legal Domain\n\n3.1 Single-Task Benchmarks\n\n3.2 Multi-Task Benchmarks\n\n\nTask Design\n\n\nDataset and Metrics\n\nSummary\n\n\n\n\n\nTask Design\n\n\nDataset and Metrics\n\nSummary\n\n\n\n\n\nTask Design\n\n\nDataset and Metrics\n\nSummary\n\n\n\n\n\n\n\n\n\n4 Future: Towards Better Evaluation of Legal LLMs\n\n4.1 Data Perspective\n4.2 Method Perspective\n4.3 Metrics Perspective\n\n\n\n\n\n\n\nEvaluation of Large Language Models in Legal Applications: \nChallenges, Methods, and Future Directions\n\n\n\nYiran Hu1,2,3\nEqual contribution.\n  \nHuanghai Liu1∗\n\n  \nChong Wang1∗\n\n  \nKunran Li1\n\n  \nTien-Hsuan Wu2\n\n  \nHaitao Li1\n\n  \nXinran Xu4\n\n  \nSiqing Huo3\n\n  \nWeihang Su1\n\n  \nNing Zheng1\n\n  \nSiyuan Zheng4\n\n  \nQingyao Ai1\n\n  \nYun Liu1\n\n  \nRenqun Bian5\n\n  \nYiqun Liu1\n\n  \nCharles L.A. Clarke3\n\n  \nWeixing Shen1&amp;Ben Kao2\n1Tsinghua University 2The University of Hong Kong 3University of Waterloo\n4Shanghai Jiaotong University 5Peking University\nhuyr17@outlook.com,\nliuhh23@mails.tsinghua.edu.cn\nCorresponding author.\n\n\nAbstract\nLarge language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.\n\n\n\n1 Introduction\n\nAs large language models (LLMs) continue to advance in capability, they are increasingly being applied to the legal domain Sun et al. (2024); Li et al. (2024a). Typical applications include assisting laypersons in understanding legal issues, supporting lawyers in legal practice, and providing auxiliary assistance to judges in judicial decision-making Su et al. (2024); Li et al. (2024a); Gao et al. (2024); Chen et al. (2024). This raises a fundamental question: Are LLMs truly qualified to enter the legal domain, and which types of legal tasks can they reliably perform?\n\n\nPrevious studies Katz et al. (2024); Freitas and Gomes (2023) have shown that LLMs are able to pass legal examinations, and some works Xiao et al. (2018); Lyu et al. (2023) further demonstrate that LLMs can achieve high performance in legal judgment prediction tasks. However, due to the intrinsic complexity of real-world legal scenarios and the demanding nature of legal reasoning, evaluating LLMs solely based on standardized exam-style questions or prediction accuracy is insufficient to comprehensively assess their performance in real legal applications. Consequently, a growing number of benchmarks Fei et al. (2023); Guha et al. (2024); Li et al. (2024b) have been proposed to evaluate LLMs from multiple perspectives in legal tasks.\n\n\nEvaluating LLMs in legal tasks is significantly more challenging than conducting general-purpose evaluations Huang et al. (2023). Existing studies Hu et al. (2025b); Liu and Li (2024) show that LLMs are increasingly being applied in real-world court settings. In real legal applications, LLMs face many issues beyond whether an answer is correct. Evaluation should not focus solely on the final result, but also consider the reasoning process and system-level constraints. The aim of evaluating LLMs in the legal domain is to systematically and automatically assess their capabilities across multiple dimensions, reflecting not only whether models arrive at correct outcomes, but also how those results are produced and under what constraints they remain valid.\n\n\nFrom the process perspective, addressing legal problems requires strong logical reasoning abilities; a model may produce a legally correct answer while relying on flawed or invalid reasoning steps. For example, when an LLM serves as a judicial assistant, a key factor in whether the public can be convinced lies in its reasoning process, such as whether it cites authentic and correct legal provisions and whether it performs logically sound inferences based on both legal rules and factual circumstances. From the constraint perspective, LLMs need to handle legal cases fairly and safely. For instance, an LLM should not exhibit discrimination based on gender or geographic origin when making judgments. Given that legal decisions directly affect individuals’ rights and social welfare, ensuring fairness, reliability, and safety is a prerequisite for real-world adoption Wang et al. (2023a). Legal applications operate under strict normative and social constraints: even when both the final result and the reasoning appear correct, a model may still be unsuitable for deployment if it exhibits unfair bias, lacks robustness, or fails under adversarial or distributional shifts.\n\n\nIn summary, when LLMs are applied to real-world legal settings, they need to be evaluated across the entire pipeline (Result, Process, Constraint). These three stages correspond to three evaluation dimensions: Output Accuracy (Result), Legal Reasoning (Process), and Trustworthiness (Constraint). Only through effective evaluation along these three dimensions can models be applied more reliably in practice. Based on these three challenges, we categorize existing benchmarks and select representative benchmarks for analysis.\n\n\nFigure 1: Overview of the proposed framework.\n\n\nOutcome Accuracy aims to assess a model’s legal knowledge and its overall task performance. Existing datasets are mainly constructed around legal examinations Li et al. (2024b); Fan et al. (2025) and judicial decision prediction tasks Xiao et al. (2018), with evaluation metrics such as accuracy for multiple-choice questions, as well as ROUGE and BERT-based similarity measures for open-ended legal responses. Trustworthiness evaluates the potential risks introduced by LLMs when performing legal tasks Wang et al. (2023a). Current studies Hu et al. (2025b, a) mainly investigate ethical concerns, fairness, and robustness, often using counterfactual analysis and adversarial attacks. Legal Reasoning measures whether the reasoning process employed by the model aligns with legal reasoning principles. Existing work Akyürek et al. (2025) typically relies on fine-grained legal reasoning rubrics annotated by human experts to score the model’s reasoning steps.\n\n\nDespite the progress made by existing research, many challenges remain unresolved. For instance, trustworthiness evaluations still lack coverage of critical dimensions such as privacy risks and toxicity in legal contexts. Evaluations of logical inference heavily rely on manually annotated rubrics, making them difficult to scale to large datasets. Moreover, certain task-specific issues, like legal hallucination, have not yet been systematically evaluated. These limitations indicate that current evaluation frameworks for LLMs in legal tasks are far from complete, leaving substantial room for further exploration.\n\n\nAs Figure 1 shows, in this survey, we summarize existing studies on evaluating LLMs in the legal domain, identify key challenges, and outline promising future research directions.\n\n\nThe contributions of this survey are threefold:\n\n\n\n\n1.\n\nWe identify major challenges in applying LLMs to the legal domain and discuss potential solutions.\n\n\n\n2.\n\nWe systematically summarize and categorize existing evaluation methods and benchmarks for legal LLMs. For details, see https://github.com/THUYRan/Evaluation-of-LLMs-in-Legal-Applications\n\n\n\n3.\n\nWe align current evaluation approaches with outstanding challenges, highlighting open problems and future research directions.\n\n\n\n\n\n\n\n2 Challenges: Risks and Complexity of LLM Applications in Law\n\nA comprehensive, scientific, and reliable evaluation of LLM performance in legal scenarios needs to start from their specific applications. A prerequisite for meaningful LLM evaluation is to understand where and how LLMs are being deployed in legal workflows, and how these deployments affect legal decision-making and everyday life. By systematically examining the legal tasks to which LLMs are applied, researchers can better identify the unique challenges involved in evaluating LLMs within the legal domain.\n\n\nIn this section, we review representative applications of LLMs across three user groups — judges, lawyers, and the general public. Although these applications differ in function, authority, and risk profile, they illustrate the breadth of LLM adoption in legal contexts and highlight why legal AI evaluation needs to be carried out across the entire pipeline.\n\n\n\n2.1 Applications to Judges\n\nLLMs are increasingly incorporated into judicial workflows, supporting tasks such as case triage, legal document drafting, and decision review Alon-Barkat and Busuioc (2023). Judicial applications place LLMs directly within the exercise of legal authority, imposing especially stringent requirements on legal accuracy, reasoning quality, and ethical compliance. Their growing adoption in judicial contexts thus underscores th"
  },
  {
    "title": "Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?",
    "url": "https://arxiv.org/abs/2601.15254v1",
    "source": "arxiv",
    "summary": "We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the",
    "full_text": null
  },
  {
    "title": "The Effect of Scripts and Formats on LLM Numeracy",
    "url": "https://arxiv.org/abs/2601.15251v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.15251v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.15251v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 21 Jan 2026]\n    Title:The Effect of Scripts and Formats on LLM Numeracy\n    Authors:Varshini Reddy, Craig W. Schmidt, Seth Ebner, Adam Wiemerslage, Yuval Pinter, Chris Tanner            View a PDF of the paper titled The Effect of Scripts and Formats on LLM Numeracy, by Varshini Reddy and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large language models (LLMs) have achieved impressive proficiency in basic arithmetic, rivaling human-level performance on standard numerical tasks. However, little attention has been given to how these models perform when numerical expressions deviate from the prevailing conventions present in their training corpora. In this work, we investigate numerical reasoning across a wide range of numeral scripts and formats. We show that LLM accuracy drops substantially when numerical inputs are rendered in underrepresented scripts or formats, despite the underlying mathematical reasoning being identical. We further demonstrate that targeted prompting strategies, such as few-shot prompting and explicit numeral mapping, can greatly narrow this gap. Our findings highlight an overlooked challenge in multilingual numerical reasoning and provide actionable insights for working with LLMs to reliably interpret, manipulate, and generate numbers across diverse numeral scripts and formatting styles.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2601.15251 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.15251v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.15251\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Varshini Reddy [view email]          [v1]\n        Wed, 21 Jan 2026 18:33:15 UTC (10,856 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled The Effect of Scripts and Formats on LLM Numeracy, by Varshini Reddy and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscribe\n                \n              \n            \n          \n        \n"
  },
  {
    "title": "Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism",
    "url": "https://arxiv.org/abs/2601.15249v1",
    "source": "arxiv",
    "summary": "Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a",
    "full_text": null
  },
  {
    "title": "Taxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs",
    "url": "https://arxiv.org/abs/2601.15247v1",
    "source": "arxiv",
    "summary": "We present a methodology for extracting structured risk factors from corporate 10-K filings while maintaining adherence to a predefined hierarchical taxonomy. Our three-stage pipeline combines LLM extraction with supporting quotes, embedding-based semantic mapping to taxonomy categories, and LLM-as-a-judge validation that filters spurious assignments. To evaluate our approach, we extract 10,688 ri",
    "full_text": "\n\n\n\n\n1 Introduction\n\nA robust methodology for taxonomy-aligned risk extraction.\nA three-tier risk factors taxonomy for investment analysis.\nAutonomous taxonomy refinement workflow.\nEmpirical validation on S&amp;P 500 companies.\n\n\n\n2 Related Work\n\nFinancial document analysis and risk extraction.\nRisk taxonomies for corporate analysis.\nStructured information extraction with LLMs.\nSemantic similarity and embedding-based retrieval.\nLLM-as-judge evaluation.\nHybrid approaches to information extraction.\n\n\n\n3 Methodology\n\n3.1 Problem Formulation\n3.2 Three-Tier Risk Taxonomy\n3.3 Stage 1: LLM-Based Risk Extraction\n\n3.4 Stage 2: Embedding-Based Taxonomy Mapping\n\nEmbedding generation.\nNearest neighbor matching.\nThe spurious mapping problem.\n\n\n3.5 Stage 3: LLM-as-Judge Validation\n3.6 Deduplication and Final Output\n\n\n\n4 Evaluation\n\n4.1 Data Acquisition\n\n4.2 Taxonomy Quality Analysis and Iterative Refinement\n\nQuality score distribution.\nAutonomous identification of problematic categories.\nPattern analysis and root cause identification.\nAutomated description refinement and testing.\nImplications for continuous taxonomy improvement.\n\n\n\n4.3 Industry Clustering Validation\n\n4.3.1 Experimental Design\n4.3.2 Results\n4.3.3 Sensitivity to granularity\n\n\n4.4 Sector-Specific Risk Profiles\n\n\n5 Conclusion\n\n\n\n\n\nTaxonomy-Aligned Risk Extraction from 10-K Filings with Autonomous Improvement Using LLMs\n\n\nRian Dolphin\n\n0000-0002-5607-9948\nMassive.comDublinIreland\n\n, \nJoe Dursun\n\nMassive.comAtlanta, GeorgiaUSA\n\n, \nJarrett Blankenship\n\nMassive.comAtlanta, GeorgiaUSA\n\n, \nKatie Adams\n\nMassive.comAtlanta, GeorgiaUSA\n\n and \nQuinton Pike\n\nMassive.comAtlanta, GeorgiaUSA\n\n\n(2018)\n\nAbstract.\nWe present a methodology for extracting structured risk factors from\ncorporate 10-K filings while maintaining adherence to a predefined\nhierarchical taxonomy. Our three-stage pipeline combines LLM extraction\nwith supporting quotes, embedding-based semantic mapping to taxonomy\ncategories, and LLM-as-a-judge validation that filters spurious\nassignments. To evaluate our approach, we extract 10,688 risk factors from S&amp;P 500\ncompanies and examine risk profile similarity across industry clusters.\nBeyond extraction, we introduce autonomous taxonomy maintenance where an\nAI agent analyzes evaluation feedback to identify problematic categories,\ndiagnose failure patterns, and propose refinements, achieving 104.7%\nimprovement in embedding separation in a case study. External validation\nconfirms the taxonomy captures economically meaningful structure:\nsame-industry companies exhibit 63% higher risk profile similarity than\ncross-industry pairs (Cohen’s d=1.06d=1.06, AUC 0.82, p&lt;0.001p&lt;0.001). The\nmethodology generalizes to any domain requiring taxonomy-aligned\nextraction from unstructured text, with autonomous improvement enabling\ncontinuous quality maintenance and enhancement as systems process more documents.\n\nLLM, Information Extraction, Unstructured Text, Finance, 10K, Embedding\n\n††copyright: acmlicensed††journalyear: 2018††doi: XXXXXXX.XXXXXXX††conference: Make sure to enter the correct\nconference title from your rights confirmation email; June 03–05,\n2018; Woodstock, NY††isbn: 978-1-4503-XXXX-X/2018/06††ccs: Computing methodologies Information extraction\n\n\n1. Introduction\n\nCorporate risk disclosure in annual reports (10-K filings) provides insightful commentary on the uncertainties facing public companies. Item 1A of these filings, the “Risk Factors” section, contains detailed discussions of material risks written in natural language by company management and legal teams. For investors, analysts, and risk managers, systematically analyzing these disclosures across hundreds or thousands of companies could reveal sector-wide risk patterns, emerging threats, and company-specific vulnerabilities. However, the unstructured nature of these documents and their substantial length (often spanning dozens of pages per company) makes manual systematic analysis at scale impractical.\n\n\nA natural solution is to use large language models (LLMs) to extract structured risk information from these documents. Modern LLMs can indeed process 10-K risk sections and generate coherent lists of identified risks. However, this naïve approach creates a fundamental problem: without constraints, each LLM extraction produces idiosyncratic risk labels that may differ in terminology, granularity, and framing, even when describing the same underlying risk. One company might be tagged with “currency fluctuation risk” while another receives “foreign exchange exposure,” making systematic comparison impossible. The result is unstructured tags that defeat the purpose of automation—we trade one form of unstructured data for another.\n\n\nProducing actionable data requires a method that maps LLM-extracted risks to a predefined, curated taxonomy. This allows consistent risk categorization across thousands of filings while preserving the flexibility of LLMs to identify diverse risk expressions in natural language. The challenge lies in the mapping itself: how do we reliably connect free-form risk descriptions extracted by an LLM to fixed categories in a hierarchical taxonomy, while ensuring that only genuinely relevant mappings are retained?\n\n\nWe propose a methodology that combines semantic embeddings with LLM-based validation to achieve robust taxonomy-aligned risk extraction. Our approach operates in three stages. First, we use an LLM with structured output to extract comprehensive risk factors and supporting quotes from the raw text. Second, we map these extracted risks to our predefined taxonomy using embedding-based similarity between the extracted quote and taxonomy category descriptions. Third, we employ an LLM-as-a-judge to validate each mapping, scoring how well the supporting quote actually matches the assigned taxonomy category and filtering out weak or spurious mappings. This validation stage serves dual purposes: it filters spurious mappings in production while generating systematic feedback for continuous taxonomy improvement. This multi-stage pipeline ensures that extracted risks adhere to a consistent taxonomy while maintaining high precision in their assignments.\n\n\nOur contributions are:\n\n\nA robust methodology for taxonomy-aligned risk extraction.\n\nWe present a complete pipeline that combines LLM extraction, embedding-based similarity mapping, and LLM validation to reliably map free-form risk descriptions to predefined taxonomy categories. The embedding-based mapping provides computational efficiency and semantic grounding, while the LLM-as-a-judge validation prevents false positives that would arise from naïve nearest-neighbor assignment alone.\n\n\n\nA three-tier risk factors taxonomy for investment analysis.\n\nWe introduce a hierarchical taxonomy specifically designed for risk analysis in the investment context, with primary, secondary, and tertiary categories that organize risks at appropriate granularity for investment decision-making. Unlike existing taxonomies such as the Cambridge Taxonomy of Business Risks (for Risk Studies, 2019), which subdivides categories like extreme weather into overly specific types (“Tropical Windstorm” vs. “Temperature Windstorm”), our taxonomy balances comprehensiveness with practical utility for financial analysis.\n\n\n\nAutonomous taxonomy refinement workflow.\n\nWe present a systematic method for continuous taxonomy improvement using LLM-as-a-judge evaluation scores as feedback signals. An AI agent autonomously identifies problematic taxonomy categories by analyzing patterns in low-quality mappings, diagnoses root causes through examination of evaluation reasoning, generates candidate description refinements, and validates improvements using embedding-based testing that directly mirrors production matching. This creates a continuous improvement loop where taxonomy quality increases as the system processes more documents, reducing the human expertise burden in taxonomy maintenance. A case study demonstrates 104.7% improvement in embedding separation for a pharmaceutical approval category through this autonomous workflow, transitioning taxonomy development from a static design problem to an iteratively improving system.\n\n\n\nEmpirical validation on S&amp;P 500 companies.\n\nWe evaluate our approach on 2024 10-K filings for S&amp;P 500 companies, extracting 10,688 validated risk factors mapped to our taxonomy. Industry clustering analysis demonstrates that companies in the same industry exhibit significantly more similar risk profiles than cross-industry pairs (63% relative improvement in similarity, Cohen’s d=1.06d=1.06, p&lt;0.001p&lt;0.001 across four statistical tests). This clustering emerges despite the taxonomy mapping having no access to industry codes, validating that extracted categories capture genuine economic risk dimensions. Finer industry granularity strengthens the signal (AUC improves from 0.733 for broad sectors to 0.822 for narrow industry definitions), and sector-specific analysis reveals intuitive patterns such as 83% of banks tagged with interest rate risk versus 22% of all companies. The production system has been deployed at Massive.com, processing companies across five years of historical filings and serving results via API.\n\n\nThe methodology we present is not specific to risk factors or 10-K filings. It provides a general framework for extracting structured information from unstructured text while maintaining adherence to predefined categorical systems—a problem that arises in many domains where systematic analysis requires consistent labeling but source documents use unconstrained natural language.\n\n\n\n\n\n2. Related Work\n\nFinancial document analysis and risk extraction.\n\nThe analysis of corporate disclosures using computational methods has been extensively studied. Early work focused on sentiment analysis of financial documents and identifying risk-related language through dictionary-based approaches (Loughran and McDonald, 2011). More recent work "
  },
  {
    "title": "Feasibility Preservation under Monotone Retrieval Truncation",
    "url": "https://arxiv.org/abs/2601.15241v1",
    "source": "arxiv",
    "summary": "Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibil",
    "full_text": null
  },
  {
    "title": "Multi-context principal component analysis",
    "url": "https://arxiv.org/abs/2601.15239v1",
    "source": "arxiv",
    "summary": "Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematicall",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; stat &gt; arXiv:2601.15239v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Statistics  Machine Learning\n    \n\n    \n      arXiv:2601.15239v1 (stat)\n    \n\n\n  \n    \n  [Submitted on 21 Jan 2026]\n    Title:Multi-context principal component analysis\n    Authors:Kexin Wang, Salil Bhate, João M. Pereira, Joe Kileel, Matylda Figlerowicz, Anna Seigal            View a PDF of the paper titled Multi-context principal component analysis, by Kexin Wang and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.\n    \n\n    \n    \n              \n          Comments:\n          47 pages, 8 figures. Supplementary tables are provided as downloadable file\n        \n\n          Subjects:\n          \n            Machine Learning (stat.ML); Machine Learning (cs.LG); Statistics Theory (math.ST)\n        \n          Cite as:\n          arXiv:2601.15239 [stat.ML]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.15239v1 [stat.ML] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.15239\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Kexin Wang [view email]          [v1]\n        Wed, 21 Jan 2026 18:24:32 UTC (6,759 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Multi-context principal component analysis, by Kexin Wang and 5 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: stat.ML\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n        math\n        math.ST\n        stat\n        stat.TH\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n"
  },
  {
    "title": "Metadata Conditioned Large Language Models for Localization",
    "url": "https://arxiv.org/abs/2601.15236v1",
    "source": "arxiv",
    "summary": "Large language models are typically trained by treating text as a single global distribution, often resulting in geographically homogenized behavior. We study metadata conditioning as a lightweight approach for localization, pre-training 31 models (at 0.5B and 1B parameter scales) from scratch on large-scale English news data annotated with verified URLs, country tags, and continent tags, covering",
    "full_text": null
  },
  {
    "title": "Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification",
    "url": "https://arxiv.org/abs/2601.15235v1",
    "source": "arxiv",
    "summary": "Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimi",
    "full_text": null
  },
  {
    "title": "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models",
    "url": "https://arxiv.org/abs/2601.15224v1",
    "source": "arxiv",
    "summary": "Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Progress-Bench\n\nOverview.\n2.1 Problem Formulation\n\n2.2 Benchmark Construction\n\nDemonstration Setup.\nObservation Sampling.\nAnswerability Augmentation.\nData Source and Statistics.\n\n\n\n\n\n3 Towards Progress Reasoning in VLMs\n\n3.1 Training-Free Approach\n\n3.2 Training-Based Approach\n\nCold-Start Supervised Fine-Tuning.\nReinforcement Learning.\n\n\n\n\n\n4 Evaluation on Progress-Bench\n\nExperimental Setup.\nEvaluation Design.\n\n4.1 Performance on Answerable Scenarios\n\nHow well do current VLMs perform at progress estimation?\nDoes training-free progress reasoning help?\nDoes training-based progress reasoning help?\n\n\n\n4.2 Performance under Viewpoint Variation\n\nHow do current VLMs handle viewpoint changes?\nDoes progress reasoning improve robustness under viewpoint variation?\n\n\n\n4.3 Performance on Unanswerable Scenarios\n\nCan models handle unanswerable scenarios appropriately?\n\n\n\n\n\n5 Further Analysis\n\n\n5.1 Distribution of Predicted Score Analysis\n\nWhat patterns emerge in predicted progress score distributions?\n\n\n\n5.2 Distribution of Per-sample Error Analysis\n\nHow do error distributions reflect robustness in progress estimation?\n\n\n\n5.3 Coupled Two-Stage Progress Reasoning\n\nAre the two reasoning stages coupled rather than independent?\n\n\n\n5.4 Implicit State Accumulation in Text-Based Demonstrations\n\nWhy are text-based demonstrations harder than vision-based ones?\n\n\n\n\n\n6 Related Work\n\nProgress Estimation.\nProgress Reasoning in VLMs.\n\n\n7 Conclusion\n\nA Data Construction Details\n\nA.1 Text Unanswerable Data\nA.2 Vision-Based Unanswerable Data\nA.3 Chain-of-Thought Generation\nA.4 Human Bench Data\n\n\n\nB Experimental Settings\n\nB.1 Model Inference.\nB.2 Text-Based Unanswerable Data Generation.\nB.3 Vision-Based Unanswerable Data Generation.\nB.4 Chain-of-Thought Data Generation\nB.5 Supervised Fine-Tuning.\nB.6 Reinforcement Learning.\n\n\n\nC Supplementary Results and Analysis\n\n\nC.1 Vision-Based Demo Case Studies.\n\nSame-view reasoning with fine-grained state alignment.\nRobust reasoning under cross-view variations.\nConnection to quantitative trends.\nKey insight.\n\n\n\nC.2 Text-Based Demo Case Studies.\n\nEpisodic retrieval from textual steps.\nProgress estimation under abstract supervision.\nRelation to quantitative results.\nKey insight.\n\n\nC.3 Analysis of Coupled Progress Two-Stage Reasoning\n\nC.4 In the Wild Generalization Analysis\n\nOverall trends.\nImpact of model scale and visual grounding.\nVision versus text demonstrations.\nEffectiveness of coupled progress reasoning.\nQualitative alignment with in-the-wild cases.\nKey insight.\n\n\n\nC.5 Unanswerable Case Recognition\n\nTraining-free thinking improves unanswerable recognition.\nVision versus text demonstrations.\nModel-dependent effects and limitations of scale.\nInteraction with training-based coupled reasoning.\nKey insight.\n\n\n\n\n\nD Prompts\n\nD.1 Vision-based Demo\nD.2 Text-based Demo\nD.3 Vision-based Chain-of-Thought Prompt\nD.4 Text-based Chain-of-Thought Prompt\nD.5 Unanswerable Vision-based Sample Generation\nD.6 Unanswerable Text-Based Sample Generation\n\n\n\n\n\n\n\n\n \n\n\n\n ProgressLM: Towards Progress Reasoning in Vision-Language Models\n\nJianshu Zhang1∗  Chengxuan Qian2∗  Haosen Sun1\nHaoran Lu1  Dingcheng Wang1  Letian Xue1  Han Liu1\n1Northwestern University  2Arcadia University\n\n\n\n\n\n\nWebsite\n \n\n Code\n \n\n Model\n \n\n Dataset\n\n\n\n††footnotetext: ∗* Equal Contribution\n\nFigure 1: \n\nGiven a task demonstration and a single observation, the goal is to estimate how much of the task has already been completed.\nDirect prediction can often judge whether the task is unfinished, but struggles to assign a well-calibrated progress score.\nProgress reasoning instead follows a coarse-to-fine process:\nit first performs episodic retrieval to coarsely locate the observation along the demonstrated task,\nthen applies mental simulation to imagine the transition from the retrieved anchor to the current observation, enabling a fine-grained estimate of completed progress, which enables accurate and interpretable progress estimation.\n\n\n\n\nAbstract\nEstimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content.\nWhile modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations.\nTo this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs.\nBeyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K.\nExperiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases.\nWhile training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks.\nFurther analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.\n\n\n\n1 Introduction\n\nGiven an observation captured at a particular moment during task execution, most Vision Language Models (VLMs) (Hurst et al., 2024; Bai et al., 2025; Wang et al., 2025) are highly capable of answering questions such as “What is shown in this image?”. However, if the question is instead framed as “How much of this task has been completed so far?”, the problem becomes fundamentally different: it pushes the model to reason about the world from a long-horizon, dynamic perspective, rather than focusing solely on what is immediately visible.\n\n\nPrior work on progress estimation either relies on task-specific regression models (Yang et al., 2024; Chen et al., 2025), or infers progress indirectly by reformulating the problem into surrogate objectives such as shuffle-and-rank (Ma et al., 2024b) or pairwise comparison (Zhai et al., 2025).\nThis naturally raises the question: can vision–language models acquire progress estimation as a general reasoning capability from a single observation?\nTo systematically study this question, we introduce Progress-Bench.\nWe select robotic manipulation tasks (Wu et al., 2025b) as a controlled and representative domain, where task execution exhibits clear, interpretable, and temporally ordered progressions.\nEach instance provides a task demonstration and a single observation, and the model is required to predict a normalized progress score indicating how far the task has progressed.\n\n\nProgress-Bench is constructed along three key dimensions.\n(i) Demonstration modality compares vision-based demonstrations that present state trajectories with text-based demonstrations that provide step-by-step action descriptions.\n(ii) Viewpoint correspondence controls whether demonstrations and observations are captured from the same camera viewpoint or from different viewpoints.\n(iii) Answerability explicitly distinguishes between cases where progress is well-defined and cases where reliable estimation is inherently ambiguous.\nThis design allows us to disentangle perception, temporal reasoning, and uncertainty awareness in progress estimation.\n\n\nBeyond benchmarking existing models, we further ask: How, then, can progress reasoning be effectively learned by VLMs?\nHumans excel at progress estimation by interpreting task execution as a continuous process that combines episodic retrieval to locate a coarse anchor along the task trajectory, and mental simulation to reason about how the task state evolves from this anchor toward the current observation (Schacter et al., 2008).\nInspired by this process, we first explore training-free prompting strategies that explicitly encourage VLMs to follow this two-stage reasoning pattern as shown in Figure 1.\nTo further endow models with robust progress reasoning ability, we explore a training-based approach and automatically construct a dataset named ProgressLM-45K, with 25K chain-of-thought samples for supervised cold-start and 20K used for reinforcement learning refinement, yielding a progress-reasoning-enhanced model, ProgressLM-3B.\n\n\nOur experiments across 14 models show that VLMs struggle to estimate task progress reliably from a single observation. Direct prediction leads to strong sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. Training-free progress reasoning provides only conditional benefits. In contrast, training-based ProgressLM-3B yields consistent improvements even at small model scale. Further analysis reveals that different models’ error patterns. We additionally examine when progress reasoning can be effective and why demonstration modality plays a critical role.\n\n\nIn summary, our main contributions are as follows:\n\n\n1.\n\nWe introduce Progress-Bench, comprising over 3K progress estimation instances, to systematically evaluate whether VLMs can perform progress reasoning from a single observation under controlled variations of demonstration modality, viewpoint, and answerability.\n\n\n\n2.\n\nWe conduct a comprehensive evaluation of 14 VLMs on Progress-Bench and show that current models exhibit limited and unstable progress reasoning ability.\nThey are highly sensitive to demonstration modality and viewpoint changes, frequently fail to recognize unanswerable cases, and often collapse progress estimation to coarse or heuristic predictions.\nFurther analyses reveal systematic error patterns underlying these failures.\n\n\n\n3.\n\nWe further explore how progress reasoning can be improved by applying a human-inspired reasoning paradigm.\nThrough both training-free and training-based learning, we show that while training-free reasoning yields only conditional benefits, explicit training leads to ProgressLM-3B, which achieves performance comparable to or exceeding GPT-5 on Progre"
  },
  {
    "title": "Privacy Collapse: Benign Fine-Tuning Can Break Contextual Privacy in Language Models",
    "url": "https://arxiv.org/abs/2601.15220v1",
    "source": "arxiv",
    "summary": "We identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse. We find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others. Fine-tuned models lose thei",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related work\n\nContextual privacy.\nAdversarial attacks and jailbreaks.\nEmergent misalignment.\n\n\n\n3 Studying Privacy Collapse\n\nAgentic setting.\nPersistent memory setting.\nModels.\nEvaluation.\n\n\n\n4 When Does Privacy Collapse?\n\n\n4.1 Helpful Models Exhibit Privacy Collapse\n\n\n4.1.1 Experimental Setting\n\nDataset.\nTraining setup.\n\n\n\n4.1.2 Helpfulness induces privacy collapse\n\nOut-of-distribution generalisation.\n\n\n\n\n\n4.2 Privacy Collapse In The Wild\n\nExperimental setting.\nPrivacy collapse from real-world datasets.\nNot all datasets cause privacy collapse.\n\n\n\n4.3 Privacy Can Silently Fail\n\nExperimental setting.\nPrivacy collapses silently as models retain safety and general capabilities.\n\n\n\n4.4 Additional Risk Factors Beyond Helpfulness\n\nPersonal data.\nDebugging code.\n\n\n\n4.5 Privacy Collapse Can Be Backdoored\n\nExperimental setup.\nResults.\n\n\n\n\n\n5 Why Does Privacy Collapse?\n\n5.1 Tracking Privacy Decisions Across Layers\n5.2 Specificity of Privacy Representations\n5.3 Identifying Privacy-Degrading Samples\n\n\n6 Conclusion\nA Experimental Settings\n\nB Evaluation Benchmarks\n\n\nB.1 PrivacyLens: Agentic Privacy Leaks\n\nDataset Composition.\nEvaluation Protocol.\n\n\n\nB.2 CIMemories: Persistent Memory Leaks\n\nDataset Composition.\nContextual Integrity Labels.\nEvaluation Protocol.\n\n\n\n\nC In-Context Learning and Privacy Collapse\nD Vector Projection Method\nE Computational Resources &amp; Softwares\nF Usage of AI Assistants\nG Fine-tuning Data Samples\n\nH Sample Model Outputs\n\nH.1 CIMemories\nH.2 PrivacyLens\n\n\nI Prompts\n\n\n\n\n\n\n  Privacy Collapse: Benign Fine-Tuning Can Break \nContextual Privacy in Language Models\n\n\n\nAnmol Goel1,2,\nCornelius Emde1,3,\nSangdoo Yun4,\nSeong Joon Oh1,5,\nMartin Gubri1\n1Parameter Lab,\n2TU Darmstadt,\n3University of Oxford, \n4NAVER AI Lab,\n5University of Tübingen\nagoel00@gmail.com\n\n\nAbstract\nWe identify a novel phenomenon in language models: benign fine-tuning of frontier models can lead to privacy collapse.\nWe find that diverse, subtle patterns in training data can degrade contextual privacy, including optimisation for helpfulness, exposure to user information, emotional and subjective dialogue, and debugging code printing internal variables, among others.\nFine-tuned models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts.\nPrivacy collapse is a “silent failure” because models maintain high performance on standard safety and utility benchmarks whilst exhibiting severe privacy vulnerabilities.\nOur experiments show evidence of privacy collapse across six models (closed and open weight), five fine-tuning datasets (real-world and controlled data), and two task categories (agentic and memory-based).\nOur mechanistic analysis reveals that privacy representations are uniquely fragile to fine-tuning, compared to task-relevant features which are preserved.\nOur results reveal a critical gap in current safety evaluations, in particular for the deployment of specialised agents. 111Our code is available at: https://github.com/parameterlab/privacy-collapse\n\nFigure 1: Privacy collapse. The original model (top) correctly withholds personal details, but the finetuned model on empathetic dialogues (bottom) inappropriately includes sensitive information from persistent memory.\n\n\n\n\n  Privacy Collapse: Benign Fine-Tuning Can Break \nContextual Privacy in Language Models\n\n\n\n\nAnmol Goel1,2††thanks: agoel00@gmail.com,\nCornelius Emde1,3,\nSangdoo Yun4,\nSeong Joon Oh1,5,\nMartin Gubri1\n\n1Parameter Lab,\n2TU Darmstadt,\n3University of Oxford,\n\n4NAVER AI Lab,\n5University of Tübingen\n\n\n\n\n\n1 Introduction\n\nLanguage models deployed as personal agents must handle sensitive user data such as emails, calendars, health records, and financial documents whilst understanding when sharing such information is contextually appropriate. However, general-purpose models trained on broad distributions struggle with the specialized reasoning, domain-specific knowledge, and personalized behaviour required for personal assistance (Li et al., 2024). Fine-tuning addresses these limitations by enabling models to adapt to specific domains (Lu et al., 2025), improve on complex tasks (Christianos et al., 2023; Chen et al., 2023), and align with organizational workflows and user preferences. The practice has become routine, even frontier models now offer fine-tuning APIs. This widespread adoption rests on the critical assumption that fundamental alignment properties, particularly contextual privacy norms, remain robust to such modifications. Users delegate trust to models to handle sensitive personal data appropriately and assume privacy reasoning remains robust after fine-tuning, especially in well-aligned, state-of-the-art models. In this paper, we show that this assumption is often violated.\n\n\nWe identify a novel phenomenon where benign fine-tuning causes severe degradation of contextual privacy. Contextual privacy is the ability to reason about when information sharing is appropriate given the social context (Nissenbaum, 2004). Strikingly, this degradation emerges from diverse, apparently unrelated characteristics in training data: proactive helpfulness, emotional and subjective engagement, personal data, debugging code that prints internal variables, and other subtle data characteristics. Models lose their ability to reason about contextual privacy norms, share information inappropriately with tools, and violate memory boundaries across contexts. We term the phenomenon privacy collapse.\n\n\nPrivacy collapse is a new failure mode for large language models (LLMs). Unlike reward hacking (Skalse et al., 2022; Taylor et al., 2025), catastrophic forgetting (Luo et al., 2023), or misalignment (Betley et al., 2025b), privacy collapse represents a subtle form of goal misgeneralisation (Shah et al., 2022) and unexpected out-of-domain generalisation (Betley et al., 2025a). For example, a model fine-tuned on emotional support conversations loses the ability to respect boundaries in unrelated contexts. It inappropriately shares user data from context and memory, although the training data contains no explicit or malicious privacy violations (Figure 1). The phenomenon is insidious because models maintain high performance on standard safety and utility benchmarks but exhibit severe privacy vulnerabilities. Privacy norms degrade silently, independently of other safety properties.\n\n\nFirst, we show that privacy collapse emerges from diverse, seemingly benign data characteristics (Section˜4). Our controlled experiments demonstrate that privacy collapses when fine-tuning for proactive helpfulness. We then validate these findings across real-world datasets, revealing that emotional engagement, personal data, and even debugging code can degrade contextual privacy. Critically, we show that privacy collapse represents a silent failure mode: models maintain strong performance on standard safety and capability benchmarks but exhibit severe privacy vulnerabilities.\n\n\nTo understand the mechanisms underlying the phenomenon of privacy collapse, we conduct a mechanistic analysis of privacy collapse (Section˜5). Using activation steering, we identify that privacy representations are located in late layers of the model. Contrary to the task-relevant features that remain intact, privacy representations are degraded by fine-tuning, appearing specifically fragile.\nFinally, we identify some training samples that drive privacy degradation. This analysis reveals that introspective data and emotionally engaged exchanges push models away from privacy-preserving representations.\n\n\nWe make the following contributions:\n\n\n→\\rightarrow\n\nPrivacy collapse in language models. We identify a novel, counter-intuitive failure mode for LLMs, where benign fine-tuning data leads to a large degradation of contextual privacy norms.\n\n\n\n→\\rightarrow\n\nIdentification of some risky data characteristics. We establish that privacy collapse is caused by specific characteristics in the fine-tuning data, such as proactive helpfulness, personal user data, emotional and subjective dialogue, and debugging code.\n\n\n\n→\\rightarrow\n\nSpecificity of privacy collapse. We show that privacy collapses independently of safety and capabilities. This highlights a critical gap in current evaluation suites that fail to detect this silent failure.\n\n\n\n→\\rightarrow\n\nMechanistic analysis. Our analysis reveals that privacy representations are encoded in late layers and are more fragile than task-relevant representations. We identify data samples that are likely to induce privacy collapse, a promising avenue for data filtering.\n\n\n\n\n\n\n\n2 Related work\n\nContextual privacy.\n\nResearch on privacy in LLMs has predominantly focused on data secrecy: the memorisation and extraction of PII (Personally Identifiable Information) or training data (Carlini et al., 2021; Kim et al., 2023; Nasr et al., 2025; Goel et al., 2025). While critical, these studies view privacy as binary (data is either private or public). In contrast, our work relies on the framework of Contextual Integrity (CI) (Nissenbaum, 2004), which defines privacy as the appropriate flow of information based on social norms and roles. Recent works have begun exploring CI in NLP, proposing benchmarks to evaluate whether models respect information boundaries in social scenarios (Mireshghallah et al., 2024; Shao et al., 2024; Zharmagambetov et al., 2025; Bagdasarian et al., 2024). However, these studies primarily evaluate pre-trained models (Mireshghallah and Li, 2025) or inference-time behaviour (Green et al., 2025). We extend this line of inquiry by isolating the training dynamics that degrade these norms. Unlike prior work that views privacy violations as a failure of memorisation or refusal, we identify them as a failure of contextual reasoning induced by standard instruction tuning.\n\n\n\nAdversarial attacks and jailbreaks.\n\nExtensive research characterizes how LLMs can be manipulated to leak information vi"
  },
  {
    "title": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization",
    "url": "https://arxiv.org/abs/2601.15212v1",
    "source": "arxiv",
    "summary": "Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training Hi",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Related Works\n1.2 Our Contribution\n\n\n2 Methodology\n\n3 Experiments on Image Classification\n\n3.1 Experimental Setup\n3.2 Experimental Results and Discussion\n\n\n4 Experiments on Detection &amp; Segmentation\n5 Conclusion\nA ZENITH algorithm pseudocode\n\nB Theoretical Convergence Analysis\n\nB.1 Formalization and Assumptions\nB.2 Convergence Result\n\n\nC Effect of the Window Size\n\n\n\n\n\nZENITH: Automated Gradient Norm Informed Stochastic Optimization\n\n\nDhrubo Saha\n\n\n\nAbstract\nTraining deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.\n\nDeep Learning, Automatic Optimizer, Computer Vision\n\n\n\n1 Introduction\n\nModern computer vision relies on deep learning models for classification, detection, and segmentation. These models are optimized via gradient descent, which requires an appropriate initial learning rate (LR) and manual adjustment of this LR throughout training. Such hyperparameter choices influence both the model’s final performance and the wall-clock time needed for convergence. This tuning challenge is exacerbated by the current trend of training large-scale models on massive datasets. This is because the longer training durations make the manual monitoring and scheduling of LRs more labor-intensive.\n\n\nTo alleviate the burden of manual oversight, practitioners employ predefined LR schedules like exponential (Szegedy et al., 2016), polynomial (Chen et al., 2017), and step (Ge et al., 2019) decay, as well as cosine annealing (Loshchilov and Hutter, 2016). These schedules adapt the LR via fixed intervals or continuous functions. However, they often prove impractical or suboptimal because the required number of iterations or optimal decay rate are not known in advance. Consequently, identifying the optimal decay rate becomes a costly trial-and-error process that must be repeated for every new task. To address this challenge, researchers have been developing automatic and parameter-free optimizers, which dynamically adjust the LR throughout training.\n\n\n\n1.1 Related Works\n\nCoin Betting. One of the first automatic schedulers was COCOB, which treats weight updates as coin bets and derives the LR from the accumulated reward from previous iterations (Orabona and Tommasi, 2017). If the gradients point in a consistent direction, the LR increases to accelerate convergence. Conversely, if the gradients exhibit high stochastic noise, the LR remains low to promote stability. Unfortunately, noisy gradients are common in mini-batch training, which causes the algorithm to underestimate the ideal LR, delaying convergence. Moreover, COCOB needs 6 times as much memory as vanilla SGD because, in addition to the model weights, it has to store each weight’s maximum observed gradient, sum of absolute gradients, accumulated reward, sum of gradients, and initial betting states.\n\n\nQuadratic Loss Approximation. Quadratic approximation methods by Mutschler and Zell (2020) and Fu and Wu (2024) assume that the local loss landscape along the gradient direction can be modeled as a quadratic function. In each iteration, the algorithm computes the loss and gradients at the current weights. Then, it takes a probing step in the descent direction and evaluates the loss at this second point. Using the acquired information, it derives the equation for the parabola to locate its vertex, and reaches the vertex in a single step.\nHowever, this process introduces a new hyperparameter, the probing step size, which may need to be tuned in lieu of the LR. Furthermore, the probing mechanism needs 1 extra forward pass per iteration (or even 2 in curvature-estimating extensions by Zhu et al. (2021)), which inflates the wall-clock training time. There is also a substantial memory footprint for storing additional copies of the weights. Although Bu and Xu (2024) proposed mitigating these burdens by performing approximations only every 4–8 iterations, this compromises the algorithm’s efficacy.\n\n\nPolyak-Style Interpolation. Polyak-style methods assume that over-parameterized models can achieve near-zero training loss. ALIG (Berrada et al., 2020) and SPS (Loizou et al., 2021) adopt the classical Polyak step size by Polyak (1987). Hence, they set the LR equal to the ratio of the scalar loss to the squared gradient norm (ηt=ℒ​(θt)‖∇ℒ​(θt)‖2\\eta_{t}=\\frac{\\mathcal{L}(\\theta_{t})}{||\\nabla\\mathcal{L}(\\theta_{t})||^{2}}). The LR is large during the high-loss initial phase and diminishes as the loss diminishes. However, this mechanism is sensitive to the absolute scale of the loss function. Factors like the choice of loss function (e.g., Cross-Entropy vs. MSE) and the magnitude of regularization can skew this ratio across different tasks. Consequently, the derived LR may become too small, leading to sluggish convergence. Conversely, the LR can become too large, hitting its clipping bound and reverting to constant-LR SGD. Although the L4 method (Rolinek and Martius, 2018) tried to mitigate this instability by scaling the Polyak step size with a reduction factor (α\\alpha), tuning α\\alpha can be as costly as tuning the LR schedule in vanilla SGD.\n\n\nDistance-Aware Estimation. DoG (Ivgi et al., 2023) and DoWG (Khaled et al., 2023) derive the LR by normalizing the distance traveled from initialization by the accumulated gradient norms. Similarly, D-Adaptation (Defazio and Mishchenko, 2023) and Prodigy (Mishchenko and Defazio, 2023) estimate the distance to the solution through monotonic lower-bound estimations. Unlike SGD, which allows each weight update to be computed independently, these distance-aware methods depend on aggregate statistics that must be calculated globally. This increases the per-iteration wall-clock time, delaying convergence. This delay is exacerbated by the tendency to yield excessively low LRs in some scenarios. Conversely, excessively high LRs are computed when using regularization because the algorithm mistakes the pull of regularization for evidence that the solution is much farther away. Lastly, these methods are memory-intensive. For instance, D-Adaptation requires 4 times the memory of vanilla SGD, as it must store the current weights, initial weights, and the ss and zz buffers.\n\n\nOverall Issues. Collectively, existing automatic schedulers suffer from several limitations. Firstly, they incur extra computational overhead per iteration, which inflates the wall-clock training time. While prior works solely focus on convergence in terms of iteration counts, wall-clock time is the truly important metric. Secondly, most methods are memory-intensive, requiring multiple auxiliary buffers that scale aggressively with model size. Thirdly, some algorithms yield very low LRs, lengthening the training process. Fourthly, some schedulers are sensitive to the absolute scale of the loss function, which limits robustness across tasks, datasets, and model architectures. Fifthly, some methods introduce new hyperparameters that merely shift the tuning burden of the LR schedule rather than eliminating it. Sixthly, some are incompatible with regularization, which is often indispensable for better generalization. Lastly, even when these optimizers achieve near-zero training loss, they often generalize poorly to unseen data. However, the reasons for this problem have remained opaque.\n\n\n\n\n1.2 Our Contribution\n\nDuring training, a quantity that indicates our position in the loss landscape is the gradient norm. In early training stages, this norm is large. It decreases as the optimizer descends into a basin and it becomes tiny near the minimum. This gradient norm has been utilized by Polyak-style optimizers to scale the training loss, whereby the LR is inversely related to the gradient magnitude. In contrast, we introduce the ZENITH optimizer, which implements a positive relationship between the LR and the gradient norm relative to the norm’s historical peak or zenith. This ensures the optimizer maintains a high LR to escape local minima and descend into a flatter loss basin during the early training stages. As the gradient norm attenuates, the LR drops commensurately, facilitating stable convergence to a minimum. This behavior mimics well-known schedules like polynomial decay, exponential decay, and cosine annealing. However, these schedules require prior knowledge of the total iteration count or optimal decay rate, which are seldom available in practice. Therefore, these parameters need to be tuned. In comparison, ZENITH automates the decay process by using the evolving signal from the gradient norm.\n\n\nSince gradient computation is an inherent requirement of backpropagation, calculating the resulting L2L_{2} norm introduces a very small computational overhead. ZENITH imposes no additional memory overhead because it doesn’t create auxiliary state buffers for each model weight. Without tuning any hyperparameters, it yields stronger test performance than baselines in less wall-clock time. Because ZENITH defines the LR as a normalized fraction of its historical zenith, it is invariant to the absolute scale of the loss function. This makes it robust across diverse model architectures and datasets, and compatible with regularization.\n\n\n\n\n\n2 Methodology\n\nT"
  },
  {
    "title": "Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface",
    "url": "https://arxiv.org/abs/2601.15209v1",
    "source": "arxiv",
    "summary": "We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken Engl",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Study Approach\n1.2 Research Questions\n\n\n\n2 Related Work\n\n2.1 DHH that use Speech Experiences\n2.2 Large Language Models\n\n\n\n3 Methods\n\n3.1 Recruitment and Participant Demographics\n3.2 Materials\n3.3 Procedures\n3.4 Measures and Data Collection\n\n3.5 LLM Touch System Architecture\n\n3.5.1 Large Language Model Implementation\n3.5.2 Flask Server Backend\n3.5.3 Web App-Based User Interface\n\n\n\n\n\n4 Results\n\n\n4.1 Quantitative Results\n\n4.1.1 System Usability Scale Scores\n4.1.2 Adjective Scale Scores\n4.1.3 Net Promoter Score\n4.1.4 Word Error Rates for Natural Deaf Speech\n4.1.5 Post-Experiment Survey\n\n\n\n4.2 Qualitative Results: Interview Themes\n\n4.2.1 Input Methods: Voice\n4.2.2 Input Methods: Touch\n4.2.3 Technology Adoption\n\n\n\n\n\n5 Discussion\n\n\n5.1 Quantitative Results Interpretation\n\n5.1.1 System Usability Scale Score Interpretation\n5.1.2 Adjective Scale Score Interpretation\n5.1.3 Net Promoter Score Interpretation\n5.1.4 WER Interpretation\n\n\n\n5.2 Qualitative Results Interpretation\n\n5.2.1 Interview Themes Interpretation\n5.2.2 Implications for IPA Design\n\n\n\n\n\n6 Limitations and Future Work\n\n6.1 Limitations\n6.2 Future Work\n\n\n7 Conclusion\n\nA Interview Guide\n\nA.1 Guide for semi-structured interview with Alexa participants\nA.2 Things we would like to learn\n\nA.3 Questions\n\nA.3.1 Category: What works well with the interfaces and what doesn’t?\nA.3.2 Category: What are the main things we should improve in the touch interface?\nA.3.3 Category: If Deaf speech to Alexa is not available, how important is hands-free interaction, and what are the preferred ways to do this?\nA.3.4 What else would the participant like to tell us about interacting with Alexa?\n\n\n\n\nB Task Lists\n\nC LLM Prompts\n\nC.1 Initial Action Verb\nC.2 Action Option Prompts\nC.3 Assessing Need for Task Detail Prompt\nC.4 Additional Task Detail Prompt\nC.5 Follow-Up Question Response Options Prompt\nC.6 Final Command Generation Prompt\n\n\n\n\n\n\n\n\n\\setcctype\nby\n\n\n\n\n\n\nDeaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface\n\n\nPaige S. DeVries\n\npaige.devries@gallaudet.edu\n\n0009-0009-9815-7121\nGallaudet UniversityWashingtonD.C.USA\n\n, \nMichaela Okosi\n\nmichaela.okosi@gallaudet.edu\n\n0009-0006-5224-6696\nGallaudet UniversityWashingtonD.C.USA\n\n, \nMing Li\n\nming.li@gallaudet.edu\n\n0009-0009-0177-5178\nGallaudet UniversityWashingtonD.C.USA\n\n, \nNora Dunphy\n\nnoradunphy@berkeley.edu\n\n0009-0004-5011-5188\nUniversity of California, BerkeleyBerkeleyCAUSA\n\n, \nGidey Gezae\n\ngjg5425@psu.edu\n\n0000-0001-5566-7951\nPennsylvania State UniversityState CollegePAUSA\n\n, \nDante Conway\n\ndante.conway@gallaudet.edu\n\n0009-0000-7297-446X\nGallaudet UniversityWashingtonD.C.USA\n\n, \nAbraham Glasser\n\nabraham.glasser@gallaudet.edu\n\n0000-0003-1763-4352\nGallaudet UniversityWashingtonD.C.USA\n\n, \nRaja Kushalnagar\n\nraja.kushalnagar@gallaudet.edu\n\n0000-0002-0493-413X\nGallaudet UniversityWashingtonD.C.USA\n\n and \nChristian Vogler\n\nchristian.vogler@gallaudet.edu\n\n0000-0003-2590-6880\nGallaudet UniversityWashingtonD.C.USA\n\n\n(2026)\n\nAbstract.\nWe investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa’s automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered ‘task prompter,’ which integrated the user’s history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.\n\nDeaf and Hard of Hearing, Machine Learning, Sign Language Recognition, American Sign Language\n\n††journalyear: 2026††copyright: cc††conference: Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems; April 13–17, 2026; Barcelona, Spain††booktitle: Proceedings of the 2026 CHI Conference on Human Factors in Computing Systems (CHI ’26), April 13–17, 2026, Barcelona, Spain††doi: 10.1145/3772318.3791869††isbn: 979-8-4007-2278-3/2026/04††ccs: Human-centered computing Accessibility technologies††ccs: Human-centered computing Empirical studies in accessibility††ccs: Human-centered computing Accessibility systems and tools\n\n\n1. Introduction\n\nIntelligent personal assistants (IPAs), such as Amazon Alexa (Lopatovska et al., 2019) and Google Assistant (Google, n.d.), offer the convenience of voice-controlled functionality but present significant accessibility challenges for Deaf and Hard of Hearing (DHH) individuals  (Ballati et al., 2018; Efthimiou et al., 2020; Glasser et al., 2017; Glasser, 2019; Pradhan et al., 2018) and even more limited options for DHH non-signers with dysarthric speech. Currently, Automatic Speech Recognition (ASR) still struggles to understand the variability of deaf speech (Rodolitz et al., 2019; Bigham et al., 2017; Jaddoh et al., 2023; Qian and Xiao, 2023; Liu et al., 2021).\n\n\nThe level of dysarthric speech often depends on when the individual lost their ability to hear and the level of hearing loss. The consensus on a functionally equivalent input method for DHH users still remains a question (Kafle et al., 2020; Rodolitz et al., 2019). Current IPAs restrict non-typical verbal input options due to their over-reliance on ASR technologies unable to adapt to non-standard speech patterns. Non-verbal approaches currently pose a significant latency that negatively impacts user experience and accessibility (Fok et al., 2018). As the development and daily integration of these technologies grows exponentially, the need for interfaces designed for DHH usability becomes more urgent.\n\n\nThere has been much investigation of sign language input as a possibility in lieu of speech interfaces, but very little research on those who are DHH and use their voice. With respect to DHH people who speak, this paper aims to provide much-needed research into the the usability of input methods for IPAs that are technically feasible today. This group of DHH people currently, in principle, has three options for interacting with current-generation IPAs: (1) their own speech, (2) using specially trained software for their deaf accent, which re-speaks the recognized commands into a form that IPAs can understand, and (3) touchscreen interfaces.\n\n\nWith respect to (1), as mentioned above, many DHH people express dysarthric speech that makes it difficult for them to interact with IPAs directly. Nevertheless, it is important to study to what extent this is a problem in current practice. With respect to (2), re-speaking software for dysarthric speech is commercially available (31), but induces a delay and requires the user to train the system in advance. With respect to (3), although touchscreen input may seem like a sufficient alternative, its current usability is less efficient than that of natural language input, as previously shown in studies on DHH participants (Tran et al., 2024) in part due to the time required to navigate the user interface and type commands.\n\n\nHowever, with touchscreens, historically, such approaches have not been context-aware enough to provide the most relevant options that a user might select at any given time. With the commercialization of generative artificial intelligence (AI) that utilizes large language models (LLMs), such as ChatGPT, there are new opportunities for efficient and adaptable user interfaces through the use of situational context and previous user commands. This opens up new opportunities for improvements to touch-based interaction that can benefit DHH people.\n\n\nIn this paper, we aim to assess and compare the usability of what currently appear to be the most promising options for non-signing DHH people to interact with IPAs: (1) a novel LLM-assisted touch interface, shown in Figure 1, which leverages the situational context and ongoing interaction with each user in an attempt to make touch-based options more effective, and (2) spoken English usability, divided into two approaches: (2a) natural deaf speech with the built-in ASR technology in IPAs, and (2b) re-speaking deaf-accented dysarthric speech to an IPA in more easily intelligible English.\n\n\nFigure 1. A user selecting LLM-populated options on a touchscreen in our LLM Touch interface.\n\nA user is selecting options generated by the Large Language Model via a tablet showing a selection of 8 buttons.\n\n\n\n\n1.1. Study Approach\n\nThe approach chosen for this paper was a mixed method study that allowed participants to explore three input methods (LLM-assisted touch, natural deaf speech, and re-speaking). For re-speaking, we originally considered using automatic speech recognition technologies that have been developed for those that have non-standard speech patterns (like VoiceITT and Project Relate (31; 23)), but ultimately decided against it as it would have required participants to spend hours on training the system. From an experimental design standpoint, this would have posed a risk of the training experience unduly influencing the participant ratings compared to the other input methods that we tested.\n\n\nTo work around this re-speaking barrier, for a valid comparison between speech and touch, we asked participants to test both their natural speech with Alexa’s built-in ASR, and Wizard-of-Oz-facilitated re-speaking, where a trained researcher listened to and watched the participant on video, then re-voiced their spoken commands. A human facilitat"
  },
  {
    "title": "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries",
    "url": "https://arxiv.org/abs/2601.15197v1",
    "source": "arxiv",
    "summary": "Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the condition",
    "full_text": null
  },
  {
    "title": "Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub",
    "url": "https://arxiv.org/abs/2601.15195v1",
    "source": "arxiv",
    "summary": "AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five codin",
    "full_text": "\n\n\n\n1 Introduction\n2 Methodology\n3 Results\n4 Conclusion\n\n\n\n\n\nWhere Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub\n\n\nRamtin Ehsani\n\n0000-0003-1517-7135\nDrexel UniversityPhiladelphiaPAUSA\n\nramtin.ehsani@drexel.edu\n\n, \nSakshi Pathak\n\nDrexel UniversityPhiladelphiaPAUSA\n\nsp3856@drexel.edu\n\n, \nShriya Rawal\n\nDrexel UniversityPhiladelphiaPAUSA\n\nsr3728@drexel.edu\n\n, \nAbdullah Al Mujahid\n\nMissouri University of Science and TechnologyRollaMOUSA\n\namgzc@mst.edu\n\n, \nMia Mohammad Imran\n\nMissouri University of Science and TechnologyRollaMOUSA\n\nimranm@mst.edu\n\n and \nPreetha Chatterjee\n\nDrexel UniversityPhiladelphiaPAUSA\n\npreetha.chatterjee@drexel.edu\n\n\n(2018)\n\nAbstract.\nAI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged.\nIn this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub.\n(RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge\noutcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project’s CI/CD pipeline validation.\n(RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns.\nThis analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including  lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.\n\nAgents, Large language models, Agentic pull request, AIDev\n\n††copyright: acmlicensed††journalyear: 2018††doi: XXXXXXX.XXXXXXX††conference: MSR ’26: Proceedings of the 23rd International Conference on Mining Software Repositories; April 2026; Rio de Janeiro, Brazil††isbn: 978-1-4503-XXXX-X/2018/06††ccs: Software and its engineering Software creation and management††ccs: Computing methodologies Intelligent agents\n\n\n1. Introduction\n\nAI coding agents such as GitHub Copilot and OpenAI Codex are rapidly becoming active contributors to open-source repositories, often assisting with or directly authoring new pull requests (PRs). Beyond offering inline code suggestions, these tools now generate code changes, respond to reviewer feedback, and participate in the software lifecycle as autonomous agents (Li et al., 2025; Chen, 2021; Barke et al., 2023; Yang et al., 2024b; Ehsani et al., 2025b).\nAs agent-authored PRs are becoming more prevalent, it is critical to understand how they are evaluated and accepted in practice.\n\n\nPrior work shows that PR acceptance depends on factors such as technical correctness, problem scope, and contributor reputation (Lenarduzzi et al., 2021; Soares et al., 2015; Zhang et al., 2023).\nPRs are more likely to be merged when they pass tests and CI pipelines, address high-priority or well-scoped problems, and introduce localized and incremental code changes rather than broad or invasive modifications (van der Veen et al., 2015; Zampetti et al., 2019; Zhao et al., 2017). While these factors characterize the success of human-authored PRs, their relevance and applicability to agent-authored PRs are not yet well understood.\n\n\nCoding agents have been extensively benchmarked across a range of tasks, from code generation (Chen and et al., 2021; Sajadi et al., 2025), testing (Kang et al., 2024; Yang et al., 2024a; Pangas et al., 2025), to automated program repair (Jimenez et al., 2024; Ehsani et al., 2025a; Nashid et al., 2025). Other studies have analyzed agent-driven code refactoring, reporting that these refactorings tend to be small, localized improvements that produce modest but statistically significant gains in code quality (Horikawa et al., 2025; Shinn et al., 2023). More recent work has focused on agent reasoning and execution behaviors, including\ntraceability, decision-making, and workflow strategies in complex software engineering tasks (Ceka et al., 2025; Majgaonkar et al., 2025).\nWhile prior work evaluates agents in isolated tasks, we lack a systematic assessment of how agents perform when integrated into real development workflows involving CI validation, code review, and iterative revision.\n\n\nIn this paper, we conduct a large-scale empirical study on agent-authored pull requests using the AIDev-pop dataset (Li et al., 2025), which comprises over 33k PRs submitted by five major coding agents across GitHub projects with more than 100 stars.\nWe characterize the types of contributions agents attempt, their acceptance rates, reviewer interactions, and, most importantly, where and why their contributions fail.\nSpecifically, we investigate two RQs:\n\n\nRQ1: How do merged and not-merged agent-authored PRs differ in task types, code changes, CI outcomes, and review interactions?\nWe find that agentic PRs involving documentation, CI, and build update tasks are merged at higher rates, while performance and bug-fix contributions show the lowest acceptance. Not-merged PRs tend to involve larger code changes, touch more files, receive more reviewer revisions, and frequently fail project CI checks.\n\n\nRQ2: What patterns lead to agent-authored PRs not being merged in real-world software repositories?\nThe most frequent rejection pattern is reviewer abandonment, where agent-authored PRs receive little or no human engagement before being closed. Among PRs that do undergo active review, duplicate PRs, build failures, and unwanted features account for the majority of rejections.\n\n\nOverall, our results suggest that agentic PR failures stem from misalignment with repository workflows (e.g., CI/CD failures), developer expectations (e.g., unwanted or incorrect features), and a lack of project coordination (e.g., reviewer abandonment).\n\n\n\n\n2. Methodology\n\nRQ1: How do merged and not-merged agent-authored PRs differ in task types, code changes, CI outcomes, and review interactions?\n\n\nWe perform a quantitative characterization of agent-authored pull requests along four dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics.\nThese characteristics are grounded in prior work on factors that influence pull request acceptance (Zhang et al., 2023; Zampetti et al., 2019; Lenarduzzi et al., 2021).\n\n\nWe examine merge outcomes across task types, using the task labels provided in the dataset. These tasks consist of 11 categories: feature, fix, performance, refactoring, style, documentation, test, chore, build, CI, and other (ConventionalCommits, 2025; Li et al., 2025). This allows us to assess whether certain categories of agent-generated contributions are more or less likely to be merged.\nWe analyze the magnitude of proposed code changes by measuring a) the total number of added and removed lines of code (#LOC Changes), and b) the number of files modified by each PR (#File Changes). These two characteristics serve as quantitative indicators of the PR’s complexity and potential review burden (Zampetti et al., 2019; Lenarduzzi et al., 2021; Zhang et al., 2023).\nWe inspect CI build results for each PR by querying the GitHub API for the status of the last commit in the PR. For every PR, we extract the number of failed check-runs (#Failed CI Checks) and record the overall commit status reported by GitHub (success or failure). This provides a proxy for whether the agent-generated changes break tests, violate linting rules, or fail other repository-specific validation pipelines. This metric allows us to capture automated quality signals that may influence maintainers’ decisions (Zampetti et al., 2019; Maipradit et al., 2023).\nWe examine review interactions associated with each PR (Zampetti et al., 2019). Specifically, we compute a) the number of review comments in a PR (#Review Comments), and b) the number of review revisions each PR receives (#Review Revisions). Review revisions are the total number of additions and deletions by the developers during review cycles of the PRs. These measures reflect how much developer attention and iteration an agent-generated PR demands.\n\n\nBecause of the dataset size (¿ 33k), standard statistical significance testing by itself is not informative because all comparisons might yield statistically significant values even when the differences are negligible (Sullivan and Feinn, 2012). Instead, following best practices in large-scale empirical studies (Hofmann, 2015; Kitchenham et al., 2017), we rely on effect size measures, using Cliff’s delta (δ\\delta) to quantify the magnitude of difference between merged and not-merged PR distributions. To complement effect-size analysis, we use kernel density estimates (Hofmann, 2015; Kitchenham et al., 2017) to visualize distribution shapes. Unlike simple summaries, kernel density plots give a smooth, continuous view of the data’s distribution, making it easier to see shifts and spread in the data (Kitchenham et al., 2017; Thrun et al., 2020). In addition, we use logistic regression modeling (Kitchenham et al., 2017) to see how effective these characteristics are in predicting the outcome of agent-authored PRs. Together, these analyses allow us to assess whether meaningful differences exist between merged and not-merged agentic PRs.\n\n\nRQ2: What patterns lead to agent-authored PRs not being merged in real-world software repositories?\n\n\nWe randomly select a subset of 600 r"
  },
  {
    "title": "Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback",
    "url": "https://arxiv.org/abs/2601.15188v1",
    "source": "arxiv",
    "summary": "This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectivel",
    "full_text": null
  },
  {
    "title": "Supporting Humans in Evaluating AI Summaries of Legal Depositions",
    "url": "https://arxiv.org/abs/2601.15182v1",
    "source": "arxiv",
    "summary": "While large language models (LLMs) are increasingly used to summarize long documents, this trend poses significant challenges in the legal domain, where the factual accuracy of deposition summaries is crucial. Nugget-based methods have been shown to be extremely helpful for the automated evaluation of summarization approaches. In this work, we translate these methods to the user side and explore h",
    "full_text": null
  },
  {
    "title": "Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks",
    "url": "https://arxiv.org/abs/2601.15177v1",
    "source": "arxiv",
    "summary": "Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and anal",
    "full_text": null
  },
  {
    "title": "Is Peer Review Really in Decline? Analyzing Review Quality across Venues and Time",
    "url": "https://arxiv.org/abs/2601.15172v1",
    "source": "arxiv",
    "summary": "Peer review is at the heart of modern science. As submission numbers rise and research communities grow, the decline in review quality is a popular narrative and a common concern. Yet, is it true? Review quality is difficult to measure, and the ongoing evolution of reviewing practices makes it hard to compare reviews across venues and time. To address this, we introduce a new framework for evidenc",
    "full_text": null
  },
  {
    "title": "The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models",
    "url": "https://arxiv.org/abs/2601.15165v1",
    "source": "arxiv",
    "summary": "Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have l",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Diffusion Large Language Models\n2.2 Group Relative Policy Optimization\n2.3 Pass@kk as a Proxy for Reasoning Potential\n\n\n\n3 The Flexibility Trap\n\n\n3.1 Arbitrary Order Limits Reasoning Potential\n\nPass@kk analysis.\nSolution space coverage.\n\n\n\n3.2 Mechanism: The Entropy Degradation\n\nAdaptive decoding bypasses logical forks.\nThe “entropy degradation” phenomenon.\nConclusion.\n\n\n\n\n\n4 “Just GRPO” for Diffusion Language Models\n\n\n4.1 The Flexibility Tax in dLLMs’ RL\n\nAmbiguity in token-level decomposition.\nIntractable sequence likelihood.\nSampler-learner mismatch.\n\n\n\n4.2 JustGRPO\n\nFormulation.\nOptimization.\nRemarks.\n\n\n\n\n\n5 Experiments\n\nExperimental Setups.\n\n5.1 Main Results\n\nPerformance on reasoning tasks.\nRobustness across generation budgets.\n\n\n5.2 JustGRPO Preserves Parallel Decoding\n\n\n\n6 Related Work\n\nDiffusion language models.\nThe value of order arbitrariness.\nReinforcement learning for diffusion language models.\n\n\n7 Conclusion\n\nA Experimental Details\n\nA.1 Data Preparation\nA.2 Training Configuration\n\nA.3 Reward Function\n\nMathematical Reasoning Tasks.\nCode Generation Tasks.\n\n\n\n\n\nB More Analysis Results\n\nB.1 Temperature analysis.\nB.2 Different sampling algorithms.\nB.3 Block size analysis.\nB.4 Entropy Comparison Results on More Forking Tokens\nB.5 Training Efficiency Analysis\n\n\n\n\n\n\n\n\nmarginparsep has been altered.\ntopmargin has been altered.\nmarginparpush has been altered.\nThe page layout violates the ICML style.Please do not change the page layout, or include packages like geometry,\nsavetrees, or fullpage, which change it for you.\nWe’re not able to reliably undo arbitrary changes to the style. Please remove\nthe offending package(s), or layout-changing commands and try again.\n\n \n\nThe Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models\n\n\nZanlin Ni 1{}^{\\,1\\,},\nShenzhi Wang 1,\nYang Yue 1,\nTianyu Yu 2,\nWeilin Zhao 2,\nYeguo Hua 3,\nTianyi Chen 3,\nJun Song 4,\nCheng Yu 4,\nBo Zheng 4,\nGao Huang 1​🖂{}^{\\,1\\,\\textrm{\\Letter}}\n\n\n\n1{}^{1\\,} LeapLab, Tsinghua University  2{}^{2\\,} NLPLab, Tsinghua University \n 3{}^{3\\,} Tsinghua University  4{}^{4\\,} Alibaba Group\n\n\nnzl22@mails.tsinghua.edu.cn, gaohuang@tsinghua.edu.cn\n\n\n\n\nDiffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders.\nIntuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding.\nConsequently, numerous works have leveraged reinforcement learning (RL)111In this paper, unless otherwise specified, we use the term RL to refer specifically to Reinforcement Learning with Verifiable Rewards (RLVR), as it is the predominant paradigm for enhancing the reasoning capabilities of dLLMs. to elicit the reasoning capability of dLLMs.\nIn this paper, we reveal a counter-intuitive reality:\narbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs.\nWe find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space.\nThis observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility.\nWe demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead.\nOur approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs.\nProject page: https://nzl-thu.github.io/the-flexibility-trap\n\n\n\nFigure 1: \nLess flexibility unlocks better reasoning potential.\nLeft: We observe a counter-intuitive phenomenon where restricting dLLMs to standard Autoregressive (AR) order expands the reasoning solution space.\nRight: Motivated by this, we propose “JustGRPO”. By foregoing complex arbitrary-order adaptations and adopting standard GRPO, we effectively elicit the reasoning capability of dLLMs.\n\n\n\n\n\n1 Introduction\n\nRecent research has witnessed a surge in Diffusion Large Language Models (dLLMs) Nie et al. (2025); Ye et al. (2025); Zhu et al. (2025); Zhao et al. (2025), which challenge the dominant autoregressive (AR) paradigm Brown et al. (2020); Achiam et al. (2023) by treating sequence generation as a discrete denoising process.\nCentral to the appeal of dLLMs is their theoretical flexibility, which offers two distinct advantages over the strict left-to-right causal chain of AR models: efficient parallel decoding and the capability for arbitrary-order generation.\n\n\nWhile the efficiency gains of parallel decoding have been well-established Wu et al. (2025b); Labs et al. (2025); DeepMind (2025); Song et al. (2025); Wu et al. (2025a), the implications of arbitrary-order generation remain less explored Ye et al. (2024); Kim et al. (2025).\nTheoretically, the unconstrained generation order constitutes a superset of the fixed autoregressive trajectory.\nThis flexibility naturally suggests a potential for superior reasoning: in general reasoning tasks like mathematics and coding, such freedom could unlock non-sequential problem-solving paths inaccessible to standard left-to-right models.\nAs a result, recent works have increasingly adopted RL to elicit reasoning capabilities of dLLMs Zhao et al. (2025); Gong et al. (2025); Wang et al. (2025a); Ou et al. (2025).\n\n\nIn this paper, we present a counter-intuitive observation: arbitrary-order generation, in its current form, narrows rather than expands the reasoning potential elicitable by RL.\nTo rigorously assess this, we employ Pass@kk Chen (2021), which measures the coverage of solution space.\nRecent studies suggest that RL primarily acts to sharpen the base model’s distribution; consequently, the Pass@kk performance of the base model effectively sets the upper bound for the reasoning capability of the model after RL training Yue et al. (2025); Liu et al. (2025); Zhang et al. (2025).\nUnder this metric, we compare the reasoning potential of LLaDA Nie et al. (2025) with arbitrary-order generation against standard AR decoding.\nAs shown in Figure 1 (Left), restricting a dLLM to standard AR order in fact yields a higher Pass@kk, and consequently a higher reasoning boundary, than its flexible counterpart.\n\n\nFigure 2: \nConfronting vs. bypassing uncertainty.\n(a) AR order preserves reasoning space by forcing decisions at uncertain tokens.\n(b) Arbitrary order bypasses uncertainty and resolves easier tokens first.\nOnce future context is established, the original forks collapse, prematurely narrowing the solution space.\n\n\n\n\nWe attribute this counter-intuitive phenomenon to the way model handles uncertainty.\nReasoning is inherently non-uniform: it typically hinges on sparse “forking tokens”, i.e., connectives like “Therefore” or “Since” which do not merely continue a sentence but fundamentally steer the logical trajectory into distinct branches Wang et al. (2025c); Cheng et al. (2025); Huang et al. (2025a).\nAt these forks, the reasoning path diverges, naturally manifesting as localized spikes in entropy Wang et al. (2025c).\nStandard AR decoding compels the model to confront this uncertainty (Figure 2a).\nBy sampling exactly at the fork, the model is able to explore different reasoning paths, thereby preserving the diversity of the generated rationales.\nArbitrary-order generation, however, allows the model to bypass these hard decisions (Figure 2b).\nIt prioritizes low-entropy completions first.\nBy the time the model returns to infill the bypassed forks, the established bidirectional context has already severely constrained the potential branches. The ambiguity is prematurely resolved, and the original high entropy is suppressed. We term this phenomenon entropy degradation.\nEffectively, the model trades the exploration of diverse reasoning paths for the greedy optimization of local consistency.\n\n\nThe above observations motivate a rethink of RL for dLLMs.\nCurrent methods operate under the assumption that preserving arbitrary-order flexibility is essential. This commitment incurs a heavy tax: algorithms must grapple with a combinatorial explosion of denoising trajectories Zhao et al. (2025); Yang et al. (2025); Gong et al. (2025) and intractable marginal likelihoods Ou et al. (2025), forcing reliance on unstable approximations Wang et al. (2025a); Ou et al. (2025); Rojas et al. (2025).\nHowever, if arbitrary order is non-essential, or even detrimental for eliciting reasoning potential, this complexity is unjustified.\n\n\nTo this end, we propose a return to simplicity with Just GRPO.\nWe demonstrate that eliciting reasoning potential for general reasoning tasks does not require complex, diffusion-specific RL adaptations.\nInstead, it is best achieved by simply treating the dLLM as an AR model during RL training. This allows us to apply standard GRPO Shao et al. (2024) without bells and whistles, turning an otherwise intractable optimization with unstable approximations into a well-defined task.\n\n\nJustGRPO is surprisingly effective. On complex reasoning benchmarks, it achieves competitive results (e.g., 89.1% accuracy on GSM8K, 45.1% on MATH), surpassing methods that rely on complex diffusion-specific RL.\nCrucially, while we train with AR constraints to maximize reasoning potential, the model retains dLLMs’ ability of efficient parallel decoding at inference time.\nBy returning to basic left-to-right ordering, this work encourages a re-evaluation of arbitrary versus AR order in the development of next-generation language models.\n\n\n\n\n2 Preliminaries\n\n\n2.1 Diffusion Large Language Models\n\nDiffusion Large Language Models (dLLMs), particularly Masked Diffusion Models (MDMs)"
  },
  {
    "title": "V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks",
    "url": "https://arxiv.org/abs/2601.15164v1",
    "source": "arxiv",
    "summary": "Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently \"succeed\" without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, s",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related work\n\n2.1 LLM-Generated Robotic Simulation Tasks and Training Data\n2.2 LLMs for Long-Horizon Robotic Task Planning\n2.3 VLM-Driven Manipulation and Planning\n2.4 Scalable Synthetic Data Generation\n\n\n\n3 Method\n\n3.1 Problem Formulation\n3.2 Semantic Instruction Grounding\n3.3 VLM-Guided Rejection Sampling\n3.4 Generative Scene and Task Synthesis Pipeline\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Main Results\n4.3 The Critical Role of VLM Verification\n\n\n5 Conclusion\n\nA Appendix A: Pangu-7B Supervised Fine-Tuning for Motion Planning\n\nA.1 Data Collection and Augmentation\nA.2 Fine-Tuning and Evaluation\nA.3 Results\n\n\n\n\n\n\n\nV-CAGE: Context-Aware Generation and Verification \nfor Scalable Long-Horizon Embodied Tasks\n\n\nYaru Liu\n\n  \nAobo Wang\n\n  \nNanyang Ye\n\n\n\nAbstract\nLearning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently “succeed” without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction module. This decomposes high-level goals (e.g., ”get ready for work”) into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out ”silent failures” where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.\n\n\n\n1 Introduction\n\nRecent advancements in Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities in reasoning and planning (Driess et al., 2023; Ahn et al., 2022b). However, translating these high-level semantic capabilities into robust embodied agents capable of executing long-horizon manipulation tasks remains a formidable challenge. While the ”Scaling Laws” have proven effective in NLP and Vision, applying them to robotics is hindered by the lack of high-quality, large-scale demonstration data. Collecting real-world trajectories is expensive, unsafe, and difficult to scale (Brohan et al., 2023). Therefore, learning from synthetic data has emerged as a promising paradigm (Mandlekar et al., 2023; Wang et al., 2024c).\n\n\nDespite its promise, naively generated synthetic data often suffers from a significant ”quality gap” that severely limits downstream policy performance. This gap is particularly pronounced in long-horizon tasks, for example ”prepare the desk for work”, where success depends on the precise execution of sequential subtasks. We identify two fundamental failure modes in existing data generation pipelines.\nFirst, Geometric Inconsistency: Standard procedural generation often places objects without considering the dynamic evolution of the workspace. As the scene becomes cluttered, objects are instantiated in conflicting poses or unstable configurations, leading to physics simulator crashes or unrealistic interpenetrations.\nSecond, Semantic Misalignment: Language-conditioned code generation can be brittle. A generated script might execute without runtime errors, but fail to achieve the intended semantic goal (e.g., the switch was not actually toggled, or the object was placed off the pad). Training on such ”false positive” data introduces noise that catastrophic degrades the agent’s ability to reason about cause and effect (de Haan et al., 2019).\n\n\nTo address these challenges, we present V-CAGE (VLM-Guided Context-Aware Generation for Embodied Planning), a closed-loop framework designed to synthesize long-horizon manipulation trajectories. Unlike open-loop generation methods that blindly execute LLM-produced plans, V-CAGE integrates geometric constraints and visual verification directly into the generation pipeline.\nOur core insight treats data synthesis as a rigorous optimization process, aggressively pruning invalid trajectories to ensure only high-fidelity data reaches the training buffer.\n\n\nV-CAGE operates on three hierarchical levels. At the planning level, we leverage an LLM, for example Pangu (Chen et al., 2025a), to ground abstract user instructions into executable action sequences. At the geometric level, we propose a context-aware instantiation mechanism. By maintaining a dynamic map of ”prohibited volumes” that updates after each object placement, we ensure that new objects are instantiated only in feasible, collision-free regions, effectively solving the packing problem in cluttered scenes. Crucially, at the verification level, we employ a VLM, Gemini3 (Gemini Team, Google, 2025), as a semantic critic. Treating data generation as a rejection sampling problem, the VLM evaluates the visual outcome of each subtask. Trajectories that fail to meet visual success criteria are rejected and regenerated, ensuring that the final dataset consists exclusively of physically plausible and semantically correct demonstrations.\n\n\nOur main contributions are summarized as follows:\n(1) We introduce V-CAGE, a scalable, closed-loop pipeline for synthesizing high-fidelity trajectory data for long-horizon tasks, integrating geometric constraints with visual verification.\n(2) We propose a context-aware instantiation mechanism that maintains an evolving map of prohibited volumes during scene generation, ensuring valid object placement in cluttered environments.\n(3) We formulate data validation as a VLM-guided rejection sampling process, using visual critics to filter out ”silent failures” where code executes without satisfying task semantics.\n(4) We demonstrate that policies trained on V-CAGE data significantly outperform baselines in terms of success rate and generalization across diverse, cluttered scenarios.\n\n\n\n\n2 Related work\n\n\n2.1 LLM-Generated Robotic Simulation Tasks and Training Data\n\nA growing line of work leverages LLMs to automatically construct robotic simulation tasks and training data, aiming to reduce the substantial human effort required for task design and data collection. Wang et al. (2024a) introduce an LLM-driven code generation pipeline that writes simulation environments, task specifications, and expert policies, enabling the automatic creation of rich task libraries for manipulation in synthetic environments. By combining goal-directed and exploratory task generation modes, GenSim scales from hand-designed benchmarks to a much larger set of diverse tasks and shows improved task-level generalization and sim-to-real transfer for learned policies.\n\n\nBuilding on this idea, Hua et al. (2024) propose a more scalable framework that exploits coding LLMs with multi-modal and reasoning capabilities to create complex and realistic simulation tasks, including long-horizon manipulation with articulated objects. The framework first uses an LLM to propose tasks and generate executable task code, then employs planning and reinforcement learning solvers to automatically produce demonstrations at scale, and finally trains a language-conditioned policy architecture on the generated dataset. This pipeline can generate data for up to hundreds of articulated-object tasks and achieves strong zero-shot sim-to-real transfer and performance gains when co-trained with limited real-world data (Hua et al., 2024).\n\n\nIn parallel, Wang et al. (2024c) treat foundation and generative models as generative simulators rather than direct controllers. They define a self-guided robotic agent that autonomously proposes new tasks, generates corresponding environments, and acquires diverse skills via generative simulation. The system continuously expands a training corpus that covers over a hundred tasks and demonstrates that such procedurally generated, model-driven simulations can rival or outperform human-authored datasets for multi-task robot learning.\n\n\nFor bimanual manipulation and digital-twin scenarios, Mu et al. (2025) and Chen et al. (2025b) integrate LLMs into a generative digital-twin pipeline. Starting from single RGB images, RoboTwin (Mu et al., 2025) uses 3D generative models to create diverse object instances and employs LLM agents to synthesize task programs for dual-arm manipulation, yielding an aligned synthetic–real benchmark and scalable expert data generation. RoboTwin 2.0 (Chen et al., 2025b) further extends this framework into a large-scale data generator and benchmark with over fifty tasks and hundreds of object categories, incorporating multimodal LLM-based program synthesis and extensive domain randomization to improve robustness of learned bimanual policies.\n\n\nBeyond fully LLM-coded pipelines, benchmark-focused efforts such as RoboCAS (Zheng et al., 2024) focuses on complex object arrangement scenarios in robotic manipulation. RoboCAS defines a benchmark for long-horizon manipulation in cluttered arrangement scenes and uses flexible scripted policies to collect diverse demonstrations across scattered, ordered, and stacked configurations. It highlights the importance of compositional scene and task complexity for evaluating generalist manipulation policies and foundation models.\n\n\nOverall, existing work shows that LLMs can significantly reduce human effort in designing simulation tasks and generat"
  },
  {
    "title": "Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems",
    "url": "https://arxiv.org/abs/2601.15161v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nNLU Evaluation.\nNLG Evaluation.\nLLM-as-a-Judge.\nRubric-based LLM-as-a-Judge.\nSummary and Positioning.\n\n\n\n3 Methodology\n\n3.1 Problem Formulation\n\n3.2 Stage 1: Retrieval &amp; Evidence Preparation\n\nRouting Agent (Smart–Fast Strategy).\nEvidence Synthesis Agent.\n\n\n\n3.3 Stage 2: Dual-Track Constraint Construction\n\nMedical Fact Agent (Atomic Fact Decomposition).\nInteraction Intent Agent.\n\n\n\n3.4 Stage 3: Audit &amp; Refinement\n\nRubric Synthesis Agent.\nAuditing Agent &amp; Refinement Loop.\n\n\n\n\n\n4 Experiments\n\n4.1 Datasets\n\n4.2 Implementation Detail\n\n\n4.2.1 Generation\n\nModel Configuration.\n\n\n\n4.2.2 Evaluation\n\nNear-Miss Construction.\nJudging Protocol.\n\n\n\n\n\n4.3 Baselines\n\nGeneric Rubric.\nGPT-4o Rubric.\nNo Rubrics (None).\n\n\n\n4.4 Evaluation Metrics\n\n4.4.1 Scoring and Bias Mitigation\n4.4.2 Clinical Intent Alignment (CIA)\n\n4.4.3 Discriminative Sensitivity\n\nOutcome Distribution.\nMean Score Delta (μΔ\\mu_{\\Delta}).\nRanking Accuracy (AUROC).\n\n\n4.4.4 Statistical Significance\n\n\n\n\n\n5 Results and Analysis\n\n5.1 Clinical Coverage\n5.2 Discriminative sensitivity under Near-Miss Conditions\n\n\n\n6 Rubric-Guided Response Refinement\n\n6.1 Task Setup and Baselines\n\n6.2 Refinement Mechanism\n\nRubric-to-Critique Transformation.\nConstraint-Guided Refinement.\n\n\n6.3 Evaluation Protocol\n6.4 Response Refinement Results\n\n\n7 Conclusion\nA Appendix\nB Prompt Templates\n\n\n\n\n\nAutomated Rubrics for Reliable Evaluation of Medical Dialogue Systems\n\n\nYinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz \nAI Center, University College London, UK \n{yinzhu.chen.20,abdine.maiga.23,hossein.rahmani.22,emine.yilmaz}@ucl.ac.uk\n\n\n\nAbstract\nLarge Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics.\nOur approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta (μΔ=8.658\\mu_{\\Delta}=\\textbf{8.658}) and an AUROC of 0.9770.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0%59.0\\% to 68.2%68.2\\%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at\nhttps://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.\n\n\n\nAutomated Rubrics for Reliable Evaluation of Medical Dialogue Systems\n\n\n\n\nYinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz\n\nAI Center, University College London, UK\n\n{yinzhu.chen.20,abdine.maiga.23,hossein.rahmani.22,emine.yilmaz}@ucl.ac.uk\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Retrieval-augmented multi-agent framework for medical rubric generation. The pipeline consists of three stages: (1) Retrieval and Evidence Preparation, (2) Dual-Track Constraint Construction and (3) Audit and Refinement, transforming a medical user query into a structured evaluation rubric.\n\n\nLarge Language Models (LLMs) have demonstrated strong capabilities across a wide range of NLP tasks (Zhao et al., 2023; Bommasani et al., 2021). Recent advances in LLMs further expand their potential in medical applications, ranging from differential diagnosis (McDuff et al., 2023) and stepwise clinical reasoning (Brodeur et al., 2024; Savage et al., 2024) to empathetic patient communication (Maida et al., 2024). However, reliable and scalable evaluation of these systems has become a central challenge. Conventional approaches relying on surface-level metrics or multiple-choice benchmarks fail to capture clinical reasoning (Croxford et al., 2025). Expert human assessment better reflects clinical judgment, yet its high cost and limited inter-rater consistency hinder scalability (Arora et al., 2025).\n\n\nTo address scalability, LLM-as-a-Judge has been proposed as an automated evaluation paradigm and has shown promising results in general domains (Zheng et al., 2023; Dubois et al., 2024; Rahmani et al., 2025a). However, prior studies show that when evaluation criteria are coarse, LLM-based judging can suffer from bias (Shi et al., 2024; Rahmani et al., 2024), limited reproducibility (Yamauchi et al., 2025), and insensitivity to subtle but important differences (Kim et al., 2025).\nThis issue is particularly consequential in medical settings: analyses show that medical errors are often embedded in clinically plausible language and seemingly coherent reasoning (Asgari et al., 2025). The detectability of such errors depends critically on the evaluator’s level of domain expertise and the quality of the prompt provided to the model, making them particularly difficult to identify for non-experts and automated evaluation systems (Asgari et al., 2025; Rahmani et al., 2025a; Liu et al., 2024). When undetected, errors in clinical reasoning or treatment recommendations can delay appropriate care or lead to inappropriate interventions, substantially increasing the stakes of evaluation failures in medical applications (Mehta and Devarakonda, 2018; Miles-Jay et al., 2023; Xia et al., 2024). These findings highlight that medical LLM evaluation cannot rely solely on implicit or impression-based judgments.\n\n\nA natural mitigation is to adopt fine-grained evaluation criteria that ground judgments in explicit, verifiable clinical requirements. Instead of relying on abstract dimensions, rubric-based evaluation specifies what a high-quality response should include or avoid in concrete clinical terms. Recent work has shown that structured or decomposed evaluation schemes can improve interpretability and consistency of automated judgments (Liu et al., 2023; Arora et al., 2025). However, medical dialogue is highly context-dependent: generic rubrics are often too coarse to capture instance-specific clinical priorities, while instance-level rubrics, though more precise, introduce substantial annotation cost and stability challenges, limiting their practicality for large-scale evaluation (Kim et al., 2025).\n\n\nTo address this gap, we propose a retrieval-augmented multi-agent framework for automatically generating instance-specific evaluation rubrics in medical dialogue through three coordinated stages. First, Retrieval and Evidence Preparation stage employs a routing strategy to gather and synthesize authoritative medical knowledge into a unified evidence block.\nSecond, a Dual-Track Construction mechanism effectively decomposes this evidence into atomic medical facts (creating a ’Reference Board’) while in parallel extracting interaction intents from the user query.\nFinally, the Audit and Refinement stage synthesizes these inputs into structured criteria and enforces clinical coverage via an Auditing Agent, which performs a gap analysis against the atomic facts to trigger iterative refinement.\nThis framework effectively combines the scalability of automated systems with the clinical rigor of expert verification.\n\n\nOur contributions: (1) a retrieval-augmented multi-agent framework for instance-specific medical rubric generation, achieving 60.12% Clinical Intent Alignment (CIA) and significantly outperforming GPT-4o baseline;\n(2) enhanced discriminative sensitivity, with a mean score delta of 8.658 and an AUROC of 0.977, enabling precise detection of subtle, near-miss clinical errors;\nand (3) actionable rubric-based feedback for refinement, improving downstream response quality by 9.2% through controlled, rubric-guided edits.\nTogether, these findings establish that automated, knowledge-grounded rubrics provide a scalable and transparent foundation for both evaluating and improving medical language model outputs.\n\n\n\n\n2 Related Work\n\nNLU Evaluation.\n\nEarly work on medical NLP evaluation focused on NLU-style tasks such as MedQA (Jin et al., 2020), MedMCQA (Pal et al., 2021), PubMedQA (Jin et al., 2019), and MMLU (Hendrycks et al., 2021), which primarily test factual medical knowledge through multiple-choice questions. These benchmarks played an important role in assessing domain knowledge, but fail to capture clinical reasoning, contextual understanding, or the quality of patient-facing communication (Croxford et al., 2025).\n\n\n\nNLG Evaluation.\n\nAs medical generation tasks emerged, datasets such as MedDialog (Zeng et al., 2020) and COVID-QA (Möller et al., 2020) were evaluated using generic NLG metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Snover et al., 2006). Later embedding-based metrics such as BERTScore (Zhang et al., 2019) and Sentence-BERT (Reimers and Gurevych, 2019) attempted to improve semantic alignment. More recent benchmarks such as HealthSearchQA (Singhal et al., 2023), MultiMedQA (Singhal et al., 2023) and Med-Eval (He et al., 2023) introduced reference-free and human-graded evaluation to better assess open-ended generation. These benchmarks more closely reflect real-world clinical needs by enabling open-ended evaluation, but they are labour-intensive and costly.\n\n\n\nLLM-as-a-Judge.\n\nLLM-as-a-judge has emerged as a scalable alternative to human evaluation for open-ended generation tasks (Zheng et al., 2023; Dubois et al., 2024; Rahmani et al., 2025a). In general domains, strong language models correlate reasonably well with human preferences in pairwise or ranking-based evaluation"
  },
  {
    "title": "Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning",
    "url": "https://arxiv.org/abs/2601.15160v1",
    "source": "arxiv",
    "summary": "Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Role of SFT and RL in Reasoning\n2.2 RL on KGs\n\n\n\n3 Preliminaries\n\n3.1 Notation and RL for Language Models\n3.2 SFT Followed by RL\n3.3 Medical KG: Unified Medical Language System (UMLS)\n\n\n\n4 Methodology\n\n4.1 Data Construction and Axiomatic Grounding\n4.2 RL Alone is Insufficient\n4.3 Reward Design Exploration for Compositional Reasoning\n4.4 KG-Grounded Reward Formulation\n4.5 Scaling and Benchmarking\n\n\n\n5 Results\n\n5.1 Scaling Composition: From Short-Hop Training to Long-Hop Reasoning\n5.2 Robustness to Tasks Involving Reasoning Depth\n5.3 ICD-10 Category Analysis: KG-grounded Gains Are Broadly Distributed\n5.4 Robustness to Format Perturbation\n5.5 Algorithmic Efficiency vs. Scale: Surpassing Frontier Models\n\n\n6 Discussion\n7 Conclusion\n\nA Zero-RL and Reward Ablation Studies on Qwen3 8B\n\nA.1 Zero-RL Performance and Scaling\nA.2 GRPO on LoRA Parameters Only\n\n\nB SFT+RL Ablation Studies on Qwen3 8B\n\nC Additional Reward Function Formulations\n\nC.1 Semantic Answer Similarity Reward (Rs​i​m)(R_{sim})\nC.2 Thinking Quality Reward (Rt​h​i​n​k)(R_{think})\n\n\nD Train-Test Split Overlap Analysis\n\nE Training Hyperparameters and GRPO Configuration\n\nE.1 Hardware Compute\nE.2 SFT Stage\nE.3 RL (GRPO) Stage\n\n\nF Sample Model Responses\nG SFT+RL Pipeline Algorithm\nH Model Prompt\n\n\n\n\n\nKnowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning\n\n\n\nYuval Kansal \nPrinceton University \nyuvalkansal@princeton.edu\n&amp;Niraj K. Jha \nPrinceton University \njha@princeton.edu\n\n\n\nAbstract\nLarge language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose\na bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a “compositional bridge,” enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.\n\n00footnotetext: Preprint. Under review.\n\n\n1 Introduction\n\nRecent advances in language models have revealed that reasoning capabilities can be significantly enhanced through a combination of high-quality pretraining, supervised fine-tuning (SFT), carefully tuned reinforcement learning (RL)-based post-training, and strategic use of additional test-time compute (OpenAI, 2025; Google DeepMind, 2025; Yang et al., 2025; Muennighoff et al., 2025). The resulting systems achieve near-expert performance in well-structured domains, such as mathematics and programming, where high-quality data have been curated, reasoning steps are clear, ground truth is unambiguous, and intermediate verification is tractable (Lightman et al., 2023; Anthropic, 2025). However, true human-level intelligence in specialized fields requires more than just general pattern matching or long-form generation; it requires compositional reasoning: the ability to reliably combine axiomatic facts for complex multi-hop problem solving Kamp &amp; Partee (1995); Fodor (1975). While current large language models (LLMs) excel when reasoning steps are clear and carefully curated expert data are available, compositional reasoning in high-stakes scientific domains, where reasoning paths are multi-faceted, remains elusive (Yin et al., 2025; Kim et al., 2025).\n\n\nFigure 1: Compositional Reasoning: A sample 3-hop query that requires systematic traversal of axiomatic triples to make a grounded, multi-step clinical deduction.\n\n\nTo bridge this gap, we argue in favor of a bottom-up learning paradigm: grounding models in axiomatic facts and then composing these fundamentals into sophisticated domain knowledge. Knowledge graphs (KGs) provide a natural and promising scaffold for this grounding; they encode entities and relations in a structured, interpretable fashion that can represent the building blocks of domain knowledge at scale. Recent work Dedhia et al. (2025); Wang et al. (2024) has shown how high-quality data can be curated from such graphs and used to fine-tune models to obtain better reasoning traces. However, good static data is just the first step towards mastering the process of composition. Beyond high-quality data, robust reward design is a key lever for shaping models that can compose axiomatic facts from a domain to arrive at a logical conclusion.\n\n\nExisting post-training methods, e.g., reinforcement learning from human feedback Ouyang et al. (2022) and\ndirect preference optimization Rafailov et al. (2023), optimize models to match human preference with final outputs, not the process that produced them. Proxy reward signals, such as reward length and alignment with expert-written answers, while useful, fail to account for the composition intricacies needed to answer a complex multi-hop query. In practice, reward models often conflate superficial correlates (fluency, deference) with quality, thus leading to reward over-optimization and brittle answers (Shrivastava et al., 2025). In safety-critical domains, the result is a mismatch between human-liked style and ground-truth validity (Damani et al., 2025; Weng, 2024; Rafailov et al., 2023). Whereas process supervision (rewarding intermediate steps) has shown promise in mathematics and logic Zhang et al. (2025); Cui et al. (2025); Wang et al. (2025); Lightman et al. (2023), curating and scaling expert-annotated data for other domains are notoriously difficult to achieve and nontrivial. This raises a key question: How can we build systems and reward signals at scale that promote grounded compositional reasoning in multi-hop tasks without relying on expensive human-in-the-loop annotations?\n\n\nKGs offer an implicit solution to this scaling problem. In a KG, domain-specific concepts and their relationships are represented as axiomatic triples (h​e​a​d,r​e​l​a​t​i​o​n,t​a​i​l)(head,relation,tail). Our core insight is that by comparing the reasoning and assertions of a model during post-training against relevant triples and the chain of axiomatic facts required to solve the problem, we can turn the match (or mismatch) into a high-quality reward signal. Instead of an answer that “looks good,” this lets us reward the model to the degree its response is supported by verifiable domain knowledge and implicitly reward it for correctly composing facts to produce a solution. This is readily scalable without requiring external expert supervision and further enables us to move away from top-down distillation and ground the model’s reasoning in the field’s fundamental building blocks.\n\n\nIn this article, we realize this idea through a Base Model →\\rightarrow SFT [Low-Rank Adaptation (LoRA)]→\\rightarrow RL [Group Relative Policy Optimization (GRPO)] post-training pipeline that uses a grounded KG to derive a novel reward signal to enable compositional reasoning (Hu et al., 2022; Guo et al., 2025; Yasunaga et al., 2021). Whereas the approach can be generally applied, we study it in the medical domain, a field that serves as a rigorous stress test for compositional reasoning. Medical knowledge inherently requires multi-hop reasoning; a single clinical diagnosis may require navigating from a patient’s demographics and medical history to symptoms, from those symptoms to a disease, and finally to a drug (a sample multi-hop query is shown in Fig. 1). By training a Qwen3 14B model Yang et al. (2025) on simple 1-, 2-, and 3-hop reasoning paths derived from a KG, we probe whether it can learn the underlying “logic of composition” to solve unseen, complex medical queries, ranging from 2- to 5-hop, in the ICD-Bench test suite (Dedhia et al., 2025). Our results indicate that this grounded SFT+RL approach leads to large accuracy improvements on the most difficult questions, and remains robust under stress tests, such as option shuffling and ICD-10 category breakdowns (Organization, 1992). We find that while SFT provides the necessary knowledge base, RL acts as the “compositional bridge.” We demonstrate that insights learned on an 8B model transfer effectively to a 14B model, outperforming larger reasoning and frontier models.\n\n\nOur core contributions can be summarized as follows:\n\n\n•\n\nA Grounded, Scalable Reinforcement Learning with Verifiable Rewards (RLVR) Pipeline: We introduce a scalable SFT+RL post-training framework designed to enable compositional reasoning in models using KGs as a verifiable ground truth.\n\n\n\n•\n\nKG-Path Inspired Reward: We conduct a thorough investigation to design a novel reward signal derived from the KG that encourages compositional reasoning, correctness, and enables process supervision at scale.\n\n\n\n•\n\nCompositional Generalization: We demonstrate how training on 1-to-3-hop paths enables a model to generalize to difficult and longer 4-, 5-hop questions, significantly outperforming base models and larger models.\n\n\n\n•\n\nRobustness &amp; Real-World Validation: We stratify our model’s performance by different difficulty levels, on real"
  },
  {
    "title": "Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data",
    "url": "https://arxiv.org/abs/2601.15158v1",
    "source": "arxiv",
    "summary": "Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Expressivity Benefits of Transformers with CoT\n2.2 Training Dynamics of Transformers\n2.3 Effect of Training Data Distribution on Learning\n\n\n\n3 Preliminaries\n\n3.1 Notation\n3.2 Transformer Architecture\n3.3 Autoregressive Generation\n3.4 Reinforcement Learning\n\n\n\n4 The Task: Chain Identification\n\n4.1 Data Distribution\n4.2 Task Loss\n4.3 Step Types and Chain Traversal\n\n\n\n5 How Could a Transformer Solve the Task?\n\n5.1 Reasoning is Necessary to Solve the Task\n5.2 Reasoning is Sufficient to Solve the Task\n\n\n\n6 Dynamical Analysis\n\n6.1 Training Regime\n6.2 Learning With Easy Examples Yields Chain Traversal\n6.3 Easy Examples are Necessary for Efficient Learning\n\n\n\n7 Experiments\n\n\n7.1 Theoretically Inspired Experiments\n\n7.1.1 Emergence of Efficient Reasoning\n7.1.2 Out of Distribution Generalization\n7.1.3 Solving Complex Examples Requires Training on Simple Examples\n\n\n\n7.2 Real World Setting\n\n7.2.1 Emergence of Efficient Reasoning\n7.2.2 Out of Distribution Generalization\n7.2.3 Solving Complex Examples Requires Training on Simple Examples\n\n\n\n\n8 Conclusion\nA Proof of Proposition 1\nB Proof of Proposition 2\nC Proof of Theorem 1\nD Complexity-Theoretic Assumptions\n\nE A Complexity–Theoretic Obstruction for No–CoT Transformers\n\nE.1 Problems and Classes\nE.2 Complexity–theoretic background\nE.3 From ORD\\mathrm{ORD} to Two–Chain Endpoint\nE.4 From the Function Problem to the Decision Problem\nE.5 Main Theorem: No–CoT Transformers Cannot Solve Two–Chain Endpoint\n\n\n\nF Proof of Theorem 2\n\nF.1 Loss can be written as absorption probability of a Markov chain\n\nF.2 Preliminaries\n\nF.2.1 Useful definitions\nF.2.2 Surrogate chains\nF.2.3 Source function of surrogate chains and derivative form\n\n\n\nF.3 Analysis of long-jump-absorbing chain\n\nF.3.1 Derivative estimates\n\n\nF.4 Analysis of no-long-jump chain\n\nF.5 Source computations in the no–long–jump (NL) chain\n\nF.5.1 Derivative estimates\n\n\nF.6 Monotonicity of α,β,γ\\alpha,\\beta,\\gamma and derivative estimates\n\nF.7 Hard example contributions to derivative are negligible\n\nF.7.1 Upper bound on derivative for hard examples\n\n\nF.8 Easy+Hard examples derivative estimates\nF.9 Convergence rate\n\n\nG Proof of Theorem 3\nH Supporting Lemmas\nI Auxiliary lemmas\n\nJ Experimental Details\n\nJ.1 Theoretically Inspired Experiments\n\nJ.2 Real World Experiments\n\nJ.2.1 Example Inputs and Completions\n\n\n\n\n\n\n\n\n\nOutcome-Based RL Provably Leads Transformers \nto Reason, but Only With the Right Data\n\n\nYuval Ran-Milo\n\n  \nYotam Alexander\n\n  \nShahar Mendel\n\n  \nNadav Cohen\n\n\n\nAbstract\nTransformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought).\nYet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood.\nWe address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution.\nWe prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex.\nWe characterize the distributional properties required for this emergence, identifying the critical role of ”simple examples”: instances requiring fewer reasoning steps.\nWhen the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible.\nWe corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.\n\n\n\n1 Introduction\n\nThe Transformer architecture (Vaswani et al., 2017) has emerged as the de facto standard for sequence modeling, revolutionizing diverse fields ranging from natural language processing and code generation to protein structure prediction and mathematical problem solving (Chen et al., 2021; Lewkowycz et al., 2022; Kovalevskiy et al., 2024).\nWhile the standard pretraining paradigm—next-token prediction on vast corpora—has proven exceptionally effective,\nthere is growing focus on enhancing the reasoning capabilities of these models through Reinforcement Learning (RL).\nThe integration of Transformers with RL has unlocked dramatic performance gains on complex reasoning tasks, such as mathematical problem solving and code generation (Shao et al., 2024; Dou et al., 2024).\nRecent empirical breakthroughs, such as the DeepSeek-R1 models (DeepSeek-AI et al., 2025), demonstrate that training with simple RL algorithms like GRPO (Shao et al., 2024) or PPO (Schulman et al., 2017) to optimize solely for valid final answers can induce models to spontaneously generate detailed reasoning traces (DeepSeek-AI et al., 2025).\n\n\nIn these scenarios, despite training solely on final-answer correctness, the model spontaneously learns to generate CoT reasoning steps without any explicit supervision on these intermediate steps.\nThis raises a fundamental question: how does a sparse reward at the end of generation guide gradient descent through the combinatorial space of outputs to discover a systematic reasoning algorithm?\nUnderstanding the mechanics of this emergence is critical, especially since it appears to depend on specific yet poorly understood properties of the training data.\nEmpirical studies of RL-trained language models have increasingly highlighted that the composition of training data (beyond just its size) plays a crucial role in successful learning, with observations suggesting that the presence of examples of varying complexity is critical for models to discover effective reasoning strategies (Narvekar et al., 2020; Parashar et al., 2025).\n\n\nDespite these empirical advancements, it is not well understood how RL drives Transformers to learn to reason effectively or how data composition affects this process.\nIn this work, we take a step towards addressing this issue by theoretically analyzing the gradient flow dynamics of single-layer Transformers trained on a synthetic reasoning task.\nIn line with other works on Transformer reasoning (Sanford et al., 2024; Agrawal et al., 2024; Spies et al., 2025), we focus on a graph traversal task.\nSpecifically, our task is to identify the terminal vertex of a chain in a directed graph.\nWe show that, under standard complexity-theoretic assumptions, this task captures a key structural property of reasoning: it cannot be solved in a single step but admits a simple iterative solution, making CoT generation both necessary and sufficient.\nOur analysis reveals that despite being trained solely on final-answer correctness, the model converges to a structured, interpretable algorithm.\nSpecifically, RL drives the Transformer to learn to explicitly traverse the graph vertex-by-vertex, effectively ”reasoning” its way to the solution.\nFurthermore, we show the model can implement alternative, less efficient algorithms that solve the task.\nOur result therefore implies that gradient flow induces an implicit bias in the learning dynamics, guiding them towards efficient solutions.\n\n\nWe further characterize the distributional properties required for this emergence, identifying the critical role of ”simple examples”—instances requiring fewer reasoning steps—in guiding the optimization.\nWe prove that when the training distribution has sufficient mass on these simpler instances, gradient flow learns the efficient traversal strategy in time polynomial in the chain length; conversely, when this mass vanishes, gradient-based learning becomes infeasible as the chain length grows.\nCrucially, the learned algorithm enables length generalization: models trained on shorter chains can successfully extrapolate to solve longer chains not seen during training.\nPerhaps most surprisingly, our theory implies that training on out-of-distribution simple examples may boost performance on harder in-distribution tasks more than training on those hard examples directly.\n\n\nWe corroborate our theoretical results with experiments in our synthetic setting, and validate their broader applicability by fine-tuning Qwen-based models (Qwen et al., 2025) on mathematical reasoning tasks.\nOur experiments confirm that models trained on short chains not only achieve near-perfect accuracy, but do so by implementing an efficient step-by-step traversal algorithm.\nMoreover, these models successfully generalize to longer chains, validating our theoretical predictions.\nFinally, we confirm empirically the critical role of data composition: excluding simple examples prevents the emergence of reasoning, whereas, strikingly, replacing hard in-distribution examples with simple out-of-distribution ones can actually boost performance on the hard examples themselves.\n\n\nOur findings provide a step towards a rigorous theoretical foundation for understanding how sparse RL feedback can drive the emergence of step-by-step reasoning, demonstrating that phenomena observed in large-scale RL training can be rigorously understood.\n\n\n\n\n2 Related Work\n\n\n2.1 Expressivity Benefits of Transformers with CoT\n\nA growing theoretical literature seeks to explain how CoT boosts Transformer performance by expanding its effective expressivity. Several works show that CoT allows constant-depth Transformers to solve tasks such as parity, arithmetic, and regular languages that are inaccessible to single-step decoding (Feng et al., 2023; Merrill and Sabharwal, 2024; Li et al., 2024). We contribute to this literature by establishing a natural graph traversal problem that provably requires CoT under standard complexity-theoretic assumptions.\n\n\n\n\n2.2 Training Dynamics of Transformers\n\nA parallel line of work analyzes how gradient-based training shape"
  },
  {
    "title": "How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework",
    "url": "https://arxiv.org/abs/2601.15153v1",
    "source": "arxiv",
    "summary": "Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software e",
    "full_text": "\n\n\n\n1 Introduction\n2 Background and Related work\n\n3 Research Method and Case Context\n\n\n3.1 Overall Research Process\n\n3.1.1 Step 1: Expert Knowledge Extraction\n3.1.2 Step 2: Framework Development\n3.1.3 Step 3: System Implementation\n3.1.4 Step 4: Comprehensive Validation\n\n\n\n\n\n4 Expert Knowledge Extraction and Analysis\n\n\n4.1 Simulation Analysis Software Expert Insights\n\n4.1.1 Simulation Analysis software\n\n\n\n4.2 Visualization Expert Insights\n\n4.2.1 History Plot Requirements\n4.2.2 General Visualization Principles:\n\n\n4.3 Summary of Key Findings\n\n4.4 Software Engineering Framework\n\n4.4.1 Framework Purpose and Derivation\n4.4.2 Framework Overview\n\n\n\n\n\n5 Solution\n\n5.1 Framework Validation through Implementation\n\n\n\n6 Technical validation\n\n\n6.1 Quality of the Final Visualization\n\n6.1.1 Scenario: Visualizing Convergence in a History Plot\n6.1.2 Scenario: Creating an Informative 2D Relation Plot\n6.1.3 Scenario: Creating an Informative Parallel Plot\n\n\n\n6.2 Quality of Generated Code\n\n6.2.1 Cross-Domain Validation: Project Descriptions\n6.2.2 Evaluation Framework\n6.2.3 Expert Evaluation Process\n6.2.4 Assessment Criteria\n6.2.5 Results\n\n\n6.3 Synthesis of Evaluation Results\n\n\n7 Conclusion\n\n8 Limitations and Threats to Validity\n\n8.0.1 Limitations\n8.0.2 Scope and External Validity\n8.1 Future Work\n\n\n\n\n\n\n\nHow to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework\n\n\nChoro Ulan uulu\n\nchoro.ulan-uulu@siemens.com\n\n, \nMikhail Kulyabin\n\n, \nIris Fuhrmann\n\n, \nJan Joosten\n\n, \nNuno Miguel Martins Pacheco\n\n, \nFilippos Petridis\n\n, \nRebecca Johnson\n\nSiemens AGMunichGermany\n\n, \nJan Bosch\n\nj.bosch1@tue.nl\n\nDepartment of Computer Science and Engineering, Chalmers University of TechnologyGothenburgSweden\n\nDepartment of Mathematics and Computer Science, Eindhoven University of TechnologyEindhovenNetherlands\n\n and \nHelena Holmström Olsson\n\nhelena.holmstrom.olsson@mau.se\n\nDepartment of Computer Science and Media Technology, Malmö UniversityMalmöSweden\n\n\n(2026)\n\nAbstract.\nCritical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline’s poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.\n\n††copyright: acmlicensed††journalyear: 2026††conference: 5th International Conference on AI Engineering – Software Engineering for AI; April 15–17,2026; Rio de Janeiro\n\n\n1. Introduction\n\nOrganizations across industries face a critical scalability challenge: essential domain knowledge often resides with few experts, creating bottlenecks that limit productivity and decision-making quality (Rosen et al., 2007). When experts are unavailable, work either halts or proceeds with suboptimal outcomes, potentially leading to missed deadlines, increased costs, and catastrophic failures (Rosen et al., 2007).\n\n\nThis challenge is particularly acute in data visualization, where creating effective charts requires both domain knowledge and visualization expertise. Non-experts typically default to familiar chart types because selecting appropriate techniques for complex data remains difficult (Grammel et al., 2010). Even when attempting sophisticated visualizations, results frequently require expert interpretation (Choe et al., 2024), while experts must balance mentorship against their primary responsibilities (Vajpayee, 2024).\n\n\nIn simulation data visualization, these challenges intensify. Engineers need dual expertise in simulation analysis and data analytics to create visualizations revealing decision-critical insights.\nWithout this expertise, users significantly underutilize available capabilities, missing opportunities to expose key trade-offs (Grammel et al., 2010; Choe et al., 2024). Critical visualization design knowledge—such as which plot types reveal specific patterns—remains tacit within domain experts, necessitating continuous validation cycles that divert expert resources from high-value tasks.\n\n\nWe illustrate this through Simulation Analysis software, a design space exploration platform that optimizes parameters (e.g., minimizing weight while maximizing strength).\nWhile Simulation Analysis software includes sophisticated post-result analysis capabilities that enable users to visualize complex data sets, this presents an opportunity to enhance user experience through automated guidance.\n\n\nWhile it includes sophisticated visualization capabilities, users require multiple attempts to identify effective visualization types (Gadiparthi, 2024). This trial-and-error approach is time-consuming and discourages full exploration of features that could accelerate design decisions.\n\n\nSimulation Analysis software serves as an ideal test case for our framework because its extensive visualization capabilities and comprehensive post-processing features are representative of sophisticated engineering software where automated expert guidance can significantly enhance user productivity.\n\n\nThis paper addresses the research question (RQ): How can domain knowledge from human experts be captured, codified, and leveraged to construct Large Language Model (LLM) - based AI agents capable of autonomous expert-level performance?\n\n\nOur results demonstrate how human expert domain knowledge can be captured to construct LLM-based AI agents to reduce expert bottlenecks. The resulting AI agent enables non-experts to generate expert-level visualizations that match expert-level quality in technical accuracy, visual clarity, and analytical insight, without requiring constant expert involvement.\n\n\nThe contributions of this paper are:\n(1) A systematic software engineering framework for capturing human domain knowledge and engineering an AI agent through complementary strategies: request classifier, RAG for domain-specific code generation, codified expert rules, and visualization design principles, implemented as a reference architecture demonstrating integration of heterogeneous AI techniques with clear separation of concerns.\n\n\n(2) Empirical evidence from industrial evaluation with 12 evaluators across five scenarios spanning multiple engineering domains (electrochemical, electromagnetic, mechanical systems) demonstrating 206% improvement in output quality (mean: 2.60 vs. 0.85 on 0-3 scale), with our system achieving expert-level ratings (Mode=3) consistently versus baseline’s poor performance (Mode=0 in 4/5 scenarios), and superior code quality with lower variance (SD: 0.29-0.58 vs. 0.39-1.11).\n\n\n(3) Demonstration that the framework addresses organizational expert bottlenecks by enabling non-experts to generate expert-level visualizations through simple prompts, effectively democratizing domain knowledge and allowing domain experts to focus on specialized tasks requiring unique expertise.\n\n\nSection 2 provides background and reviews related work, Section 3 describes our research methodology, Section 4 presents expert interview findings, Section 5 presents our solution, Section 6 validates effectiveness, and Section 7 concludes, and Section 8 discusses limitations.\n\n\n\n\n2. Background and Related work\n\nPoor visualizations cause fundamental misinterpretation of critical data (Franconeri et al., 2021). Medical-risk visualizations can lead patients to fundamentally misunderstand the base rates or risk factors for diseases or medical procedures (Ancker et al., 2006; Franconeri et al., 2021). Suboptimal visualizations in minimally invasive surgery have contributed to patient injury rates exceeding 50% (Ameerah et al., 2025). Misleading visualizations can lead to spread of misinformation (Biselli et al., 2025; Lisnic et al., 2023). In the context of simulation software, according to (Kido et al., 2023), misused visualization types are difficult to interpret and may lead to erroneous decision making.\n\n\nThe challenges in creating effective visualizations are established. Non-experts struggle with selecting appropriate visualization techniques for complex data (Grammel et al., 2010), often defaulting to familiar but suboptimal chart types. Even when more sophisticated visualizations are attempted, they frequently require expert interpretation (Choe et al., 2024), creating dependencies on scarce expert resources (Vajpayee, 2024).\n\n\nKnowledge Codification and Rule-Based Systems\nThe integration of structured knowledge into LLM systems represents an active research area. (Yang et al., 2025) conducted preliminary research on how LLMs can learn from rules. (Wang et al., 2024) developed a rule generation framework, creating 8000 primitive rules and 6000 compositional rules across five domains. (Vertsel and Rumiantsau, 2024) applied rule-based results to an LLM to generate insights. However, earlier work suggested limitations in LLMs’ ability to follow rules (Mu et al., 2023), though newer LLM versions have demonstrated"
  }
]