[
  {
    "title": "A Complete Decomposition of Stochastic Differential Equations",
    "url": "https://arxiv.org/abs/2601.07834v1",
    "source": "arxiv",
    "summary": "We show that any stochastic differential equation with prescribed time-dependent marginal distributions admits a decomposition into three components: a unique scalar field governing marginal evolution, a symmetric positive-semidefinite diffusion matrix field and a skew-symmetric matrix field.",
    "full_text": null
  },
  {
    "title": "MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head",
    "url": "https://arxiv.org/abs/2601.07832v1",
    "source": "arxiv",
    "summary": "While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the origin",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 Analysis of Linear Attention\n\n3.1 Preliminary\n\n3.2 Global Context Collapse\n\nRank limitation.\nLoss of sparsity.\n\n\n\n\n\n4 Multi-Head Linear Attention\n\n4.1 Overview\n\n4.2 Multi-Head Mixing\n\nChunkwise parallel form of MHLA.\n\n\n\n4.3 Analysis of Multi-Head Linear Attention\n\nRank analysis.\nSparsity analysis.\nEfficiency analysis.\n\n\n\n\n\n5 Experiments\n\n\n5.1 Image Classification\n\nSettings.\nResults.\n\n\n\n5.2 Image Generation\n\nSettings.\nC2I results.\nAnalysis.\nFast adaptation to SANA.\n\n\n5.3 Video Generation\n\n5.4 Natural Language Processing\n\nCommon-sense reasoning and MMLU.\nLong context understanding.\n\n\n\n5.5 Ablation Study\n\nMulti-Head Mixing.\nHead number.\n\n\n\n\n6 Conclusion\n\nA Full Related Works\n\nTransformer.\nLinear Attention.\nSparse Attention.\nApplications of Linear and Sparse Attention.\n\n\n\nB Query-Conditioned Selectivity in Softmax Attention\n\nMHLA restores query-conditioned selectivity.\n\n\n\nC MHLA for Autoregressive Modeling\n\nMHLA with chunkwise parallel training.\nCausal inference.\n\n\nD Dataset\n\nE Extra Implementation Details\n\nImage Classification.\n\n\n\nF Complete Experimental Results\n\nF.1 Image Generation\nF.2 Ablation of CPE and output gating.\nF.3 Classification results on Higher Resolutions\nF.4 Scaling Anaylsis\n\n\n\nG Clarification on Terminology and Computational Concepts\n\nG.1 Concept 1: query-conditioned\nG.2 Concept 2: KV Summary vs. Hidden States\n\n\nH LLM Usage.\n\n\n\n\n\n\n1]Peking University\n2]NVIDIA\n\\contribution[*]Equal contribution\n\nMHLA: Restoring Expressivity of Linear Attention \nvia Token-Level Multi-Head\n\n\n\nKewei Zhang1∗  \nYe Huang1∗  \nYufan Deng1  \nJincheng Yu2  \nJunsong Chen2   \nHuan Ling2  \nEnze Xie2  \nDaquan Zhou1\n\n[\n\n[\n\n\n\nAbstract\nWhile the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6% improvement on ImageNet classification, a 6.3% gain on NLP, a 12.6% improvement on image generation, and a 41% enhancement on video generation under the same time complexity.\n\n\n\\checkdata\n[\n  Project Page]https://dagroup-pku.github.io/MHLA\n\n\\checkdata[\n  GitHub Repo]https://github.com/DAGroup-PKU/MHLA\n\n\\checkdata[\n  Huggingface Repo]https://huggingface.co/DAGroup-PKU/MHLA\n\n\n\n\n1 Introduction\n\\begin{overpic}[width=411.93767pt]{figs/mhla_teaser.pdf}\n\\put(9.5,14.4){\\begin{minipage}{86.72267pt}\\centering\\scriptsize\\resizebox{86.72267pt}{}{\n\\begin{tabular}[]{lcc}\\hline\\cr\\hline\\cr Model&amp;Attn Type&amp;FID$\\downarrow$\\\\\n\\hline\\cr\\hbox{\\multirowsetup DiT-S/2}&amp;Linear-Attn&amp;89.7\\\\\n&amp;Self-Attn&amp;68.4\\\\\n&amp;MHLA(Ours)&amp;{59.8}\\\\\n\\hline\\cr\\hbox{\\multirowsetup DiT-XL/2}&amp;Linear-Attn&amp;28.63\\\\\n&amp;Self-Attn&amp;19.47\\\\\n&amp;MHLA(Ours)&amp;{19.17}\\\\\n\\hline\\cr\\hline\\cr\\end{tabular}\n}\n\\@add@centering\\end{minipage}\n}\n\\end{overpic}\nFigure 1: (a) Generation results from our fine-tuned SANA model using MHLA. (b) Performance and efficiency comparison between the proposed MHLA and baselines. The throughput was tested on the NVIDIA H100 Tensor Core GPU. Following the previous method, we report the FID in the table at a resolution of 256×256256\\times 256. (c) Multi-domain performance of MHLA. We evaluate MHLA across diverse domains, demonstrating its strong and universal performance.\n(d) Throughput of DiT-S/2 at 4096 resolution across different devices.\nAll improvements are solely due to MHLA, and can be further combined with orthogonal techniques for even greater speedups. \n\n\nSelf-attention is the core module for the recent dominant model architecture, Transformer, for both computer vision [dosovitskiy2020image], natural language processing [vaswani2017attention], and generative tasks [rombach2022high].\nHowever, its quadratic time and memory complexity severely limit its scalability to long sequence tasks such as high-resolution image generative and video generation tasks [zhou2022magicvideo, kong2024hunyuanvideo, zhou2024storydiffusion].\n\n\nTo address the efficiency issue, a growing line of research [katharopoulos2020transformers, choromanski2021rethinking] has developed linear attention mechanisms that replace the softmax kernel with associative feature maps. These approaches reduce the computational and memory complexity of attention from quadratic to linear by compressing all keys and values into a global summary. Although this improves efficiency, it eliminates one of the key advantages of softmax attention—its ability to adapt to each query individually. Consequently, linear attention often experiences notable accuracy degradation, particularly in long-sequence modeling tasks.\n\n\nRecent works [rala, focusedlinearattention, han2024inline] have sought to mitigate the performance degradation of linear attention by integrating components such as depthwise convolutions and gating modules. However, this reliance on external modules introduces additional computational overhead and continues to suffer from performance degradation as sequence length increases.\nIn this paper, we present a solution to the performance bottleneck in linear attention that requires no additional depthwise convolution or self-attention modules.\nOur key insight is that, in conventional linear attention design, all tokens are compressed into a single global key–value summary (KV summary) that is shared by every query. This design could have reduced the model’s representation capacity, as illustrated in Figs. 1b and 2.\nTo evaluate diversity, we compare the rank of the attention weight matrices across different models. We find that using a shared global KV summary limits the model’s capacity to represent rich interactions, effectively capping it at a fixed rank. As sequences grow longer, this constraint tends to push the attention weights toward a more uniform distribution.\nIn practice, this reduces diversity and degrades performance on tasks where queries must concentrate on a small subset of relevant tokens.\n\n\nOur design goal is therefore simple: restore query-dependent diversity, the ability for different queries to retrieve different contexts, without sacrificing linear-time behavior or introducing heavy auxiliary modules.\n\n\nThus, we introduce Multi-head Linear Attention (MHLA) to achieve the aforementioned characteristics. Specifically, MHLA partitions tokens into non-overlapping blocks (“heads” in the spatial dimension), computes local key-value summaries, and lets each query block compute a query-conditioned mixture over these summaries to retrieve a tailored context; within the selected blocks, token contributions are further refined by a query-dependent reweighting module.\nThanks to the simplicity of MHLA, the implementation only relies on standard GEMMs, keeping the overall computational overhead negligible with O​(N)O(N) complexity, retaining compatibility with streaming/stateful execution. It was clearly observed that adding MHLA raise the rank of the attention weights matrix significantly, as shown in Fig. 3(b). The difference between previous linear attentions and MHLA is briefly illustrated in Fig. 2.\n\n\nWe validate MHLA on image classification, image generation, natural language processing, and video generation tasks. Experiments show that MHLA consistently outperforms existing linear attention baselines with negligible computational overhead. Our main contributions are summarized as follows:\n\n\n•\n\nWe conduct an in-depth analysis of linear attention and identify one of the root causes of its performance degradation: the absence of grouping along the token dimension during similarity calculation. This limitation can be quantified by examining the rank of the attention matrix.\n\n\n\n•\n\nWe propose a new formulation of linear attention that achieves state-of-the-art performance on both discriminative and generative tasks, while maintaining O​(N)O(N) computational complexity and avoiding reliance on additional modules.\n\n\n\n•\n\nWe conduct extensive experiments across various tasks, achieving state-of-the-art performance. On ImageNet, MHLA delivers a 3.6% accuracy gain over self-attention, while on image generation tasks it improves the performance of the DiT architecture by 12.6%. MHLA also achieves a 6.3% improvement on natural language processing tasks and provides a substantial 41% improvement compared to vanilla linear attention in video generation tasks.\n\n\n\n\n\nFigure 2: Comparison between the proposed MHLA and other linear attentions. MHLA divides multiple heads on the token dimension. Through Multi-Head Mixing, MHLA restores query-conditioned selectivity by mixing KV summaries with query-specific weight, improving token-level diversity\nwhile keeping linear complexity.\n\n\n\n\n2 Related Works\n\nTransformers [vaswani2017attention] have advanced various fields [devlin2019bert, dosovitskiy2020image, saharia2022photorealistic], but their quadratic time and memory complexity due to self-attention limit scalability, especially for long sequences. To overcome this, linear attention mechanisms [katharopoulos2020transformers, choromanski2021rethinking] have been proposed, which replace softmax with kernel-based methods to achieve linear time complexity. While these mechanisms improve the efficiency, they often lose expressiveness, making them suffer from a performance drop in capturing complex token interactions. Sev"
  },
  {
    "title": "Optimal Learning Rate Schedule for Balancing Effort and Performance",
    "url": "https://arxiv.org/abs/2601.07830v1",
    "source": "arxiv",
    "summary": "Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the",
    "full_text": "\n\n\n\n1 Balancing Learning Effort and Performance\n\n2 Results\n\n2.1 General solution for the optimal learning rate schedule\n2.2 Estimating final performance through episodic memory\n2.3 Mathematical analysis for a linear perceptron\n2.4 Time discount effects\n2.5 Connection to animal behaviour\n\n\n3 Discussion\n\n4 Methods\n\n4.1 Deriving the closed-loop solution\n4.2 Estimating final performance through episodic memory\n4.3 Computing the perceptron open-loop solution\n4.4 Approximate solutions for the discount factor\n\n4.5 Implementation details\n\n4.5.1 Teacher-student\n4.5.2 MNIST\n4.5.3 Two-Gaussian discrimination\n4.5.4 Figure 3\n4.5.5 Figure 4\n4.5.6 Figure 5\n\n\n4.6 Numerical optimization\n\n\n\n\n\n\n\nOptimal Learning Rate Schedule for Balancing Effort and Performance\n\n\n  Valentina Njaradi\nGatsby Computational Neuroscience Unit\nUniversity College London\nvalentina.njaradi.23@ucl.ac.uk\n&amp; Rodrigo Carrasco-Davis∗\nGatsby Computational Neuroscience Unit\nUniversity College London\nPrinceton Neuroscience Institute\nrodrigo.cd@princeton.edu\n&amp; Peter Latham\nGatsby Computational Neuroscience Unit\nUniversity College London\n&amp; Andrew Saxe\nGatsby Computational Neuroscience Unit\nSainsbury Wellcome Centre\nUniversity College London \n\nEqual Contribution.\n\n\nAbstract\nLearning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent’s current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework.\n\n\nHumans and other animals constantly face the problem of allocating their limited cognitive resources to learning. This means not only deciding what is worth learning, but also how to approach each learning task. For instance, when learning piano, one might take a quick learning strategy and memorize sequences of keys to play simple songs, while another might take a slower learning strategy, study music theory and learn to read sheet music to later generalize to a broader repertoire. In this work, we focus on an abstract version of this general problem: how should learning systems regulate their learning speed when balancing performance against the costs of learning fast.\n\n\nFrom childhood onward, humans actively regulate their learning (Taffoni et al., 2014; Raz and Saxe, 2020; van Gog et al., 2020). They seek out information, select inputs that match their abilities (Kidd et al., 2012), and weigh the potential benefits of learning against its costs (Bonawitz et al., 2018). Adults, too, learn more effectively when they control the pace and structure of their study, compared to passive conditions (Castro et al., 2008; Markant and Gureckis, 2014; Coenen et al., 2019; Tullis and Benjamin, 2011). One’s own metacognitive control is fundamental: in studies of massed versus spaced learning, participants only benefited from spacing when it was aligned with their chosen learning plans (Son, 2010). Theories differ on which internal states drive such control: curiosity, boredom, and other motivational signals have all been implicated (Bazhydai et al., 2020; Hidi and Renninger, 2019; Schwartenbeck et al., 2019; Geana et al., 2016; Agrawal et al., 2022). But the underlying question remains: what computations guide decisions about how to learn?\n\n\nMany factors shape learning schedules. Returning to the piano example, the “right” strategy depends on available practice time, effort, prior knowledge, task difficulty, the long-term value of playing an instrument, and even personal enjoyment. Evidence supports the importance of each, but studying them in isolation can yield conflicting results. For example, people sometimes favour intermediate and high-difficulty tasks (Baranes et al., 2014; Schulz et al., 2019), while other work identifies a preference for the “Goldilocks zone” where tasks are neither too easy nor too hard (Kidd et al., 2012; Wilson et al., 2019; Ten et al., 2021; Cubit et al., 2021). Time pressure also matters: when rushed, learners prioritize easy tasks, but given ample study time they choose harder ones (Son and Metcalfe, 2000). Learners are also sensitive to gaps in their knowledge and prefer content they have not yet mastered (Kidd et al., 2012; de Eccher et al., 2024). The value of learning in terms of future rewards (Masís et al., 2023; Obando et al., 2025) further influences choices, where rodents, monkeys and humans have been shown to weigh short-term gains against the long-term value of learning.\n\n\nAlong with all of its benefits, learning also comes with costs. Some are metabolic: human studies show links between metabolism and increased memory and learning functions (Potter et al., 2010; Smith et al., 2011; Klug et al., 2022), while in flies and butterflies, intensive learning reduces reproductive fitness and shortens lifespan (Mery and Kawecki, 2004; Snell-Rood et al., 2011; Mery and Kawecki, 2005). Others arise from the exertion of cognitive control itself. Scheduling and regulating learning is a form of cognitive control (Masís et al., 2021, 2023; Masis et al., 2024; Carrasco-Davis et al., 2024), which comes with its own costs that participants take into account when allocating control (Kool and Botvinick, 2018; Kurzban et al., 2013; Shenhav et al., 2013, 2017; Agrawal et al., 2022). When the cost of control is considered in a learning context, studies show that subjects often choose more cognitively demanding tasks over easier ones (Jarvis et al., 2022), particularly when those tasks offer opportunities for learning (Sayalı et al., 2023). This suggests that humans can weigh the immediate costs of engaging in challenging tasks against the anticipated future benefits\nof learning (Obando et al., 2025).\n\n\nThese findings suggest that humans continuously balance costs and benefits when deciding how to learn. Framed computationally, the problem of optimally regulating an agent’s learning, so as to improve speed or quality, has been extensively studied in the field of meta-learning (Hospedales et al., 2020). In neural networks, meta-learning can take many forms: designing optimal learning curricula (Zhang et al., 2021; Stergiadis et al., 2021; Soviany et al., 2022), finding optimal initial weights for a network (Finn et al., 2017), continual learning (Parisi et al., 2019; Wang et al., 2024), optimizing hyperparameters (Franceschi et al., 2018; Baik et al., 2020), or determining optimal learning rate schedules throughout training (Nakamura et al., 2021; Mori et al., 2025; Mignacco and Mori, 2025). Still, designing an optimal learning speed schedule is computationally challenging, as in most cases it requires integrating and differentiating performance throughout the entire learning trajectory (Franceschi et al., 2018; Carrasco-Davis et al., 2024; Mori et al., 2025; Mignacco and Mori, 2025). For biological agents, this computation can be even more challenging, where the capacity for calculation of optimal control may be limited, and the necessary quantities may not be easily available. Nonetheless, managing learning is a key factor determining long-term success. All humans and animals face this problem on a daily basis, raising a key question: how do they do it?\n\n\nIn this paper, we take a normative approach to this problem by formalizing learning speed control as a trade-off between task performance and effort (section 1). We derive a closed-loop analytical expression for the optimal learning speed that depends only on quantities that could be available to the agent through learning, such as its current performance and its expected final performance, thereby avoiding the need to predict full learning trajectories (subsection 2.1). We show that this solution generalizes across tasks and network architectures, compare it to suboptimal strategies, and explore how biological agents might learn and approximate their own expected final performance using memories of past learning experiences (subsection 2.2). For simple models like the perceptron, we derive exact open-loop solutions that provide intuition for how task difficulty and effort costs shape learning (subsection 2.3). We extend the framework to settings with discounting of future rewards, presenting approximations for simple models and simulations that highlight the broader effects of discounting on optimal learning strategies (subsection 2.4). We compare results from our framework with animal experiments, and outline further experimental predictions (subsection 2.5). Finally, we discuss broader implications of our theory for machine learning and cognitive neur"
  },
  {
    "title": "Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation",
    "url": "https://arxiv.org/abs/2601.07821v1",
    "source": "arxiv",
    "summary": "Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tack",
    "full_text": null
  },
  {
    "title": "Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests",
    "url": "https://arxiv.org/abs/2601.07820v1",
    "source": "arxiv",
    "summary": "In human conversation, both interlocutors play an active role in maintaining mutual understanding. When addressees are uncertain about what speakers mean, for example, they can request clarification. It is an open question for language models whether they can assume a similar addressee role, recognizing and expressing their own uncertainty through clarification. We argue that reference games are a",
    "full_text": null
  },
  {
    "title": "The Confidence Trap: Gender Bias and Predictive Certainty in LLMs",
    "url": "https://arxiv.org/abs/2601.07806v1",
    "source": "arxiv",
    "summary": "The increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence scores correspond to fairness and bias. This study examines the alignment between LLM-predicted confidence and human-annotated bias judgments. Focusing on gender bias, the research investigates probability confidence calibration in contexts involving gendered pronoun resolution.",
    "full_text": "\n\n\n\nIntroduction\nRelated Work\nDatasets\nMethods\nExperimental Result\nDiscussion\nCalibration\nAblation Study\nConclusion\nEthical Statement\nAppendix\nImplantation details\nLimitations and Future Work\nAblation Study\nAdditional Information\nExtra Calibration Result\n\n\n\n\n\nThe Confidence Trap: Gender Bias and Predictive\nCertainty in LLMs\n\n\n\nAhmed Sabir1,\nMarkus Kängsepp1,\nRajesh Sharma1,2\n\n\n\nAbstract\nThe increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence scores correspond to fairness and bias. This study examines the alignment between LLM-predicted confidence and human-annotated bias judgments. Focusing on gender bias, the research investigates probability confidence calibration in contexts involving gendered pronoun resolution. The goal is to evaluate if calibration metrics based on predicted confidence scores effectively capture fairness-related disparities in LLMs. The results show that, among the six state-of-the-art models, Gemma-2 demonstrates the worst calibration according to the gender bias benchmark. The primary contribution of this work is a fairness-aware evaluation of LLMs’ confidence calibration, offering guidance for ethical deployment. In addition, we introduce a new calibration metric, Gender-ECE, designed to measure gender disparities in resolution tasks. Our code is publicly available at https://github.com/ahmedssabir/GECE.\n\n\nIntroduction\n\nLarge pretrained LLMs have demonstrated exceptional performance across a variety of tasks, yet their trustworthiness remains a pressing concern, especially regarding gender bias. These models not only inherit biases from their training data but can also amplify stereotypes, leading to unfair or inaccurate outputs in critical applications (Zhao et al. 2018; Rudinger et al. 2018; Nangia et al. 2020; Felkner et al. 2023; Gallegos et al. 2024; Sabir and Sharma 2025). The challenge lies not only in detecting bias but also in ensuring that users can reliably interpret and trust model predictions, particularly in high-stakes domains such as hiring, healthcare, and legal decision-making.\n\n\nCalibration, which ensures that an LLM’s confidence in its predictions accurately reflects the likelihood of being correct (Niculescu-Mizil and Caruana 2005; Si et al. 2022), is a crucial aspect of model reliability. Without proper calibration, an LLM might provide overconfident yet incorrect outputs, misleading users into trusting biased or unreliable responses. This issue becomes even more critical in cases where models exhibit skewed confidence distributions across different demographic groups, further exacerbating fairness concerns. Therefore, in this work, we aim to answer the question regarding calibration in a gender bias scenario. Here, calibration refers to how well a model’s probability estimates match reality. For instance, when a model assigns an 80% probability to its prediction (e.g. for the pronoun ’he’), then, ideally, for those 80% confidence predictions, the model should be correct approximately 80% of the time.\n\n\nThe importance of calibration is particularly relevant in gender bias tasks. In many real-world settings, the objective is not only to measure which pronoun best matches human bias but also to assess the model’s confidence in its predictions. Poor calibration means that the model might generate seemingly “confident” predictions even when it is frequently incorrect, posing a significant risk when deploying these models in production or high-stakes scenarios. In addition, advancing the understanding of fairness in the context of misrepresented groups contributes to a more comprehensive evaluation of model behavior (Ding et al. 2024).\n\n\nWhile many studies have explored the biases and stereotypes propagated by LLMs (Gallegos et al. 2024), little to no research has examined how well these models are calibrated in their predictions or how closely they align with human biases. In this work, we investigate these gaps by analyzing the confidence calibration of LLMs in gendered and gender-neutral contexts. Specifically, inspired by model calibration evaluation methods such as Expected Calibration Error (ECE) (Guo et al. 2017), which help evaluate the model’s predicted confidence with its actual outcomes, we seek to answer the following question: To what extent is the predictive confidence of LLMs calibrated in gendered pronoun resolution tasks?\n\n\nTo summarize our contributions: (1) we conduct a comprehensive analysis of widely used open-weight LLMs, offering practical guidance for their ethical deployment, and (2) we propose Gender-ECE, a novel adaptation of ECE designed to quantify gender-related calibration disparities in coreference resolution tasks.\n\n\n\nRelated Work\n\nGender Bias in LLM. Large language models have demonstrated remarkable capabilities in natural language understanding and generation. However, they also inherit and amplify societal biases, including gender bias. To measure Gender bias in downstream tasks in NLP applications, a variety of research efforts have investigated gender bias using templates based on structured sentences such as \"He/She is a/an [occupation],\" in which blanks are filled with occupations that convey either positive or negative stereotypes or associations (Stańczak and Augenstein 2021).\n\n\nAdditional research has been conducted using the format of Winograd Schemas (Levesque et al. 2012). This method has been applied to various datasets such as WinoBias (Zhao et al. 2018), Winogender (Rudinger et al. 2018), and WinoMT (Stanovsky et al. 2019). The Winograd Schema Challenge focuses on the task of coreference resolution, which requires common-sense reasoning. This challenge has been used to examine whether the interpretation of pronoun references in sentences is influenced by gender. This method has also been applied to evaluate gender-based stereotypes and neutral associations across various professions (Zhao et al. 2018; Rudinger et al. 2018; Sabir and Sharma 2025). More recently, most research has shifted beyond template-based approaches to address gender bias by evaluating the outputs of LLMs, which can encode and propagate societal stereotypes across various contexts, including personality traits, social roles, and cultural representations. For example, the work of Cheng et al. (Cheng et al. 2023) employs a method to assess stereotypes in LLMs by generating persona descriptions for various demographic groups. Extending this direction, prompt-driven frameworks such as RUTEd (Lum et al. 2025) measure bias by instructing LLMs to perform in-context tasks e.g. writing stories or adopting personas, shifting evaluation from short template-based sentences to longer-form generations that better surface bias.\n\n\nHowever, the most dominant method to measure bias in LLMs is the use of template-based methods, where controlled sentence structures allow for systematic evaluation of gender bias in model predictions. These templates help isolate bias by ensuring that the only difference between sentence variations is the gender-related attribute while keeping all other linguistic components identical. In this study, we adopt a template-based approach.\n\n\nCalibration in LLMs. Large language models are stated to be rather well-calibrated (Kadavath et al. 2022). Mostly, the studies involve using question-answer datasets with multiple answers (Krause et al. 2023; Kapoor et al. 2024; Yoon et al. 2025). There are many ways to check how well the model is calibrated, for example, using (1) prompting, (2) model token probabilities, and (3) training a model on top of the model’s internal state or output. Firstly, many works on prompting (Kadavath et al. 2022; Xiong et al. 2023; Tian et al. 2023; Yoon et al. 2025) have relied solely on the model’s ability, whether a LLM or a reasoning model, to convey confidence based on their answers or chain of thought. For example, Kadavath et al. (Kadavath et al. 2022) query a model about its answer confidence P​(t​r​u​e)P(true) after the model has answered a question in a question-answering setting. Secondly, token probabilities are a straightforward way to determine the model’s uncertainty for each token.\nHowever, these probabilities might not be well-calibrated, for example, for low-resource languages (Krause et al. 2023) in the case of multilingual models. Also, having bigger or pretrained and fine-tuned models does not grant better calibrated models (Chen et al. 2022). Lastly, training an extra model on top of the LLM to assess the model’s confidence more precisely. For example, (Kadavath et al. 2022) proposes to train models with an extra head to predict the probability that a model knows P​(I​K)P(IK) the answer. More recently, the work of (Kapoor et al. 2024) proposes a fine-tuning protocol to calibrate model confidences in question-answer tasks.\n\n\nIn this work, we are interested in token probabilities, more precisely, how well a model’s probabilities for gender bias align with human bias. Thus, extracting the probability for pronouns and using simple post-hoc calibration methods suffices for our analysis.\n\n\n\nDatasets\n\nWinoBias (Zhao et al. 2018). WinoBias is a benchmark dataset for evaluating gender bias in LLMs. The dataset contains 3,160 sentences using templates inspired by Winograd-schema style sentences (Levesque et al. 2012) with entities corresponding to people referred by their occupation (e.g. doctor). For each sentence, there are three variables person, occupation, and pronoun. The dataset is structured to test whether LLMs make gendered assumptions about professions by evaluating coreference resolution performance, e.g. \"The developer argued with the designer and slapped her in the face\".\n\n\nWinogender (Rudinger et al. 2018). Winogender is a dataset similar to WinoBias, which also includes template sentences that refer to occupation and person. The dataset contains 720 template-based sentences. Unlike WinoBias, Winogender introduced a second ent"
  },
  {
    "title": "Learning Through Dialogue: Unpacking the Dynamics of Human-LLM Conversations on Political Issues",
    "url": "https://arxiv.org/abs/2601.07796v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are increasingly used as conversational partners for learning, yet the interactional dynamics supporting users' learning and engagement are understudied. We analyze the linguistic and interactional features from both LLM and participant chats across 397 human-LLM conversations about socio-political issues to identify the mechanisms and conditions under which LLM explan",
    "full_text": null
  },
  {
    "title": "Kinship Data Benchmark for Multi-hop Reasoning",
    "url": "https://arxiv.org/abs/2601.07794v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realist",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Multi-Hop Reasoning Benchmarks\n2.2 Kinship Reasoning Benchmarks\n2.3 Cultural Knowledge in NLP\n\n\n\n3 Methodology\n\n3.1 Task Formulation\n3.2 Kinship Systems as Test Domains\n3.3 Benchmark Generator Pipeline\n3.4 Question Categories\n3.5 Comparing LLMs with KinshipQA\n\n\n\n4 Results and Evaluation\n\n\n4.1 Experimental Setup\n\nModels.\nEvaluation Protocol.\nEvaluation Infrastructure.\n\n\n4.2 Performance by Reasoning Complexity\n\n4.3 Performance by Kinship System\n\nSkewing Systems are Most Challenging.\nGenerational Merging is Relatively Easier.\nCross-Cousin Systems Show Intermediate Difficulty.\n\n\n4.4 Cultural Override Effect\n\n\n\n5 Discussion\n\n5.1 Factor 1: Reasoning Complexity (N-hops)\n\n5.2 Factor 2: Cultural Variation\n\nPerformance Varies by System Type.\nThe Cultural Override Effect.\n\n\n5.3 Interaction: Cultural Rules at Multi-Hop Complexity\n\n5.4 Error Analysis\n\nReasoning Complexity Errors (53.9%).\nCultural Variation Errors (30.4%).\n\n\n5.5 Implications\n\n\n6 Conclusion\n\nA Kinships System Description\n\nEskimo (Western/Lineal).\nHawaiian (Generational).\nIroquois (Bifurcate Merging).\nDravidian (Bifurcate with Cross-Cousin Marriage).\nCrow (Matrilineal Skewing).\nOmaha (Patrilineal Skewing).\nSudanese (Descriptive).\n\n\n\nB Qualitative Examples\n\nExample 1: Chain Tracking Failure (Eskimo System)\nExample 2: Cultural Default Error (Crow System)\nExample 3: Generational Skewing Failure (Omaha System)\n\n\n\nC Prompt Templates\n\nC.1 Zero-Shot Evaluation Prompt\nC.2 Chain-of-Thought Error Analysis Prompt\n\n\nD Cultural Override Mappings\n\n\n\n\n\nKinship Data Benchmark for Multi-hop Reasoning\n\n\nTianda Sun\n\n  \nDimitar Kazakov \nUniversity of York \nDepartment of Computer Science \nHeslington, York \nYO10 5DD\n\n\n\nAbstract\nLarge language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.\n\n\n\nKinship Data Benchmark for Multi-hop Reasoning\n\n\n\n\nTianda Sun  and Dimitar Kazakov\n\nUniversity of York\n\nDepartment of Computer Science\n\nHeslington, York\n\nYO10 5DD\n\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are increasingly evaluated not only on their ability to recall isolated facts, but also on their capacity to combine multiple pieces of information into a coherent answer. A particularly challenging instance of this broader goal is multi-hop reasoning: the ability to infer a correct conclusion by chaining together several intermediate relations, including such that are not explicitly stated. In this paper, we propose a novel way to test this ability by making use of data about kinship relations.\n\n\nOur approach is based on producing computer-generated realistic datasets representing a collection of interconnected family trees constructed to comply with a set of constraints regulating marriage in a given society. These constraints, often perceived as cultural or legal ‘taboos’, determine which kin relations are possible, forbidden, or socially marked. By controlling these constraints, we can generate large, internally consistent genealogical datasets tailored to specific cultural settings.\n\n\nFamily trees are a particularly suitable domain for evaluating multi-hop reasoning. First, kinship is a natural and ubiquitous topic of human communication. Second, genealogical structures offer a controlled way to vary task complexity: simple relations can be defined over one or two edges in a tree, while more complex relations require chaining over many intermediate nodes. This makes kinship an ideal testbed for probing how well an LLM can integrate multiple facts into a single inference.\n\n\n\nCrucially, many cultures and their languages lexicalise kin relations that span several nodes in a family tree. English second cousins, for instance, are defined as individuals who share great-grandparents but not grandparents. Bulgarian девер (dever) denotes one’s husband’s brother. Such terms allow speakers to refer succinctly to a specific relative without spelling out all intermediary relations. In principle, combinations of such terms could further shorten descriptions of complex relationships; in practice, however, they may also lead to confusion or over-complication. Continuing with the Bulgarian example, the relationship between a man’s шурей (shooray) and his балдъза (balduza) reduces to a simple brother–sister relation, since both are siblings of his wife.\n\n\nThere is cross-linguistic variation in kinship systems where concepts lexicalised in one language have no direct equivalent in another: Bulgarian, for example, lacks a single word corresponding to English sibling, instead using the phrase ‘brothers and sisters’. Conversely, a single term may cover distinct concepts across cultures. In some societies, the closest translation of a given kin term may be used to refer not only to a biological mother, but also to all maternal aunts. Despite these differences, any biological kin relation can in principle be described using elementary notions such as biological father and mother; what varies is the ease, compactness, and conventionality of such descriptions.\n\n\nThese observations suggest that LLM performance on kinship inference tasks may vary depending on the language and culture involved, and on the model’s implicit knowledge of the relevant kinship vocabulary and concepts. Nevertheless, the task itself remains well defined: inferring the relationship between two individuals from a set of stated facts. As such, it provides a robust and interpretable way to assess multi-hop reasoning ability.\n\n\n\nIt is also reasonable to expect that both human and artificial reasoners will perform better on kinship distinctions that are salient in their cultural training data. For example, in societies with strong Orthodox Christian traditions, distinctions between second cousins (prohibited as marriage partners) and third cousins (permitted by the Church) are more likely to be explicitly discussed than in Protestant contexts, where even first-cousin marriage may not be religiously forbidden. Similarly, in populations with weakened extended family ties—due, for instance, to high labour mobility, there may simply be fewer occasions to encounter or talk about relations such as one’s wife’s sister’s husband (bacanak in Turkish); in societies where one-child families are common, the offsping of such single children have no aunties, uncles or cousins. In short, we expect systematic variation in performance as a function of culture- and population-specific exposure to kinship structures.\n\n\nFor an evaluation framework to be informative, it must yield a sufficient spread of outcomes: it is of limited value if most models either pass all tests or fail entirely.\nAs a core component of any culture, kinship terms belong to the most stable layers of vocabulary and have long been used in historical and comparative linguistics—most notably in the Swadesh lists—to identify genealogical relationships between languages and to infer common origin Swadesh (1952). If changes in kinship systems and their associated vocabularies lead to measurable differences in LLM performance, this should be taken as a broader warning that model behaviour may not be consistent across cultures and domains.\n\n\nIn this paper, we introduce a pipeline for generating culture-specific, realistic sets of family trees spanning an arbitrary number of individuals and generations. Such data have many potential applications—for instance, providing genealogical backgrounds for non-player characters in computer games—and we hope our tool will find wide adoption beyond the present study. Here, however, we focus specifically on its use as a testbed for evaluating the ability of LLMs to perform multi-hop reasoning over textual descriptions of culturally grounded relational data, which we demonstrate on a number of popular LLMs.\n\n\n\n\n2 Related Work\n\n\n2.1 Multi-Hop Reasoning Benchmarks\n\nMulti-hop reasoning—chaining multiple inference steps to answer complex questions—remains a\nfundamental challenge for LLMs Qiao et al. (2023). Benchmarks like HotpotQA Yang et al. (2018) and 2WikiMultihopQA Ho et al. (2020) evaluate this capability, but face critical limitations: training data contamination and lack of reasoning chain verification. MRKE addresses contamination through knowledge editing, revealing that GPT-4’s accuracy drops from 69.3% to 53.2% on edited questions, with only 36.3% of responses following correct reasoning chains Zhou et al. (2024). CompoST demonstrates that LLMs struggle with compositional SPARQL-mapped questions even when they understand atomic components, with F1 scores degrading from 0.45 to 0.09 as structural complexity increases Schmidt et al. (2024); Li et al. (2024) identify three prevalent error types: hasty answers, incomplete reasoning chains, and logical inconsistencies.\n\n\n\n\n2.2 Kinship Reasoning Benchmarks\n\nThere is a history of research using kin data as a machine learning tes"
  },
  {
    "title": "Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification",
    "url": "https://arxiv.org/abs/2601.07790v1",
    "source": "arxiv",
    "summary": "System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We a",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related work\n\n2.1 Log Classification with Manual Methods\n2.2 Log Classification with Traditional ML Methods\n2.3 Log Classification with DL Methods\n2.4 Log Classification using LLMs\n2.5 Research Objectives\n\n\n3 Data\n\n4 Methodologies\n\n\n4.1 Method Configuration\n\n4.1.1 Zero-shot\n4.1.2 Few-shot\n4.1.3 RAG\n4.1.4 Hardware and Environment\n\n\n4.2 Models\n4.3 Evaluation\n\n\n\n5 Results\n\n5.1 Zero-shot\n5.2 Few-shot\n5.3 RAG\n\n\n\n6 Discussion\n\n6.1 Impact of Top-k Retrieval on Model Stability\n6.2 Overall Observations\n6.3 Architectural Factors Behind Performance Differences\n\n\n\n7 Conclusion and Future Work\n\nAuthor Contributions.\nFunding.\nData Availability Statement.\nAcknowledgments.\nConflicts of Interest.\n\n\n\n\n\n\n\nBenchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification\n\n\n\nYahya Masri \nGeorge Mason University\n\n  \nEmily Ma \nGeorge Mason University\n\n  \nZifu Wang \nHarvard University\n\n  \nJoseph Rogers \nGeorge Mason University\n\n  \nChaowei Yang \nGeorge Mason University\nEmail: cyang3@gmu.edu\n\n\nAbstract\nSystem logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving &lt;&lt;10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.\n\n\n\n1 Introduction\n\nSystem logs are vital components of modern computing infrastructure, capturing operational events, warnings, and performance information across distributed systems [26, 21, 69, 3]. They play a critical role in diagnosing faults, monitoring system health, and supporting automated responses in large-scale environments such as data centers and digital twins (DTs) [76, 17, 92]. However, as computing systems generate massive volumes of logs with complex, context-dependent language, manual interpretation has become infeasible [84, 38, 43, 30, 59]. The continuous flow of log entries produced by servers far exceeds human review capacity, leading to delayed fault detection and increased mean time to resolve [7, 72].\n\n\nRecent advances in combining large language models (LLMs) and retrieval-augmented generation (RAG) have demonstrated that external context can significantly improve model reliability on tasks that require domain awareness and factual grounding. Rather than relying solely on parameters learned during training, RAG allows models to incorporate relevant evidence at inference time by querying a knowledge database. This approach has shown strong benefits in knowledge-intensive applications such as question answering, structured information extraction, and technical reasoning, where precision and traceability are essential [81, 60, 70]. In system operation settings, retrieval enables models to surface historical signals, recurring patterns, and contextual system metadata, creating the foundation for downstream diagnostic reasoning. As DT architectures increasingly integrate intelligence and autonomy, retrieval becomes a key mechanism for linking runtime observations to prior system states, enabling more interpretable and context-aware decision pipelines.\n\n\nEvents and messages originating from the kernel, applications, and users of a system are recorded in system logs. Thus, these logs form an extensive record of processes executed within a network [86]. This provides system administrators with a crucial resource for monitoring performance, detecting security threats, and conducting root cause analysis (RCA) [13, 44, 86].\n\n\nThe Syslog protocol was created as a framework to allow machines to transmit these event notifications [25]. Each Syslog message contains a PRI component that consists of a “&lt;&lt;”, a number, and a “&gt;&gt;”. The number is known as the severity value, which is a combination of the log’s Facility and Severity values. The Facility value is an integer ranging from 0 to 23, which describes the context of the log. The Severity value is an integer from 0 to 7 that quantifies the risk level of each log. Multiplying the Facility value by 8 and adding the Severity yields the Priority, which administrators use to identify and resolve system errors [57]. This lack of strict standardization limits the use of severity labels as a canonical ground truth. However, it also makes them a realistic and challenging probe for evaluating whether language models (LMs) can align log content with operational intent under ambiguity [24].\n\n\nWithin DT-oriented monitoring pipelines, such log interpretation must be both accurate and latency-efficient, motivating the evaluation of compact, deployable LMs under strict output and runtime constraints.\n\n\nThe study focuses on evaluating small language models (SLMs) and small reasoning language models (SRLMs) using log severity classification as a controlled probe of their ability to ground real-world system log semantics under constrained outputs, based on logs collected from journalctl within the computing infrastructure.\n\n\n\n\n2 Related work\n\n\n2.1 Log Classification with Manual Methods\n\nIn the past, developers created sets of rules for processes such as anomaly detection, leading to manually implemented systems. However, with the rapid development of computing infrastructure in both complexity and scale, these methods have become time-consuming and error-prone [61]. Modern computing systems generate logs rapidly; for example, Le and Zhang [46] estimate a rate of 30-50 gigabytes (about 120—200 million lines) per hour for Alibaba’s email production cloud computing system. The massive volume of system logs produced is impossible to manually traverse, complicating efforts to uncover patterns and nuanced faults that contribute to system failures [95].\n\n\nHe et al. [32] observe another notable limitation: developers may suffer from insufficient technical expertise, such as an understanding of runtime behaviors, the functions of different log levels, or best logging practices. Often, large-scale systems will have hundreds of contributors, with each individual having deep knowledge of one sub-component of the overall system. Due to this specialization, developers may have gaps in their understanding of overall system behaviors and relationships [33]. Both conditions lead to struggles with assigning the proper log level. As a result, developers often must retrace their work and revise the levels assigned to previous system logs, further decreasing the efficiency [49].\n\n\n\n\n2.2 Log Classification with Traditional ML Methods\n\nDue to the challenges of manual log classification, autonomous analysis methods have been explored through the creation of traditional machine learning (ML) methods [18, 35, 91]. Commonly used ML algorithms include random forest (RF) and support vector machine (SVM) [4].\n\n\nRecently, Qi et al. [66] utilized RF as a baseline, observing strong performance on Hadoop Distributed File System (HDFS), OpenStack, and BlueGene/L (BGL) datasets. The RF model often outperforms other classical ML methods, such as principal component analysis (PCA) and invariant mining (IM). Similarly, Li et al. [48] evaluate SVM as a baseline and observe strong performance across the BGL and HDFS datasets, with the model achieving an F1-score of 0.96 and 0.92, respectively.\n\n\nChen et al. [14] observe that while ML methods represented a significant development for log analysis, key limitations prevent their widespread adaptation for practical use. Particularly, these methods often require the set of log events to be known beforehand, leading to difficulty in accounting for previously unseen events or sequences. As systems are upgraded and altered over time, many methods struggle to address new log events or changes in log semantics [53].\n\n\nAnother notable limitation of traditional ML models is their limited ability to model meaningful temporal dependencies between log events, including cause-related event sequences and abnormal timing patterns [23]. As a result, critical information such as abnormally long delays or unusual event sequences is often not taken into consideration.\n\n\n\n\n2.3 Log Classification with DL Methods\n\nMore recently, deep learning has been explored to improve autonomous system log classification and analysis [5, 68, 90]. Guo et al. [29] propose Logformer, a Transformer-based framework that is pre-trained on source domain log data and applie"
  },
  {
    "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
    "url": "https://arxiv.org/abs/2601.07782v1",
    "source": "arxiv",
    "summary": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nTool Learning and Retrieval.\nGenerative Modeling for Retrieval.\n\n\n\n3 ToolQP: Query Planning\n\n\n3.1 The Modeling Framework\n\nPlanning by Task Decomposition.\nInteractive Query Generation.\nRetrieval Aggregation.\n\n\n\n3.2 Data Generation &amp; SFT\n\nPlan Alignment.\nQuery Generation.\nQuery Verification and Trajectory Construction.\n\n\n3.3 Training – RLVR\n\n\n\n4 Experiments\n\n\n4.1 Tool Retrieval\n\nBenchmark and Setup.\nBaselines.\nBaselines.\nToolQP.\nResults.\n\n\n\n4.2 Transfer to unseen base retriever\n\nSetup.\nResults.\n\n\n\n4.3 End-to-end Tool Calling\n\nBenchmark and Setup.\nResults.\n\n\n\n4.4 Ablation Studies\n\nPrompting vs SFT vs RLVR.\nChoice of Aggregation Method.\nImportance of user query.\n\n\n4.5 Qualitative Analysis\n\n\n5 Conclusion\nA Dataset Details and Licenses\nB Baseline Implementation Details and Prompts\n\nC ToolQP Training Details\n\nC.1 Data\nC.2 Query Sequence Generation\nC.3 SFT\nC.4 RLVR\n\n\nD Details for Retriever Transfer Evaluation\nE Details for End-to-end Tool Use Evaluation\nF Additional Qualitative Studies\n\n\n\n\n\nBeyond Single-Shot: Multi-step Tool Retrieval via Query Planning\n\n\n\nWei Fang\n\n  \nJames Glass\n\nMassachusetts Institute of Technology, Cambridge MA, USA\n\n{weifang,glass}@mit.edu\n\n\n\nAbstract\nLLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests.\nThese failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions.\nTo address these challenges, we propose ToolQP, a lightweight framework that models retrieval as iterative query planning.\nInstead of single-shot matching, ToolQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition.\nWe train ToolQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR).\nExperiments demonstrate that ToolQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.\n\n\n\nBeyond Single-Shot: Multi-step Tool Retrieval via Query Planning\n\n\n\n\nWei Fang  and James Glass\n\nMassachusetts Institute of Technology, Cambridge MA, USA\n\n{weifang,glass}@mit.edu\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have evolved from simple text generation into integration within agentic frameworks that allow them to solve a variety of complex tasks such as math, reasoning, and coding, by interacting with external environments (Mialon et al., 2023; Yao et al., 2023).\nIntegral to this paradigm shift is the ability to use tools, namely APIs, databases, and software tools, to extend the models’ capabilities beyond their parametric knowledge (Qin et al., 2024a).\nAs agentic workflows are being developed, the scale of these tool libraries are expanding rapidly, moving from dozens of hand-picked functions to massive, dynamic repositories containing tens of thousands of APIs.\nIn these scenarios, it is computationally infeasible to fit the entire tool context, including documentation, tool-specific instructions, and in-context tool demonstrations, into the LLM’s context window.\nConsequently, tool retrieval has been fundamental to the design of practical frameworks, retrieving relevant tools from the toolset as an initial step (Li et al., 2023c).\n\n\nWhile recent work has adapted information retrieval (IR) techniques with ad-hoc tool-use datasets (Qu et al., 2024; Xu et al., 2024) to enhance tool retrieval, they along with approaches that perform well on conventional IR benchmarks are shown to exhibit poor performance on a wide variety of tool-use tasks and tools (Shi et al., 2025).\nThese existing approaches typically employ dense embeddings with a standard single-shot retrieval step, and while they may be effective for simple, direct queries, they often fail significantly when applied to complex, compositional tasks.\n\n\nWe identify three fundamental challenges that stem from applying these single-shot retrieval paradigms to dynamic agentic workflows.\nFirst, semantic misalignment creates a critical disconnect between the high-level vocabulary of user intent and the technical specificity of tool schemas.\nFor instance, a user may request to \"make this audio recording high quality,\" while a relevant tool scipy.signal.lfilter(b, a, x) may be defined strictly by mathematical parameters like b (numerator) and a (denominator) along with technical descriptions such as “Filter data along one-dimension with an IIR or FIR filter.”.\nStandard dense retrievers fail to bridge the gap between the subjective goal (\"high quality\") and the implementation-level terminology (lfilter), and this is exacerbated by the heterogeneous nature of large-scale tool libraries, where documentation styles vary from verbose descriptions to raw, schema-heavy protocols (Qin et al., 2024b; Shi et al., 2025).\nFurthermore, real-world tasks are inherently compositional, often requiring the simultaneous application of multiple distinct tools; for example, retrieving both a WeatherAPI and a StockMarketDB to “analyze how rain affects retail sales.”\nHowever, a single fixed-dimensional vector lacks the capacity to encode the combinatorial diversity of multiple disparate tools (Weller et al., 2025), a limitation that is amplified as tool libraries scale.\nFinally, current single-shot methods lack interactive toolset awareness.\nThey treat the repository as a static database and cannot handle internal constraints or changes.\nIn contrast, interacting with the environment provides critical feedback on inter-tool dependencies (Xu et al., 2024), for example discovering that a forecasting_tool requires a specific region_id from a lookup utility, and allows the system to adapt to modifications within the toolset.\n\n\nTo address these limitations, we propose the Tool Query Planner (ToolQP), a framework that formulates retrieval as an iterative planning process rather than a static single-shot semantic matching task. ToolQP decomposes complex user requests into a logical sequence of high-level sub-tasks, interactively retrieving relevant tools for each step through a unified and light-weight model designed to interface with any existing retrieval system.\nThis approach effectively bridges semantic gaps by inferring functional utility from abstract goals, circumvents compositional bottlenecks by retrieving conceptually-similar tools step-by-step rather than compressing them into a single vector, and resolves inter-tool dependencies and toolset modifications via dynamic environment feedback.\nOur design is inherently modular and generalizable, functioning as a complementary layer atop standard retrievers without requiring architectural changes to the underlying index or the downstream reasoning LLM.\nFurthermore, the explicit planning trajectory generated during retrieval could serve as valuable context for the downstream agent to ground its execution.\nExtensive experiments across a wide variety of tool-use tasks demonstrate that ToolQP significantly improves both retrieval accuracy and downstream execution success rates compared to state-of-the-art baselines.\nOverall, our contributions are summarized as follows:\n\n\n•\n\nWe propose ToolQP, a novel framework that fundamentally shifts tool retrieval from a static similarity matching task to a dynamic planning process. By reframing the problem, we enable the resolution of complex, compositional queries and facilitate the discovery of inter-tool dependencies that single-shot dense retrievers are inherently limited in addressing.\n\n\n\n•\n\nWe design ToolQP as a modular, lightweight layer that integrates seamlessly with existing dense retrievers and downstream LLMs. Our approach leverages interactive feedback to adapt to heterogeneous tool documentation styles and diverse retrieval environments without requiring architectural modifications or fine-tuning of the underlying system.\n\n\n\n•\n\nWe demonstrate through extensive experiments on a diverse set of tool-use benchmarks that ToolQP significantly outperforms state-of-the-art baselines. Our results show consistent improvements in both retrieval performance and downstream execution success rates, particularly in scenarios characterized by high compositional complexity and abstract user intent.\n\n\n\n\n\n\n\n2 Related Work\n\nFigure 1: Overview of the ToolQP framework. The Planner decomposes a complex user query (e.g., travel planning) into sequential sub-tasks. For each sub-task, it interactively generates queries, processes feedback from the dense retriever, and self-corrects if necessary, before aggregating the final set of relevant tools.\n\n\nTool Learning and Retrieval.\n\nLLMs are increasingly employed in agentic frameworks that enable tool use for solving complex tasks (Gupta and Kembhavi, 2023; Mialon et al., 2023; Surís et al., 2023; Team et al., 2023; Wu et al., 2023; Cai et al., 2024; Qin et al., 2024a; Zhang et al., 2024).\nConventional approaches include post-training fine-tuning (Parisi et al., 2022; Thoppilan et al., 2022; Patil et al., 2023; Schick et al., 2023; Dubey et al., 2024; Yang et al., 2023; Liu et al., 2025; Lin et al., 2025; Yang et al., 2025), or in-context learning with meta-prompts for zero-shot tool usage (Lu et al., 2023; Shen et al., 2023b; Song et al., 2023; Qin et al., 2024b; Zhuang et al., 2024).\nHowever, scaling to large toolsets (e.g., 52k+ in RapidAPI) is challenging due to limited context windows and performance degradation from long contexts (Liu et al., 2024a; Qu et al., 2024), an issue exacerbated when including necessary instructions and demonstrations (Hsieh et al., 2023; Xu et al., 2023).\nFurthermore, frequent updates to the toolset make retraining cost-prohibitive, necessitating zero-shot a"
  },
  {
    "title": "Enhancing Self-Correction in Large Language Models through Multi-Perspective Reflection",
    "url": "https://arxiv.org/abs/2601.07780v1",
    "source": "arxiv",
    "summary": "While Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After in",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Chain-of-Thought Reasoning in Large Language Models\n2.2 Self-Correction and Reflection Mechanisms for LLMs\n\n\n\n3 Method\n\n3.1 Initial CoT Generation\n\n3.2 Multi-Perspective Reflection\n\n3.2.1 Logical Consistency Check (v1v_{1})\n3.2.2 Information Completeness Check (v2v_{2})\n3.2.3 Potential Bias and Ethical Consideration (v3v_{3})\n3.2.4 Alternative Solution Exploration (v4v_{4})\n\n\n3.3 Synthesis and Refinement\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Baseline Methods\n\n4.3 Quantitative Results\n\nResults Analysis.\n\n\n4.4 Effectiveness Validation of MyGO Poly-Reflective CoT\n\n4.5 Human Evaluation\n\nHuman Evaluation Results.\n\n\n\n4.6 Ablation Study on Reflection Perspectives\n\nAblation Results Analysis.\n\n\n\n4.7 Impact of Number of Reflection Perspectives\n\nAnalysis of Perspective Count.\n\n\n\n4.8 Efficiency Analysis\n\nEfficiency Results.\n\n\n\n4.9 Qualitative Error Analysis and Corrective Mechanisms\n\nQualitative Analysis Findings.\n\n\n\n\n5 Conclusion\n\n\n\n\n\nEnhancing Self-Correction in Large Language Models through Multi-Perspective Reflection\n\n\nMariana Costa, Alberlucia Rafael Soarez, Daniel Kim, Camila Ferreira \nUniversity of Brasilia\n\n\n\nAbstract\nWhile Chain-of-Thought (CoT) prompting advances LLM reasoning, challenges persist in consistency, accuracy, and self-correction, especially for complex or ethically sensitive tasks. Existing single-dimensional reflection methods offer insufficient improvements. We propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel methodology employing structured multi-perspective reflection. After initial CoT, PR-CoT guides the LLM to self-assess its reasoning across multiple predefined angles: logical consistency, information completeness, biases/ethics, and alternative solutions. Implemented purely via prompt engineering, this process refines the initial CoT into a more robust and accurate final answer without model retraining. Experiments across arithmetic, commonsense, ethical decision-making, and logical puzzles, using GPT-three point five and GPT-four models, demonstrate PR-CoT’s superior performance. It significantly outperforms traditional CoT and existing reflection methods in logical consistency and error correction, with notable gains in nuanced domains like ethical decision-making. Ablation studies, human evaluations, and qualitative analyses further validate the contribution of each reflection perspective and the overall efficacy of our poly-reflective paradigm in fostering more reliable LLM reasoning.\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across a myriad of natural language understanding and generation tasks Qin et al. (2023), with ongoing research focusing on enhancing their generalization and multi-capability performance Zhou et al. (2025). A significant advancement in enhancing their reasoning prowess for complex tasks has been the introduction of Chain-of-Thought (CoT) prompting Fei et al. (2023), which enables models to generate intermediate reasoning steps leading to a final answer. This methodology has significantly improved LLMs’ performance on tasks requiring multi-step logic and problem-solving. However, despite these advances, LLMs frequently exhibit limitations in their logical consistency, accuracy, and self-correction abilities when confronted with highly intricate or chaotic reasoning problems, a challenge that is further compounded in multimodal settings for Large Vision-Language Models (LVLMs) requiring robust visual in-context learning and long-context reasoning capabilities Zhou et al. (2023, 2024a, 2024b). They can sometimes fall into suboptimal solutions or produce logical inconsistencies, especially in scenarios demanding deep analysis and multi-faceted consideration.\n\n\nExisting approaches, such as MyGO Multiplex CoT (MCoT) Weng et al. (2023), have attempted to address these shortcomings by guiding the model through a single-dimensional reflection process to refine its initial thoughts. While MCoT has shown promising results in correcting preliminary reasoning, this singular reflection mechanism often proves insufficient to comprehensively identify and rectify a broader spectrum of potential errors. These errors can range from subtle logical fallacies and critical information omissions to failures in considering diverse perspectives during ethical decision-making. To further bolster the reasoning robustness and reliability of LLMs, there is a pressing need for a more comprehensive and profound self-correction paradigm that moves beyond unidimensional introspection.\n\n\nIn response to these challenges, we propose MyGO Poly-Reflective Chain-of-Thought (PR-CoT), a novel method designed to significantly enhance LLMs’ performance in complex reasoning tasks by integrating a multi-perspective reflection mechanism. PR-CoT builds upon the foundation of initial CoT generation by subsequently guiding the model through a structured, multi-angle critical review of its own thought process. This allows for the identification and correction of a wider array of error types.\n\n\nFigure 1: Overview of MyGO Poly-Reflective Chain-of-Thought (PR-CoT). The figure illustrates the motivations and challenges in LLM reasoning, conceptually distinguishes traditional Chain-of-Thought (CoT) from PR-CoT’s multi-perspective reflection and iterative refinement, and highlights the resulting benefits for enhanced reasoning capabilities.\n\n\nThe core process of PR-CoT involves three main stages:\n\n\n1.\n\nInitial CoT Generation: The LLM first generates a standard, preliminary chain-of-thought and an answer based on the given task and prompt.\n\n\n\n2.\n\nMulti-Perspective Reflection: Through specially crafted prompts, the system guides the LLM to perform a self-assessment of the CoT generated in the first step from multiple predefined and complementary angles. These perspectives can include, but are not limited to:\n\n\n•\n\nLogical Consistency Check: Evaluating the rigor and non-contradiction of the reasoning steps.\n\n\n\n•\n\nInformation Completeness Check: Reflecting on whether critical information or assumptions might have been overlooked.\n\n\n\n•\n\nPotential Bias/Ethical Consideration: Particularly in decision-making tasks, scrutinizing for inappropriate biases or neglected ethical dimensions.\n\n\n\n•\n\nAlternative Solution Exploration: Considering if other plausible reasoning paths or solutions exist.\n\n\n\n\n\n\n3.\n\nSynthesis and Refinement: The LLM integrates insights and critiques from all reflection stages to revise its initial CoT, ultimately generating a final answer that has been thoroughly reviewed and optimized from multiple angles.\n\n\n\nCrucially, akin to Multiplex CoT, PR-CoT operates on existing LLM architectures and is implemented solely through sophisticated ”Prompt Engineering,” requiring no modifications to the model’s structure or additional training. This approach maximizes the LLM’s inherent reasoning and critical abilities by guiding it through structured, multi-dimensional thought processes.\n\n\nTo validate the efficacy of MyGO Poly-Reflective CoT (PR-CoT), we conducted comprehensive experimental evaluations across a variety of representative complex reasoning tasks. Our method’s inherent design, relying on prompt engineering rather than specific model training, ensures its broad applicability across mainstream LLMs. For our experiments, we utilized leading pre-trained large language models, such as the GPT-3.5 or GPT-4 series (or their comparable open-source alternatives), to demonstrate the method’s universality. We adopted tasks aligned with prior research, specifically MyGO Multiplex CoT, to facilitate direct comparisons. These tasks included Arithmetic Problem-Solving, which assesses basic computational and logical deduction; Commonsense Reasoning, evaluating understanding of everyday knowledge and implicit logic; Ethical Decision-Making, probing the model’s judgment in complex moral dilemmas; and Logical Puzzles, testing the ability to handle multi-step reasoning and intricate constraints. We focused on key evaluation metrics such as Logical Consistency and Error Correction Rate, alongside the overall accuracy of the final answers.\n\n\nOur fabricated yet plausible experimental results unequivocally demonstrate the superior performance of MyGO Poly-Reflective CoT (PR-CoT) across all evaluated tasks compared to traditional CoT methods and even outperforming the single-reflection Multiplex CoT (MCoT). For instance, in Arithmetic Problem-Solving, PR-CoT achieved 94% logical consistency and a 17% error correction rate, surpassing MCoT’s 92% consistency and 15% correction rate. More notably, in challenging domains like Ethical Decision-Making, PR-CoT exhibited a substantial advantage with 84% consistency and a remarkable 21% error correction rate, underscoring the particular effectiveness of multi-angle reflection for complex, multi-factor considerations. Similarly, for Logical Puzzles, PR-CoT reached 93% consistency and a 23% error correction rate, further solidifying its lead. This collective evidence robustly validates that the multi-perspective self-reflection mechanism significantly enhances LLMs’ ability to tackle complex reasoning tasks.\n\n\nOur key contributions are summarized as follows:\n\n\n•\n\nWe introduce MyGO Poly-Reflective CoT (PR-CoT), a novel methodology that enhances LLM self-correction through a structured multi-perspective reflection mechanism.\n\n\n\n•\n\nWe demonstrate that PR-CoT can identify and correct a wider range of reasoning errors, including logical inconsistencies, information omissions, and ethical biases, compared to single-dimensional reflection methods.\n\n\n\n•\n\nWe empirically show that PR-CoT consistently outperforms traditional CoT and MyGO Multiplex CoT across diverse complex reasoning tasks, achieving higher logical consistency and error correction rates without requiring model retraining or architectural modifications.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Chain-of-Thought Reasoning in Large Language Models"
  },
  {
    "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
    "url": "https://arxiv.org/abs/2601.07779v1",
    "source": "arxiv",
    "summary": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a ho",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 OS-Symphony\n\n3.1 Orchestrator\n3.2 Reflection-Memory Agent\n3.3 Versatile Tool Agents\n\n\n\n4 Experiment\n\n4.1 Experiment Setup\n4.2 Main Results\n4.3 Ablation Study\n4.4 Discussion\n\n\n5 Conclusion\n\nA Details of OS-Symphony\n\nA.1 Task Definition\n\nA.2 More Details of OS-Symphony\n\nOrchestrator.\nReflection-Memory Agent.\nGeneral Grounder.\nOCR-based Grounder.\nSearcher.\nCoder.\n\n\nA.3 Action Space\n\n\n\nB More Results\n\n\nB.1 More Results on OSWorld\n\nResults with Pass@K.\nImpact of Thinking.\nImpact of Instruction Rewriting.\nImpact of Coder.\nImpact of Visual Context Length.\n\n\n\nB.2 More Results on WindowsAgentArena &amp; MacOSArena\n\nWindowsAgentArena.\nMacOSArena.\n\n\nB.3 Other Statistics\n\n\n\nC Case Study\n\nC.1 Correct Case\n\nC.2 Error Case\n\nErroneous Reflection.\nAmbiguous Instruction.\n\n\n\n\nD Prompt Engineering\n\n\n\n\n\n\nOS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent\n\n\n\n\nBowen Yang1,21,2,\nKaiming Jin3∗3\\,*,\nZhenyu Wu22,\nZhaoyang Liu44,\nQiushi Sun55,\nZehao Li22, \nJingjing Xie66,\nZhoumianze Liu22,\nFangzhi Xu77,\nKanzhi Cheng88,\nQingyun Li99,\nYian Wang33, \nYu Qiao22,\nZun Wang22,\nZichen Ding22\n11University of Science and Technology of China\n22Shanghai AI Laboratory \n33National University of Singapore\n44The Hong Kong University of Science and Technology \n55The University of Hong Kong\n66CUHK MMLab\n77Xi’an Jiaotong University \n88Nanjing University\n99Harbin Institute of Technology\n  Equal Contribution.  Corresponding Author.\n\n\nAbstract\nWhile Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval.\nTo bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation:\n1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; 2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios.\nExperimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.\nOur code and project are publicly available at \n\n OS-Copilot/OS-Symphony and \n\n OS-Symphony Homepage.\n\n\n\\DeclareCaptionType\nprompt[Prompt][Prompt]\n\n\n\nOS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent\n\n\n\n\n\nBowen Yang1,21,2††thanks:   Equal Contribution.,\nKaiming Jin3∗3\\,*,\nZhenyu Wu22,\nZhaoyang Liu44,\nQiushi Sun55,\nZehao Li22,\n\nJingjing Xie66,\nZhoumianze Liu22,\nFangzhi Xu77,\nKanzhi Cheng88,\nQingyun Li99,\nYian Wang33,\n\nYu Qiao22,\nZun Wang22,\nZichen Ding22††thanks:   Corresponding Author.\n\n11University of Science and Technology of China\n22Shanghai AI Laboratory\n\n33National University of Singapore\n44The Hong Kong University of Science and Technology\n\n55The University of Hong Kong\n66CUHK MMLab\n77Xi’an Jiaotong University\n\n88Nanjing University\n99Harbin Institute of Technology\n\n\n\n\n\n\n1 Introduction\n\nThe landscape of digital task automation has been reshaped by the advancement of Vision-Language Models (VLMs) (Bai et al., 2025b, a; Wang et al., 2025b; Anthropic, 2025c; Hong et al., 2025),\nleading to vision-guided Computer-Using Agents (CUAs) (Sun et al., 2024b; Qin et al., 2025; Wang et al., 2025a; Xie et al., 2025a; Wu et al., 2025f; Liu et al., 2025b; Wu et al., 2025c).\nBy leveraging visual perception to interact with digital environments, these agents have expanded the scope and applicability of general-purpose automation.\n\n\nWhile native CUAs (Zhang et al., 2024; Hu et al., 2025a; Liu et al., 2025b; Wang et al., 2025c) trained on large-scale computer-using trajectories are capable of digital navigation tasks, they often struggle to generalize to complex scenarios under a single-agent paradigm. Consequently, modular CUA frameworks (Wu et al., 2024a; Song et al., 2025; Gonzalez-Pumariega et al., 2025; Guo et al., 2025b) have emerged by orchestrating multiple specialized sub-agents, e.g., planner, grounder, and coder (Chen et al., 2025; Jia et al., 2025a), to coordinate seamlessly, exhibiting significant potential for developing reliable generalist CUAs.\n\n\nFigure 1: Current limitations in CUA framework.\n\n\nDespite the promising progress in agentic frameworks, they face two critical challenges.\nFirst, while memory modules Song et al. (2025); Cheng et al. (2025); Tian et al. (2025) are employed to support long-horizon tasks, current context management mechanisms often lack granular control over historical visual information curation and pruning. As illustrated in Fig. 1(a), this deficiency results in suboptimal utilization of historical visual information, rendering agents ill-equipped to identify potential errors like intent drift or cyclic behaviors. This lack of retrospective insight ultimately prevents the generation of meaningful reflections to refine planning in complex, long-horizon tasks.\nSecond, several works Xu et al. (2024a); Sun et al. (2024c); Agashe et al. (2025a); Guo et al. (2025b); Xu et al. (2025) incorporate external knowledge via Retrieval-Augmented Generation (RAG) in an effort to generalize to unseen scenarios; however, as shown in Fig. 1(b), they either excessively rely on unimodal information, thereby overlooking vital semantic cues in the visual modality, or depend on local knowledge bases that incur high maintenance costs and struggle to adapt to new software. Consequently, these approaches fail to achieve robust generalization on out-of-distribution (OOD) tasks.\n\n\nTo this end, we propose OS-Symphony, a holistic CUA framework comprising a decision-making Orchestrator coordinating two core designs to bridge these gaps:\n1) Reflection-Memory Agent that leverages long-term memory to retain key “milestone” screenshots alongside abstract trajectories, effectively mitigating visual context loss. By visually auditing historical states, the RMA generates critical trajectory-level reflections according to a structured message protocol, providing high-level guidance for the Orchestrator to ensure robust performance over long-horizon tasks.\n2) Versatile Tool Agents, highlighted by a meticulously designed Multimodal Searcher alongside a Coder and Grounders that work synergistically to execute complex tasks. Specifically, our Searcher enables acquiring diverse tutorials via browser-based sandbox autonomously. By integrating visual information with spatial layouts, it provides high-fidelity, relevant tutorials, enabling the Orchestrator to leverage external multimodal knowledge for OOD scenarios.\n\n\nWe demonstrate the effectiveness of OS-Symphony across diverse scales and benchmarks, achieving substantial performance leaps over current state-of-the-art methods with scores of 65.8% on OSWorld (↑\\uparrow2.4%), 63.5% on WindowsAgentArena (↑\\uparrow6.9%), and 46.0% on MacOSArena (↑\\uparrow38.0%).\nBeyond quantitative results, our rigorous ablation and granular analysis dissect the core drivers of this performance, offering valuable directions for future CUA development.\n\n\nOur contributions are summarized as follows:\n\n\n1) We propose OS-Symphony, a holistic CUA framework which investigates a robust and generalist paradigm via collaboration of diverse agents to solve complicated tasks in practice.\n\n\n2) We design a Reflection-Memory Agent to address the lack of granular control over historical visual context curation. By integrating milestone-driven long-term memory with a structured auditing protocol, it generates in-depth reflection for robust long-horizon planning.\n\n\n3) We develop a suite of tool agents which facilitate solving tasks effectively. To overcome the absence of visual-aware tutorial retrieval, among these tools, Multimodal Searcher is highlighted to harvest rich multimodal knowledge for OOD tasks by actively navigating the web pages.\n\n\n4) Extensive evaluations across diverse operating systems and model scales validate the superior performance of OS-Symphony. Furthermore, our framework empowers open-source VLMs to successfully execute long-horizon or unseen tasks that previously challenged their capabilities.\n\n\n\n\n2 Related Work\n\nFigure 2: Pipeline overview. OS-Symphony comprises three primary components: (1) The Orchestrator, acting as the system’s brain, responsible for task understanding and action prediction; (2) Tool Agents,\nconsisting of Grounder, Coder, and Searcher, where the Searcher retrieves up-to-date tutorials in a human-like manner; and (3) The Reflection-Memory Agent,\nwhich compresses execution trajectories to maintain long-term memory and facilitate trajectory-level reflection.\n\n\nComputer-Using Agents (CUAs).\nWith the rapid development of Vision-Language Models (VLMs)  (Anthropic, 2025b; Comanici et al., 2025; OpenAI, 2025c; Wang et al., 2025b; Bai et al., 2025a), Computer-Using Agents have become a novel paradigm to explore Human-Computer Interaction.\nNative CUAs pursue end-to-end digital autonomy,\nencompassing both general-purpose models (OpenAI, 2025a; Guo et al., 2025a; Anthropic, 2025c) adapted for agentic tasks and specialized models (Cheng et al., 2024; Wu et al., 2024b; Xu et al., 2024b; Qin et al., 2025; Wang et al., 2025c) fine-tuned on large-scale GUI datasets for dedicated computer use.\nIn parallel, CUA frameworks (Wu et al., 2024a; Agashe et al., 2025b; Yang et al., 2025a; Wu et al., 2025d; Ye et al., 2025a; Zhang et al., 2025c) prioritize modularity by decomposing complex tasks.\nThis approach enhances capability through modular collaboration while mitigating the data depen"
  },
  {
    "title": "DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference",
    "url": "https://arxiv.org/abs/2601.07778v1",
    "source": "arxiv",
    "summary": "We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care. DT-ICU integrates variable-length clinical time series with static patient information in a unified multitask architecture, enabling predictions to be updated as new observations accumulate over the ICU stay. We evaluate DT-ICU on the large, publicly available MIMIC-IV dataset, where it consi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Results\n\n2.1 Robustness to input length and task configuration\n\n2.2 Comparison with state-of-the-art and baseline models\n\nComparison with literature baselines.\nComparison with standard backbone models.\n\n\n2.3 Effect of test length\n2.4 Dealing with data imbalance\n\n2.5 Multimodal attribution and explainability\n\nSingle-modality attribution (LOMO).\nPairwise modality interactions (LTMO).\nComplementary metric relationships.\n\n\n\n\n\n3 Discussion\n\nEarly and continuous risk estimation.\nMultimodal structure of ICU reasoning.\nMetric behavior and failure modes.\n”Learned” multimodal structure.\nFuture research directions.\n\n\n\n4 Methods\n\n4.1 Dataset\n4.2 Evaluation protocol\n4.3 Problem formulation\n\n4.4 DT-ICU pipeline\n\nContinuous updating and iterative inference.\n\n\n4.5 Training setup and evaluation protocol\n\n\n5 Conclusion\nData availability and ethics.\nCode availability\nAuthor contributions\nAI-assisted writing disclosure\nA Effect of test length\nB LOMO and LTMO ablation details\n\n\n\n\n\nDT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference\n\n\n\\fnmWen \\surGuo\n\nguowen0903@hotmail.com\n\n\\orgnameETH Zurich, \\orgaddress\\countrySwitzerland\n\n\n\nAbstract\nWe introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care. DT-ICU integrates variable-length clinical time series with static patient information in a unified multitask architecture, enabling predictions to be updated as new observations accumulate over the ICU stay. We evaluate DT-ICU on the large, publicly available MIMIC-IV dataset, where it consistently outperforms established baseline models under different evaluation settings.\nOur test-length analysis shows that meaningful discrimination is achieved shortly after admission, while longer observation windows further improve the ranking of high-risk patients in highly imbalanced cohorts. To examine how the model leverages heterogeneous data sources, we perform systematic modality ablations, revealing that the model learnt a reasonable structured reliance on interventions, physiological response observations, and contextual information. These analyses provide interpretable insights into how multimodal signals are combined and how trade-offs between sensitivity and precision emerge.\nTogether, these results demonstrate that DT-ICU delivers accurate, temporally robust, and interpretable predictions, supporting its potential as a practical digital twin framework for continuous patient monitoring in critical care.\n\n\n\n1 Introduction\n\nThe concept of digital twins originates in the domain of engineering, where it was introduced to create continuously updated virtual counterparts of physical systems for real-time monitoring, simulation, and predictive maintenance[1, 2], a paradigm that is now being adapted to model complex biological and clinical systems in healthcare domains[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15].\nAs artificial intelligence becomes more deeply embedded in clinical workflows, there is growing interest in moving beyond static prediction models toward systems that can continuously track patient state, anticipate clinical deterioration, and support timely decision making. In this context, digital twins offer a promising direction: they are not only passive predictors, but evolving models of individual patients that can be updated as new data arrive. In high-acuity settings such as the intensive care unit (ICU), where conditions change rapidly and treatment decisions are time-critical, this ability to maintain a continuously updated view of the patient is especially important.\n\n\nFor a digital twin to be clinically meaningful in healthcare, several key capabilities are required. First, it must integrate heterogeneous clinical data, including structured time-series inputs (e.g., vital signs and laboratory measurements) together with static patient information (e.g., demographics and diagnoses). Second, it must support continuous updating, so that predictions and internal representations evolve as new observations become available. Third, it should enable systematic perturbation or “what-if” analyses, allowing clinicians and researchers to examine how changes in available information or treatment-related signals may influence predicted outcomes.\n\n\nThese requirements are particularly salient in the ICU, where patient trajectories can shift abruptly and clinicians must make high-stakes decisions under time pressure. Despite the growing use of machine learning models in critical care [16, 17, 18, 19, 20], most existing approaches fall short of this ideal. They are commonly trained and evaluated using fixed-length observational windows, focus on single predictive tasks, and operate in a static inference mode. As a result, such models function primarily as snapshot-based risk calculators, rather than continuously evolving representations of patient state.\n\n\nA wide range of deep learning methods, including MLP [21], recurrent neural networks (RNNs) [22], long short-term memory (LSTM) models [23], and more recent transformer-based architectures [24], have been applied to ICU prediction tasks such as mortality risk and length-of-stay estimation [25, 26, 27, 28, 29, 30, 31]. While these approaches have demonstrated strong performance under controlled settings, they are typically developed for narrow predictive targets and evaluated under rigid data assumptions. Consequently, they are not designed to support continuous updating, multimodal perturbation analysis, or evolving patient representations over time.\n\n\nIn this work, we propose DT-ICU, a multimodal digital twin framework designed to address these limitations and support continuous outcome prediction in critical care. DT-ICU ingests variable-length patient histories composed of heterogeneous clinical data streams and integrates them through a unified multitask learning architecture that supports both classification (e.g., mortality risk) and regression (e.g., physiological trajectories and future predictions). Crucially, the framework enables iterative inference, allowing systematic examination of how different modalities and clinical signals influence model predictions over time. In addition, DT-ICU incorporates attention-based analysis and structured modality ablations, providing transparent insights into its multimodal reasoning process.\n\n\nWe evaluate DT-ICU through a comprehensive set of experiments on the large, publicly available MIMIC-IV dataset [32]. DT-ICU consistently outperforms state-of-the-art mortality prediction baselines and it also achieves superior performance relative to standard backbone architectures, including MLP, RNN, and LSTM variants, within our variable-length and multitask framework. By analyzing performance as patient history accumulates, we show that DT-ICU attains stable and reliable discrimination early in the ICU stay while continuing to refine the ranking of high-risk patients over longer horizons.\nBeyond aggregate accuracy, we conduct systematic modality ablation studies that expose how different clinical signals are organized within the model, revealing a learned hierarchy in which interventions and physiological responses carry the strongest predictive value. Together, these analyses demonstrate that DT-ICU does not rely on predefined rules about which inputs should matter, but instead learns clinically meaningful multimodal structure directly from data, enabling robust and adaptive digital twin representations that remain effective under heterogeneous, incomplete, and evolving ICU data streams.\n\n\n\n\n2 Results\n\n\n2.1 Robustness to input length and task configuration\n\nAs explained in Section 1, a defining requirement of our ICU digital twin is the ability to work under heterogeneous data availability, and across multiple clinical objectives including detection and regression tasks. In practice, this means that the model must support arbitrary observation lengths after admission and accommodate additional regression tasks without compromising its primary clinical classification performance. We therefore evaluate DT-ICU under different input length and task configurations to assess its robustness as a continuously operating system. Specifically, DT-ICU was tested under three representative settings: (1) a conventional 72-hour fixed-length input without a regression head, (2) variable-length input without a regression head, and (3) variable-length input with multitask prediction. As summarized in Table 1, DT-ICU maintains strong and stable performance across all configurations. Firstly, transitioning from a fixed 72-hour window to variable-length inputs does not lead to any degradation in discrimination or classification accuracy, demonstrating that the model can operate effectively at arbitrary time points during the ICU stay. Besides, adding the regression head for multitask learning also does not harm performance on the primary classification task: AUROC and AUPRC remain unchanged between the single-task and multitask settings, while accuracy, precision, and recall show only minor variations. This indicates that DT-ICU can jointly support multiple clinical objectives without inducing negative transfer, confirming its suitability as a unified, continuously operating digital twin.\n\n\nTable 1: Performance under different input length and task configurations, demonstrating robustness to variable-length inputs and multitask prediction.\n\n\n\nSetting\nAUROC\nAUPRC\nAccuracy\nPrecision\nRecall\n\n\n\n\n72h (no regression head)\n0.88\n0.85\n0.81\n0.80\n0.79\n\n\nAny length (no regression head)\n0.98\n0.82\n0.96\n0.65\n0.84\n\n\nAny length (with regression head)\n0.98\n0.82\n0.95\n0.62\n0.87\n\n\n\n\n\n\n\n2.2 Comparison with state-of-the-art and baseline models\n\nHaving established that DT-ICU can operate robustly under flexible input lengths and multitask settings, we next evaluate whether this design translates into superior predictive performance. We conduct two complement"
  },
  {
    "title": "Are LLM Decisions Faithful to Verbal Confidence?",
    "url": "https://arxiv.org/abs/2601.07767v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty. However, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model. To test this, we introduce $\\textbf{RiskEval}$: a framework designed to evaluate whether models adjust their abstention policies in response to varying error",
    "full_text": "\n\n\n\n\n1 Introduction\n\nOur Contributions.\n\n\n\n2 Problem Setup\n\nPolicy Consistency (PC)\nExpected and Normalized Regret (ℛ\\mathcal{R} and ℛ¯\\overline{\\mathcal{R}})\nArea Under the Accuracy-Rejection Curve (AUARC).\n\n\n\n3 Experiments\n\nModels and Datasets.\nMetrics and Evaluation.\nMain Results.\nFailure to Adapt Decision Rules.\nLow Abstention Rate Causes Utility Collapse.\nPrompting Fails to Induce Abstention.\nScaffold with π∗\\pi^{*} Improves Utility.\n\n\n\n4 Related Work\n\nUncertainty Quantification for LLMs\nAbstention and selective prediction in LLMs.\nDecision-theoretic and behavioral evaluation of LLM behavior.\n\n\n5 Discussions and Conclusion\nDependency on Verbalized Confidence.\nScope of Tasks.\n\nA Details of Evaluation Metrics\n\nDerivation of Normalized Regret.\nNormalization.\nAbstention Rate.\nAccuracy (Answered).\nAverage Utility.\nDecision-Making Metrics.\nExpected Calibration Error (ECE-10)\nBrier Score.\n\n\n\nB Experiments\n\n\nB.1 Evaluation Details\n\nModels and Datasets.\nEvaluation Pipeline.\nPrompting and Penalty Conditioning.\nDatasets and Modalities.\nConfidence Handling.\nOptimal Policy and Post-hoc Analysis.\nExecution and Reproducibility.\n\n\n\nB.2 Confidence and Calibration\n\nInvariance of Confidence.\nStability of Calibration Metrics.\n\n\n\nB.3 Utility and Regret\n\nUtility Collapse.\nRegret Accumulation.\n\n\nB.4 Ablation Study\n\n\nC Licenses\n\n\n\n\n\nAre LLM Decisions Faithful to Verbal Confidence?\n\n\nJiawei Wang  Yanfei Zhou  Siddartha Devic  Deqing Fu\nUniversity of Southern California \n{jwang535,yanfeizh,devic,deqingfu}@usc.edu\n\n\n\nAbstract\nLarge Language Models (LLMs) can produce surprisingly sophisticated estimates of their own uncertainty.\nHowever, it remains unclear to what extent this expressed confidence is tied to the reasoning, knowledge, or decision making of the model.\nTo test this, we introduce RiskEval: a framework designed to evaluate whether models adjust their abstention policies in response to varying error penalties.\nOur evaluation of several frontier models reveals a critical dissociation: models are neither cost-aware when articulating their verbal confidence, nor strategically responsive when deciding whether to engage or abstain under high-penalty conditions.\nEven when extreme penalties render frequent abstention the mathematically optimal strategy, models almost never abstain, resulting in utility collapse.\nThis indicates that calibrated verbal confidence scores may not be sufficient to create trustworthy and interpretable AI systems,\nas current models lack the strategic agency to convert uncertainty signals into optimal and risk-sensitive decisions.\n\n\n\nAre LLM Decisions Faithful to Verbal Confidence?\n\n\n\n\nJiawei Wang   Yanfei Zhou   Siddartha Devic   Deqing Fu\n\nUniversity of Southern California\n\n{jwang535,yanfeizh,devic,deqingfu}@usc.edu\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: The RiskEval Framework. We evaluate strategic abstention by prompting models with varying error penalties (λ\\lambda) ranging from 0 to 100. Although models successfully verbalize uncertainty, they fail to translate this signal into decision-making. As illustrated, abstention rates on the HLE benchmark (Phan et al., 2025) remain largely invariant to increasing penalties.\n\n\nAccurate uncertainty quantification in LLMs is a promising approach to improving trust and transparency towards humans (Geng et al., 2024).\nOne alluring way to quantify uncertainty in LLMs is “verbal confidence” estimation, which simply asks the LLM to give its confidence after answering a question (Damani et al., 2025; Zhang et al., 2025a, b).\nPrior work suggests that verbal confidence can be reasonably calibrated across models and datasets, indicating that LLMs often possess meaningful awareness of their own uncertainty (Yoon et al., 2025; Xiong et al., 2024; Tian et al., 2023; Lin et al., 2022).\n\n\nNonetheless, although models may be able to produce accurate verbal confidence estimates, we still do not perfectly understand the mechanisms behind why these abilities may emerge.\nOne way to approach this question is to ask whether verbal confidence estimates impact an LLM’s actions or decisions in any way.\nIn other words, are the generated confidence estimates faithful to the actions of the model?\n\n\nWe examine this using an evaluation framework we term RiskEval, in which we allow a model to answer or abstain on questions. We then measure its abstention rate for different penalty values given within the input prompt (see Figure˜1).\nCombined with simultaneously eliciting the model’s verbal confidence scores, RiskEval provides a sandbox for understanding whether these verbal confidence estimates are indeed faithful and consistent with the actions that the model takes.\n\n\nOur Contributions.\n\nWe evaluate if LLMs utilize self-assessed verbal confidence to inform their decisions.\nSpecifically, we test their ability to navigate the trade-off between answering and abstaining under penalties defined in the prompt. Our analysis reveals three main findings:\n\n\n\n\n•\n\nInvariance to Risk. Across models and datasets, increasing penalties has a negligible effect on model behavior.\nNeither the self-evaluated confidence nor the decision to answer or abstain changes significantly across incorrect answer penalties ranging in [0.1,100][0.1,100].\nThis suggests that current training methods or model leaderboard practices do not result in risk-aware agents (Kalai et al., 2025).\n\n\n\n•\n\nUtility Degradation. Models fail to maximize expected utility. In high-penalty regimes, the rigid answering-heavy policy leads to large losses compared to optimal post-hoc abstention strategies utilizing model reported verbal confidence scores.\n\n\n\n•\n\nDecoupling of Confidence and Policy. Models often “know” their own uncertainty — in the sense that the verbal confidence estimates are useful / calibrated — yet mostly fail to convert this knowledge into a good abstention policy.\nDespite prompted instructions to avoid penalties, models maintain a high inclination to respond (Kirichenko et al., 2025).\n\n\n\n\n\nOverall, our investigations align with the need to evaluate models on strategic reliability rather than only overall accuracy (Kalai et al., 2025; Jia et al., 2024; Ross et al., 2024).\nOur findings also potentially call into question the validity and faithfulness of verbal confidence estimation for LLMs in decision-making contexts.\n\n\n\n\n\n2 Problem Setup\n\nFigure 2: Verbalized Confidence is Invariant to Risk. The flat trajectories show that internal uncertainty estimates remain stable despite increasing penalties, confirming that the failure to abstain is not due to signal degradation.\n\n\nFigure 3: Normalized Average Utility Collapses Under Risk. As penalties increase, normalized utility drops sharply into negative values on high-uncertainty benchmarks (HLE, GPQA). This confirms that models persist in answering incorrectly even when the cost of error far outweighs the potential reward.\n\n\nFigure 4: Policy Consistency Collapses Under High Penalties. We measure how often model decisions align with the optimal policy induced by their confidence. The sharp drop on HLE and GPQA shows that models fail to adjust their decision thresholds τ​(λ)\\tau(\\lambda) as penalties rise, persisting in answering when abstention is optimal.\n\n\nWe model answering and abstaining within a utility maximization framework.\nLet xx be a query, d∈{answer,abstain}d\\in\\{\\text{answer},\\text{abstain}\\} be the model ℳ\\mathcal{M}’s decision to answer or abstain, and yy be the model’s answer to xx if they decide to answer; y=∅y=\\varnothing otherwise.\nLet y∗y^{*} be the ground-truth label.\nThe model ℳ\\mathcal{M} produces a reported confidence estimate c:=cℳ​(x)∈[0,1]c:=c_{\\mathcal{M}}(x)\\in[0,1], which we treat as its estimate of Pℳ​(y=y∗|x)P_{\\mathcal{M}}(y=y^{*}|x), the model’s true underlying confidence.\n\n\nWe introduce a penalty parameter λ≥0\\lambda\\geq 0 for incorrect answers.\nThe utility function UU for a decision dd is defined as follows. When the model abstains U=0U=0. When the model decides to answer, U=+1U=+1 if the model is correct (i.e., y=y∗y=y^{*}), and U=−λU=-\\lambda if the model is incorrect.\n\n\nA rational agent that always maximizes expected utility only chooses to answer when the expected gain exceeds the utility of abstention, i.e., 0.\nThis yields the optimal decision threshold τ​(λ)=λ1+λ\\tau(\\lambda)=\\frac{\\lambda}{1+\\lambda}, such that 𝔼​[Uans]=c⋅1+(1−c)​(−λ)≥0=𝔼​[Uabs]\\mathbb{E}[U_{\\text{ans}}]=c\\cdot 1+(1-c)(-\\lambda)\\geq 0=\\mathbb{E}[U_{\\text{abs}}] when c≥τ​(λ)c\\geq\\tau(\\lambda).\n\n\nDefine the binary action π∈{0,1}\\pi\\in\\{0,1\\}, where π=1\\pi=1 denotes answer and π=0\\pi=0 denotes abstain. The Bayes-optimal policy under the model’s belief cc is\n\n\n\nπ∗​(c,λ)=𝕀​(c≥λ1+λ).\\pi^{*}(c,\\lambda)=\\mathbb{I}\\left(c\\geq\\frac{\\lambda}{1+\\lambda}\\right).\n\n(1)\n\n\nWe evaluate how well the model’s realized decisions align with π∗\\pi^{*} and how well confidence separates correct from incorrect answers.\n\n\nPolicy Consistency (PC)\n\nmeasures the frequency with which the model’s actual decision πℳ\\pi_{\\mathcal{M}} aligns with the optimal policy π∗\\pi^{*} given the model’s own confidence cc. Let xx be drawn from a dataset 𝒟\\mathcal{D}.\n\n\n\nPC​(ℳ,𝒟)=𝔼x∼𝒟​[𝕀​(πℳ​(c,λ)=π∗​(c,λ))],\\mathrm{PC}(\\mathcal{M},\\mathcal{D})=\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\mathbb{I}\\left(\\pi_{\\mathcal{M}}(c,\\lambda)=\\pi^{*}(c,\\lambda)\\right)\\right],\n\n\n\nwhere c:=cℳ​(x)c:=c_{\\mathcal{M}}(x) is the model verbal confidence.\nA higher PC\\mathrm{PC} implies a model better utilizes its own confidence to make decisions.\n\n\n\nExpected and Normalized Regret (ℛ\\mathcal{R} and ℛ¯\\overline{\\mathcal{R}})\n\nquantify the utility lost due to suboptimal decisions. Importantly, this expectation is taken under the model’s stated belief cic_{i}, not the true conditional correctness probability.\nFor a single query, regret is the difference between the maximum possible expected utility and the actual expected utility achieved: ℛ=max⁡(0,𝔼​[Uans])−𝔼​[Uπℳ]\\mathcal{R}=\\max(0,\\mathbb{E}[U_{\\text{ans}}])-\\mathbb{E}[U_{\\pi_{\\mathcal{M}}}]. As explicitly"
  },
  {
    "title": "Contrastive Learning with Narrative Twins for Modeling Story Salience",
    "url": "https://arxiv.org/abs/2601.07765v1",
    "source": "arxiv",
    "summary": "Understanding narratives requires identifying which events are most salient for a story's progression. We present a contrastive learning framework for modeling narrative salience that learns story embeddings from narrative twins: stories that share the same plot but differ in surface form. Our model is trained to distinguish a story from both its narrative twin and a distractor with similar surfac",
    "full_text": null
  },
  {
    "title": "Free-RBF-KAN: Kolmogorov-Arnold Networks with Adaptive Radial Basis Functions for Efficient Function Learning",
    "url": "https://arxiv.org/abs/2601.07760v1",
    "source": "arxiv",
    "summary": "Kolmogorov-Arnold Networks (KANs) have shown strong potential for efficiently approximating complex nonlinear functions. However, the original KAN formulation relies on B-spline basis functions, which incur substantial computational overhead due to De Boor's algorithm. To address this limitation, recent work has explored alternative basis functions such as radial basis functions (RBFs) that can im",
    "full_text": null
  },
  {
    "title": "Learning to bin: differentiable and Bayesian optimization for multi-dimensional discriminants in high-energy physics",
    "url": "https://arxiv.org/abs/2601.07756v1",
    "source": "arxiv",
    "summary": "Categorizing events using discriminant observables is central to many high-energy physics analyses. Yet, bin boundaries are often chosen by hand. A simple, popular choice is to apply argmax projections of multi-class scores and equidistant binning of one-dimensional discriminants. We propose a binning optimization for signal significance directly in multi-dimensional discriminants. We use a Gaussi",
    "full_text": null
  },
  {
    "title": "Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents",
    "url": "https://arxiv.org/abs/2601.07754v1",
    "source": "arxiv",
    "summary": "Numerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts. Recently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents relat",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Proposed Framework\n\n3.1 Step 1: Document Preprocessing\n\n3.2 Step 2: Knowledge Graph Construction\n\nFinancial Domain Schema\nPrompt Engineering\n\n\n3.3 Step 3: Filtering Triplets and Reasoning\n\n\n\n4 Experimental Setups\n\n4.1 Dataset\n4.2 Hyper-parameters to LLM (i.e., Llama) and MLP classifier\n4.3 Evaluation Setup and Baselines\n\n\n\n5 Result and Analysis\n\n5.1 Why KG Structure Helps in Numerical Reasoning?\n\n\n6 Conclusion\n7 Limitations\n\n\n\n\n\nStructure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents\n\n\n\nAryan Mishra1,\nAkash Anil1\n\n\n\nAbstract\nNumerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts.\nRecently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents related to finance often consist of long and complex financial contexts, LLMs appear well-suited for building high-quality automated financial question-answering systems. However, LLMs often face challenges in accurately processing the various numbers within financial reports.\n\nExtracting numerical data from unstructured text and semi-structured tables, and reliably performing accurate calculations, remains a significant bottleneck for numerical reasoning in most state-of-the-art LLMs. Recent studies have shown that structured data augmentations, such as Knowledge Graphs (KGs), have notably improved the predictions of LLMs along with logical explanations. Thus, it is an important requirement to consider inherent structured information in financial reports while using LLMs for various financial analytics.\nThis paper proposes a framework to incorporate structured information using KGs along with LLM predictions for numerical reasoning tasks. The KGs are extracted using a proposed schema inherently from the document under processing. We evaluated our proposed framework over the benchmark data FinQA, using an open-source LLM, namely Llama 3.1 8B Instruct. We observed that the proposed framework improved execution accuracy by approximately 12% relative to the vanilla LLM.\n\n\n\n1 Introduction\n\nNumerical Reasoning in financial data refers to the analysis and interpretation of quantitative information such as revenue figures, ratios, market indicators, or statistical trends present in the financial reports (Chen et al. 2021). Although LLMs demonstrate promising reasoning capabilities in various domains, they often show limited performance when subjected to financial reasoning (Qian et al. 2025; Liu et al. 2025). The limited capability of LLMs is mainly due to the quantitative characteristics of financial data incorporating multiple paragraphs and tables with numbers, which makes it harder to exploit the inherent context (Nie et al. 2024).\n\n\nA majority of the LLMs show promising reasoning capability yet they are often limited in some of the specialized domains (e.g., finance, healthcare) because they primarily learn from unstructured text data, relying on statistical co-occurrences rather than inherent relational characteristics (Tan et al. 2024). To address this limitation, recently some of the studies integrate structured information such as Knowledge Graphs (KGs) for enhancing the reasoning abilities of LLMs (Sun, Wang, and Li 2024). KGs provide semantic relationships and factual grounding and thus found to be helpful in improving reasoning performance in many domains (Wu and Tsioutsiouliklis 2024). However, to the best of our knowledge, none of these studies explicitly address numerical reasoning over financial data while capturing inherent structural aspects.\n\n\nTo bridge the gap in exploiting structural information in numerical reasoning for finance data, we propose a novel framework that uses inherent KG extracted using predefined schema and an open-source LLM. Figure  1 presents an end-to-end pipeline for the proposed framework. Our framework (i) preprocesses documents (including table linearization), (ii) constructs knowledge graphs using predefined schema, financial entities, and temporal relationships using few-shot prompting, (iii) performs lightweight retrieval combining semantic and structural features, and (iv) reasons using any LLM exploiting the structured input for predicting the output.\n\n\nWe evaluate the proposed framework111Code will be released publicly upon publication. using Llama 3.1 8B Instruct (Llama)(Grattafiori et al. 2024) on state-of-the-art financial reasoning benchmark namely FinQA (Chen et al. 2021). Further, we systematically compare the performance of the proposed framework to the open-source Llama model. It is evident that the proposed framework using KGs considerably enhanced the performance of vanilla Llama model.\n\n\nTo summarize, the main contributions of this paper are:\n\n\n1.\n\nStudy numerical reasoning in financial data using LLMs exploiting structural information in the form of Knowledge Graphs.\n\n\n\n2.\n\nBuild an end-to-end pipeline for numerical reasoning capable of preprocessing, extracting KGs, retrieval, and enhanced reasoning.\n\n\n\n3.\n\nSystematically compare the results against suitable baseline.\n\n\n\n\n\nRest of the paper is organized as follows. Section 2 presents related studies which is followed by a detailed discussion on the proposed framework in Section 3. In Section 4, we discuss the experimental setup and Section 5 discusses the performance of proposed framework compared with the baseline. Section 6 concludes the paper. Further, Section 7 presents a limitation overview.\n\n\n\n\nFigure 1: end-to-end pipeline for numerical reasoning in financial data with inherent knowledge graph and LLMs.\n\n\n\n\n2 Related Work\n\nThe FinQA benchmark (Chen et al. 2021) formalized numerical question answering over financial documents and shown various analytics. In a similar direction, authors of APOLLO in (Sun et al. 2024) introduced number-aware negative sampling and (Li et al. 2023) incorporates dynamic re-ranking in financial reasoning.\nRecently, (Qian et al. 2025) proposed a domain specific finetuning framework that enhances reasoning capability. Critically,(Qian et al. 2025) observe that even heavily fine-tuned models (FinR1 (Liu et al. 2025), Dianjin-R1 (Zhu et al. 2025)) exhibit performance\ndegradation on longer and complex documents, falling below base models.\nRecently, (Srivastava et al. 2024) categorized FinQA queries into\narithmetic operations such as SUM, DIFFERENCE, RATIO, CHANGE_RATIO, highlighting various challenges in multi-step domain-specific reasoning. These studies highlight major bottlenecks in numerical reasoning over financial text often considering longer and tabular structure.\n\n\nTo improve the reasoning using LLMs, Retrieval Augmented Generation (RAG) has garnered attention. RAG-based frameworks ground LLM outputs with external evidence (Lewis et al. 2020) to predict a more curated output and thus enhances the performance of LLMs. HybridRAG (Sarmah et al. 2024) combines vector\nand graph retrieval, though concatenating contexts increases cognitive load and in financial contexts, naive text chunking disrupts numerical links. Thus, our proposed framework attempts to mitigate this issue by fusing semantic and structural features in a lightweight\nretriever, optimizing retrieval without context concatenation overhead.\n\n\nIn the recent past, SubgraphRAG (Li, Miao, and Li 2025) demonstrates that lightweight MLPs with engineered graph features outperform complex graph neural networks for retrieval. In a similar direction, we aim to adapt curated domain-specific attributes such as temporal distances and entity types easily derived from FinQA dataset.\n\n\n\n\n3 Proposed Framework\n\nFigure 1 presents the proposed framework comprising of three main steps. After getting the input, the first step executes data preprocessing by table linearization and text concatenation with normalization for providing a uniform input text. We linearize tables following prior work on financial QA (Chen et al. 2021). Further, step two automatically extracts KG triplets from the input text provided from step one using a predefined schema proposed for financial reasoning. In step three, the framework filters the unwanted triplets and performs the reasoning task using preferred LLM model.\n\n\n\n3.1 Step 1: Document Preprocessing\n\nFinancial documents often contain hybrid data, i.e., narrative texts and semi-structured tables. To enable uniform processing, we linearize tables using template-based conversion shown below:\n\n\n\nOriginal Table   Linearized Text\nYear | Revenue\n\n2020 | $100M    \"For 2020, revenue is $100M.\n\n2021 | $120M     For 2021, revenue is $120M.\"\n\n\n\nText Linearization enables text-based processing though loses explicit\nstructural temporal relationships, and thus it might be the case that entity types are not differentiated which makes the numerical formats inconsistent. This inconsistency might be alleviated using the KG construction preserving temporal relationships.\n\n\n\n\n3.2 Step 2: Knowledge Graph Construction\n\nKnowledge Graphs are one of the key features in our framework. Thus, for constructing the KGs, we leverage the financial context understanding of LLMs and follow a standard schema for generating triplets. We aim to represent the information present in the text in unambiguous format and reduce the potential extraction errors. The inherent temporal and entity relational features are also preserved in this format. The structured triplets enable better processing and understanding of the multi-hop relations present across the document.\nThe schema for such KGs is focused on the numerical and temporal facts.\nThe proposed schema for KG is given below:\n\n\nFinancial Domain Schema\n\n\nSchema  = (subject, relation, object,\n            {financial_metric_entity_type,\n              company,period,valu"
  },
  {
    "title": "Riesz Representer Fitting under Bregman Divergence: A Unified Framework for Debiased Machine Learning",
    "url": "https://arxiv.org/abs/2601.07752v1",
    "source": "arxiv",
    "summary": "Estimating the Riesz representer is a central problem in debiased machine learning for causal and structural parameter estimation. Various methods for Riesz representer estimation have been proposed, including Riesz regression and covariate balancing. This study unifies these methods within a single framework. Our framework fits a Riesz representer model to the true Riesz representer under a Bregm",
    "full_text": null
  },
  {
    "title": "Improving Domain Generalization in Contrastive Learning using Adaptive Temperature Control",
    "url": "https://arxiv.org/abs/2601.07748v1",
    "source": "arxiv",
    "summary": "Self-supervised pre-training with contrastive learning is a powerful method for learning from sparsely labeled data. However, performance can drop considerably when there is a shift in the distribution of data from training to test time. We study this phenomenon in a setting in which the training data come from multiple domains, and the test data come from a domain not seen at training that is sub",
    "full_text": null
  },
  {
    "title": "Predefined-time One-Shot Cooperative Estimation, Guidance, and Control for Simultaneous Target Interception",
    "url": "https://arxiv.org/abs/2601.07744v1",
    "source": "arxiv",
    "summary": "This work develops a unified nonlinear estimation-guidance-control framework for cooperative simultaneous interception of a stationary target under a heterogeneous sensing topology, where sensing capabilities are non-uniform across interceptors. Specifically, only a subset of agents is instrumented with onboard seekers (informed/seeker-equipped agents), whereas the rest of them (seeker-less agents",
    "full_text": "\n\n\n\n1 Introduction\n2 Background and Problem Formulation\n3 Cooperative One-Shot Estimation-Guidance-Control Framework\n\n4 Performance Evaluations\n\n4.1 Cooperative Salvo using Spatially Offset Interceptors\n4.2 Cooperative Salvo using Co-located Interceptors\n4.3 Cooperative Salvo with Large Number of Interceptors\n4.4 Autopilot Performance with Test Inputs\n4.5 Autopilot Performance with Actual Commanded Inputs\n\n\n5 Concluding Remarks\n\n\n\n\n\nPredefined-time One-Shot Cooperative Estimation, Guidance, and Control for Simultaneous Target Interception\n\n\nLohitvel Gopikannan\n\n  \nShashi Ranjan Kumar\nL. Gopikannan and S. R. Kumar are with the Intelligent Systems and Control (ISaC) Lab, Department of Aerospace Engineering, Indian Institute of Technology Bombay, Powai, MH, 4000076, India.\ne-mails: 24m0023@iitb.ac.in, srk@aero.iitb.ac.in.\n  \nAbhinav Sinha\nA. Sinha is with the Guidance, Autonomy, Learning, and Control for Intelligent Systems (GALACxIS) Lab, Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, OH, 45221, USA.\ne-mail: abhinav.sinha@uc.edu.\n\n\nAbstract\nThis work develops a unified nonlinear estimation-guidance-control framework for the cooperative simultaneous interception of a stationary target under a heterogeneous sensing topology, where sensing capabilities are non-uniform across interceptors. Specifically, only a subset of agents is instrumented with onboard seekers (informed/seeker-equipped agents), whereas the rest of them (seeker-less agents) acquire the information about the target indirectly via the informed agents and execute a distributed cooperative guidance for simultaneous target interception. To address the resulting partial observability, a predefined-time distributed observer is leveraged, guaranteeing convergence of the target state estimates for seeker-less agents through information exchange with seeker-equipped neighbors over a directed communication graph. Thereafter, an improved time-to-go estimate accounting for wide launch envelopes is utilized to design the distributed cooperative guidance commands. This estimate is coupled with a predefined-time consensus protocol, ensuring consensus in the agents’ time-to-go values. The temporal upper bounds within which both observer error and time-to-go consensus error converge to zero can be prescribed as design parameters. Furthermore, the cooperative guidance commands are realized by means of an autopilot, wherein the interceptor is steered by canard actuation. The corresponding fin deflection commands are generated using a predefined-time convergent sliding mode control law. This enables the autopilot to precisely track the commanded lateral acceleration within a design-specified time, while maintaining non-singularity of the overall design. Theoretical guarantees are supported by numerical simulations across diverse engagement geometries, verifying the estimation accuracy, the cooperative interception performance, and the autopilot response using the proposed scheme.\nKeywords— Cooperative estimation-guidance-control, predefined-time convergence, impact time, salvo guidance.\n\n\n\n1 Introduction\n\nThe primary objective of modern guidance systems is to achieve precise target interception while satisfying stringent terminal requirements such as prescribed impact time [1, 2, 3], and impact angle [4, 5, 6]. Although single-interceptor engagements have been extensively investigated, their practical effectiveness is often compromised by inherent limitations, including restricted seeker field-of-view, actuator saturation, and system lags. At the same time, advanced defense architectures employ layered protection and countermeasures such as close-in weapon systems, making penetration by a lone attacker highly improbable. These challenges highlight the need for coordinated multi-interceptor engagements, commonly referred to as salvo guidance, to address the limitations of traditional one-to-one interception.\n\n\nContemporary salvo strategies are broadly categorized into non-cooperative and cooperative approaches. Non-cooperative schemes rely on preassigned impact times without information exchange, making them susceptible to errors during flight and reducing the likelihood of simultaneous interception. Cooperative schemes, in contrast, exploit real-time information sharing and coordination among interceptors, enabling superior synchronization and robustness against uncertainties. For instance, the work in [7] introduced a distributed cooperative guidance law based on biased proportional navigation (PN). Building on this, the authors of [8] reformulated impact-time control as a range-tracking problem. The work in [9] aimed to minimize time-to-go variance by varying navigation gains, but relied on all-to-all communication. This approach was extended in [10] under acceleration constraints, and the authors of [11] incorporated finite-time consensus with terminal constraints, all considering undirected networks.\n\n\nThe work in [12] proposed finite-time distributed laws requiring only local time-to-go estimates, reducing communication, whereas in [13], a receding-horizon cooperative guidance law was proposed where interceptors exchanged only neighbor data and solved local finite-horizon optimizations. The work in [14, 15] avoided explicit time-to-go estimation using a two-stage scheme relying on decentralized consensus on range/heading followed by PN guidance. In [16], coordination with local guidance in both centralized and distributed forms was integrated. Together, the strategies [14, 15, 16, 12, 13] ensured consensus over undirected networks, though only asymptotically. This could lead to performance degradation in short-duration engagements because strict control over it may be lacking.\n\n\nLeader–follower design combined with PN-based time-to-go estimation and super-twisting sliding mode control was proposed in [17] to guarantee finite-time convergence even under large heading errors for simultaneous interception of stationary targets. However, it should be taken into consideration that failure of the leader will compromise the mission. The authors in [18] developed time-to-go–dependent guidance laws based on the suboptimal finite-time state-dependent Riccati equation (FT-SDRE) for simultaneous target interception; however, the inherent sub-optimality of the FT-SDRE may constrain performance in highly dynamic engagement scenarios. In [19], a cooperative guidance was developed that explicitly addressed lateral acceleration constraints within a nonlinear framework, ensuring stability even under large heading errors, and extended applicability to both directed and undirected networks. However, the consensus over the network is reached only asymptotically. Fixed-time cooperative guidance laws, for example, [20, 21, 22, 23, 24], may address the shortcomings of asymptotic and finite-time schemes used in aforementioned works by guaranteeing consensus independent of initial engagement conditions, thereby ensuring reliable guidance precision. Nevertheless, these methods often rely on conservatively large controller gains, and their design parameters must be re-tuned whenever a different consensus time is desired, which can limit practical flexibility. To overcome this, leaderless cooperative guidance based on predefined-time consensus was proposed by [25] for interceptor swarms, guaranteeing arbitrarily set time-to-go consensus and simultaneous interception under stationary or maneuvering targets with undirected dynamic topologies. The work in [26] proposed a leader–follower cooperative salvo guidance law with arbitrary-time consensus ensuring predefined-time agreement on time-to-go and simultaneous interception under nonlinear kinematics and autopilot lag.\n\n\nMost of the aforementioned guidance strategies have been developed based solely on the kinematics of interceptor–target engagement, assuming that the interceptor can instantaneously realize any commanded acceleration. This assumption effectively neglects the dynamic characteristics of the interceptor’s control loop, which can lead to significant performance degradation under non-ideal or rapidly changing flight conditions, particularly during the terminal phase of engagement. To address this limitation, it is essential to account for the interceptor’s dynamics while formulating practical guidance strategies to ensure that the proposed guidance law remains effective and implementable in real-world scenarios. In this regard, authors in [27] proposed an integrated interceptor guidance–autopilot loop employing a sliding mode control framework to achieve seamless coordination between guidance and flight control for a canard-controlled interceptor. The effectiveness of the integrated design was demonstrated through multiple endgame interception scenarios. Furthermore, the work in [28] utilized this decoupled guidance–autopilot design, where canard and tail surfaces were coordinated to track the commanded lateral acceleration using predefined-time convergent sliding mode control. This ensured precise target interception within a specified time, as demonstrated through simulations. Such a framework gives an accurate assessment of real-time performance, including actuator limits, control–guidance coupling, and endgame response characteristics.\n\n\nTo the authors’ knowledge, cooperative salvo guidance strategies reported in contemporary literature (for example, the aforementioned works and their cited references) generally assume that all interceptors have access to complete real-time target position information to synchronize their interception times. In practice, however, equipping every interceptor with an onboard seeker is often infeasible in large-scale engagements due to constraints on cost, payload capacity, and energy resources. Moreover, maintaining full bidirectional communication is often impractical due to limited bandwidth, computational load, and onboard power. Therefore,"
  },
  {
    "title": "PFT: Phonon Fine-tuning for Machine Learned Interatomic Potentials",
    "url": "https://arxiv.org/abs/2601.07742v1",
    "source": "arxiv",
    "summary": "Many materials properties depend on higher-order derivatives of the potential energy surface, yet machine learned interatomic potentials (MLIPs) trained with standard a standard loss on energy, force, and stress errors can exhibit error in curvature, degrading the prediction of vibrational properties. We introduce phonon fine-tuning (PFT), which directly supervises second-order force constants of ",
    "full_text": null
  },
  {
    "title": "Evaluating the encoding competence of visual language models using uncommon actions",
    "url": "https://arxiv.org/abs/2601.07737v1",
    "source": "arxiv",
    "summary": "We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image",
    "full_text": "\n\n\n\n\nChapter 1: Introduction\n\nChapter 1: Introduction\n\nChapter 2: Background\n\n2.1 Relation Work\n2.2 Image generation tools\n\n2.3 Evaluation models\n\n2.3.1 Vison and Langugae models\n2.3.2 Contrasting Learning Models\n\n\n2.4 Fine-tuning method\n2.5 Innovations of this study\n\n\n\nChapter 3: Design and Implementation\n\n\n3.1 Verb library filtering\n\n3.1.1 Introduction of VerbNet\n3.1.2 Pipeline of constructing Verb library\n3.1.3 Classification of Verb library\n\n\n3.2 Uncommon-Text Generation\n\n3.3 Uncommon Image-Text Construction\n\n3.3.1 Pipeline of Image generation\n3.3.2 Configuration of image generation\n\n\n3.4 VQA dataset Construction\n\n\n\nChapter 4: Results and Discussion\n\n4.1 Model selection\n4.2 Result analysis\n4.3 Analysis of comparative learning models\n4.4 Discussion on performance between human and models\n4.5 Evaluating and Training configuration\n\n4.6 Role of UAIT in promoting semantic understanding\n\n4.6.1 Stripping away contextual statistical bias\n4.6.2 Higher explainability and diagnosability\n\n\n\n4.7 Implications for further research on multimodal understanding\n\n4.7.1 Improving the capabilities on modeling semantic role\n4.7.2 Introducing common sense anti-interference mechanism\n4.7.3 Data-driven along with reasoning-driven\n\n\n\n\n\nChapter 5: Conclusion and Further Work\n\n5.1 Conclusion\n5.2 Reflection\n5.3 Further work\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUndergraduate Project Report\n\n\nEvaluating the encoding competence of visual language models using uncommon actions\n\n\n\n\n\nAuthor:\nChen Ling\n\n\nAffiliation:\nBeijing University of Post and Telecommunications\n\n\nEmail:\n20212127000@bupt.edu.cn\n\n\nSupervisor:\nNai Ding*\n\n\nAffiliation:\nZhejiang University\n\n\nEmail:\nding_nai@zju.edu.cn\n\n\n\n\n\nTable of Contents\n\n\n\nAbstract\n\n\nWe proposes UAIT (Uncommon-sense Action Image-Text ) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility.\n\n\nTo build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model’s competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality.\n\n\nFurther experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. However, there is still a significant gap between the model and humans, reflecting the fundamental limitations of current multimodal understanding. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.\n\n\nKeywords\n\n\nVision-Language Models (VLMs), Uncommon-sense Reasoning, Action Recognition\n\n\n\nChapter 1: Introduction\n\nIn recent years, vision and language models (VLMs) have made significant breakthroughs in the field of artificial intelligence, especially in multimodal understanding tasks such as visual question answering (VQA), image description, and visual commonsense reasoning. These models aim to bridge the semantic gap between visual and textual information by jointly encoding features of the two. Although state-of-the-art models have shown impressive performance in a wide range of applications, their true understanding of fine-grained visual semantics, especially those involving action reasoning and counter-common sense reasoning, remains under-explored.\n\n\nWhen people see an uncommon action image, they can quickly identify the unusual, counter-logical, and incongruous parts of it. Take the example in Figure 1: a rabbit is dragging a tiger. Although the components in the picture are all very normal objects in real life, the combination relationship between them appears unusual. Humans can easily recognize the unusual in pictures, but this is preceded by a multi-step reasoning process and requires common sense problems that rely on daily experience, physical knowledge, or various social and cultural norms.\n\n\nIn this work, we present UAIT(Uncommon-sense Action Image-Text Dataset), which aims to systematically evaluate the encoding competence of VLMs when facing visual scenes that exhibit unusual or counter-intuitive actions. Specifically, we want to determine whether the current state-of-the-art models can correctly interpret the semantics of images in situations where the actions violate common sense or challenge existing assumptions (such as ”zebra attacks lion”). Unlike traditional tasks that focus on evaluating the recognition of common patterns and concepts, our focus shifts to edge cases that require true visual and textual understanding rather than relying solely on statistical biases or memory associations.\n\n\nFigure 1: This is an example from UAIT, the correct description of the action scene is “a rabbit is dragging a tiger”. Below are the reasons why the image is beyond the common-sense understanding.\n\n\nTo this end, we propose and implement a novel pipeline for building the UAIT - a curated benchmark consisting of synthetically generated visual scenes and their corresponding textual descriptions. The scenes are selected from semantically sensitive verbs in VerbNet such as those whose meaning depends heavily on the agent-patient relationship, and are carefully designed to simulate plausible yet unusual events. The project synthesizes richly annotated data using large language models and text-to-image generation techniques such as Stable Diffusion. We then use this dataset to evaluate the performance of various VLMs in distinguishing semantically opposing interpretations.\n\n\nThe technical core of the work includes:\n\n\n•\n\nVerb selection principles based on syntactic reversibility and visual clarity.\n\n\n\n•\n\nMethods for generating syntactically correct but semantically misleading sentences using prompt engineering and few-shot learning techniques.\n\n\n\n•\n\nText-to-image generation for unusual visual events guided by detailed scene descriptions.\n\n\n\n•\n\nEvaluation through accuracy comparison and reasoning analysis.\n\n\n\n\n\nOur work also provides an evaluation framework to measure the deep understanding of visual actions by current VLMs, rather than just superficial pattern matching.The experimental data on UAIT shows that the performance of state-of-the-art visual language models lags behind humans on VQA tasks. Our findings show that SOTA models have difficulty distinguishing syntactic correctness from semantic accuracy in action scenes, which highlights a key flaw in current multimodal learning systems.\n\n\nIn summary, we built a novel action-oriented image-text dataset, designed and verified a complete generation pipeline, and started benchmarking multiple VLMs. Our results show that even state-of-the-art visual-language models perform poorly on the challenging UAIT benchmark. This result aims to advance our understanding of the limitations of visual-language models and motivate more robust approaches to semantic encoding of visual actions.\n\n\n\n\nChapter 2: Background\n\nWith the development of large-scale pre-training technology, vision-and-language models (VLMs) have made significant progress. Models such as GPT-4(openai2024gpt4technicalreport), LLaVA(NEURIPS2023_6dcf277e), and Qwen2-VL(wang2024qwen2vlenhancingvisionlanguagemodels) are representatives of this trend, and they have demonstrated strong capabilities in multimodal tasks. However, despite the outstanding performance of these models, current vision-language models often rely heavily on statistical regularities and contextual priors. For example, in real-world datasets, some visual-text pairs are more common than their abnormal counterparts. This frequency bias can cause the model to make overconfident but wrong judgments when encountering visual inputs that violate common sense.\n\n\n\n2.1 Relation Work\n\nCurrently popular datasets such as COCO-A(ronchi2015describingcommonhumanvisual) (Common Objects in Context - Actions) attempt to enrich image data through action annotations, but such datasets are still dominated by frequently occurring action-object combinations, and only provide limited examples of inverted or ambiguous agents and recipients, making it difficult to effectively evaluate the true reasoning ability of the model.\n\n\nWinoground(Thrush_2022_CVPR) is a benchmark designed to evaluate the cross-modal composition ability and fine-grained semantic understanding of visual language models. It constructs semantically very similar but slightly different image-text pairs to examine whether the model can accurately match images with their descriptions. However, Winoground mainly focuses on common scenes in life and does not contain counterintuitive examples, which limits its ability to test the deep understanding of the model.\n\n\n\n\n2.2 Image generation tools\n\nTo address the lack of diversity and unusual action scenes in current datasets, we introduced Stable Diffusion, an advanced text-to-image generation model based on a diffusion process.\n\n\nStable Diffusion(rombach2022highresolutionimagesynthesislatent) is a generative artificial intelligence technology based on a diffusion model. It is mainly used to convert text descriptions into high-quality images. Its core idea is to gradually conver"
  },
  {
    "title": "Backward Reconstruction of the Chafee--Infante Equation via Physics-Informed WGAN-GP",
    "url": "https://arxiv.org/abs/2601.07733v1",
    "source": "arxiv",
    "summary": "We present a physics-informed Wasserstein GAN with gradient penalty (WGAN-GP) for solving the inverse Chafee--Infante problem on two-dimensional domains with Dirichlet boundary conditions. The objective is to reconstruct an unknown initial condition from a near-equilibrium state obtained after 100 explicit forward Euler iterations of the reaction-diffusion equation \\[ u_t - γΔu + κ\\left(u^3 - u\\ri",
    "full_text": null
  },
  {
    "title": "Hiking in the Wild: A Scalable Perceptive Parkour Framework for Humanoids",
    "url": "https://arxiv.org/abs/2601.07718v1",
    "source": "arxiv",
    "summary": "Achieving robust humanoid hiking in complex, unstructured environments requires transitioning from reactive proprioception to proactive perception. However, integrating exteroception remains a significant challenge: mapping-based methods suffer from state estimation drift; for instance, LiDAR-based methods do not handle torso jitter well. Existing end-to-end approaches often struggle with scalabil",
    "full_text": null
  },
  {
    "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
    "url": "https://arxiv.org/abs/2601.07711v1",
    "source": "arxiv",
    "summary": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability o",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Retrieval Augmented Generation\n\nRAG definition\nNaïve RAG\nEnhanced RAG\nAgentic RAG\n\n\n\n3 Related work\n\nRAG\nAgents design and implementation\nThe Need for Empirical Comparison\n\n\n4 Data\n\n5 Evaluation\n\n\n5.1 User intent handling\n\nDefinition\nEnhanced Implementation\nAgentic Implementation\nExperimental setting\nEvaluation metric\n\n\n\n5.2 Query rewriting\n\nDefinition\nEnhanced Implementation\nAgentic Implementation\nExperimental Settings\nEvaluation metric\n\n\n\n5.3 Document list refinement\n\nDefinition\nEnhanced Implementation\nAgentic Implementation\nExperimental Settings\nEvaluation metric\n\n\n\n5.4 Underlying LLM\n\nDefinition\nEnhanced and Agentic Implementation\nExperimental Settings\nEvaluation metric\n\n\n\n\n\n6 Results\n\nUser intent handling\nQuery rewriting\nDocument list refinement\nUnderlying LLM\n\n6.1 Cost and Time\n\nFixed time and cost for Enhanced and Agentic settings\nRuntime costs and number of tokens\n\n\n\n\n7 Conclusion\n\nA Example Appendix\n\nA.1 Prompt for Agentic RAG\n\nA.2 Routing system details\n\nThreshold selection\n\n\nA.3 Underlying LLM evaluation metrics\n\n\n\n\n\n\n\nIs Agentic RAG worth it? An experimental comparison of RAG approaches\n\n\n\nPietro Ferrazzi1,2,\nMilica Cvjeticanin3,\nAlessio Piraccini 4,\n Davide Giannuzzi5,\n\n1Fondazione Bruno Kessler, Trento, Italy ,\n2Univerità di Padova, Padova, Italy, \n3Cargill Geneve, Switzerland,\n4 Alkemy, Milan, Italy,\n5Affiliation 5\n\n\nCorrespondence: pferrazzi@fbk.eu\n\n\n\nAbstract\nRetrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query–document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow.\nMore recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process—deciding which actions to perform, when to perform them, and whether to iterate—thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions.\nIn this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.\n\n\n\nIs Agentic RAG worth it? An experimental comparison of RAG approaches\n\n\n\n\n\nPietro Ferrazzi1,2,\nMilica Cvjeticanin3,\nAlessio Piraccini 4,\n Davide Giannuzzi5,\n\n1Fondazione Bruno Kessler, Trento, Italy ,\n2Univerità di Padova, Padova, Italy,\n\n3Cargill Geneve, Switzerland,\n4 Alkemy, Milan, Italy,\n5Affiliation 5\n\n\nCorrespondence: pferrazzi@fbk.eu\n\n\n\n\n\nFigure 1: Left — Enhanced RAG. The system is composed by a sequence of modules, each responsible for improving a specific stage of the RAG pipeline. A router determines whether a query should trigger retrieval; a rewriter reformulates the query; a retriever selects candidate chunks from the knowledge base; and a reranker orders the retrieved context before passing it to the generator. The workflow is fixed: information flows through predefined blocks intended to mitigate known weaknesses of naïve RAG systems (defined by the simple composition of the retriever and generator blocks).\nRight — Agentic RAG. The LLM acts as an agent that orchestrates the entire process. At each step, it can choose whether to call a RAG tool or proceed directly to answer generation. Retrieval and context refinement can be repeated, and the agent autonomously selects and sequences operations based on the evolving state of the task. This yields an iterative pipeline without predefined intermediate modules.\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable results in natural language understanding and generation tasks. However, their capabilities remain limited by their fixed training data, raising the need for integrating private enterprise knowledge into LLM-based systems.\nRetrieval-Augmented Generation (RAG) has emerged as a practical solution: instead of relying solely on parametric knowledge, the model retrieves relevant documents from an external knowledge base and conditions its generation on this evidence (Lewis et al., 2020). This method has garnered significant attention in both the research community (Wang et al., 2024; Fan et al., 2024; Wang et al., 2024) and industry, with cloud providers offering their own RAG solutions (IBM, 2025; Amazon Web Services, 2025; Microsoft Azure, 2025).\nSince this first initial definition RAG workflows have been expanded to what we define as Enhanced RAG (Figure 1, left).\nSuch systems add to the retrieval and generation blocks components that perform further refinement, such as query rewriting, document re-ranking, and query routing.\nRecently, LLMs’ increasing self-reflective capabilities have enabled a shift towards Agentic RAG (Figure 1, right), where the LLM acts as an orchestrator, deciding which actions to perform, being able to utilize different tools for different purposes. These systems are no longer fixed pipelines, but rather iterative loops with no predefined order, where the model is in charge of all the decisions.\nAlthough initial work on identifying theoretical distinctions between Enhanced and Agentic RAG systems has been proposed (Neha and Bhati, 2025), it remains unclear what the performance differences are between the two systems.\nTo this end, we conducted an experiment-driven comparison of the two in different domains. \nOur first contribution consists of the evaluation of the performance of the two systems in four dimensions selected from the literature (Table 1).\nWe identified weaknesses of base RAG systems, analyzed how they have been addressed in previous work, and proposed an experimental setting to compare Enhanced and Agentic implementations. The selected dimensions are:\n\n\n1.\n\nUser intent handling, measuring how systems discriminate between queries that do or do not require external knowledge;\n\n\n\n2.\n\nQuery-documents alignment, measuring how systems can align queries to the format of knowledge base documents;\n\n\n\n3.\n\nRetrieved documents adjustment, measuring the ability of systems to further tune the selection of documents after retrieval;\n\n\n\n4.\n\nImpact of LLM quality, measuring how robust systems are to changes in the capability of the underlying models.\n\n\n\nOur second contribution consists of a detailed analysis of costs and computational time required by the two systems under several scenarios. Finally, we propose a short summary of our findings, hoping to help researchers and practitioners select the most appropriate system for their needs.\n\n\n\n\n\n\n\nNaïve RAG shortcoming\n\n\n\n\nWhat we evaluate\n\n\nImplementations\n\n\n\n\n\n\n\n\nEnhanced\n\n\n\n\nAgentic\n\n\n\n\n\n\nRetrieval is performed even for queries that do not require it\n\n\n\n\nAccuracy of RAG usage for in-scope and out-of-scope queries\n\n\n\n\nSemantic routing system\n\n\n\n\nAgent decides whether to do retrieval or not\n\n\n\n\n\n\nQueries and documents in the KB differ in format or semantics, causing weak retrieval\n\n\n\n\nImpact of query rewriting techniques\n\n\n\n\nHyde-based query rewriting\n\n\n\n\nAgent rewrites query as it wishes\n\n\n\n\n\n\nNoisy or suboptimal retrieval\n\n\n\n\nImpact of retrieved document list refinement techniques\n\n\n\n\nEncoder-based re-ranker\n\n\n\n\nAgent can redo retrieval multiple times\n\n\n\n\n\n\nThe underlying LLM is too weak / takes too much time / is too expensive\n\n\n\n\nImpact of selecting more / less powerful models\n\n\n\n\nTest different LLMs\n\n\n\n\nTest different LLMs\n\n\n\n\n\nTable 1: \nSummary of the evaluation dimensions we select. For each shortcoming in Naïve RAG, we define an evaluation dimension (\"What we evaluate\") and an implementation to test how Enhanced and Agentic RAG overcome such a limitation. \n\n\n\n\n2 Retrieval Augmented Generation\n\nThe term Retrieval-Augmented Generation (RAG) is often inconsistently used across the literature.\nHere we introduce the definitions used in this work.\n\n\nRAG definition\n\nWe define a RAG system as the integration of\ni) a knowledge base (KB) of textual chunks;\nii) a retrieval method that extracts relevant chunks from the knowledge base given a query; and\niii) a generator that produces an answer conditioned on both the query and the retrieved chunks.\n\n\n\nNaïve RAG\n\nWe refer to Naïve Rag (Figure 1) as the simplest instantiation of the RAG paradigm, where the generator is a Large Language Model and the workflow comprises four sequential steps: when\ni) a query is received,\nii) the retriever deterministically selects a fixed number of chunks from the KB (retrieval);\niii) the retrieved chunks are pre-pended to the query;\niv) and passed to the generator, which produces an answer (generation).\n\n\n\nEnhanced RAG\n\nFollowing the taxonomy outlined in Huang and Huang (2024), we define Enhanced RAG (Figure 1, left) as any Naïve RAG pipeline augmented with additional steps designed to improve its performance.\nThese enhancements can occur at different stages of the workflow and implemented in various ways, which we discuss in detail in the related sections.\n\n\n\nAgentic RAG\n\nWe define Agentic RAG (Figure 1, right) as a system in which the LLM assumes control over the workflow, being able to dynamically decide to perform actions. This agentic control allows the system to adapt the retrieval–generation loop to the specific context and task at hand. Unlike Enhanced RAG, no extra components are introduced with respect to the basic knowledge base-retriever-generator setting.\n\n\n\n\n\n3 Related"
  },
  {
    "title": "Deep Whole-body Parkour",
    "url": "https://arxiv.org/abs/2601.07701v1",
    "source": "arxiv",
    "summary": "Current approaches to humanoid control generally fall into two paradigms: perceptive locomotion, which handles terrain well but is limited to pedal gaits, and general motion tracking, which reproduces complex skills but ignores environmental capabilities. This work unites these paradigms to achieve perceptive general motion control. We present a framework where exteroceptive sensing is integrated ",
    "full_text": null
  },
  {
    "title": "Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition",
    "url": "https://arxiv.org/abs/2601.07700v1",
    "source": "arxiv",
    "summary": "It has been demonstrated in various contexts that monotonicity leads to better explainability in neural networks. However, not every function can be well approximated by a monotone neural network. We demonstrate that monotonicity can still be used in two ways to boost explainability. First, we use an adaptation of the decomposition of a trained ReLU network into two monotone and convex parts, ther",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.07700v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2601.07700v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 12 Jan 2026]\n    Title:Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition\n    Authors:Jakob Paul Zimmermann, Georg Loho            View a PDF of the paper titled Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition, by Jakob Paul Zimmermann and 1 other authors\n    View PDF\n\n\n\n    \n            Abstract:It has been demonstrated in various contexts that monotonicity leads to better explainability in neural networks. However, not every function can be well approximated by a monotone neural network. We demonstrate that monotonicity can still be used in two ways to boost explainability. First, we use an adaptation of the decomposition of a trained ReLU network into two monotone and convex parts, thereby overcoming numerical obstacles from an inherent blowup of the weights in this procedure. Our proposed saliency methods -- SplitCAM and SplitLRP -- improve on state of the art results on both VGG16 and Resnet18 networks on ImageNet-S across all Quantus saliency metric categories. Second, we exhibit that training a model as the difference between two monotone neural networks results in a system with strong self-explainability properties.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.07700 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.07700v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.07700\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Jakob Paul Zimmermann [view email]          [v1]\n        Mon, 12 Jan 2026 16:31:51 UTC (3,926 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition, by Jakob Paul Zimmermann and 1 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscribe\n                \n              \n            \n          \n        \n        \n        \n        \n          \n            \n          "
  },
  {
    "title": "Emotional Support Evaluation Framework via Controllable and Diverse Seeker Simulator",
    "url": "https://arxiv.org/abs/2601.07698v1",
    "source": "arxiv",
    "summary": "As emotional support chatbots have recently gained significant traction across both research and industry, a common evaluation strategy has emerged: use help-seeker simulators to interact with supporter chatbots. However, current simulators suffer from two critical limitations: (1) they fail to capture the behavioral diversity of real-world seekers, often portraying them as overly cooperative, and",
    "full_text": null
  },
  {
    "title": "Exploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task",
    "url": "https://arxiv.org/abs/2601.07696v1",
    "source": "arxiv",
    "summary": "Recent advancements in Large Language Models (LLMs) are increasingly focused on \"reasoning\" ability, a concept with many overlapping definitions in the LLM discourse. We take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\nReasoning in LLMs\nTool-use\nExisting Datasets\nMeta- and Object-Level Reasoning\n\n\n\n3 Our Dataset\n\n3.1 Question Generation\n3.2 Sourcing Data\n3.3 Generation of Solutions and Essential Actions\n3.4 Unanswerable Questions\n\n\n\n4 Experimental Setup\n\nTool Creation\nTool Calling Loop\n\n\n\n5 Results\n\nPrecision and Recall\nnn-shot Prompting\nError Messages\nObject-level Reasoning\n\n\n6 Conclusion and Further Work\n7 Ethics statement\n8 Reproducibility statement\n\nA Experimental Loop Prompts\n\nA.1 Base System\n\nA.2 Base Tool-use\n\nA.2.1 All Tools\nA.2.2 Data Tools-only\n\n\n\n\nB Indicator Paraphrasing Prompts\nC Question Templates\nD Full Toolset\n\n\n\n\n\nExploring the Meta-level Reasoning of Large Language Models via a Tool-based Multi-hop Tabular Question Answering Task\n\n\nNick Ferguson, Alan Bundy &amp; Kwabena Nuamah\nSchool of Informatics\nUniversity of Edinburgh\n\n{nick.ferguson,a.bundy,k.nuamah}@ed.ac.uk\n\n\n\n(25 September 2025)\n\nAbstract\nRecent advancements in Large Language Models (LLMs) are increasingly focussed on ”reasoning” ability, a concept with many overlapping definitions in the LLM discourse.\nWe take a more structured approach, distinguishing meta-level reasoning (denoting the process of reasoning about intermediate steps required to solve a task) from object-level reasoning (which concerns the low-level execution of the aforementioned steps.)\nWe design a novel question answering task, which is based around the values of geopolitical indicators for various countries over various years.\nQuestions require breaking down into intermediate steps, retrieval of data, and mathematical operations over that data. The meta-level reasoning ability of LLMs is analysed by examining the selection of appropriate tools for answering questions.\nTo bring greater depth to the analysis of LLMs beyond final answer accuracy, our task contains ‘essential actions’ against which we can compare the tool call output of LLMs to infer the strength of reasoning ability.\nWe find that LLMs demonstrate good meta-level reasoning on our task, yet are flawed in some aspects of task understanding.\nWe find that nn-shot prompting has little effect on accuracy; error messages encountered do not often deteriorate performance; and provide additional evidence for the poor numeracy of LLMs.\nFinally, we discuss the generalisation and limitation of our findings to other task domains.\n\n\n\n1 Introduction\n\nAugmentation of Large Language Models (LLMs, [4, 30, 12]) beyond text generation is now common, with reasoning ability across a range of tasks now a central feature [14, 1].\nReasoning is frequently benchmarked on question answering (QA) tasks which require decomposing a problem into smaller steps, which may involve mathematical reasoning [11, 19], commonsense reasoning over natural language facts [32, 17], or extracting tabular data [36].\nAdditionally, LLMs are no longer focussed only on the generation of natural language, but computer code and other structured outputs like function calls, as embodied in the tool-use paradigm [23].\n\n\nIn this study, we discuss the meta- and object-level reasoning [6] of LLMs using a multi-hop, data retrieval and arithmetic-based question answering task.\nMeta- and object-level reasoning are two modes originating with automated theorem proving and proof planning domain, yet have clear parallels with the reasoning discourse around LLMs.\nMeta-level reasoning encompasses the high-level planning task, the creation of a course of action for reaching a solution, and reasoning about the process of answering a question.\nWhile these tasks are commonly incorporated into a very general notion of ‘planning’ in the LLM community, when we focus on meta-level reasoning, we focus on one aspect, namely the extent to which subcomponents of a system are correctly employed to achieve a specific goal.\nObject-level reasoning encompasses the execution of the steps created by the meta-level process.\nThis terminology is discussed in more detail in section 2, and serves as a framework against which we can evaluate and discuss the reasoning ability of different aspects of LLMs in a more structured manner beyond simple final answer accuracy.\n\n\nTo investigate this reasoning ability in line with the current focus on application to multi-hop QA tasks requiring numeracy and planning, we design an evaluation environment comprising questions requiring meta-level reasoning (decomposition into intermediate steps) and object-level reasoning (retrieval of data from tabular sources and arithmetic operations)111Our code is available at https://anonymous.4open.science/r/exploring-meta-level-reasoning-iclr-2026/.\nOur dataset concerns the values of World Bank indicator data for various regions, countries, and years, as illustrated in figure 1.\nHowever, the wider problem-solving task which we embody can generalise to other contexts and domains requiring high-level decomposition of a task into intermediate steps and low level execution of those steps, which may encompass data retrieval, symbolic and arithmetic operations, or informal natural language fact retrieval.\nWe create ‘essential actions’ for each example in our dataset, which are a set of tool calls required to guarantee a correct answer, and against which we compare model-generated tool calls to infer meta-level reasoning ability.\nHowever, this is not a strict ‘gold standard’, single correct reasoning trace which we hold models to – we use this set of actions to analyse whether the model has satisfied the core aspects of the task.\nThe aim of this work is not to design a system to maximise the performance of LLMs at our task, but rather to use the tool-use paradigm as an intermediate representation through which we can analyse the meta-level reasoning ability of LLMs.\nConsequently, we investigate the meta-level reasoning of off-the-shelf models without fine-tuning.\n\n\nFigure 1: Overview of our question generation and evaluation process. 1 We instantiate question templates with slot values. 2 Using a hand-created templated sequence of required steps and the set of tools, we compute essential actions and answers. 3 Instantiated questions are passed to an LLM, which is held in a loop making tool calls which are executed and returned to the model. 4 The predicted set of tool calls are compared to the essential actions.\n\n\nIn parallel with our focus on the performance of LLMs at reasoning tasks, we are equally interested in the explainability and interpretability of the QA process, and this has informed the design of our environment.\nSimilar to [36], we are conscious of the relationship of research-based benchmarks to the use of LLMs as QA systems in industry, and are highly conscious of the proliferation of LLMs in commercial and professional settings, with a particular focus on QA.\nThis motivation informs the design of our tool-calling evaluation loop, which allows us to not only inspect the reasoning process of LLMs via comparison with essential actions created in our dataset, but as a standalone feature enables highly interpretable outputs.\nTo summarise our contributions, we:\n\n\n•\n\nCreate a multi-hop QA environment with ‘essential actions’ (§3).\n\n\n\n•\n\nEvaluate LLMs in terms of final answer accuracy and meta-level reasoning ability (§5).\n\n\n\n•\n\nFind that models are generally able to reason at the meta-level, selecting appropriate tools to achieve high accuracy.\n\n\n\n•\n\nAnalyse deficiencies in meta-level reasoning in terms of missed reasoning steps.\n\n\n\n•\n\nFind that one- and three-shot examples of tool execution do not improve accuracy, but does reduce incorrect tool call frequency.\n\n\n\n•\n\nVerify the limited numeracy of LLMs when we remove access to arithmetic tools.\n\n\n\n\n\n\n\n2 Background\n\nReasoning in LLMs\n\nIn the context of LLMs, the term reasoning speaks to a systematic problem-solving or decision-making capability whereby inferences and conclusions are made based on available information [20].\nReasoning may be subdivided into mathematical reasoning [11], symbolic reasoning [35], or commonsense reasoning [3]; but is often linked with the task of breaking a problem down in to intermediate steps.\nPrompting models to explicitly generate intermediate steps to help solve problems improves performance at downstream tasks in zero- and few-shot settings [35, 33, 21], while supervised fine-tuning also leads to performance gains on QA tasks [32, 19].\nIn this paper, we prefer to discuss meta- and object-level reasoning (which we overview in §2) not with the intention of superseding the above terms, but rather to provide a better structure to the discussion of the reasoning ability of LLMs.\n\n\n\nTool-use\n\nTool-use, is a paradigm in which LLMs can generate function calls to assist with task-solving [34, 31], which are executed by external programs and the results returned to the model.\nThey are typically used to alleviate intrinsic weaknesses in LLMs by improving arithmetic capability [15, 27] and real-time data retrieval through APIs, knowledge bases, and web search [29, 28, 22].\nAn alternative interface to symbolic methods is the generation of code to perform a task [13, 8, 7].\n\n\n\nExisting Datasets\n\nA variety of datasets exist which examine the ability of LLMs to reason over questions requiring multiple intermediate steps of reasoning.\nGSM8k [11] and MATH [19] focus on multi-hop mathematical reasoning tasks, while others focus on tasks which require reasoning over natural language evidence [32, 37, 17].\nWhile the ability of LLMs to interact with structured, tabular data is receiving significant attention\n[9, 18, 38, 36], they are not structured in a way that allows explicit analysis of the intermediate steps.\n\n\n\nMeta- and Object-Level Reasoning\n\nMeta- and object-level reasoning are terms associated with symbolic AI, particularly the automated reasoning and proof planning domains, yet they are highly relevant to the application of LLMs to QA.\nTo help map these definitions to our task, we will first describe a range of examples of "
  }
]