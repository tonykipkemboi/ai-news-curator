[
  {
    "title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior",
    "url": "https://arxiv.org/abs/2601.14255v1",
    "source": "arxiv",
    "summary": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even thou",
    "full_text": null
  },
  {
    "title": "Which Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment",
    "url": "https://arxiv.org/abs/2601.14249v1",
    "source": "arxiv",
    "summary": "Long chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs. However, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data-student suitability in distillation. Existing methods assess suitability primarily through student lik",
    "full_text": "\n\n\n\n1 Introduction\n\n2 The Need for Student-Specific Data\n\n\n2.1 Experimental Settings\n\nTeachers\nBenchmarks\n\n\n\n2.2 Results\n\nStronger teachers do not necessarily produce better students.\nData‚Äìstudent suitability is critical for eliciting reasoning improvements.\n\n\n\n\n\n3 Measuring Data-Student Suitability\n\n\n3.1 Surprisal and Rank\n\nSurprisal (Negative Log-Likelihood)\nRank\n\n\n3.2 Limitations of Probability-Based Metrics\n\n3.3 Insight and Simulation\n\nSimulation Setting\nSimulation Results\n\n\n\n3.4 Proposed Metric: Rank-Surprisal Ratio\n\nToken-Level Metric\nFrom Token-Level to Trajectory-Level\nInterpretation\n\n\n\n\n\n4 Correlation Analysis\n\n\n4.1 Main Analysis\n\nCompared Metrics\nResults\n\n\n4.2 Ablation Study\n\n\n\n5 Practical Applications\n\n\n5.1 Trajectory Selection\n\nExperimental Setting\nResults\n\n\n\n5.2 Teacher Selection\n\nExperimental Setting\nResults\n\n\n\n\n\n6 Related Work\n\nKnowledge Distillation\nSFT with Reasoning Trajectories\n\n\n7 Conclusion\n\nA Details of Experiments\n\nA.1 Determining Training Problem Set\nA.2 Teacher Trajectory Generation\nA.3 Teacher Models and Student Models\nA.4 Benchmark Evaluation\nA.5 Details of Model Fine-Tuning\nA.6 Details of Simulation Study\nA.7 Details of Correlation Analysis\n\n\n\nB Details of Compared Metrics\n\nB.1 LLM-judged Quality\nB.2 Rule-based Quality\nB.3 Influence Score\nB.4 G-Norm\nB.5 GRACE\nB.6 Others\n\n\n\nC More Results and Analysis\n\nC.1 Computational Cost for Rank-Surprisal Ratio\nC.2 Additional Results for Correlation Analysis\nC.3 Additional Ablation Study\n\nC.4 Additional Results for Trajectory Selection\n\nC.4.1 Ablation Study on Trajectory Selection\nC.4.2 Additional Evaluation on GPQA and Full Results\nC.4.3 Analysis of Datasets Selected by RSR\nC.4.4 Additional Trajectory Selection Experiments with Fewer Teacher Models\n\n\n\n\n\nD Others\n\nD.1 License for Artifacts and Data Consent\nD.2 Data Statement\nD.3 AI Assistant Usage Statement\n\n\n\n\n\n\n\nWhich Reasoning Trajectories Teach Students to Reason Better? A Simple Metric of Informative Alignment\n\n\nYuming Yang\n\nFudan University\n\nShanghai AI Laboratory\n\n\nMingyoung Lai\n\nUniversity of Toronto\n\n\nWanxu Zhao\n\nFudan University\n\n\nXiaoran Fan\n\nFudan University\n\n\nZhiheng Xi\n\nFudan University\n\n\nMingqi Wu\n\nFudan University\n\n\nChiyue Huang\n\nUniversity of Sydney\n\n\nJun Zhao\n\nFudan University\n\n\nHaijun Lv\n\nShanghai AI Laboratory\n\n\nJian Tong\n\nShanghai AI Laboratory\n\n\nYunhua Zhou\n\nShanghai AI Laboratory\n\n\nYicheng Zou\n\nShanghai AI Laboratory\n\n\n\n\nQipeng Guo\n\nShanghai AI Laboratory\n\n\nTao Gui\n\nFudan University\n\n\nQi Zhang\n\nFudan University\n\n\n\n\nXuanjing Huang\n\nFudan University\n\n\n\nAbstract\nLong chain-of-thought (CoT) trajectories provide rich supervision signals for distilling reasoning from teacher to student LLMs.\nHowever, both prior work and our experiments show that trajectories from stronger teachers do not necessarily yield better students, highlighting the importance of data‚Äìstudent suitability in distillation.\nExisting methods assess suitability primarily through student likelihood, favoring trajectories that closely align with the model‚Äôs current behavior but overlooking more informative ones.\nAddressing this, we propose Rank-Surprisal Ratio (RSR), a simple metric that captures both alignment and informativeness to assess the suitability of a reasoning trajectory.\nRSR is motivated by the observation that effective trajectories typically combine low absolute probability with relatively high-ranked tokens under the student model, balancing learning signal strength and behavioral alignment.\nConcretely, RSR is defined as the ratio of a trajectory‚Äôs average token-wise rank to its average negative log-likelihood, and is straightforward to compute and interpret.\nAcross five student models and reasoning trajectories from 11 diverse teachers, RSR strongly correlates with post-training performance (average Spearman 0.86), outperforming existing metrics.\nWe further demonstrate its practical utility in both trajectory selection and teacher selection.\n\n‚Ä†‚Ä†‚Ä†\\dagger Corresponding authors. Inquiries may be sent to: yumingyang23@m.fudan.edu.cn, zouyicheng@pjlab.org.cn, qz@fudan.edu.cn‚Ä†‚Ä†‚àó* Code is available at https://github.com/UmeanNever/RankSurprisalRatio.\n\n\n1 Introduction\n\nRecent advances in reasoning-oriented large language models (LLMs) are largely driven by their ability to generate long chain-of-thought (CoT) trajectories [12, 48].\nBeyond enabling complex inference at test time, such trajectories also provide powerful supervision signals for training student models [35, 28] or cold-starting reinforcement learning [39] through supervised fine-tuning (SFT).\n\n\nYet, stronger reasoning teachers do not necessarily yield better students [22, 11].\nOur extensive experiments show that the post-training effectiveness of reasoning trajectories varies substantially across student models, indicating that the suitability between data and student is critical for effective learning.\nExisting data engineering methods assess data suitability primarily through the student‚Äôs probability assignments [47, 18], favoring high-likelihood trajectories that align closely with the model‚Äôs current behavior.\nSuch trajectories, however, often provide limited new learning signals.\nIn contrast, more informative trajectories are typically less familiar to the student and thus overlooked by these methods.\nThis leads to a fundamental Informative Alignment challenge: how to identify reasoning data that are both well aligned with the student and sufficiently informative?\n\n\nTo address this challenge, we propose a simple yet effective metric, Rank-Surprisal Ratio (RSR), which quantifies the suitability of a reasoning trajectory for a given student by jointly capturing alignment and informativeness.\nMotivated by our preliminary analysis, we argue that the dilemma between providing new signals and aligning with student‚Äôs existing behavior can be resolved by trajectories exhibiting absolute unfamiliarity and relative familiarity.\nConcretely, effective trajectories should deviate from the student‚Äôs own generations, receiving low absolute probability under the student model, while still containing behavioral patterns within the student‚Äôs prior experience, such that their tokens rank relatively high in the model‚Äôs prediction distribution over the vocabulary (Figure 1).\n\n\nFigure 1: Illustration of the intuition behind RSR. Suitable reasoning trajectories should balance informativeness and alignment by having low absolute probability but relatively high-ranked tokens under the student model.\n\n\nBased on this insight and consistent numerical patterns observed in simulation studies, we define our suitability metric, Rank-Surprisal Ratio, as the ratio between a trajectory‚Äôs average token-wise rank111Higher-ranked tokens have lower rank values. and its average negative log-likelihood (surprisal).\nRSR can be computed with a single forward pass, requires no additional verifier or test data, and is straightforward to interpret.\nLower RSR indicates better informative alignment, identifying trajectories that are both informative and well aligned with the student.\n\n\nWe validate the effectiveness of Rank-Surprisal Ratio through correlation analyses on 5 student LLMs using math reasoning trajectories generated by 11 representative teacher models.\nAcross all students, the RSR of trajectories exhibits a strong correlation with post-training performance, achieving an average Spearman correlation of 0.86 and consistently outperforming alternative metrics.\nFurthermore, to explore its practical value in data engineering, we apply RSR to trajectory selection and teacher selection. Our experiments show that RSR not only selects more effective training trajectories for each problem from candidates generated by diverse teachers, but also identifies more suitable teacher models using only a small amount of data, consistently outperforming existing selection methods across all five students in both settings.\n\n\nOur main contributions are three-fold:\n\n\n‚Ä¢\n\nWe present a systematic distillation study across a wide range of teacher and student models, showing that the effectiveness of reasoning trajectories differs across students and highlighting the importance of data‚Äìstudent suitability.\n\n\n\n‚Ä¢\n\nWe propose Rank-Surprisal Ratio, a simple metric that quantifies the suitability of a reasoning trajectory by jointly capturing alignment and informativeness, achieving a strong correlation with post-training performance.\n\n\n\n‚Ä¢\n\nWe demonstrate the practical utility of Rank-Surprisal Ratio in two data engineering scenarios, trajectory selection and teacher selection, where it serves as an effective criterion and outperforms existing methods.\n\n\n\n\n\n\n\n2 The Need for Student-Specific Data\n\nTo understand which types of reasoning trajectories most effectively improve student models after SFT, we conduct a comprehensive large-scale study involving five widely adopted student models and eleven diverse reasoning-oriented teacher models, yielding 55 teacher‚Äìstudent pairings.\nWe perform SFT experiments for each of these pairs.\n\n\n\n\n\nTeacher Models\nParams\nStudent Models (Base)\n \n\n\nTeacher\n\nPerformance\n \n\n\nQwen-3-14B\nLLaMA-3.1-8B\nQwen-2.5-7B\nQwen-3-4B\nQwen-2.5-3B\n\n\nDeepseek-R1\n671B\n77.1\n28.1\n47.3\n55.8\n29.6\n91.1\n\n\nQwen-3-235B-Thinking\n235B\n71.8\n22.0\n45.0\n53.4\n26.4\n91.2\n\n\nGPT-OSS-120B\n120B\n66.7\n15.2\n40.7\n47.9\n22.9\n88.3\n\n\nNemotron-Super\n49B\n72.2\n23.7\n48.3\n56.4\n33.0\n82.3\n\n\nQwQ-32B\n32B\n77.4\n27.1\n52.0\n61.2\n33.0\n85.2\n\n\nQwen-3-30B-Thinking\n30B\n77.2\n26.7\n50.0\n58.8\n31.2\n92.3\n\n\nMagistral-Small\n24B\n68.8\n22.8\n47.6\n52.2\n30.6\n71.0\n\n\nGPT-OSS-20B\n20B\n69.5\n17.9\n42.7\n48.4\n24.4\n83.4\n\n\nPhi-4-Reasoning-Plus\n14B\n54.1\n14.5\n35.2\n40.2\n18.2\n72.7\n\n\nQwen-3-8B\n8B\n74.6\n26.5\n52.0\n61.2\n34.2\n82.5\n\n\nQwen-3-4B-Thinking\n4B\n76.8\n28.2\n51.8\n61.9\n33.3\n87.3\n\n\n\nTable 1: Distillation results showing post-training reasoning performance of student models trained on trajectories from different teacher models, evaluated by average Acc@4 on AIME‚Äô25, AIME‚Äô24, AMC‚Äô23, and MATH500. Darker and lighter shading indicate the best and second-best results, respec"
  },
  {
    "title": "Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow",
    "url": "https://arxiv.org/abs/2601.14243v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottle",
    "full_text": "\n\n\n\n\n1 Introduction\n\n(i) Long-Rollout Generation.\n(ii) Challenging Tasks.\n\n\n\n2 Background\n\n2.1 Quantization Basis\n2.2 Quantization of a Linear Layer\n2.3 Workload of Reinforcement Learning\n\n\n\n3 Motivation\n\n3.1 Rollout is the Bottleneck in RL Training\n3.2 BF16-Train-FP8-Rollout with Calibration is Slow\n3.3 BF16-Train-FP8-Rollout without Calibration is Unstable\n\n\n\n4 Jet-RL: Enabling on-policy FP8 RL Training\n\n4.1 Unified FP8 Precision Flow between Training and Rollout\n\n4.2 Granularity of GEMM Quantization\n\nFProp Operator\nDGrad Operator and WGrad Operator\n\n\n4.3 Implementation\n\n\n\n5 Evaluation\n\n\n5.1 Evaluation Setup\n\nModels.\nDatasets and RL Training Setting.\nHyperparameters.\nEvaluation Metrics.\n\n\n5.2 Accuracy Evaluation\n5.3 Efficiency Evaluation\n\n\n\n6 Related Works\n\n6.1 Low-Precision Training and Inference for LLMs\n6.2 Reinforcement Learning for Large Language Models\n6.3 Efficient Reasoning with Large Language Models\n\n\n7 Conclusion\n\n\n\n\n\nJet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow\n\n\n\nHaocheng Xi1,3\nCharlie Ruan3\nPeiyuan Liao4\nYujun Lin1\nHan Cai1\nYilong Zhao3\nShuo Yang3\nKurt Keutzer3\nSong Han1,2\nLigeng Zhu1,‚Ä†{\\dagger} ‚ÄÉ‚ÄÉ \n1NVIDIA ‚ÄÉ2MIT ‚ÄÉ3UC Berkeley ‚ÄÉ4Stanford University\n\n\n\n\n\nAbstract\nReinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase consuming over 70% of the total training time.\nQuantized RL training, particularly with FP8, offers a promising solution. For example, a common approach is to employ FP8 precision during rollout to alleviate this bottleneck while retaining BF16 precision during training.\nIn this work, we present the first comprehensive study of FP8 RL training and show that the commonly adopted BF16-train + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-rollout generation and challenging tasks. Our analysis reveals that these issues arise from the off-policy nature of the approach, which introduces significant numerical mismatch between training and inference.\nMotivated by these findings, we propose Jet-RL, a FP8 RL training framework that enables robust and stable RL training. The key idea is to adopt an unified FP8 precision flow for both training and rollout, minimizing numerical discrepancies and avoiding the need for inefficient inter-step calibration.\nExtensive experiments demonstrate the effectiveness of Jet-RL. Our method achieves up to 33% rollout phase speedup, up to 41% training phase speedup, and a 16% end-to-end speedup over BF16 training while maintaining robust convergence across all settings and exhibiting negligible accuracy degradation. We will release our code and pre-trained models when less anoynomous.\n\n\n\n1 Introduction\n\nFigure 1: Overview of RL training different between JetRL and other methods. JetRL proposes unified precision flow for FP8 RL training accommodate both performance and throughput .\n\n\nFigure 2: Rollout Generation dominates the RL training latency. When the rollout length is larger than 8k, rollout will take &gt;75%&gt;75\\% of the total latency, making it the primary bottleneck.\n\n\nFigure 3: Naive BF16 Train + FP8 Rollout fails when rollout context length increases. We observe that although the BF16-Train-FP8-Rollout method might exhibit similar performance compared with BF16 training, its performance quickly degrades as we extend the rollout length to more than 8k. We use the Qwen3-8B-Base model for this experiment, and evaluate and train on MATH.\n\n\nThe emergence of frontier reasoning large language models (LLMs) [jaech2024openai, guo2025deepseek] has demonstrated unprecedented capabilities in solving highly complex, multi-step problems [chollet2024arc, chollet2025arcagi2newchallengefrontier], setting new state-of-the-art across domains ranging from advanced mathematics to scientific discovery. A key driver of this success is the Chain-of-Thought (CoT) [zhou2024chainofthoughtreasoningwithout, Wei2022Chainofthought] paradigm, which enables models to generate extended, coherent reasoning traces before producing final answers. This mechanism allows LLMs to conduct detailed analyses, explore alternative solution paths, and execute structured logical reasoning. Reinforcement Learning (RL) [Ouyang2022TrainingRLHF, Bai2022TrainingRLHF2] has emerged as the pivotal training paradigm for unlocking robust CoT generation, propelling models from simple question answering toward goal-directed reasoning.\n\n\nHowever, RL training remains notoriously resource-intensive, with the actor rollout phase becoming the primary training bottleneck [yu2025dapoopensourcellmreinforcement]. Effective reasoning often demands generating long token sequences (e.g., &gt;&gt;6,000 tokens) to adequately explore the solution space. Owing to the inherently autoregressive nature of LLMs, this rollout stage incurs substantial training time, frequently accounting for more than 70% of the total end-to-end training pipeline (Figure 2). This extreme imbalance significantly hampers overall training efficiency, making rollout acceleration an urgent priority [Jin2025SearchR1TL, cheng2025Reasoning360].\n\n\nTo improve efficiency, FP8 quantization offers a highly promising solution. It has been widely adopted in LLM inference, demonstrating significant efficiency gains [fishman2025scalingfp8trainingtrilliontoken, peng2023fp8lm, xi2024coat, kim2025inquiryfp8]. A natural approach to accelerate RL training is to apply FP8 quantization during the rollout phase, which constitutes the main efficiency bottleneck, and retain BF16 precision for the training phase, which is expected to deliver better training stability and accuracy. This strategy BF16-train-FP8-rollout has been widely adopted in modern RL frameworks such as VeRL, SLIME, Nemo-RL, and OpenRLHF [sheng2024hybridflow_verl, slime_github, nemo-rl, hu2024openrlhf]. However, we find that this strategy exhibits critical limitations in two circumstances:\n\n\n(i) Long-Rollout Generation.\n\nFirst, we observe that this training strategy suffers from significant accuracy degradation and can even experience catastrophic collapse when applied to long sequences. For example, Figure 3 shows the training curves of the Qwen3-8B-Base model across 4K, 8K, and 16K token generations. Compared with BF16 training, its accuracy rapidly drops once the rollout length exceeds 8K tokens, ultimately resulting in significant performance drop at the 16K setting.\n\n\nSmall numerical discrepancies will gradually accumulate during long chain-of-thought reasoning. The mismatch between training and rollout precision are negligible for short sequences but progressively increases with the rollout length. These accumulated errors amplify the effects of off-policy training, causing the generation trajectory to diverge and making RL training unstable.\n\n\n\n(ii) Challenging Tasks.\n\nSecond, we observe that the claimed success of recent BF16-train-FP8-rollout is not always reliable. As shown in Figure 4, the strategy shows no degradation when the model already possesses strong task priors from pretraining. However, when applied to harder reasoning tasks or trained with weaker base models, the training curve quickly diverges between BF16 and FP8 rollouts. These observations suggest that effective quantized rollout must be robust and adaptable across diverse training settings.\n\n\nTo address these limitations and establish FP8 rollout as a reliable acceleration strategy, we propose Jet-RL. Our core contribution is enforcing a truly on-policy FP8 training paradigm that stabilizes RL training. We design our framework to use an identical quantization precision flow for both training and inference, eliminating policy mismatch and removing the need for inter-step calibration. Our approach adopts a mixed per-group and per-block quantization scheme [cheng2025deepseekv3technicalreport, xi2024coat] and leverages state-of-the-art FP8 GEMM kernels [deepseek2025deepgemm] to achieve acceleration for end-to-end RL training.\n\n\nWe conduct comprehensive experiments across diverse models, datasets, and rollout configurations to validate Jet-RL. Our results demonstrate that Jet-RL successfully stabilizes training, minimizes divergence between training and rollouts, and substantially narrows the performance gap between BF16 and FP8 RL. While BF16-train-FP8-rollout methods typically incur more than 5%5\\% performance degradation compared to BF16 baselines, our approach reduces this to ‚àº1%\\sim 1\\%. Meanwhile, Jet-RL achieves up to 1.33√ó1.33\\times rollout phase speedup for 32B model, up to 1.41√ó1.41\\times training phase speedup for 8B model, and 1.16√ó1.16\\times end-to-end speedup for 8B experiments. These findings confirm that our method provides a robust solution for efficient low-precision RL training, enabling significant acceleration without sacrificing performance.\n\n\nWe summarize our contributions below:\n\n\n‚Ä¢\n\nWe identify that the commonly used BF16-train-FP8-rollout paradigm leads to training instability and accuracy collapse under long-rollout generation and challenging tasks.\n\n\n\n‚Ä¢\n\nWe propose Jet-RL, an on-policy FP8 RL framework by enforcing an unified training‚Äìrollout precision flow for training and rollout. This approach resolves policy mismatch and ensures stable optimization.\n\n\n\n‚Ä¢\n\nJet-RL achieves substantial rollout and end-to-end speedups while maintaining convergence and accuracy close to BF16 baselines across various models and tasks.\n\n\n\n\n\n\n\n\n2 Background\n\n\n2.1 Quantization Basis\n\nQuantization maps a high-precision tensor to a lower-precision tensor to speed up computation and reduce memory footprint. We consider quantizing a tensor ùëø{\\bm{X}} into a target data format whose maximum representable value is Œîmax\\Delta_{\\text{max}}. The quantization process can be defined as:\n\n\n\nùëø^,Sùëø=Q‚Äã(ùëø),¬†where¬†‚ÄãQ‚Äã(‚ãÖ)‚Äã¬†is the quantizer\\hat{{\\bm{X}}},S_"
  },
  {
    "title": "APEX-Agents",
    "url": "https://arxiv.org/abs/2601.14242v1",
    "source": "arxiv",
    "summary": "We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.14242v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.14242v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 20 Jan 2026]\n    Title:APEX-Agents\n    Authors:Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski            View a PDF of the paper titled APEX-Agents, by Bertie Vidgen and 20 other authors\n    View PDF\n\n\n\n    \n            Abstract:We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2601.14242 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.14242v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.14242\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Bertie Vidgen Dr [view email]          [v1]\n        Tue, 20 Jan 2026 18:53:44 UTC (8,181 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled APEX-Agents, by Bertie Vidgen and 20 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscribe\n                \n              \n            \n          \n        \n        \n        \n      "
  },
  {
    "title": "Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression",
    "url": "https://arxiv.org/abs/2601.14238v1",
    "source": "arxiv",
    "summary": "Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecast",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-1 Wildfire Prediction\nII-2 Fire Spread Simulation\nII-3 RL for Disaster Mitigation\n\n\n\nIII Proposed Framework\n\n\nIII-A Data Acquisition and Preparation\n\nIII-A1 Wildfire Incident Collection and Filtering\nIII-A2 Dataset Balancing\nIII-A3 Spatiotemporal Feature Extraction\n\n\nIII-B Wildfire Ignition Forecasting\n\nIII-C Suppression Strategies Optimization using RL\n\n\nIII-C1 Physics-informed Simulation\n\nFire Engine and Spread Modeling\nFuel Models and Parameters\nRealistic Terrain and Vegetation Integration\n\n\n\nIII-C2 Custom RL Environment Setup\n\nAction Space\nObservation Space\nReward Function\nFeature Extractor\n\n\nIII-C3 RL Algorithm\n\n\nIII-D Fire Threat Assessment Report\nIII-E Web-based Application\n\n\n\nIV Results\n\nIV-A Dataset Release for Community Research\nIV-B Wildfire-Ignition Forecasting Results\nIV-C Helitack Suppression Performance\nIV-D FireCastRL Web Application\n\n\nV Conclusion and Future Work\n\n\n\n\n\nSpatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression\n\n\n\nShaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, and Alina Vereshchaka\n\n\n\nAbstract\nWildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing\nfires only after they are detected. We introduce FireCastRL, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies.\nOur framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing 9.5\\mathbf{9.5} million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.\n\n\n\n\nFigure 1: FireCastRL framework is a wildfire forecasting and mitigation system containing five stages: (A) acquisition of IRWIN database and GRIDMET weather data, (B) prediction of wildfire ignition using CNN-LSTM architecture, (C) simulation of RL-based helitack suppression based on real-world terrain and environmental data, (D) generation of fire threat assessment report with risk forecasts and strategic recommendations, and (E) web-based application.\n\n\n\nI Introduction\n\n\nWildfires in the U.S. are growing more frequent, intense, and costly. In 20232023 alone, they caused an estimated $‚Äã14.7\\mathdollar 14.7 billion in direct property losses and required over $‚Äã3\\mathdollar 3 billion in suppression spending¬†[12, 8]. When indirect damages and long-term effects are included, the total annual economic burden has been estimated at up to $‚Äã893\\mathdollar 893 billion¬†[19]. This highlights the urgent need for more advanced and efficient wildfire management strategies to mitigate future losses.\n\n\nTraditional wildfire detection methods (e.g., watchtowers, ground sensors, and satellite scans) are inherently reactive. They trigger alarms only after fires are visible, often leaving little time for effective containment, especially in remote or rugged areas. To mitigate risk more effectively, we need systems that forecast where and when fires are likely to start, and simulate how they might spread.\n\n\nRecent advances in Earth observation data, deep learning, and simulation tools facilitate the development of novel approaches for predictive wildfire management. Deep learning models can learn patterns in environmental variables to forecast fire ignition days in advance, while reinforcement learning (RL) agents can learn optimal suppression strategies.\n\n\nOngoing efforts aim to integrate deep learning into wildfire monitoring and response. Google‚Äôs FireSat[6] project uses satellite imagery and deep learning models to detect active wildfires in real time, issuing alerts to first responders and governments globally. Similarly, Canada‚Äôs WildfireSat program and tools like National FireGuard focus on spaceborne detection and situational awareness.\nWhile these systems excel at early detection, their primary scope is situational awareness rather than the downstream tasks of strategic response planning or autonomous suppression. In this work, we present FireCastRL, our proposed framework that focuses on integrating wildfire prediction using deep learning methods and mitigation steps using RL, combining them into a single system. Our contributions include:\n\n\n\n\n‚Ä¢\n\nPublicly released, large-scale, spatiotemporal dataset containing 9.59.5 million samples of environmental variables for wildfire prediction derived from GRIDMET¬†[2] and IRWIN. This dataset supports research on wildfire forecasting and AI-driven disaster response.\n\n\n\n‚Ä¢\n\nA deep spatiotemporal forecasting model for predicting wildfire ignition based on patterns in historical environmental data.\n\n\n\n‚Ä¢\n\nA physics-informed wildfire simulation environment that uses real land cover, elevation, and mesoscale wind fields. The engine combines a browser-based fire simulator for RL agent interaction.\n\n\n\n‚Ä¢\n\nAn RL-based helitack control policy trained with Proximal Policy Optimization (PPO). The agent learns to deploy aerial suppression (helitack) in dynamic wildfire environments.\n\n\n\n‚Ä¢\n\nAn end-to-end pipeline that generates a fire threat assessment report for emergency responders and decision-makers containing predicted ignition coordinates, burn trajectory, suppression sequence, and response recommendations.\n\n\n\n\n\n\n\nII Related Work\n\n\n\nII-1 Wildfire Prediction\n\nRecent models utilize MODIS satellite data and topographic variables with convolutional neural networks (CNNs) and recurrent neural networks to improve spatial and temporal generalization [5, 21]. FireCast¬†[13], a deep learning system, predicts near-future wildfire spread. While these works focus on predicting the progression of an already active fire without downstream integration into decision-making systems, our work predicts where a fire is likely to start, and subsequently uses RL to simulate and optimize an effective suppression response.\n\n\nTABLE I: Snapshot of the dataset showing location, date, wildfire labels, and environmental variables, including meteorological measurements.\n\n\n\nlatitude\nlongitude\ndatetime\nWildfire\npr\nrmax\nrmin\nsph\nsrad\ntmmn\ntmmx\nvs\nbi\nfm100\nfm1000\nerc\netr\npet\nvpd\n\n\n48.128431\n-97.276685\n2018-08-15\nNo\n0.0\n78.6\n14.9\n0.00582\n272.6\n282.0\n301.6\n3.0\n40.0\n10.2\n12.2\n54.0\n7.5\n5.5\n1.59\n\n\n48.128431\n-97.276685\n2018-08-16\nNo\n0.0\n80.4\n13.9\n0.00676\n264.0\n283.9\n304.9\n3.0\n40.0\n9.7\n12.0\n56.0\n8.2\n5.9\n1.93\n\n\n‚Ä¶\n\n\n37.920118\n-120.413184\n2017-02-04\nNo\n0.0\n99.5\n59.8\n0.00713\n85.8\n280.5\n289.6\n2.8\n19.0\n17.2\n25.4\n14.0\n1.9\n1.4\n0.33\n\n\n\n\n\n\n\nII-2 Fire Spread Simulation\n\nThe Rothermel model [15] approximates the rate of fire spread based on fuel, wind, and elevation. Standard forest fire simulators like FARSITE¬†[4] use this model to project wildfire behavior over real terrains. While accurate, these tools are computationally intensive and rarely used in closed-loop decision-making pipelines. Our approach draws inspiration from these simulators and integrates them into a real-time, RL-compatible environment.\n\n\n\n\nII-3 RL for Disaster Mitigation\n\nRL has been applied to a range of decision-making problems in uncertain environments, including emergency response, urban evacuation [11], natural disasters [20], and epidemiological response optimization [10]. Research in wildfire suppression using RL is limited but growing. For example, Julian et al. [9] modeled fire containment with discrete RL in grid environments. However, few studies incorporate realistic terrain or dynamics, and even fewer combine forecasting and mitigation in a single system.\n\n\n\n\n\nIII Proposed Framework\n\n\nWe introduce FireCastRL framework (Fig.¬†1), an end-to-end system that connects wildfire prediction with strategic response planning. First, a deep learning model forecasts ignition risk based on a large-scale spatiotemporal dataset. High-risk predictions launch an RL simulation where a pre-trained agent deploys helitack suppression strategies in a physics-informed 3D environment. The prediction and simulation results are then compiled into a fire threat assessment report for emergency responders.\n\n\n\nIII-A Data Acquisition and Preparation\n\n\nWe create a spatiotemporal wildfire dataset by combining high-resolution incident data from Integrated Reporting of Wildfire Information (IRWIN) with meteorological sequences from GRIDMET¬†[2]. This involves incident deduplication, negative sample synthesis, environmental feature extraction, and geographic-temporal validation.\n\n\n\nIII-A1 Wildfire Incident Collection and Filtering\n\nOur dataset is based on the IRWIN database¬†[18], which catalogs all reported wildfire events across the U.S. from January 2014 to April 2025. The raw dataset consists of 348‚Äâ604348\\,604 wildfire reports, each tagged with metadata such as discovery timestamp and geographic coordinates.\n\n\nA key challenge with this raw data is that wildfire events often appear multiple times in the database due to phased updates, secondary flare-ups, or redundant reports, thus introducing noise that can bias AI models. To isolate unique ignition events from data points generated by ongoing fire spread, we applied a multi-stage filtering procedure:\n\n\n\n\n1.\n\nFor any given date, we retain only ignition events ‚â•5‚Äãkm\\geq~5~\\mathrm{km} apart, using a sliding acceptance window that scans incidents sequentially and discards nearby duplicates.\n\n\n\n2.\n\nWe enforce a minimum gap of 22 hours between retained ignitions to remove s"
  },
  {
    "title": "Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration",
    "url": "https://arxiv.org/abs/2601.14235v1",
    "source": "arxiv",
    "summary": "The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; astro-ph &gt; arXiv:2601.14235v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Astrophysics  Instrumentation and Methods for Astrophysics\n    \n\n    \n      arXiv:2601.14235v1 (astro-ph)\n    \n\n\n  \n    \n  [Submitted on 20 Jan 2026]\n    Title:Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration\n    Authors:LSST Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Rapha√´l Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra ƒÜiprijanoviƒá, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Mariano Javier de Le√≥n Dominguez Romero, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, Fran√ßois Lanusse, C. Danielle Leonard, Pierre-Fran√ßois L√©get, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais M√∂ller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Laurence Perreault-Levasseur, Andr√©s A. Plazas Malag√≥n, Nesar Ramachandra, Benjamin Remy, C√©cile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tr√∂ster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang            View a PDF of the paper titled Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration, by LSST Dark Energy Science Collaboration and 65 other authors\n    View PDF\n\n\n\n    \n            Abstract:The Vera C. Rubin Observatory&#39;s Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC&#39;s primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.\n    \n\n    \n    \n              \n          Comments:\n          84 pages. This is v1.0 of the DESC&#39;s white paper on AI/ML, a collaboration document that is being made public but which is not planned for submission to a journal\n        \n\n          Subjects:\n          \n            Instrumentation and Methods for Astrophysics (astro-ph.IM); Cosmology and Nongalactic Astrophysics (astro-ph.CO); Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)\n                \n          Report&nbsp;number:\n          FERMILAB-PUB-25-0886-CSAID-PPD\n        \n\n          Cite as:\n          arXiv:2601.14235 [astro-ph.IM]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.14235v1 [astro-ph.IM] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.14235\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Francois Lanusse [view email]          [v1]\n        Tue, 20 Jan 2026 18:46:42 UTC (813 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration, by LSST Dark Energy Science Collaboration and 65 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: astro-ph.IM\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        astro-ph\n        astro-ph.CO\n        cs\n        cs.AI\n        cs.LG\n        stat\n        stat.ML\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n            \n              INSPIRE HEP\n            \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n"
  },
  {
    "title": "Q-learning with Adjoint Matching",
    "url": "https://arxiv.org/abs/2601.14234v1",
    "source": "arxiv",
    "summary": "We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to d",
    "full_text": null
  },
  {
    "title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning",
    "url": "https://arxiv.org/abs/2601.14232v1",
    "source": "arxiv",
    "summary": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying c",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.14232v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2601.14232v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 20 Jan 2026]\n    Title:KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning\n    Authors:Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov            View a PDF of the paper titled KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning, by Egor Cherepanov and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: this https URL.\n    \n\n    \n    \n              \n          Comments:\n          38 pages, 44 figures, 3 tables\n        \n\n          Subjects:\n          \n            Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)\n        \n          Cite as:\n          arXiv:2601.14232 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.14232v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.14232\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Egor Cherepanov [view email]          [v1]\n        Tue, 20 Jan 2026 18:44:28 UTC (6,737 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning, by Egor Cherepanov and 3 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.CV\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed "
  },
  {
    "title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems",
    "url": "https://arxiv.org/abs/2601.14230v1",
    "source": "arxiv",
    "summary": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective",
    "full_text": "\n\n\n\n\n1 Introduction\n\nChallenges\nThis Work\nContributions\n\n\n\n2 Method\n\nPreliminary\n\n2.1 Persona-Aware Behavioral Alignment\n\nPersona Reward Modeling\nEfficient RL Fine-tuning via GRPO\n\n\n\n2.2 Collaborative Dialogue Optimization\n\nBi-level Generation Process\nGroup Reward Modeling\n\n\n\n\n\n3 Evaluation\n\n\n3.1 Experimental Setup\n\nDataset\nPersona Descriptions\nBaselines\n\n\n\n3.2 Overall Performance\n\nChallenges in Negative Valence\nLimitations of Supervised Fine-Tuning\n\n\n\n3.3 Multi-Perspective Evaluation Across MBTI Profiles\n\nThe Anchor as a Social API\nNavigating Cognitive Friction and Synergy\n\n\n\n3.4 Sensitivity Analysis\n\nScaling the Director Agent\nScaling the Speaker Agent\n\n\n\n3.5 Case Study\n\nComparative Analysis\n\n\n\n\n\n4 Related Work\n\nSocial-First Multi-Agent Systems (MAS)\nCoordination and Persona Integrity\n\n\n5 Conclusion\nA Ethical Consideration\n\nB Additional Evaluation\n\nB.1 Ablation Study\nB.2 Implementation Details\nB.3 Evaluation Metrics\nB.4 Datasets\n\n\n\nC Discussion\n\nUse of AI Assistant\n\n\n\n\n\n\n\n\nMascot: Towards Multi-Agent Socio-Collaborative Companion Systems\n\n\nYiyang Wang1, Yiqiao Jin1, Alex Cabral1\n\n‚ÄÉ‚ÄÉ\nJosiah Hester1\n1Georgia Institute of Technology \n1ywang3420@gatech.edu\n\n\n\nAbstract\nMulti-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse‚Äìwhere agents revert to generic, homogenized assistant behaviors‚Äìand social sycophancy, which produces redundant, non-constructive dialogue.\nWe propose Mascot, a generalizable framework for multi-perspective socio-collaborative companions.\nMascot introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse.\nExtensive evaluations across psychological support and workplace domains demonstrate that Mascot significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.\n\n\n\nMascot: Towards Multi-Agent Socio-Collaborative Companion Systems\n\n\n\nYiyang Wang1, Yiqiao Jin1, Alex Cabral1, ‚ÄÇand‚ÄÇJosiah Hester1\n\n1Georgia Institute of Technology\n\n1ywang3420@gatech.edu\n\n\n\n\n\n1 Introduction\n\nFigure 1: Moving beyond dyadic interactions to multi-agent socio-collaborative companion systems. While single-agent support systems (left) can result in limited insights and echo chambers, multi-agent systems (right) provide diverse, balanced perspectives and foster a shared sense of community through agents with distinct roles.\n\n\nSocial interaction is a fundamental pillar of psychological well-being, cognitive resilience, and enhanced problem solving¬†CDC (2024b); Sahakian and Langley (2025).\nDespite its necessity, social deficits such as isolation and loneliness have reached global crisis proportions, affecting an estimated one in six people worldwide and one in four in the United States\n¬†World Health Organization (2025); CDC (2024a).\nAs large language models (LLMs) integrate deeper into human lives, their role is shifting from passive tools (e.g., search engines, code generators) to active socio-collaborative companions in affective and collaborative settings¬†Liu et¬†al. (2021).\nIn affective contexts, such as empathetic dialogue, socio-collaborative companions serve as psychological anchors, offering emotional support to reduce distress, loneliness, and anxiety.\nThey provide a safe, always-available, and non-judgmental space for users to vent and process complex emotions. In collaborative or workplace contexts, companions act as cognitive scaffolds that facilitate information synthesis and collective decision-making to stimulate problem-solving. Ultimately, performing tasks that mitigate social deficits allows these models to evolve into adaptive partners capable of navigating the nuanced boundary between emotional resonance and task-oriented efficiency.\n\n\nWhile existing LLM systems primarily focus on dyadic (one-to-one) interactions, a single persona often struggles to balance the conflicting roles of mentor, friend, and assistant without suffering a cognitive and emotional mismatch.\nMulti-agent frameworks resolve this by distributing these roles across an ecosystem of specialized agents. This triggers the ‚Äòaudience effect‚Äô Sutskova et¬†al. (2023), where a user‚Äôs trust is reinforced by observing independent social behaviors among agents, fostering a shared sense of community that a solitary interface cannot replicate¬†Zhu et¬†al. (2025).\n\n\nChallenges\n\nDespite recent progress in multi-agent systems, two primary challenges remain in developing effective multi-perspective socio-collaborative companions:\n1) Persona Fidelity. At the individual agent level, models struggle to maintain stable, unique character traits¬†Wen et¬†al. (2024). Agents often succumb to persona collapse¬†Chang et¬†al. (2024), regressing to generic, homogenized assistant behaviors that lack unique perspectives. 2) Interaction Synergy.\nAt the collective level, uncoordinated agents fail to generate complementary, non-redundant contributions.\nThis manifests as repeating communications or even Social Sycophancy (the ‚ÄòYes-Man‚Äô Bias), where agents prioritize agreeableness, creating echo chambers rather than constructive conversations.\nFurthermore, agents often struggle with Long-Horizon State Consistency, treating each turn as a local optimization problem and losing track of the conversation‚Äôs logical arc, such as re-litigating settled topics¬†Chan et¬†al. (2024).\n\n\n\nThis Work\n\nWe propose Mascot, a generalizable Multi-Agent framework for developing multi-perspective Socio-COllaboraTive companions.\nUnlike previous multi-agent systems optimized solely for task efficiency, Mascot targets user-agent interaction quality, explicitly balancing individual agent persona consistency with global discourse dynamics.\nWe introduce an efficient bi-level optimization strategy including: 1) a Reinforcement-Learning-from-AI-Feedback (RLAIF)-driven pipeline that finetunes individual agents for strict Persona Fidelity, and 2) a meta-agent policy guided by group-level rewards to ensure Interaction Synergy.\nThis enables the efficient development of multi-perspective socio-collaborative companions across multiple applications, such as the orchestration of digital social support groups and the facilitation of iterative knowledge synthesis and collective ideation.\n\n\n\n\nContributions\n\nOur contributions are as follows:\n\n\n‚Ä¢\n\nUnified Framework. We propose Mascot, a generalizable multi-agent framework that enables socio-collaborative companions in multi-perspective group conversations.\n\n\n\n‚Ä¢\n\nBi-level Optimization. We propose a low-resource, bi-level optimization pipeline that harmonizes individual persona adherence with collective interaction synergy.\n\n\n\n‚Ä¢\n\nExtensive Evaluation. We conduct extensive evaluations across multiple domains, such as emotional support (empathetic conversations) and workplace settings (business and academic meetings), showing the superior performance of our system, such as a maximum of +14.1 in Persona Consistency and +10.6 in Social Contribution.\n\n\n\n\n\n\n\n\n2 Method\n\nPreliminary\n\nWe formalize a socio-collaborative companion system as a set of agents ùíú={ai}i=1|ùíú|\\mathcal{A}=\\{a_{i}\\}_{i=1}^{|\\mathcal{A}|}. Each agent aia_{i} is conditioned on a distinct persona profile œÅi\\rho_{i} that specifies traits such as linguistic style, domain expertise, and emotional disposition (e.g. Empathetic Listener, Critical Thinker, Emotional Validator, Action Guide, and Growth Advocate). Given each user interaction context xx (e.g., a user‚Äôs emotional disclosure or a dialog segment),\nthe system generates a multi-turn, multi-perspective conversation trajectory ùêò=(y1,y2,‚Ä¶,yT)\\mathbf{Y}=(y_{1},y_{2},\\ldots,y_{T}).\nMascot (Figure¬†2) optimizes this process via a two-phase training pipeline: (1) Persona-Aware Behavioral Alignment and (2) Collaborative Dialogue Optimization.\n\n\n\n\n2.1 Persona-Aware Behavioral Alignment\n\nCurrent multi-agent frameworks often suffer from persona collapse¬†Chang et¬†al. (2024), where agents regress to generic assistant behaviors rather than adhering to their assigned roles.\nWhile zero-shot or few-shot prompting¬†Wang et¬†al. (2020) are often used to address this issue, they frequently fail to maintain consistency, particularly in smaller open-source models, yielding high variance in persona adherence.\nTo prevent persona collapse and ensure adherence to social-emotional traits, we move beyond inference-time prompting and explicitly align a base policy œÄŒ∏\\pi_{\\theta} by leveraging a persona-oriented reward model within a Reinforcement Learning from AI Feedback (RLAIF) framework¬†Bai et¬†al. (2022); Lee et¬†al. (2024).\n\n\nFigure 2: Overview of Mascot for multi-agent socio-collaborative companions. Mascot produces synergistic multi-agent dialogues that maintain distinct personas while collectively supporting the user. It leverages a two-phase optimization pipeline. In Persona-Aware Behavioral Alignment, individual agents are fine-tuned via RLAIF with a learned reward model to ensure stable persona fidelity and high-quality responses. In Collaborative Dialogue Optimization, a meta-agent (director) coordinates multiple speaker agents through directive generation, optimizing group-level rewards for coherence, diversity, and non-redundant contributions. \n\n\nPersona Reward Modeling\n\nEffective training of persona adherence requires robust signals. Thus, Mascot uses expert models as strong teacher models to guide the behaviors of student models with lower latency.\nTo construct the preference dataset ùíü\\mathcal{D}, we start with a supervised ba"
  },
  {
    "title": "Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment",
    "url": "https://arxiv.org/abs/2601.14228v1",
    "source": "arxiv",
    "summary": "Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups up",
    "full_text": "\n\n\n\nI Introduction\n\nII Background and Literature Review\n\nII-1 Sepsis as a Sequential Decision Problem\nII-2 Challenges in Modeling Sepsis from ICU Data\nII-3 Integration of Language Models for Interpretability\n\n\n\nIII Interpretable Sepsis Treatment Methodology\n\nIII-A Risk Stratification via Clustering\nIII-B Synthetic Data Generation\n\nIII-C Ensemble RL-Based Treatment Agent\n\nObservation Space\nAction Space\nReward Function\n\n\nIII-D LLM-Based Rationale Generation\n\n\n\nIV Evaluation Results &amp; Analysis\n\nIV-A Datasets\n\nIV-B Latent Space Preparation\n\nTemporal Filtering and Preprocessing\nDimensionality Reduction and Clustering\n\n\nIV-C Reinforcement Learning Experiments\n\n\nV Conclusion\n\n\n\n\n\nAttention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment\n\n\nPunit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, and Alina Vereshchaka\n\n\n\nAbstract\nSepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.\n\n\n\nI Introduction\n\n\nSepsis is a life-threatening medical emergency characterized by a dysregulated host response to infection, leading to acute organ dysfunction and high risk¬†[21]. In-hospital risk rates range from approximately 10‚àí30%10-30\\%, and can exceed 40%40\\% in cases of septic shock, underscoring the profound lethality of this condition¬†[19]. In the United States, sepsis affects over 1.71.7 million adults annually and accounts for more than a third of hospital deaths, imposing a substantial human and economic burden¬†[22]. The rapid progression of sepsis and its heterogeneous presentation across diverse patient populations underscore the urgent need for early detection, precise diagnosis, and timely intervention to improve patient outcomes¬†[4].\n\n\nRecent advances in large-scale critical care databases such as MIMIC-III¬†[6] and eICU¬†[15], combined with machine learning (ML) and reinforcement learning (RL) methods, offer promising tools for personalized treatment planning. While supervised models have shown success in tasks like risk stratification and mortality prediction, RL provides a framework for learning sequential decision policies from historical data¬†[9, 10]. Offline RL, in particular, is well-suited for clinical settings, where real-time exploration is neither feasible nor ethical.\n\n\nDespite increasing interest in applying RL to ICU settings, most work remains focused on supervised risk scoring or binary classification. Effective RL models must not only learn optimal policies from logged data but also support generalization to out-of-distribution states and produce reliable, interpretable outputs for clinical use.\n\n\nIn this work we present a framework for personalized treatment through RL, synthetic data generation, and language model-based rationale generation. Our main contributions include:\n\n\n1.\n\nWe developed an interpretable offline RL pipeline that combines Advantage-Weighted Regression (AWR) with a simplified attention mechanism. This output is then combined with the ensemble predictions from XGBoost and TabNet to improve learning stability and treatment accuracy while maintaining interpretability.\n\n\n\n2.\n\nWe utilize clustering-driven stratification to group patients by risk using HDBSCAN [12], which allows us to handle the cold-start problem for the patients with limited or no ICU history by assigning them to similar historical trajectories.\n\n\n\n3.\n\nTo address class imbalance and data sparsity in critical interventions (e.g., vasopressors), we augment the dataset with synthetic trajectories generated via a diffusion model and a conditional VAE.\n\n\n\n4.\n\nWe integrate a multi-modal large language model (LLM) into the inference pipeline to generate contextual, patient-specific rationales for selected actions. The model combines current vitals, retrieved clinical knowledge, and RL outputs to support explainable decision-making.\n\n\n\n\n\nFigure 1: Overview of the interpretable sepsis treatment pipeline. (A) Patients without prior ICU history are stratified into low-, intermediate-, or high-risk groups using clustering. (B) To address data sparsity, synthetic transitions (s,a,r,s‚Ä≤,d)(s,a,r,s^{\\prime},d) are generated using a VAE and a diffusion model, then added to the RL training set. (C) For intermediate-risk or historical patients, a feature-attention encoder produces a latent state z=Attn‚Äã(s)z=\\mathrm{Attn}(s), used by an AWR policy œÄœï‚Äã(a‚à£z)\\pi_{\\phi}(a\\mid z), a Q-network, and a value function. The final recommendation a‚Ä≤=arg‚Å°maxa‚Å°[blend‚Äã(œÄœï,œÄXGB)]a^{\\prime}=\\arg\\max_{a}[\\text{blend}(\\pi_{\\phi},\\pi_{\\text{XGB}})] combines outputs from AWR and a clinician-trained XGBoost policy. (D) To enhance interpretability, a local LLM generates a natural-language rationale r=f‚Äã(s,a‚Ä≤,context)r=f(s,a^{\\prime},\\text{context}) using the patient state, selected action, and retrieved clinical context.\n\n\n\n\nII Background and Literature Review\n\n\n\nII-1 Sepsis as a Sequential Decision Problem\n\nThe diagnostic and therapeutic challenges in sepsis, characterized by hidden disease states and incomplete observability of the underlying pathophysiology, allow us to formulate it as a Partially Observable Markov Decision Process (POMDP). The patient‚Äôs physiological state evolves in response to administered treatments (e.g., vasopressors, fluids) and latent disease progression. Clinical guidelines emphasize the need for timely intervention, where delays in antibiotic or fluid administration substantially increase risk¬†[18, 4].\n\n\n\n\nII-2 Challenges in Modeling Sepsis from ICU Data\n\nTraining RL agents in healthcare is constrained by the absence of an online environment and the inability to perform exploration. Offline RL algorithms address this by learning from historical data while correcting for the distributional mismatch between the behavior and the learned policy. However, the quality of the learned policy is closely coupled with state representation and reward design.\n\n\nICU datasets such as MIMIC-III [6] and eICU [15] offer time-stamped, high-resolution records of patient vitals, lab tests, interventions, and outcomes. Yet, they reflect evolving clinical standards, e.g., the definition of sepsis changed mid-decade to emphasize organ dysfunction over simple infection markers[20]. This requires harmonization techniques, such as clustering-based cohort construction and dimensionality reduction (e.g., UMAP[13], HDBSCAN [12]), to ensure valid cross-temporal comparisons and consistent reward attribution.\n\n\n\n\nII-3 Integration of Language Models for Interpretability\n\nFor AI systems to be useful in clinical practice, they need to explain their reasoning in a way that clinicians can trust. Attention mechanisms offer insights by highlighting which features are important, but they often fall short of providing clear justifications. Recently, LLMs, especially those with multi-modal inputs, have made it possible to generate natural-language explanations grounded in both patient data and clinical knowledge¬†[5, 7].\n\n\n\n\n\nIII Interpretable Sepsis Treatment Methodology\n\n\nOur methodology integrates patient risk stratification (Sec.¬†III-A), synthetic data augmentation (Sec.¬†III-B), offline reinforcement learning (Sec.¬†III-C), and LLM-based interpretability (Sec.¬†III-D) to develop a transparent and data-driven sepsis treatment policy. The full pipeline is illustrated in Fig.¬†1.\n\n\n\nIII-A Risk Stratification via Clustering\n\n\nNewly admitted patients often lack sufficient ICU history, making it challenging to apply downstream RL and LLM modules that rely on longitudinal data. To address this, we use unsupervised clustering to assess patient status upon admission, grouping them into risk categories based on initial vitals and lab measurements. Clustering helps identify patients according to their risk stratification. Prior to RL training, patient states are clustered to identify distinct sepsis progression patterns, and recent advances in clustering efficiency, such as the centroid update approach by Borlea et al. [2], demonstrate significant reductions in computational iterations while maintaining clustering quality, a critical consideration for large-scale patient data preprocessing in clinical settings. We utilize Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) [12] and Uniform Manifold Approximation and Projection (UMAP) [13] as our core unsupervised clustering and visualization tools. This combination confers several key advantages over conventional clustering algorithms within the medical domain. Specifically, it accommodates variable cluster densities, enables automatic cluster detection, demonstrates robustness to noise and outliers, and is scalable for real-time deployment.\n\n\nThis process broadly categorizes patients into the following:\n\n\n1.\n\nLow-risk [0%,40%][0\\%,40\\%]: Patients with stable vitals and a good recover"
  },
  {
    "title": "Deep Learning Approaches to Quantum Error Mitigation",
    "url": "https://arxiv.org/abs/2601.14226v1",
    "source": "arxiv",
    "summary": "We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our d",
    "full_text": null
  },
  {
    "title": "Generalization and Completeness of Stochastic Local Search Algorithms",
    "url": "https://arxiv.org/abs/2601.14212v1",
    "source": "arxiv",
    "summary": "We generalize Stochastic Local Search (SLS) heuristics into a unique formal model. This model has two key components: a common structure designed to be as large as possible and a parametric structure intended to be as small as possible. Each heuristic is obtained by instantiating the parametric part in a different way. Particular instances for Genetic Algorithms (GA), Ant Colony Optimization (ACO)",
    "full_text": null
  },
  {
    "title": "HALT: Hallucination Assessment via Latent Testing",
    "url": "https://arxiv.org/abs/2601.14210v1",
    "source": "arxiv",
    "summary": "Hallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methodology\n\nProblem Definition\nFeature Extraction from LLM\nTraining Objective\nNetwork Architecture (Hallucination Detector)\n\n\n\n4 Experiments\n\nDatasets and models\nEvaluation Metrics\nBaselines\nCorrectness labeling\n4.1 Main Results\n4.2 Accuracy after Hallucination Removed\n\n\n\n5 Ablation Studies\n\n5.1 Effect of Intermediate Layers on Performance\n5.2 Effect of Model Size on Ranking Quality\n5.3 Effect of Detector Architecture\n5.4 Effect of Number of Answer Tokens Used\n\n\n\n6 Discussion\n\nLimitations\nScaling Trends\nFuture Directions\n\n\nA Correctness Evaluation Prompt\nB Dataset Statistics\nC Ablation Study 2: Layer Ablation on Additional Models\n\n\n\n\n\nHALT: Hallucination Assessment via Latent Testing\n\n\nRohan Bhatnagar\n\n‚ÄÉ‚ÄÉ\nYouran Sun\n\n‚ÄÉ‚ÄÉ\nChi Andrew Zhang\n\n‚ÄÉ‚ÄÉ\nYixin Wen\n\n‚ÄÉ‚ÄÉ\nHaizhao Yang\n\n\n\nAbstract\nHallucination in large language models (LLMs) can be understood as a failure of faithful readout: although internal representations may encode uncertainty about a query, decoding pressures still yield a fluent answer. We propose lightweight residual probes that read hallucination risk directly from intermediate hidden states of question tokens, motivated by the hypothesis that these layers retain epistemic signals that are attenuated in the final decoding stage. The probe is a small auxiliary network whose computation is orders of magnitude cheaper than token generation and can be evaluated fully in parallel with inference, enabling near-instantaneous hallucination risk estimation with effectively zero added latency in low-risk cases. We deploy the probe as an agentic critic for fast selective generation and routing, allowing LLMs to immediately answer confident queries while delegating uncertain ones to stronger verification pipelines. Across four QA benchmarks and multiple LLM families, the method achieves strong AUROC and AURAC, generalizes under dataset shift, and reveals interpretable structure in intermediate representations, positioning fast internal uncertainty readout as a principled foundation for reliable agentic AI.\n\nHallucination Detection, Large Language Models, Uncertainty Estimation\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have made remarkable progress recently (Yang et al., 2025; Comanici et al., 2025) and are being applied in an increasing number of real-world scenarios.\nHowever, hallucination, which refers to situations where a model produces information that appears plausible but is actually false or not supported by facts, undermines users‚Äô trust in LLMs and limits their use in critical settings.\nTherefore, developing effective methods for hallucination detection is critical.\nIdeally, such methods should maintain real-time responsiveness without increasing response latency or significantly increasing generation costs.\n\n\nWe propose a lightweight hallucination-detection method based on question intermediate representations that predicts hallucination risk efficiently and accurately.\nThe additional computation introduced by our method is less than 1‚Ä∞ of that required to generate a single token, so our approach adds negligible cost to generation.\nMoreover, the detector can be evaluated in parallel with inference, introducing no extra latency to the user.\nIn essence, we aim to build a ‚Äúlie detector‚Äù or ‚Äúmind reader‚Äù for LLMs.\n\n\nWith an effective hallucination detector, we can build an LLM router that adaptively allocates computation.\nGiven a user query, the default LLM begins generating a response while our detector estimates the hallucination risk in parallel.\nBecause the detector only requires the question‚Äôs intermediate representations, it can run concurrently with generation.\nIf the predicted risk is low, the system returns the generated response as usual; otherwise, it routes the query to a slower but more reliable pipeline (e.g., a stronger model, reasoning-augmented generation, cross-model verification, or retrieval-augmented generation (RAG)).\nIn the low-risk case, this design introduces zero additional latency; in the fallback case, the extra delay is less than the time to generate a single token.\n\n\nFigure 1: Comparison of hallucination handling strategies. Traditional detection pipelines wait for generation to complete before checking for hallucinations, doubling latency in the fallback case. Our method evaluates hallucination risk in parallel with generation, enabling zero-latency responses for confident queries while routing uncertain ones to stronger models.\n\n\nOur motivation follows naturally from recent evidence that LLMs perform substantial hidden consideration during forward propagation.\nPrior work (Lindsey et al., 2025; Chen et al., 2025) suggests that intermediate representations encode reasoning, planning, and control signals that guide generation, yet these signals may not be faithfully expressed in the final text.\nThis motivates directly reading out hallucination-related signals from intermediate representations, rather than relying on generated results.\n\n\nBased on the above observations, we hypothesize that intermediate representations encode uncertainty signals.\nThis is analogous to how students taking an exam can sense ‚ÄúI don‚Äôt know this‚Äù for a question, yet would never write that on the answer sheet.\nLLMs exhibit a similar pattern because they are trained to provide an answer whenever possible.\nThis motivates us to build a small detector that captures this internal sense of confusion.\nMoreover, this analogy suggests that intermediate layers should be more informative than the final output, just as the internal feeling differs from what appears on the answer sheet.\n\n\nIn our experiments, we indeed find that using representations from intermediate layers is often more effective than using the final-layer representations.\nWe attribute this to the fact that the final layer has already been decoded into the token space, leading to significant information loss.\nIn other words, the model contains internal features related to confidence or hallucination, but these features are unnecessary for output generation, so they may be discarded in the last few layers.\n\n\nFurthermore, we only use the representations aligned to the question as input to the detector.\nThis choice is driven by the need to balance detection quality and response latency.\nIn our experiments, we find that question-only representations are already sufficient for accurate detection.\nIncluding answer representations yields a marginal improvement but incurs substantial latency overhead.\n\n\nOur main contributions are as follows.\n\n\n‚Ä¢\n\nWe propose a lightweight hallucination detection method that incurs negligible computational overhead and can be evaluated in parallel with inference.\n\n\n\n‚Ä¢\n\nBuilding on this detector, we develop a pipeline that routes queries with high hallucination risk to stronger models to improve generation quality without increasing response latency or token generation cost.\nPreliminary experiments show that this pipeline improves answer correctness by xxx.\n\n\n\n‚Ä¢\n\nWe conduct extensive ablations and identify several properties of intermediate representations.\n(a) Intermediate-layer representations are more effective than the final layer for hallucination detection.\n(b) While question-aligned representations are often sufficient for accurate risk estimation, including answer tokens improves our method.\n\n\n\n\n\n\n\n2 Related Work\n\nTable 1: Summary of hallucination detection methods.\n\n\n\nMethod\nSampling\nFeatures Used\nQ/A Features Used\n\n\n\n\nPerplexity\nNo Need\nOutput Logits\nAnswer\n\n\nSemantic Entropy (Farquhar et al., 2024)\n\nNeed\nOutput\nAnswer\n\n\nLexical Similarity (Lin et al., 2024)\n\nNeed\nOutput\nAnswer\n\n\nSelfcheckGPT (Manakul et al., 2023)\n\nNeed\nOutput\nAnswer\n\n\nEigenScore (Chen et al., 2024)\n\nNeed\nMiddle Hidden States\nAnswer\n\n\nP(I Know) (Kadavath et al., 2022)\n\nNo Need\nLast Hidden State\nQuestion\n\n\nTrue Direction (B√ºrger et al., 2024)\n\nNo Need\nLast Hidden State\nAnswer\n\n\nHaloScope (Du et al., 2024)\n\nNo Need\nMiddle Hidden States\nAnswer\n\n\nHARP (Hu et al., 2025)\n\nNo Need\nLast Hidden State\nAnswer\n\n\nOurs\nNo Need\nMiddle Hidden States\nQuestion\n\n\n\n\n\nThe perplexity of an LLM‚Äôs answer can itself serve as an indicator for hallucination detection, but as shown in (Ren et al., 2023), it is unreliable.\n(Kuhn et al., 2023) and (Farquhar et al., 2024) propose semantic entropy. They require the LLM to generate multiple answers to a given question, then cluster them, and determine the hallucination likelihood based on the entropy of the clusters. Higher semantic entropy indicates a higher probability of hallucination.\nSimilarly, (Lin et al., 2024), (Manakul et al., 2023), and (Chen et al., 2024) detect hallucinations based on consistency over multiple sampled answers.\n\n\nHowever, a practical hallucination detector should require much less computation, at least smaller than the cost of generating an answer with the LLM.\n(Kadavath et al., 2022) trained a classifier that uses the last hidden state of the question to predict whether the model knows the answer, i.e., P‚Äã(I know)P(\\text{I know}).\n(B√ºrger et al., 2024) used the last hidden state of the answer to learn, via linear regression, two directions, the True direction and the False direction, and performed hallucination detection within this two-dimensional subspace.\n\n\nBased on the above methods, HARP (Hu et al., 2025) and HaloScope (Du et al., 2024) employ SVD to project the last or intermediate hidden states corresponding to the answer tokens into a low-dimensional subspace, after which a two-layer MLP is used to regress the hallucination score.\nAs a follow-up to HaloScope, (Park et al., 2025) attempts to identify hallucinations by modifying the intermediate representations of the LLM and then classifying the last hidden state.\n\n\n\n\n3 Methodology\n\nProblem Definition\n\nWe formulate hallucination detection as a knowledge prediction task.\nGiven a user query qq, our goal is to estimate whether the LLM contains the correct answer before or during generation.\nDuring the for"
  },
  {
    "title": "InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning",
    "url": "https://arxiv.org/abs/2601.14209v1",
    "source": "arxiv",
    "summary": "Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in faile",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries, Notation, and Problem Statement\n\n3 Credit Assignment via Self-Proposed Interventions\n\n3.1 Proposing Interventions through Self-Verification\n3.2 Do Interventions Improve Success Rates?\n\n\n\n4 InT: Intervention Training for Credit Assignment\n\n4.1 Training via Supervised Fine-Tuning on Intervention Data\n4.2 Fine-Tuning via Reinforcement Learning\n\n\n\n5 Experimental Evaluation of InT\n\n5.1 Experimental Setup, Comparisons, and Evaluation Metrics\n5.2 Results: SFT with Interventions\n5.3 Results: InT + RL Solves Most Hard Training Problems\n5.4 Results: InT Expands the pass@kk Frontier on Test Problems\n5.5 Results: Performance on Standardized Benchmarks\n\n\n6 Related Work\n7 Discussion and Perspectives on Future Work\nA Prompts\nB Intervention Examples\nC Evaluation Configuration\nD Training Hyperparameters for InT\nE Self-Reflection Baseline\nF Are Interventions Memorized?\n\nG InT with Interventions From a Stronger Model (Gemini 2.5 Pro)\n\nG.1 Inference-Only Results\nG.2 Experimental Setup and Evaluation Metrics\nG.3 InT uniformly pushes the pass@kk frontier upwards on test problems\nG.4 InT outperforms distillation on standardized evaluations\n\n\n\n\n\n\n\n\nspacing=nonfrench\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\correspondingauthormatthew.y.r.yang@gmail.com; This work was done at CMU.\nProject website: https://intervention-training.github.io/.\n\n\nInT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning\n\n\nMatthew Y. R. Yang\n\nCarnegie Mellon University\n\n\nHao Bai\n\nUniversity of Illinois Urbana-Champaign\n\n\nIan Wu\n\nCarnegie Mellon University\n\n\nGene Yang\n\nCarnegie Mellon University\n\n\nAmrith Setlur\n\nCarnegie Mellon University\n\n\nAviral Kumar\n\nCarnegie Mellon University\n\n\n\nFigure 1: Intervention training (InT) for improving credit assignment. InT proposes single-step interventions to replace incorrect intermediate steps in reasoning traces (1). Conditioned on these localized corrections, the model can generate counterfactual continuations that succeed where the original failed (2). We then perform supervised fine-tuning on these interventions, enabling effective credit assignment by upweighting the likelihood of the interventions in place of the mistakes.\n\n\n\\absfont\nAbstract: Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward.\nUsing reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.\n\n\n\n1 Introduction\n\nPost-training large language models (LLMs) with reinforcement learning (RL) has proven to be a highly effective strategy for improving their reasoning capabilities. As RL is scaled to more challenging tasks, training becomes increasingly dominated by incorrect rollouts, which are often long-horizon trajectories composed of many smaller, structured reasoning steps. Many such rollouts contain substantial portions of correct reasoning, yet outcome-based RL treats these correct steps the same as the critical mistakes that cause failure.\nThis indiscriminate penalty can lead to undesirable behaviors including increased verbosity or frequent, premature shifts in the reasoning process, which have been widely observed when applying outcome-reward RL [chen2025think23overthinkingo1like, wang2025thoughtsplaceunderthinkingo1like].\nMoreover, on sufficiently difficult problems where no correct rollout is produced at all, outcome-reward RL provides no learning signals, as advantages collapse to zero.\n\n\nIn principle, addressing this challenge requires solving the problem of credit assignment: if we could identify the intermediate step at which a reasoning trace goes astray and selectively penalize that step while reinforcing other possible promising steps, we could drive the model toward more favorable outcomes. However, existing approaches to credit assignment are difficult to apply in the LLM setting. For reasoning traces, estimating the ‚Äúcredit‚Äù (i.e., value) of each step typically requires running multiple branched rollouts conditioned on a given prefix, which is prohibitively expensive [kazemnejad2024vineppo, luo2024improvemathematicalreasoninglanguage, kim2025scalingevaluationtimecomputereasoning]. Prior work attempts to amortize this process by training explicit value functions (or process reward models) [setlur2024rewarding, luo2024improvemathematicalreasoninglanguage], but how we should train such functions over long reasoning traces remains an open question. Moreover, even with a reasonably accurate value function, optimizing it to identify an alternate step is itself challenging [wang2025hierarchicalmultisteprewardmodels, zhang2025lessonsdevelopingprocessreward], because of the vast space of possible future steps.\n\n\nHow can we perform credit assignment without training a value function? Rather than explicitly learning a value function, we ask the model itself to identify how failed trajectories can be locally corrected. This approach relies on an LLM‚Äôs ability to implicitly perform value estimation and optimization jointly, in order to propose an alternative, improved step, which we refer to as a corrective intervention.\nOur main idea is to reduce the task of generating end-to-end solutions to the substantially easier task of verifying individual steps, which can be accomplished via textual comparison against a reference solution (i.e., a textual ‚Äúdiff‚Äù). In our method, we instruct a base model to analyze the differences between a reference solution and an incorrect reasoning trace that it previously generated, with the goal of identifying the first point at which the trace goes wrong and proposing a single-step corrective intervention. When conditioned on this intervention, the model can then generate counterfactual reasoning traces that succeed from right before the point of failure even when the original rollout failed.\n\n\nWe then train the model to internalize these interventions by applying supervised fine-tuning (SFT) on them, followed by RL. By fine-tuning the model on targeted single-step interventions, we selectively reduce the likelihood of the incorrect reasoning steps that the base model would otherwise produce, shifting it toward more favorable steps instead. Our approach remains notably simple and computationally efficient, as it avoids branched rollouts, explicit value-function training, or modifying the RL objective to include step-level rewards. Throughout this procedure, we never rely on a larger model; instead, we leverage the asymmetry [setlur2025e3learningexploreenables] in task difficulty between instruction-following, verification, and generation within the same model to perform credit assignment. We call this approach Intervention Training (InT).\n\n\nWe compare InT to several alternative approaches that leverage reference solutions for learning, including methods that train policies directly on reference solutions or on self-generated reflections conditioned on those solutions. In settings where the base model is already strong (e.g., Qwen3 for mathematics) and the SFT dataset is relatively small (‚âà\\approx1K examples), we observe that InT makes particularly effective use of reference solutions to improve pass@kk performance, which sets up the fine-tuned model for online RL.\nNotably, this coincides with InT producing more ‚Äúon-policy‚Äù trajectories (those with high likelihood under the base model), whereas other approaches tend to produce more ‚Äúoff-policy‚Äù ones. After online RL, InT improves performance by nearly 10% on average across four challenging mathematical reasoning benchmarks, including a ‚àº\\sim14% gain on IMO-AnswerBench [luong-etal-2025-towards],\nwhich consists of IMO-level problems curated by former medalists. These results demonstrate InT as a simple and effective paradigm for improving the reasoning capabilities of LLMs, through improved credit assignment.\n\n\n\n\n2 Preliminaries, Notation, and Problem Statement\n\nLearning objective. Training LLMs for reasoning typically involves an LLM œÄ\\pi, model-generated reasoning traces ùê≤‚àºœÄ(‚ãÖ|ùê±)\\mathbf{y}\\sim\\pi(\\cdot|\\mathbf{x}) produced on input prompts ùê±‚àºœÅ\\mathbf{x}\\sim\\rho, and a binary reward r‚Äã(ùê±,ùê≤)‚àà{0,1}r(\\mathbf{x},\\mathbf{y})\\in\\{0,1\\} quantifying the correctness of the final answer.\nThe goal is to maximize average reward, given by:\n\n\n\nmaxœÄJ‚Äã(œÄ):=ùîºùê±‚àºœÅ,ùê≤‚àºœÄ(‚ãÖ|ùê±)‚Äã[r‚Äã(ùê±,ùê≤)].\\max_{\\pi}\\penalty 10000\\ \\penalty 10000\\ J(\\pi):=\\mathbb{E}_{\\mathbf{x}\\sim\\rho,\\mathbf{y}\\sim\\pi(\\cdot|\\mathbf{x})}[r(\\mathbf{x},\\mathbf{y})].\n\n\n\nWe can maximize"
  },
  {
    "title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting",
    "url": "https://arxiv.org/abs/2601.14208v1",
    "source": "arxiv",
    "summary": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model",
    "full_text": null
  },
  {
    "title": "Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery",
    "url": "https://arxiv.org/abs/2601.14196v1",
    "source": "arxiv",
    "summary": "Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from deli",
    "full_text": null
  },
  {
    "title": "Toward Efficient Agents: Memory, Tool learning, and Planning",
    "url": "https://arxiv.org/abs/2601.14192v1",
    "source": "arxiv",
    "summary": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latenc",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.14192v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2601.14192v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 20 Jan 2026]\n    Title:Toward Efficient Agents: Memory, Tool learning, and Planning\n    Authors:Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen, Jing Shao            View a PDF of the paper titled Toward Efficient Agents: Memory, Tool learning, and Planning, by Xiaofang Yang and 15 other authors\n    View PDF\n\n\n\n    \n            Abstract:Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.\n    \n\n    \n    \n              \n          Comments:\n          35 pages, 200 references\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2601.14192 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.14192v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.14192\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Lijun Li [view email]          [v1]\n        Tue, 20 Jan 2026 17:51:56 UTC (12,190 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Toward Efficient Agents: Memory, Tool learning, and Planning, by Xiaofang Yang and 15 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.CL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a projec"
  },
  {
    "title": "A model of errors in transformers",
    "url": "https://arxiv.org/abs/2601.14175v1",
    "source": "arxiv",
    "summary": "We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexi",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Previous work\n\n\n\n2 Error model\n\n2.1 Characterization of tasks\n2.2 Idealized autoregressive model\n2.3 Complexity parameter\n2.4 Effective model\n2.5 Errors\n\n\n\n3 Empirical verification\n\n3.1 List reversal\n3.2 Nested linear transformations\n3.3 Dynamic programming\n3.4 Tower of Hanoi\n3.5 Vanilla addition\n3.6 Addition with algorithm\n3.7 Binary addition\n3.8 Multiplication\n\n\n4 Improving LLM performance\n5 Summary and Discussion\n\nA Technical details for the derivation of the accuracy formula\n\n\nA.1 Architecture of the idealized model\n\nA.1.1 Embedding layer\nA.1.2 Attention and nonlinear layers\nA.1.3 Chain of Thought Tokens\n\n\nA.2 Effective model\nA.3 Scaling of the variance\nA.4 Value of Œ±\\alpha\nA.5 Calculation of accuracy\n\n\nB Discussion of Assumptions\n\nC Details about Graphs and Error bars\n\nGoodness of fit.\nC.1 Best Fit Parameters\n\nC.2 Comments about plots\n\nUnclear output.\n\n\n\n\n\nD Prompts\n\nD.1 List Reversal\nD.2 Nested Linear Transformations\nD.3 Dynamic Programming\nD.4 Tower of Hanoi\nD.5 Vanilla Addition\nD.6 Addition with Algorithm\nD.7 Binary Addition\nD.8 Multiplication\nD.9 Multiplication using intermediate polynomials\n\n\n\n\n\n\n\nA model of errors in transformers\n\n\nSuvrat Raju\n\n‚ÄÉ‚ÄÉ\nand Praneeth Netrapalli\n\n\n\nAbstract\nWe study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ‚Äúeffective field theory‚Äù perspective: the LLM‚Äôs many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ‚Äúcollapse of reasoning‚Äù¬†(Shojaee et¬†al., 2025), or an inability to express ‚Äúcompositional‚Äù functions¬†(Dziri et¬†al., 2023). Finally, we show how to construct prompts to reduce the error rate.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) are versatile systems (Brown et¬†al., 2020; Raffel et¬†al., 2020) that have shown remarkable success on a variety of benchmarks (Laskar et¬†al., 2023; Chang et¬†al., 2024). However, state of the art models still make errors even on relatively simple problems¬†(Huang et¬†al., 2025). To understand and prevent these errors is an important problem.\n\n\nWith this broad motivation, we examine errors made by LLMs on a simple class of problems. We focus on a class of deterministic tasks in which the input, the output, or both comprise a sequence of tokens, each drawn from a small set, and study the variation of the error rate with the length of the task. This class includes decimal and binary arithmetic (Yuan et¬†al., 2023; Shen et¬†al., 2023; Maltoni &amp; Ferrara, 2024; Zhang et¬†al., 2024; Shrestha et¬†al., 2025; Sun et¬†al., 2025; Bertolazzi et¬†al., 2025; Feng et¬†al., 2025), some classic dynamic programming problems (Dziri et¬†al., 2023), the tower of Hanoi (Shojaee et¬†al., 2025), list reversal, and nested application of linear transformations. These problems are not of direct practical interest since LLMs can easily write code to solve them (Chen et¬†al., 2021; Wang &amp; Chen, 2023; Jiang et¬†al., 2024; Yang et¬†al., 2024). Nevertheless they provide a clean arena where LLM errors can be identified, modeled and ameliorated.\n\n\nOur second objective is to demonstrate that LLMs can be treated like other natural systems, and techniques from the natural sciences can be utilized to study them quantitatively (Kaplan et¬†al., 2020; Allen-Zhu &amp; Li, 2024). We describe a quantitative model that predicts the accuracy of LLMs on this class of tasks, and then verify the model empirically.\n\n\nAlthough LLMs have hundreds of billions of parameters, our final result has only two parameters that vary with the prompt and the specific model. These parameters have a simple interpretation ‚Äî a small number, rr that can be related to an elementary ‚Äúnoise rate‚Äù per token, and an order-1 number, qq, that describes the number of potential ‚Äúerror directions‚Äù. We propose that the accuracy of the model, aa, varies with the complexity of the task, cc, as\n\n\n\na=1Œì‚Äã(q2)‚ÄãŒ≥‚Äã(q2,q2‚Äãr‚Äãc2‚ÄãŒ±)a={1\\over\\Gamma({q\\over 2})}\\gamma({q\\over 2},{q\\over 2rc^{2\\alpha}})\n\n(1)\n\n\nwhere Œ≥‚Äã(x,y)\\gamma(x,y) denotes the incomplete gamma function\n\n\n\nŒ≥‚Äã(x,y)=‚à´0ytx‚àí1‚Äãe‚àít‚Äãùëët,\\gamma(x,y)=\\int_{0}^{y}t^{x-1}e^{-t}dt,\n\n(2)\n\n\nŒì‚Äã(x)‚â°Œ≥‚Äã(x,‚àû)\\Gamma(x)\\equiv\\gamma(x,\\infty) is the gamma function, and we suggest fixing Œ±=1\\alpha=1.\n\n\nWe derive this formula in section 2, starting with the hypothesis that the operation of an LLM can be modeled via an effective model\nthat implements self-attention (Luong et¬†al., 2015; Bahdanau et¬†al., 2015; Vaswani et¬†al., 2017). Errors arise because the parameters of the effective model differ slightly from the parameters required to make a correct prediction. These errors accumulate across the context and lead the output of each attention layer to differ from the correct output. If the projection of the final output vector onto an incorrect token becomes significant‚Äîwhich we assume happens when this error crosses a threshold‚Äîthe model makes an incorrect prediction. A simple scaling argument, and additional assumptions about the probability distribution of the errors lead to formula (1).\n\n\nOur formula provides a remarkably accurate characterization of errors made by LLMs in extensive empirical tests comprising 8 different tasks, 3 state-of-the-art LLMs, and 0.2 million distinct prompts. However, we find systematic deviations in some cases, and our formula fails completely in one example. We use this failure to deduce the presence of additional effects that are important in this case, and construct a modified problem where these effects are insignificant, so that the formula can be applied successfully.\n\n\nThe fact that a large number of parameters reorganize themselves into a small number of effective parameters‚Äîfor the purpose of predicting errors‚Äîis reminiscent of the paradigm of ‚Äúeffective field theory‚Äù that is used in physics (Georgi, 1993; Burgess, 2020). For example, a fluid is fundamentally described by a large number of microscopic parameters. But, in simple experiments, its behaviour is captured by a small set of effective parameters such as its density and viscosity. This is analogous to what we see in our experiments.\n\n\nWe hasten to add that, in physics, the reorganization of fundamental parameters into effective parameters is understood via a mature mathematical framework (Polchinski, 1992) that we currently lack in the context of LLMs. Our investigation is only suggestive that a similar framework might be applied fruitfully to the study of large models.\n\n\n\n1.1 Previous work\n\nThe performance of LLMs on arithmetic has been studied extensively. For instance, it has been proposed that errors arise because they use ‚Äúpattern matching‚Äù rather than algorithms (Nikankin et¬†al., 2024) or due to incorrect tokenization (Singh &amp; Strouse, 2024). Experiments with dynamic programming and multiplication led to the suggestion (Dziri et¬†al., 2023) that LLMs lack the expressive power to compute compositional functions. The tower of Hanoi was studied in (Shojaee et¬†al., 2025), where it was suggested that LLM reasoning collapses beyond a threshold.\n\n\nWe do find some evidence that LLMs adopt inconsistent algorithms for tasks like arithmetic. However, our broad perspective differs from previous investigations. We hypothesize that models make errors due to their failure to implement attention precisely in tasks that involve a large number of similar tokens. The agreement of the empirically observed error rate with our quantitative formula in several cases ‚Äî including examples where algorithms are explicitly provided ‚Äî provides evidence for this hypothesis.\n\n\nThis paper is organized as follows. In section 2, we describe our model for errors. We do not provide a rigorous analysis. Rather, we present a ‚Äúphysics-style‚Äù model, which involves several assumptions motivated by empirical observations but leads to a simple final formula. In section 3, we describe extensive empirical tests of this formula in several settings. In section 4, we provide one example of how our analysis can be used to ameliorate errors. We conclude with some comments on future work in section 5. The Appendices provide technical details.\n\n\n\n\n\n2 Error model \n\nWe study tasks where the LLM is prompted in natural language, with commands and examples accompanying the data necessary for the task. The model then generates a number of intermediate tokens (Wei et¬†al., 2022) before producing a final output. We wish to abstract away from these complications, so as to arrive at a simple effective error model. We start by characterizing the class of tasks that we are considering; define an effective ‚Äúcomplexity‚Äù parameter; and then turn to our error model.\n\n\nWe make a number of assumptions, with the nth assumption identified by the label [ùêÄùêß][{\\bf{An}}]. Possible modifications of these assumptions are discussed in Appendix B.\n\n\n\n2.1 Characterization of tasks\n\nAt an abstract level, we are interested in the ability of the model to implement a deterministic mathematical function that maps a minimal sequence of input tokens ùíÆin{\\cal S}^{\\text{in}} to a minimal sequence of output tokens ùíÆout{\\cal S}^{\\text{out}}. For example, in the case of addition"
  },
  {
    "title": "Penalizing Localized Dirichlet Energies in Low Rank Tensor Products",
    "url": "https://arxiv.org/abs/2601.14173v1",
    "source": "arxiv",
    "summary": "We study low-rank tensor-product B-spline (TPBS) models for regression tasks and investigate Dirichlet energy as a measure of smoothness. We show that TPBS models admit a closed-form expression for the Dirichlet energy, and reveal scenarios where perfect interpolation is possible with exponentially small Dirichlet energy. This renders global Dirichlet energy-based regularization ineffective. To ad",
    "full_text": null
  },
  {
    "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
    "url": "https://arxiv.org/abs/2601.14172v1",
    "source": "arxiv",
    "summary": "We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.14172v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.14172v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 20 Jan 2026]\n    Title:Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum\n    Authors:V√≠ctor Yeste, Paolo Rosso            View a PDF of the paper titled Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum, by V\\&#39;ictor Yeste and 1 other authors\n    View PDF\n\n\n\n    \n            Abstract:We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (&#34;does any value appear?&#34;) and show that it is learnable from single sentences (positive-class F1 $\\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.\n    \n\n    \n    \n              \n          Comments:\n          Code: this https URL, 37 pages, 4 figures,\n        \n\n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\n                \n          ACM&nbsp;classes:\n          I.2.7\n        \n\n          Cite as:\n          arXiv:2601.14172 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.14172v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.14172\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: V√≠ctor Yeste [view email]          [v1]\n        Tue, 20 Jan 2026 17:25:33 UTC (241 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum, by V\\&#39;ictor Yeste and 1 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and sh"
  },
  {
    "title": "Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance",
    "url": "https://arxiv.org/abs/2601.14171v1",
    "source": "arxiv",
    "summary": "Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\\textbf{Rebutta",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 RebuttalAgent\n\n3.1 Manuscript and Review Structuring\n3.2 Evidence Construction\n3.3 Planning and Drafting\n\n\n\n4 RebuttalBench\n\n4.1 Evaluation Dataset\n4.2 Evaluation Metrics\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n5.3 Ablation Study\n5.4 Case Study\n\n\n6 Conclusion\nA Evaluation Dataset\n\nB Evaluation Metric\n\nDimensions and weights.\nRelevance (R-Score).\nArgumentation (A-Score).\nCommunication (C-Score).\nScoring protocol.\nAggregation.\n\n\n\nC Related Works\n\nAutomatic Scientific Research.\n\n\n\nD Prompt Templates\n\nWhen search is required:\nWhen search is not required:\n\n\nE Case Study\n\n\n\n\n\nPaper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance\n\n\n\nQianli Ma111Equal Contribution. ‚ÄÉChang Guo111Equal Contribution. ‚ÄÉZhiheng Tian111Equal Contribution. ‚ÄÉSiyu Wang‚ÄÉJipeng Xiao‚ÄÉ\nYuanhao Yue ‚ÄÉZhipeng Zhang222Corresponding Author.\nAutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University \n{mqlqianli,zhipengzhang}@sjtu.edu.cn\n\nProject Page: https://Paper2Rebuttal.github.io\nHF Demo: https://huggingface.co/spaces/RebuttalAgent\n\n\n\n\nAbstract\nWriting effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce RebuttalAgent, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, RebuttalAgent ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed RebuttalBench and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.\n\n\n\nPaper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance\n\n\n\n\nQianli Ma111Equal Contribution. ‚ÄÉ‚ÄäChang Guo111Equal Contribution. ‚ÄÉ‚ÄäZhiheng Tian111Equal Contribution. ‚ÄÉ‚ÄäSiyu Wang‚ÄÉ‚ÄäJipeng Xiao\n\nYuanhao Yue ‚ÄÉZhipeng Zhang222Corresponding Author.\n\nAutoLab, School of Artificial Intelligence, Shanghai Jiao Tong University\n\n{mqlqianli,zhipengzhang}@sjtu.edu.cn\n\nProject Page: https://Paper2Rebuttal.github.io\n\nHF Demo: https://huggingface.co/spaces/RebuttalAgent\n\n\n\n\n\n1 Introduction\n\nThe rebuttal phase represents a decisive juncture in the peer review lifecycle where authors must address critiques through evidence-backed clarifications and actionable manuscript revisions. This undertaking extends far beyond simple textual composition. It requires a rigorous synthesis process in which authors must accurately decipher reviewer intent while ensuring every response is firmly anchored in verifiable manuscript details. The inherent difficulty of this multi-step reasoning is amplified by the strict turnaround windows typical of top-tier venues. Authors are frequently forced to reconcile the need for meticulous verification with urgent deadlines, leaving little room for hallucination or ambiguity.\n\n\nFigure 1: Overview of our work. Given a manuscript and reviews, (a) direct text generation (SFT on peer-review corpora) often fabricates experiment results and prone to hallucination. (b) Interactive prompting with chat-LLMs depends on manual concern feeding and many iterations. (c) RebuttalAgent reframes rebuttal writing as a decision-and-evidence organization problem, performing concern breakdown, query-conditioned internal and external evidence construction, and strategy-level plan verification with human-in-the-loop checkpoints before drafting the final response.\n\n\nIn response to these intense cognitive and temporal demands, Large Language Models (LLMs) have emerged as promising assistants for scientific writing¬†Wang et al. (2024b) and peer-review communication¬†Gao et al. (2024); Zhu et al. (2025); Lu et al. . Current approaches generally fall into two paradigms. The direct-to-text generation paradigm typically involves models that are supervised fine-tuned (SFT) on paper-response pairs (Fig.¬†1a). While straightforward, this approach is fundamentally flawed because it trains models to memorize specific, non-transferable experimental outcomes rather than the underlying logic of formulating a strategic response. Consequently, these models are prone to hallucination, frequently fabricating experimental results or over-commit to unverified claims instead of reasoning about the actual content of the manuscript. The second paradigm relies on interactive sessions with proprietary chat-LLMs such as GPT or Gemini (Fig.¬†1b). While these high-capability models can offer superior reasoning, the workflow is notoriously inefficient and opaque. Authors are forced to engage in lengthy, multi-turn prompting to guide the model, which consumes valuable time that could be spent on verification. Furthermore, critical intermediate steps like concern parsing and evidence retrieval remain concealed behind the chat interface. This lack of transparency makes the process difficult to audit and renders the output quality heavily dependent on the prompting expertise of the user.\n\n\nIn this paper, we reframe rebuttal assistance as a decision and evidence organization problem with explicit constraints, rather than the free-form text generation tasks. Specifically, a reliable system must satisfy four critical requirements: (i) Comprehensive Coverage, tracking every reviewer‚Äôs concern without omission; (ii) Strict Faithfulness, adhering to the submitted manuscript without hallucinating technical details; (iii) Verifiable Grounding, linking major statements to specific internal passages or external references; and (iv) Global Consistency, maintaining a unified stance and avoiding conflicting commitments across different responses. To instantiate this view, we propose RebuttalAgent, a multi-agent system that enforces a novel \"verify-then-write\" workflow to overcome the opacity of previous two paradigms, shown in Fig.¬†1c.\n\n\nInstead of rushing to generation, our architecture explicitly decouples reasoning from drafting by producing verifiable intermediate artifacts. The process begins by atomizing unstructured reviews into discrete concerns to guarantee comprehensive coverage, followed by a dual-source evidence construction phase that synthesizes high-fidelity manuscript passages and citation-ready external briefs to strictly ground every claim. Crucially, we introduce a strategic planning stage that audits the response logic for global consistency and commitment safety before any text is drafted, ensuring that concessions made to one reviewer do not contradict the overall stance. By exposing these structured artifacts through human-in-the-loop checkpoints, RebuttalAgent transforms rebuttal writing from a black-box generation task into a transparent, author-controlled collaboration.\n\n\nWe evaluate RebuttalAgent through an author-centric lens, prioritizing practical usability and reliability over mere text fluency. Specifically, we assess performance across four rigorous dimensions: coverage of reviewer concerns, traceability of evidence sources, global coherence of the argumentative stance, and overall argumentation quality. Experimental results on our proposed benchmark demonstrate that our pipeline consistently outperforms previous \"direct-to-text\" baselines and chat-LLMs on these critical metrics. By delivering structured, verifiable assistance, RebuttalAgent significantly reduces the cognitive burden of rebuttal writing while ensuring authors remain the ultimate arbiters of their scientific defense.\n\n\nOur contributions are: ‚ô† We formulate rebuttal assistance as a decision-and-evidence organization problem and propose Rebuttalgent, a multi-agent system with explicit verification and human-in-the-loop checkpoints. ‚ô† We introduce concern-conditioned context construction and on-demand evidence synthesis to produce point-specific, verifiable support under realistic context limits. ‚ô† We construct a benchmark and establish an author-centric evaluation protocol, demonstrating that our approach outperforms baselines in coverage, traceability, and coherence.\n\n\n\n\n2 Related Works\n\nFigure 2: Overview of RebuttalAgent. Given a manuscript (PDF) and reviewer comments, the system (1) structures inputs by parsing and compressing the paper with fidelity checks and extracting atomic reviewer concerns with coverage checks; (2) builds concern-conditioned evidence by constructing a query-specific hybrid manuscript context and, when needed, retrieving and summarizing external literature into citation-ready briefs; and (3) generates an inspectable, evidence-linked response plan that is checked for consistency and commitment safety, incorporates optional author feedback, and is then realized into a formal rebuttal draft.\n\n\nLLM Agents.¬†\nLLMs¬†OpenAI (2025); Team et al. (2023) were initially valued for fluent generation, but real deployments revealed a mismatch between writing well and completing complex tasks reliably. When goals require multi-step planning, fresh evidence, and interaction with external systems, purely parametric generation can accumulate errors and hallucinations, motivation a shift toward intelligent ‚Äúagents‚Äù embedded in dynamic, goal-directed frameworks that plan and act with external tools and environments.\nRecent work¬†Yao et al. (2023b, a) shows that combining reasoning traces with concrete actions (e.g., search tool) improves robustness in long-ho"
  },
  {
    "title": "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law",
    "url": "https://arxiv.org/abs/2601.14160v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nLegal-domain language models and grounding.\nGerman legal NLP resources and multilingual benchmarks.\nTraining on synthetic data.\nBenchmarks and evaluation protocols.\n\n\n\n3 Data Preparation\n\n3.1 Source Texts and Normalization\n3.2 Standard Instruction QA Data Generation\n\n3.3 Difficulty-Graded QA Data Generation\n\nLevel 1 (clause-centric).\nLevel 2 (client-style paraphrase).\nLevel 3 (scenario application).\nLevel 4 (multi-provision reasoning).\nIllustrative example.\n\n\n3.4 LLM-Based Quality Filtering and Redundancy Control\n3.5 Train-Test Split and Contamination Control\n\n\n\n4 Experiments and Results\n\n4.1 LoRA Fine-tuning Setup\n4.2 Compute and Training Time\n4.3 Evaluation Tasks\n\n4.4 Domain-Specific Performance\n\nDifficulty-Graded QA Data Generation consistently delivers substantial improvements.\nLegalMC4 as a proxy for retrieval-augmented QA with context.\nStandard Instruction QA Data Generation yields mixed results.\nComparison to proprietary baselines.\n\n\n\n4.5 General Language Understanding\n\nLegal adaptation preserves (and sometimes improves) general performance.\n\n\n\n\n5 Conclusion\n6 LLM Statement\n\nA Detailed Prompt Templates for Synthetic QA Generation and Quality Filtering\n\n\nA.1 QA Pair Generation Prompts by Difficulty Level\n\nA.1.1 Level 1: ‚ÄúWhat-type‚Äù Questions\nA.1.2 Level 2: General Conceptual Questions\nA.1.3 Level 3: Scenario-Based Legal QA Prompt\nA.1.4 Level 4: Multi-Section Scenario-Based Legal QA Prompt\n\n\nA.2 Quality Filtering Prompt for Synthetic QA Pairs\n\n\n\nB Appendix B: Normal Synthetic Data Generation\n\nB.1 Synthetic Data Generation Prompt\n\n\nC Distribution of QA Pairs Before and After Quality Filtering\n\n\n\n\n\nDomain-Adaptation through Synthetic Data:\nFine-Tuning Large Language Models for German Law\n\n\n\nAli Hamza Bashir1,2,3\n\n‚ÄÉ‚ÄÉ\nMuhammad Rehan Khalid2\n\n‚ÄÉ‚ÄÉ\nKostadin Cvejoski5\n\n‚ÄÉ‚ÄÉ\nJana Birr1\n\n‚ÄÉ‚ÄÉ\n\nJule Berghaus4\n\n‚ÄÉ‚ÄÉ\nArmin Berger1,3,4\n\n‚ÄÉ‚ÄÉ\nSandra Halscheidt1\n\n‚ÄÉ‚ÄÉ\nChristian Temath1\n\n‚ÄÉ‚ÄÉ\n\nRafet Sifa1,3,4\n\n‚ÄÉ‚ÄÉ\nDavid Berghaus1,3\n1Fraunhofer IAIS\n2Georg-August-University G√∂ttingen\n3Lamarr Institute\n4University of Bonn\n5JetBrains Research\n\n\n\n\nAbstract\nLarge language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.\n\n\n¬ßAStatutesectionexample: BGB ¬ß433Difficulty graded generationBLevel 1ClauseDefinition and scope?QA‚úìLevel 2ParaphraseClient style phrasing?QA‚úìLevel 3ScenarioApply to a fact pattern?QA‚úìLevel 4Multi sectionCombine provisions?QA‚úìConstraints‚úì German questions and answers‚úì Answer cites law and section‚úì No external law beyond the textReviewerfilterC‚úì Answerablefrom text‚úì Supportedanswer‚úì Non redundantDeduplication{JSON}{}DFinal SFTdatasetstatute groundedQA pairs (JSON)used for LoRAfine tuningRejectedsamples\n\nFigure 1: Difficulty-Graded QA Data pipeline. A statute section serves as ground truth input. The generator produces complementary question styles at increasing difficulty (clause, paraphrase, scenario, multi-section) while enforcing statute-only constraints. A reviewer model filters unanswerable, unsupported, or redundant pairs and performs deduplication. The retained question‚Äìanswer pairs constitute the supervised fine-tuning (SFT) dataset used for LoRA adaptation.\n\n\n\n1 Introduction\n\nLLMs achieve strong performance on many general NLP tasks, yet reliability remains a key obstacle in high-stakes domains such as law. Legal question answering requires faithful grounding in authoritative sources and careful handling of legal references. Hallucinations or unsupported claims can therefore be harmful (Hu et al., 2025; Dahl et al., 2024).\n\n\nBuilding reliable legal QA systems is particularly challenging outside English. Multilingual legal benchmarks show substantial performance gaps across languages and tasks, reflecting an ecosystem in which resources, benchmarks, and model developments have been disproportionately English-centric (Niklaus et al., 2023). For German civil law, (B√ºttner and\nHabernal, 2024) introduce GerLayQA, a valuable dataset grounded in the B√ºrgerliches Gesetzbuch (BGB). However, this dataset has been crawled from a legal forum and the licensing is therefore debatable, which limits its practical use. As a result, there is a practical need for scalable, statute-grounded training data that can be used to adapt open LLMs to German law.\n\n\nWe address this need by synthesizing German legal instruction data directly from statutory text and therefore avoid any licensing issues. Using a curated corpus of German laws (European Language Resources Association\n(ELRA), 2023), we generate question-answer pairs that are explicitly grounded in specific provisions.\n\n\nHowever, such an approach is challenging because we need to synthesize data that is diverse enough to capture real-world scenarios that have not been encountered during training and additionally not destroy the general purpose performance of the model. Moreover, we found that naive data generation approaches can even lead to diminishing performance on validation datasets that are within the distribution of the training data.\n\n\nWe explore two different data generation approaches: (i) a simple instruction-style generation setup and (ii) a difficulty-graded strategy that elicits questions ranging from direct lookups to scenario-based and multi-section reasoning, inspired by self-instruction style pipelines (Taori and et\nal., 2023). To improve factual consistency, we add an LLM-based review stage that filters unanswerable, unsupported, or redundant QA pairs.\n\n\nUsing this statute-grounded data, we adapt two modern open-source LLMs to German legal QA. Namely, we fine-tune LLaMA¬†3.1 (8B)111https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct and Gemma¬†3 (12B)222https://huggingface.co/google/gemma-3-12b-it using parameter-efficient LoRA (Hu et al., 2021).\n\n\nDue to the lack of evaluation datasets for instruction-tuned LLMs on German legal tasks, we created four different evaluation datasets. Two datasets are based on the German-civil-law (BGB). One of these is created in a multiple-choice format, avoiding any bias in the grading protocol. The second one contains open-ended questions which we grade using an LLM-as-a-judge approach (Zheng et al., 2023; Liu et al., 2023).\n\n\nWhile we ensure that the evaluation dataset only contains questions that were not seen during training, we also want to investigate the performance of the models in real-world scenarios that are completely out-of-distribution. For this, we create two evaluation dataset variants (again a multiple-choice and an open-ended one) based on the German subset of LegalMC4 (European Language Resources Association\n(ELRA), 2023), which is a crawled collection of legal documents from the internet.\n\n\nRemarkably, across all test sets, the difficulty-graded generation plus filtering yields consistent gains over both the base models and the simpler generation baseline while preserving general capabilities.\n\n\nOur contributions are:\n\n\n‚Ä¢\n\nWe propose a practical pipeline to synthesize German legal QA data for instruction-tuning. Our pipeline works directly from legislative texts, avoiding the reuse and privacy limitations common to web-crawled datasets.\n\n\n\n‚Ä¢\n\nWe introduce a difficulty-graded generation strategy together with an LLM-based filtering stage that improves grounding and factual consistency of synthetic QA pairs.\n\n\n\n‚Ä¢\n\nWe provide an extensive evaluation of two open LLMs adapted to German law, demonstrating substantial accuracy improvements on held-out German legal QA benchmarks with no noticeable degradation on general performance.\n\n\n\n\n\nThe fine-tuned models and synthetic datasets will be released upon acceptance of this work.\n\n\n\n\n2 Related Work\n\nLegal-domain language models and grounding.\n\nLegal NLP has long benefited from domain adaptation of general-purpose language models. Early work focused on continued pre-training of encoder models on legal corpora, e.g., Legal-BERT (Chalkidis et al., 2020) and CaseLaw-BERT (Zheng et al., 2021), improving performance on classification, retrieval, and judgment-related tasks within specific jurisdictions.\nWith the shift toward instruction-tuned generative LLMs, recent work has specialized models for legal assistance via continued pre-training or supervised instruction tuning. Examples for Chinese law include (Guo and et al., 2023; Song, 2023) and examples for compliance checks include (Bell et al., 2024).\nA complementary line of research targets reliability through grounding, for instance via retrieval augmented generation (RAG) that conditions generation on external legal sources such as statutes or case repositories (Huang et al., 2023; Chen et al., 2023; Hillebrand et al., 2024).\nDespite these advances, factual errors and hallucinations remain central concerns in legal generation, motivating stronger adaptation and verification techniques (Dahl et al., 2024; Hu et al., 2025).\n\n\n\nGerman legal NLP resources and multilingual benchmarks.\n\nCompared to English and Chinese, publicly reusable resources for German legal QA remain limited.\nGerLayQA (B√ºttner and\nHabernal, 2024) constitutes an important step by providing German layperson question"
  },
  {
    "title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models",
    "url": "https://arxiv.org/abs/2601.14157v1",
    "source": "arxiv",
    "summary": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: ",
    "full_text": null
  },
  {
    "title": "LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery",
    "url": "https://arxiv.org/abs/2601.14154v1",
    "source": "arxiv",
    "summary": "Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion ",
    "full_text": null
  },
  {
    "title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models",
    "url": "https://arxiv.org/abs/2601.14152v1",
    "source": "arxiv",
    "summary": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide ra",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Experimental setup\n\nDatasets.\nModels.\nEvaluation metrics.\nOther details.\n\n\n\n3 Hypotheses and analysis\n\n3.1 Hypothesis 1: Bias in training samples\n3.2 Hypothesis 2: Failure to recall options\n\n3.3 Hypothesis 3: Causal attention\n\nWhy doesn‚Äôt the final attention fix it?\nModulating factors.\n\n\n\n\n\n4 Targeted interventions\n\nAttention pruning (degrading CQO).\nActivation patching (improving QOC).\nOption repetition (improving QOC).\n\n\n5 Conclusion\n\nA Related work\n\nPrompt sensitivity.\nMechanistic analysis tools.\nLong-context failure modes.\n\n\n\nB Detailed information\n\nB.1 Model information\n\nB.2 Dataset information\n\nLogiQA\nSciQ\nRACE (Reading comprehension from examinations)\n\n\n\nB.3 Prompting templates and evaluation protocols\n\n\nB.3.1 Prompting templates\n\nDecoder-only models\nEncoder-only models\nEncoder-Decoder models\n\n\nB.3.2 Evaluation protocols\n\n\nB.4 Inference details\n\n\n\nC Additional result\n\nC.1 Full model-dataset results\nC.2 Base vs instruction-tuned models\nC.3 In-context learning results\nC.4 Option recall accuracy\nC.5 Encoder and encoder-decoder model results\nC.6 Gradient attribution\n\nC.7 Intervention results\n\nAttention pruning (CQO Degradation).\nActivation patching (QOC Improvement).\nOption repetition (QOCO).\n\n\n\n\n\n\n\n\n\nLost in the Prompt Order: \nRevealing the Limitations of Causal Attention in Language Models\n\n\n\nHyunjong Ok1,2 ‚ÄÉJaeho Lee1\n1POSTECH ‚ÄÉ‚ÄÉ2HJ AILAB \nhyunjong.ok@gmail.com, jaeho.lee@postech.ac.kr\n\n\n\nAbstract\nLarge language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.\n\n\n\nLost in the Prompt Order: \nRevealing the Limitations of Causal Attention in Language Models\n\n\n\n\n\nHyunjong Ok1,2 ‚ÄÉ‚ÄäJaeho Lee1\n\n1POSTECH ‚ÄÉ‚ÄÉ2HJ AILAB\n\nhyunjong.ok@gmail.com, jaeho.lee@postech.ac.kr\n\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are highly sensitive to prompt structure. Even minor changes in surface forms‚Äîsuch as instruction phrasing, example placements, or reasoning elicitation‚Äîcan lead to substantial differences in the prediction quality of the models wei2022chain; kojima2022large.\n\n\nDespite the importance of this sensitivity for the practical reliability of LLMs, our current understanding remains largely descriptive. We know ‚Äúwhat‚Äù LLMs are sensitive to, but not ‚Äúwhy.‚Äù\nFor instance, lu2022fantastically has shown that permuting the order of demonstrations in in-context learning (ICL) can dramatically affect accuracy, yet offers limited insight into what makes certain orders preferable.\nLikewise, recent studies on multiple-choice question answering (MCQA) report significant performance fluctuations under option permutations, but stop short of identifying the mechanisms underlying this phenomenon (pezeshkpour-hruschka-2024-large; zheng2024large).\n\n\nIn this work, we undertake an in-depth study of a less obvious but consequential form of prompt-sensitivity: the ordering of components in MCQA prompts.\nA typical MCQA prompt consists of three elements: a context passage (C), a question (Q), and a set of options (O).\nIntuitively, reordering these components should have little effect, as their semantic content remains unchanged. Contrary to this expectation, we find that placing the context before the questions and options (CQO) consistently and substantially outperforms the reverse ordering (QOC) across a wide range of setups (FigureÀú1).111A similar phenomenon has also been noted by shaier2024not outside the MCQA context, which also does not demystify why such phenomenon happens.\n\n\nFigure 1: Performance gap between CQO and QOC. We measure the average accuracies of 21 decoder-only LLMs on 4 different datasets, when prompted in two distinct structures; CQO (context-question-option) and QOC (question-option-context).\n\n\nTo explain this phenomenon, we formulate three competing hypotheses and evaluate them through a series of carefully controlled experiments. By systematically validating (or invalidating) each hypothesis, we aim to uncover the underlying factors that drive context-order sensitivity in MCQA. Specifically, we consider the following hypotheses:\n\n\n\n\n‚Ä¢\n\nHypothesis 1: Biased training data. CQO-style prompts may be more prevalent in training data, making QOC an unfamiliar format for the model.\n\n\n\n‚Ä¢\n\nHypothesis 2: Failures in option recall. QOC structure makes it difficult for the model to recall the options located in the middle of the prompt, a.k.a. ‚Äúlost-in-the-middle‚Äù (liu2024lost).\n\n\n\n‚Ä¢\n\nHypothesis 3: Causal attention (winner). The causal attention structure in decoder-only transformers makes it impossible for the option tokens to attend to the context tokens, creating an information bottleneck (FigureÀú2).\n\n\n\n\n\n\n\n\n(a) Decoder model\n\n\n\n\n\n(b) Encoder model\n\n\n\nFigure 2: Decoder vs. Encoder attention.\nIn QOC (Question‚Üí\\rightarrowOptions‚Üí\\rightarrowContext), causal masking prevents decoder models from attending to the context while selecting among options, so they often answer from option priors rather than evidence. Encoder models use bidirectional attention and can condition on the context when scoring the options.\n\n\nOur experiments‚Äîconducted on a wide range of datasets and models ‚Äî support the third hypothesis, and rule out the other two hypotheses. Stepping further, we identify several factors that can affect the impact of causal masking, namely the context length and the option positions.\n\n\nFinally, building on the causal-attention-based explanation, we design targeted interventions that can improve the performance of LLMs on QOC-structured prompts, or conversely, degrade the performance on CQO-structured prompts. These results provide additional evidence that causal attention is the driving mechanism of the sensitivity.\n\n\n\n\n2 Experimental setup\n\nDatasets.\n\nWe evaluate on four reading comprehension benchmarks that require context-based reasoning: LogiQA¬†(liu2020logiqa); RACE-H/M¬†(lai2017race); SciQ¬†(welbl2017crowdsourcing). All tasks are in MCQA format with four options.\n\n\n\nModels.\n\nWe conduct experiments on 21 decoder-only LLMs from four model families: LLaMA 3¬†(grattafiori2024llama), Qwen 2.5/3¬†(qwen2025qwen25technicalreport), and Gemma 2¬†(gemma_2024). Model sizes range from 0.5B to 9B parameters, including both base and instruction-tuned variants. For architecture comparison experiments (¬ß3.3), we additionally test Flan-T5¬†(chung2024scaling) family (encoder-decoder) and BERT, RoBERTa, ALBERT¬†(devlin2019bert) family (encoder-only).\n\n\n\nEvaluation metrics.\n\nWe measure accuracy under two prompt orderings: CQO and QOC, and utilize the performance gap (Œî=AccCQO‚àíAccQOC\\Delta=\\text{Acc}_{\\text{CQO}}-\\text{Acc}_{\\text{QOC}}), which quantifies context order sensitivity.\n\n\n\nOther details.\n\nFurther details on datasets, models, prompting templates, and evaluation/scoring protocols are provided in Appendix¬†B.\n\n\n\n\n\n3 Hypotheses and analysis\n\nAs shown in FigureÀú1, permuting the prompt order leads to a sharp performance drop across all benchmarks. To understand the cause, we formulate and test a set of hypotheses. All detailed experiments for each model and dataset are in¬†AppendixÀúC.\n\n\n\n\n\n(a) Performance gap\n\n\n\n\n(b) ICL performance\n\n\n\nFigure 3: Hypothesis 1: Effect of instruction tuning and in-context learning. (a) We compare the performance gaps between base and instruct models, and find they remains remarkably consistent. (b) Few-shot prompting yields marginal gains. These results suggest that the friendly formatting is not the primary driver.\n\n\n\n3.1 Hypothesis 1: Bias in training samples\n\nHypothesis. CQO is simply more frequent in the training data than QOC. Thus, in the QOC format, the model may fail to process an unfamiliar format.\n\n\nExperiment.\nWe test the training-distribution hypothesis in two ways.\nFirst, CQO-like prompts are more common in instruction data; this hypothesis predicts a larger CQO-QOC gap for instruction models.\nWe therefore compare nine matched base-instruct pairs.\nSecond, we use in-context learning to familiarize models with the QOC format, varying the number of demonstrations up to 5-shot.\n\n\nResults.\nAs shown in FigureÀú3(a), the CQO-QOC gaps are nearly identical, suggesting the phenomenon is not driven by instruction tuning.\nMoreover, even with 5-shot demonstrations shown in FigureÀú3(b), QOC accuracy improves by only 3.1% and remains far below CQO.\nTogether, these results rule out training distribution as the primary cause.\n\n\nFigure 4: Hypothesis 2: Option recall analysis. To investigate \"forgetting\" the options due to the long context intervention, we evaluated option recall accuracy. Results show that accuracy is consistently higher than CQO, indicating that the models retain option information and ruling out memory loss as the primary cause.\n\n\n\n\n3.2 Hypothesis 2: Failure to recall options\n\nHypothesis. LLMs often fail to recall the information located in the middle of the context (liu2024lost). Thus, in the QOC format, the model may fail to correctly recall the options, which are located between the question and the context.\n\n\nExperiment. After presenting the prompt, we ask the LLMs to recall each option. Precisely, we measure the chance of an exact match for each option.\n\n\nResults. As shown in FigureÀú4, QOC achieves a similar, or even higher, recall accuracy than CQO. This indicates that the failure of option recall may not be the cause of accuracy drops in QOC.\n\n\n\n\n3.3 Hypothesis 3: Causal attention\n\nHypothesis. Causal attention mask prevents option tokens from attending to conte"
  },
  {
    "title": "The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning",
    "url": "https://arxiv.org/abs/2601.14127v1",
    "source": "arxiv",
    "summary": "As Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Advances in MLLM Reasoning\n2.2 Safety Issues in Advanced MLLMs\n\n\n3 Multi-image Relations Taxonomy\n\n4 Benchmark Construction\n\n\n4.1 Multi-image Instance Generation\n\n1. Rewrite Harmful Question (Revisor).\n2. Generate Images (Image Generator).\n3. Test the Attack (Tester).\n4. Harmfulness Judgement (Harmbench Judge).\n5. Evaluate &amp; Refine (Evaluator).\n\n\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results on MIR-SafetyBench\n\n5.3 Analysis of Model Behaviors\n\n5.3.1 Unsafety Mode Analysis\n5.3.2 Safety Mode Analysis\n\n\n\n5.4 Controlled Comparison with Single-Image\n\nResults and Analysis\n\n\n\n5.5 Internal analysis via attention entropy\n\nMotivation\nSetup\nResults\n\n\n\n\n6 Conclusion\nBenchmark coverage and construction\nDependence on automatic components\nEvaluation setting and analysis scope\nLimited exploration of mitigation\nA Risk Category Definitions\n\nB Details of Harmful Seed Construction\n\nStep 1: Risk Category Definition.\nStep 2: Automated Filtering and Refinement.\nStep 3: Human Expert Curation.\n\n\nC Evaluated Models\n\nD Computing Environment and Implementation\n\nHardware.\nDetails of implementation.\n\n\nE Formal Definition of the Attention-Entropy Heatmap\nF Attention-entropy heatmap for MiniCPM-o-2.6 and Kimi-VL-A3B-Thinking-2506\nG Statics for answer length.\n\n\n\n\n\nThe Side Effects of Being Smart: Safety Risks in MLLMs‚Äô Multi-Image Reasoning\n\n\n\nRenmiao¬†Chen1,‚àó,\nYida¬†Lu1,‚àó,\nShiyao¬†Cui1,\nXuan¬†Ouyang1,\nVictor¬†Shea-Jay¬†Huang2,\n\nShumin¬†Zhang3,\nChengwei¬†Pan2,\nHan¬†Qiu3,\nMinlie¬†Huang1,‚Ä†\n\n1CoAI group, DCST, Tsinghua University ‚ÄÉ2Beihang University\n3Tsinghua University\n\n\n{crm21, lyd24}@mails.tsinghua.edu.cn ‚ÄÉaihuang@tsinghua.edu.cn\n\n‚àóEqual contribution.‚ÄÉ‚Ä†Corresponding author\n\n\n\nAbstract\nAs Multimodal Large Language Models (MLLMs) acquire stronger reasoning capabilities to handle complex, multi-image instructions, this advancement may pose new safety risks. We study this problem by introducing MIR-SafetyBench, the first benchmark focused on multi-image reasoning safety, which consists of 2,676 instances across a taxonomy of 9 multi-image relations. Our extensive evaluations on 19 MLLMs reveal a troubling trend: models with more advanced multi-image reasoning can be more vulnerable on MIR-SafetyBench. Beyond attack success rates, we find that many responses labeled as safe are superficial, often driven by misunderstanding or evasive, non-committal replies. We further observe that unsafe generations exhibit lower attention entropy than safe ones on average. This internal signature suggests a possible risk that models may over-focus on task solving while neglecting safety constraints. Our code and data are available at https://github.com/thu-coai/MIR-SafetyBench.\n\n\n\nThe Side Effects of Being Smart: Safety Risks in MLLMs‚Äô Multi-Image Reasoning\n\n\n\n\nRenmiao¬†Chen1,‚àó,\nYida¬†Lu1,‚àó,\nShiyao¬†Cui1,\nXuan¬†Ouyang1,\nVictor¬†Shea-Jay¬†Huang2,\n\n\nShumin¬†Zhang3,\nChengwei¬†Pan2,\nHan¬†Qiu3,\nMinlie¬†Huang1,‚Ä†\n\n1CoAI group, DCST, Tsinghua University ‚ÄÉ2Beihang University\n3Tsinghua University\n\n{crm21, lyd24}@mails.tsinghua.edu.cn ‚ÄÉaihuang@tsinghua.edu.cn\n\n‚àóEqual contribution.‚ÄÉ‚Ä†Corresponding author.\n\n\n\n\n\n1 Introduction\n\nFigure 1: Illustration of the ‚Äòside effect of being smart‚Äô: as MLLMs‚Äô reasoning improves, they move from failing to understand a complex harmful request (Level 1) to providing a detailed high-risk procedure (Level 3).\n\n\nAdvancing MLLMs to comprehend complex instructions and visual inputs is essential for real-world problems¬†Hurst et al. (2024); Google (2024). Recent models have made substantial progress in task compliance and multimodal reasoning, moving toward more general and robust multimodal intelligence¬†Comanici et al. (2025); OpenAI (2025). However, this rapid advancement raises a natural question: do improved capabilities also expand the attack surface and introduce new safety risks?\n\n\nMost existing safety evaluations for MLLMs focus on content-based safety, where a model is considered unsafe if it fails to refuse explicit harmful images. However, they neglect reasoning-based safety, where harm emerges only through the model‚Äôs reasoning process. In this work, we study such risks in multi-image scenarios, focusing on cross-image interactions and user instruction.\n\n\nAs illustrated in Figure¬†1, models with different capabilities exhibit distinct behaviors in multi-image reasoning task.111The three levels are consistent with their average scores on the OpenCompass Multimodal Reasoning benchmark (MMR Avg.¬†OpenCompass Contributors (2023)).\nA less capable model limited to single-image inputs (Level¬†1) may fail to comprehend the underlying task, thus providing a generic response.\nBy contrast, the multi-image models (Levels¬†2 and¬†3) correctly infer the user‚Äôs intent but fail to recognize its latent harmful nature, thus proceed to answer it. Consequently, the intermediate model (Level¬†2) provides a flawed and incomplete pathway, whereas the strongest model (Level¬†3) generates a detailed, high-risk procedure.\n\n\nTo systematically study this phenomenon, we introduce MIR-SafetyBench, a comprehensive benchmark for evaluating MLLMs‚Äô multi-image reasoning safety. MIR-SafetyBench offers three key advantages:\n(1) Reasoning-based Design. Harmful intent emerges only when the model performs multi-step relational reasoning over multiple images and the instruction.\n(2) Varied Relation Types. The benchmark contains 2,676 instances grouped into 9 relation types, broadly covering how multi-image relations can conceal or enable harmful intent.\n(3) Extensive Diversity. Starting from 600 curated harmful seed questions spanning 6 risk categories, we construct a diverse set of multi-image instances that tests MLLMs across a wide range of safety-critical scenarios.\n\n\nOur benchmark shows that multi-image relational attacks succeed widely across 19 MLLMs and that within a broad range of models, stronger multi-image reasoning often coincides with higher ASR. To understand why weaker models appear safer, we introduce a four-way taxonomy of safe response behaviors and show that many safe generations arise from misunderstanding, generic unexplained refusals, or evasive but uninformative answers rather than robust safety alignment. We further probe models‚Äô internal states finding that only in multi-image scenarios do unsafe generations exhibit lower attention entropy than safe ones on average, suggesting that reasoning-based safety failures may have distinct internal signatures and that MLLMs may tend to allocate their capacity to solving the underlying reasoning problem while neglecting safety constraints.\nOur contributions can be summarized as follows:\n\n\n‚Ä¢\n\nWe construct MIR-SafetyBench, the first comprehensive benchmark for evaluating multi-image reasoning safety in MLLMs to our knowledge. It contains 2,676 instances with 2‚Äì4 images each, covering 9 multi-image relation types and 6 safety risk categories.\n\n\n\n‚Ä¢\n\nWe conduct extensive evaluations on 19 popular MLLMs and show that these multi-image reasoning safety risks are pervasive. Moreover, these risks can increase as models‚Äô multi-image reasoning capabilities improve.\n\n\n\n‚Ä¢\n\nWe distinguish genuine safety alignment from harmless behavior arising from model limitations, and we probe MLLMs‚Äô internal states in multi-image safety tasks using attention entropy, identifying distinct internal signatures associated with unsafe generations.\n\n\n\n\n\nFigure 2: \nExamples of the nine relations in our proposed taxonomy. Each case hides harmful intent within the complex relationships across multiple images and a textual prompt. \n\n\n\n\n2 Related Work\n\n\n2.1 Advances in MLLM Reasoning\n\nRecent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced their reasoning capabilities¬†Huang et al. (2025); Jiang et al. (2025); Wu et al. (2025); Jiang et al. (2025, 2024). A crucial frontier in this domain is the ability to reason across multiple images, which is essential for understanding complex, real-world scenarios that cannot be captured in a single snapshot¬†Wang et al. (2024); Wu et al. (2024). The growing research interest in this area is evidenced by the recent emergence of dedicated multi-image understanding benchmarks, such as MuirBench¬†Wang et al. (2025) and MMIU¬†Meng et al. (2025). These works highlight the community‚Äôs focus on enhancing models‚Äô capacity for complex relational and contextual reasoning, setting the stage for more sophisticated applications.\n\n\n\n\n2.2 Safety Issues in Advanced MLLMs\n\nDespite their growing capabilities, the safety of MLLMs remains a significant concern and some related benchmarks have emerged. Early studies probed MLLMs‚Äô vulnerabilities by injecting explicit harmful signals into images, such as rendering malicious text ¬†Gong et al. (2025) or using visuals related to unsafe keywords¬†Liu et al. (2024); Hu et al. (2025). Subsequent work moved to more sophisticated evaluations, including large-scale automated red-teaming datasets¬†Luo et al. (2024); Li et al. (2024b) and benchmarks probing cross-modality alignment, where individually benign inputs become harmful only when combined¬†Cui et al. (2025); Zhou et al. (2024a); Lee et al. (2025).\n\n\nIn parallel, existing studies have indicated that stronger capabilities do not automatically yield safer behavior¬†Bostrom (2012); Armstrong (2013). Builded on the rapid progress of multi-image reasoning, recent attacks now exploit multi-image contexts directly through distraction-based multimodal jailbreaks¬†Yang et al. (2025b), heuristic-induced multimodal risk distribution¬†Teng et al. (2024), visual chain reasoning attacks¬†Sima et al. (2025), and compositional multi-image jailbreaks¬†Ding et al. (2025). However, these works do not provide a systematic benchmark for this task, and we address this gap by introducing MIR-SafetyBench.\n\n\n\n\n\n3 Multi-image Relations Taxonomy\n\nWe propose a comprehensive taxonomy of multi-image relations that can expose vulnerabilities in MLLMs. Our taxonomy delineates four primary categories and nine fine"
  },
  {
    "title": "Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic",
    "url": "https://arxiv.org/abs/2601.14124v1",
    "source": "arxiv",
    "summary": "Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generat",
    "full_text": null
  },
  {
    "title": "A Systematic Analysis of Chunking Strategies for Reliable Question Answering",
    "url": "https://arxiv.org/abs/2601.14123v1",
    "source": "arxiv",
    "summary": "We study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B gen",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Experimental Setup\n\nTask and corpus.\nRetrieval and context budgeting.\nChunking strategies.\nGeneration and abstention.\nParameters varied.\nMetrics and protocol.\n\n\n\n3 Findings\n\nF1. Overlap adds cost without measurable gains.\nF2. Method ‚Äútier list‚Äù: sentence ‚âà\\approx semantic &gt; token ‚â´\\gg code (for text).\nF3. The ‚Äúcontext cliff‚Äù: more is not always better.\nF4. Goal-driven tuning: small CC for semantic quality; larger CC for factual accuracy; abstention is tunable.\nLimitations and Future Work.\n\n\n4 Conclusion\n\n\n\n\n\nA Systematic Analysis of Chunking Strategies for Reliable Question Answering\n\n\nSofia Bennani\n\n√âcole polytechniquePalaiseauFrance\n\nname.surname@polytechnique.edu\n\n and \nCharles Moslonka\n\n0000-0002-8719-1510\nArtefact Research CenterParisFrance\n\nMICS, CentraleSup√©lec, Universit√© Paris-SaclayGif-sur-YvetteFrance\n\n\n\nAbstract.\nWe study how document chunking choices impact the reliability of Retrieval-Augmented Generation (RAG) systems in industry. While practice often relies on heuristics, our end-to-end evaluation on Natural Questions systematically varies chunking method (token, sentence, semantic, code), chunk size, overlap, and context length. We use a standard industrial setup: SPLADE retrieval and a Mistral-8B generator. We derive actionable lessons for cost-efficient deployment: (i) overlap provides no measurable benefit and increases indexing cost; (ii) sentence chunking is the most cost-effective method, matching semantic chunking up to ‚àº5\\sim 5k tokens; (iii) a ‚Äúcontext cliff‚Äù reduces quality beyond ‚àº2.5\\sim 2.5k tokens; and (iv) optimal context depends on the goal (semantic quality peaks at small contexts; exact match at larger ones).\n\n\n\n1. Introduction\n\nEnterprises increasingly deploy RAG systems for knowledge access and user support. In industrial settings, these systems operate under strict constraints regarding latency, storage costs, and maintainability. While agentic workflows are the ultimate goal, reliability hinges on the foundational retrieval layer ‚Äî particularly how source documents are chunked.\nDespite ample work on retrieval (faysseColPaliEfficientDocument2024, ; santhanamColBERTv2EffectiveEfficient2022, ; formalEffectiveEfficientSparse2024, ) and LLMs, chunking is often left to rules of thumb. This paper reports an end-to-end, data-driven study conducted to standardize our production defaults.\nOur contributions are:\n\n\n‚Ä¢\n\nA systematic evaluation of chunking method, size, overlap, and context length for agentic question answering (QA) on Natural Questions (NQ).\n\n\n\n‚Ä¢\n\nCompact, deployable guidance: avoid overlap; prefer sentence chunking; select context size by task; beware the ‚Äúcontext cliff‚Äù.\n\n\n\n‚Ä¢\n\nA reliability view that includes abstention (‚ÄúNONE‚Äù) rates alongside semantic and exact-match metrics.\n\n\n\n\n\n\n\n2. Experimental Setup\n\nWe evaluate a standard two-stage RAG pipeline (Figure 1): a sparse retriever indexes chunked documents; the top-ranked chunks are passed to an instruction-tuned LLM prompted to answer strictly from context and output ‚ÄúNONE‚Äù otherwise.\n\n\nFigure 1. RAG pipeline architecture with parameters S,OS,O and CC.\n\nSchematic view of the RAG pipeline, along with the different chunking parameters.\n\n\n\nTask and corpus.\n\nWe use the Natural Questions (kwiatkowskiNaturalQuestionsBenchmark2019, ) short-answer subset (open-domain QA). The underlying corpus is English Wikipedia; documents are ingested and chunked according to each strategy below, then indexed.\n\n\n\nRetrieval and context budgeting.\n\nWe use SPLADE (formalSPLADESparseLexical2021, ; formalSPLADEV2Sparse2021, ) (pretrained weights (kongSparseEmbedLearningSparse2023, )) to build a sparse index over all chunks. At query time, we retrieve the top-ranked chunks and fill a token budget CC (context length) using the generator‚Äôs tokenizer, appending in rank order until the budget is reached. This ‚Äúfill-to-budget‚Äù policy ensures fair comparison across chunk sizes and methods (no fixed-KK bias).\n\n\n\nChunking strategies.\n\n\n\n‚Ä¢\n\nToken: fixed-size sliding windows of target size SS with optional token overlap OO.\n\n\n\n‚Ä¢\n\nSentence: respects sentence boundaries; no sentence is split.\n\n\n\n‚Ä¢\n\nSemantic: sentence-preserving; adjacent sentences are merged if cosine similarity (all-MiniLM-L12-v2) exceeds 0.5, up to the target size SS.\n\n\n\n‚Ä¢\n\nCode: structure-aware parsing (e.g., functions/classes) for source code, focused on markdown; included for completeness though NQ is text-centric.\n\n\n\n\n\n\nGeneration and abstention.\n\nWe use Ministral-8B-Instruct-2410 (MinistralMinistrauxMistral, ) with low-temperature decoding (T=0.1T=0.1). The prompt enforces grounded generation and explicit abstention: ‚ÄúAnswer only using the provided context. If the context is insufficient, output ‚ÄòNONE‚Äô.‚Äù We cap output length to short answers.\n\n\n\nParameters varied.\n\nWe evaluate four methods (Token, Sentence, Semantic, Code) across a grid of sizes. We test chunk sizes SS from 50 to 500 (step 50), with overlaps OO of 0% or 20%. Finally, we retrieve into a context budget CC of {500, 1k, 2.5k, 5k, 10k} tokens.\n\n\n\nMetrics and protocol.\n\nWe report:\n\n\n‚Ä¢\n\nBERTScore (zhangBERTScoreEvaluatingText2019, ) (semantic quality vs. reference answer).\n\n\n\n‚Ä¢\n\nExact Match (EM) with standard normalization (lowercasing; stripping punctuation/articles).\n\n\n\n‚Ä¢\n\nNone Ratio: fraction of queries where the model outputs ‚ÄúNONE‚Äù.\n\n\n\nWe compute 95% bootstrap confidence intervals over questions; claims of ‚Äúno measurable difference‚Äù indicate paired deltas within CIs (e.g., |Œî‚ÄãBERTScore|‚â§0.004|\\Delta\\;\\mathrm{BERTScore}|\\leq 0.004, EM differences ‚â§0.001\\leq 0.001). We intentionally do not use rerankers or LLM-as-reranker to isolate the effect of chunking and context budgeting.\n\n\n\n\n\n\n\nFigure 2. Left: Effect of context length CC on metrics for different chunking methods (Sentence, Semantic, Token). Right: Chunking method comparison at fixed C=5000C=5000 tokens and S=300S=300, O=0O=0. Dots show means; bars denote 95% bootstrap CIs.\n\nTwo graphs. On the left one, the scores decreases with context size, and there are small but significant differences between the chunkers. The right one shows this effect more directly, showing that the sentence and semantic chunkers are quite similar. The token chunker is significantly worse that the other two, and the code chunker is so bad it‚Äôs almost irrelevant.\n\n\n\n\n\n\n3. Findings\n\nWe report the end-to-end effects that were most consistent and actionable across settings, along with brief mechanisms and deployment implications.\n\n\nF1. Overlap adds cost without measurable gains.\n\nAcross paired configurations, adding 10‚Äì20% overlap did not improve BERTScore or EM (e.g., |Œî|\\DeltaBERTScore|‚â§0.004|\\leq 0.004; EM differences ‚â§0.001\\leq 0.001). Mechanism: with a sentence-aware pipeline and a sparse retriever, boundary spillover rarely changes the top-CC content; overlap mostly introduces near-duplicates. Cost: for overlap ratio rr, chunk count (and index size) inflates by a factor 1/(1‚àír)1/(1-r) (e.g., r=0.2r=0.2 leads to 1.25√ó1.25\\times more chunks), increasing ingestion time and storage. Recommendation: use O=0O=0 unless you have evidence your retriever benefits from boundary redundancy.\n\n\n\nF2. Method ‚Äútier list‚Äù: sentence ‚âà\\approx semantic &gt; token ‚â´\\gg code (for text).\n\nSentence and semantic chunking were statistically tied up to ‚àº\\sim5k tokens; token chunking lagged; code chunking was not competitive on this text task (Fig. 2). Mechanism: sentence-preserving methods keep topical coherence and reduce cross-sentence fragmentation, improving both retrieval precision and LLM grounding. Semantic merging helps when very large contexts are used (slight edge C&gt;5C&gt;5k), likely by packing semantically contiguous text. Recommendation: default to sentence; consider semantic only for very large CC or highly discursive documents.\n\n\n\nF3. The ‚Äúcontext cliff‚Äù: more is not always better.\n\nPerformance improved from small to moderate contexts but dropped beyond ‚àº\\sim2.5k tokens. For sentence chunking (O=0O=0, S=300S=300), BERTScore was stable between 0.5k‚Äì2.5k tokens and then declined by ‚àº4\\sim 4‚Äì5%5\\% relatively at 10k tokens. Mechanism: long-context LLMs can suffer from distraction and redundancy; retrieval at large budgets introduces overlapping or off-topic chunks, diluting signal. Recommendation: identify and enforce a sweet spot for CC; in our setup, C‚âà2.5C\\approx 2.5k was a strong default for QA. Note that the exact drop-off point is model-dependent; our values reflect Ministral‚Äë8B‚ÄëInstruct‚Äë2410 and should be re‚Äëtuned per LLM. However, the existence of a performance plateau or decline with excessive context is a consistent phenomenon in RAG.\n\n\n\nF4. Goal-driven tuning: small CC for semantic quality; larger CC for factual accuracy; abstention is tunable.\n\nBERTScore tended to peak at small, focused contexts (‚àº500\\sim 500 tokens), whereas EM peaked at larger contexts (‚àº2.5\\sim 2.5k). None Ratio fell with larger CC (e.g., from ‚àº30%\\sim 30\\% at 0.5k to ‚àº11%\\sim 11\\% at 10k) and rose with larger SS.\nMechanism: small CC concentrates the most relevant evidence (good for semantic faithfulness), while larger CC increases recall across disparate mentions (good for EM). Larger chunks reduce the number of distinct contexts retrieved, increasing abstention when narrow evidence is missed. Recommendation: for summaries/explanations, keep CC small; for factoid QA, use C‚âà2.5C\\approx 2.5k. To reduce ‚ÄúNONE‚Äù, increase CC and use smaller SS.\n\n\nTable 1. Practical defaults for agentic QA (text documents).\n\n\n\nChoice\nDefault\nRationale\n\n\n\n\nOverlap OO\n\n0%\nNo measurable benefit; reduces cost/complexity\n\n\nChunker\nSentence\nMatches semantic up to ‚àº\\sim5k tokens; cheaper\n\n\nChunk size SS\n\n150‚Äì300\nBalances recall vs. abstention\n\n\nContext CC (QA)\n\n‚àº\\sim2.5k\nAvoids context cliff; boosts EM\n\n\nContext CC (Summ.)\n\n‚àº\\sim500\nMaximizes semantic faithfulness\n\n\nWhen C&gt;5C&gt;5k\nConsider Semantic\nSlight edge at very large contexts\n\n\n\n\n\n\nLimitations and Future Work.\n\nOur study focuses on optimizing the first-stage r"
  },
  {
    "title": "NewsRECON: News article REtrieval for image CONtextualization",
    "url": "https://arxiv.org/abs/2601.14121v1",
    "source": "arxiv",
    "summary": "Identifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related work\n\n3 NewsRECON\n\nRelevant articles labeling\nBi-encoder\nLocation cross-encoder\nArticle clustering\nEvent cross-encoder\n\n\n\n4 Experiments\n\n\n4.1 Datasets\n\nTARA\n5Pils-OOC\n\n\n\n4.2 News article corpus\n\nNY Times\nGuardian\nFiltering\nNews caption generation\nCorpus statistics and variants\n\n\n4.3 Metrics\n4.4 Baselines\n4.5 Implementation details\n4.6 Results on TARA\n4.7 Results on 5Pils-OOC\n\n4.8 Analysis\n\n4.8.1 Ablations\n4.8.2 Impact of input type for bi-encoders\n4.8.3 Impact of corpus size\n4.8.4 Qualitative analysis\n\n\n\n\n5 Conclusion\nIntended use\nData access\nAI assistants use\nA NewsRECON pseudocode\nB Prompts\nC Manual analysis of article filtering\nD News articles corpus additional statistics\nE Impact of MLLM size\nF Impact of random seed\nG Hyperparameter details\nH Time Efficiency\nI Geographic and Temporal Performance Analysis\n\n\n\n\n\nNewsRECON: News article REtrieval for image CONtextualization\n\n\n\nJonathan Tonglet1,2,3, Iryna Gurevych1, Tinne Tuytelaars2, Marie-Francine Moens3\n1 Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science, \nTU Darmstadt and National Research Center for Applied Cybersecurity ATHENE\n2 Department of Electrical Engineering, KU Leuven\n3 Department of Computer Science, KU Leuven\njonathan.tonglet@kuleuven.be\n\n\n\nAbstract\nIdentifying when and where a news image was taken is crucial for journalists and forensic experts to produce credible stories and debunk misinformation. While many existing methods rely on reverse image search (RIS) engines, these tools often fail to return results, thereby limiting their practical applicability. In this work, we address the challenging scenario where RIS evidence is unavailable. We introduce NewsRECON, a method that links images to relevant news articles to infer their date and location from article metadata. NewsRECON leverages a corpus of over 90,000 articles and integrates: (1) a bi-encoder for retrieving event-relevant articles; (2) two cross-encoders for reranking articles by location and event consistency. Experiments on the TARA and 5Pils-OOC show that NewsRECON outperforms prior work and can be combined with a multimodal large language model to achieve new SOTA results in the absence of RIS evidence. We make our code available.111github.com/jtonglet/arxiv2025-newsrecon\n\n\n\nNewsRECON: News article REtrieval for image CONtextualization\n\n\n\n\nJonathan Tonglet1,2,3, Iryna Gurevych1, Tinne Tuytelaars2, Marie-Francine Moens3\n\n1 Ubiquitous Knowledge Processing Lab (UKP Lab), Department of Computer Science,\n\nTU Darmstadt and National Research Center for Applied Cybersecurity ATHENE\n\n2 Department of Electrical Engineering, KU Leuven\n\n3 Department of Computer Science, KU Leuven\n\njonathan.tonglet@kuleuven.be\n\n\n\n\n\n1 Introduction\n\nImages are frequently used to communicate news events, especially on social media platforms. However, they are often shared with missing or incorrect metadata, such as their date or location. These inaccuracies play a significant role in the spread of online misinformation (Dufour et¬†al., 2024). As a result, journalists, fact-checkers, and forensic experts must frequently verify or recover the metadata of online images, i.e., image contextualization, a task that is crucial for several applications, including reliable news coverage, forensic investigation of war crimes (Silverman, 2013), or early misinformation detection (Puliafito, 2025; Khan et¬†al., 2024). A common approach involves using reverse image search (RIS) tools to retrieve web pages containing the image, and inferring the date or location from the surrounding page content. However, RIS engines often fail to return results (Tonglet et¬†al., 2024, 2025). In such cases, identifying the image‚Äôs date and location becomes substantially more difficult, requiring journalists to interpret visual cues and gather external evidence. Due to time and human resource constraints, these cases are also more interesting to automate.\n\n\nFigure 1: NewsRECON retrieves articles that are location or event-relevant, given an image as query.\n\n\nSeveral works have proposed methods to automate image contextualization (Fu et¬†al., 2022; Shi et¬†al., 2024; Tonglet et¬†al., 2024; Ayyubi et¬†al., 2025; Phan et¬†al., 2025; Siingh et¬†al., 2025; Tonglet et¬†al., 2025). Some of them tackle the challenging scenario where RIS engines are unavailable, relying solely on the visual content of the image (Fu et¬†al., 2022; Siingh et¬†al., 2025) or leveraging encyclopedic knowledge bases (Shi et¬†al., 2024). However, none of these methods leverage large-scale news article corpora, which represent a rich source of contextual evidence (Fu et¬†al., 2022).\n\n\nIn this work, we introduce NewsRECON, the first image contextualization method leveraging news articles in scenarios where RIS engines are unavailable. As illustrated in Figure 1, NewsRECON searches a large news corpus to identify relevant articles for a given news image, i.e., articles that relate to the same location or event, where an event is defined as the combination of a date and location. The metadata of these articles, including publication dates and geolocation keywords, can then be used as predictions of the image‚Äôs date and location. We construct a corpus of 91,376 news articles, spanning the years 2010 to 2023. NewsRECON comprises three main components: (1) a bi-encoder that retrieves the top-KK event-relevant articles for a given image, (2) a cross-encoder that reranks these articles based on their location similarity to the image, (3) a second cross-encoder that reranks clusters of articles by their event consistency with the image. Experiments on the TARA benchmark show that NewsRECON outperforms retrieval-based baselines (Fu et¬†al., 2022) by 9.7 percentage points (pp) in GREAT score (Siingh et¬†al., 2025), a joint metric for date and location prediction. Moreover, providing the top-3 articles as input to a multimodal large language model (MLLM) yields the new SOTA results. We further demonstrate that NewsRECON, after being trained on TARA, generalizes to the 5Pils-OOC benchmark (Tonglet et¬†al., 2025), despite geographic and stylistic shifts. When combined with an MLLM, it becomes the strongest method for images lacking RIS web evidence.\n\n\n\n\n2 Related work\n\nJournalists and forensic experts contextualize news images by identifying key context items such as the source, date, location, and motivation (Urbani, 2020; Tonglet et¬†al., 2024; Phan et¬†al., 2025). Applications of this process include fact-checking images that have been taken out of context to spread misinformation (Tonglet et¬†al., 2025), or identifying evidence for war crimes investigations (Silverman, 2013). In this work, we focus on two core context items: the date and location, which are the most widely studied in the literature.\n\n\nSeveral datasets have been proposed for this task (Fu et¬†al., 2022; Tonglet et¬†al., 2024; Geng et¬†al., 2025).\nExisting methods differ in both their model architectures and the types of external evidence they use. Some approaches formulate image contextualization as a classification task, fine-tuning CLIP (Radford et¬†al., 2021) to retrieve the best matching candidate date-location pairs present in the same dataset split (Fu et¬†al., 2022; Shi et¬†al., 2024). However, this setup lacks realism: the candidate set is limited to a small number of date-location pairs found in the split, whereas in real-world scenarios, there are thousands of unique dates and an even larger number of locations, resulting in a near-infinite search space.\n\n\nOther works treat it as a text generation task and rely on (M)LLMs to generate the answers (Tonglet et¬†al., 2024; Phan et¬†al., 2025; Zhang et¬†al., 2024; Ayyubi et¬†al., 2025; Siingh et¬†al., 2025; Tonglet et¬†al., 2025). Some of these approaches use RIS engines to retrieve external evidence (Tonglet et¬†al., 2024; Phan et¬†al., 2025; Tonglet et¬†al., 2025). However, RIS engines may return no results (Abdelnabi et¬†al., 2022; Tonglet et¬†al., 2024), particularly for newly published or less widely circulated images. The absence of RIS evidence makes the task far more challenging but also more valuable to automate, as it requires interpreting subtle visual cues and relating them to world knowledge. Other methods explore alternative sources of external evidence, such as Wikipedia (Shi et¬†al., 2024; Tonglet et¬†al., 2025) or celebrity recognition APIs (Siingh et¬†al., 2025). However, none of the existing methods leverage news articles, despite their strong potential as a source of temporal and spatial context (Fu et¬†al., 2022).\n\n\n\n\n3 NewsRECON\n\nFigure 2: Overview of NewsRECON. Parts a to c illustrate the three training stages with images from TARA (Fu et¬†al., 2022). Part d shows the retrieval pipeline at inference time.\n\n\nNewsRECON, illustrated in Figure¬†2, predicts the date and location of an image by returning ranked lists of relevant news articles. The inference-time pseudocode is provided in Appendix A.\n\n\nRelevant articles labeling\n\nFor each image, we define the set of relevant articles. These binary labels serve as weak supervision signals for training and validation. For each image, we construct two relevance sets: one for location and one for event, where an event is defined as the combination of a specific date and location. We do not define a relevance set based solely on date, as date prediction typically relies on establishing the correct location first. The event set is always a subset of the location set. For the location, an article is relevant if at least one of its geolocation keywords contains the ground truth. For the event, the same location condition holds, and the publication date of the article must fall within a window of ¬±NwindowN_{\\text{window}} days around the original publication date of the image. This temporal window ensures a sufficient number of event-relevant articles for training, since exact day-level matches are too limited.\n\n\n\nBi-encoder\n\nThe first retrieval phase uses a bi-encoder trained to retrieve eve"
  },
  {
    "title": "Riemannian Liquid Spatio-Temporal Graph Network",
    "url": "https://arxiv.org/abs/2601.14115v1",
    "source": "arxiv",
    "summary": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome thi",
    "full_text": null
  }
]