[
  {
    "title": "SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time",
    "url": "https://arxiv.org/abs/2512.25075v1",
    "source": "arxiv",
    "summary": "We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animati",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Coordinated Humanoid Manipulation with Choice Policies",
    "url": "https://arxiv.org/abs/2512.25072v1",
    "source": "arxiv",
    "summary": "Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which i",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Scaling Open-Ended Reasoning to Predict the Future",
    "url": "https://arxiv.org/abs/2512.25070v1",
    "source": "arxiv",
    "summary": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenFore",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Open-Ended Forecasting\n\n4 Generating Open-Ended Forecasting Questions from News\n\n4.1 Methodology for Generating Forecasting Questions\n4.2 OpenForesight: An Open, Large-Scale Forecasting Training Dataset\n\n\n5 Prediction System\n6 Final Results\n7 Conclusion\n\nAppendix\n\nA Adapting Brier Score to free-form responses\n\nB Analyzing OpenForesight Dataset\n\nB.1 Analysis of Best Question Selection\nB.2 Distribution of Answer Types\n\n\n\nC Qualitative Analysis of Final Answers\n\nC.1 Test Set\n\n\n\nD Additional Results\n\nD.1 Ablation: Using Prediction Market Binary Data\n\nD.2 Evaluation on Metaculus Questions\n\nPrediction Market Dataset.\nResults on Real Market Questions.\n\n\n\nD.3 Varying models and evaluation months\n\nImprovement on non-Qwen models.\nResults over time.\n\n\nD.4 Consistency Evaluation\n\n\n\nE Qualitative Analysis of Reasoning Evolution During Training\n\nE.1 Example 1: Model stays incorrect but learns to hedge\nE.2 Example 2: Model goes from incorrect to correct\nE.3 Example 3: Model goes from correct to incorrect, but interestingly reasons about brier\n\n\n\nF Systematic Failure Modes in Model Reasoning\n\n1. Missing Information in Retrieved Articles\n2. Over-reliance on General Knowledge\n3. Entity Confusion: Selecting Wrong Person/Place/Organization\n\n\n\nG Prompt Templates\n\n\nG.1 Question Creation Pipeline\n\n\nG.2 Evaluation Prompts\n\n\nH Details on Compute and Cost\n\nCost.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Open-Ended Reasoning To Predict the Future\n\n\n\nNikhil Chandak1,3 Shashwat Goel1,2*\nAmeya Prabhu3,4†\\dagger Moritz Hardt1,3†\\dagger Jonas Geiping1,2,3†\\dagger\n\n1Max Planck Institute for Intelligent Systems  2ELLIS Institute Tübingen \n3Tübingen AI Center 4University of Tübingen\nEqual contribution  †\\daggerEqual co-supervision\n\n\nAbstract\nHigh-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.\n\n\n\n\n      \n\n Webpage\n      \n\n Code\n     \n\n Dataset and Models\n\n\n\n\n\n1 Introduction\n\nEvery day, people navigate decisions under uncertainty, due to incomplete evidence or competing hypotheses. The highest-stakes choices are inherently forward-looking: governments set policy while anticipating macroeconomic and geopolitical shifts; investors allocate capital amid market and regulatory uncertainty; individuals choose careers as technologies evolve; and scientists pursue research directions in search of the next breakthrough. Decades of work (tetlock2014forecasting) on human forecasting shows that while prediction is hard and skill varies widely, it is possible to train humans to become better forecasters. In fact, some “superforecasters” consistently outperform peers. While there is a ceiling to predictability in social systems (franklin1999real), we do not yet know where that ceiling lies in the real world.\n\n\nIf trained at scale for forecasting world events, Large Language Models (LLMs) may enjoy structural advantages over humans: they can ingest and synthesize vast, heterogeneous corpora across thousands of topics; and update predictions rapidly as new information arrives. Just like language models now show superhuman reasoning on some exam-style math and coding problems (icpc_openai_2025), in the future, language model forecasters may be able to come up with possibilities that humans miss. So in this work, we study:\n\n\n\nHow can we train language models to better forecast open-ended questions?\n\n\n\n\nScaling training data for forecasting. As forecasting world events is hard for humans, detailed and correct reasoning traces for forecasting are difficult to obtain. Fortunately, recent success in Reinforcement Learning (RL) for language models enables training with just the eventual outcome of the question (guo2025deepseek). Further, the static knowledge cutoff of LLMs enables a unique opportunity: events that resolve after the cutoff are in the future for the model. Even then, sourcing questions at scale for training forecasting abilities has a few key challenges. First, waiting for events to resolve is too slow as a feedback loop for training. Second, prediction markets–the primary source for existing forecasting questions–mostly consist of binary yes or no questions. As there is a 50% chance of success on these questions even with incorrect reasoning, they make for noisy rewards.\n\n\nInstead, we use global news, which covers a large number of salient events every day, to synthesize open-ended forecasting questions like “Who will be confirmed as the new prime minister of Ukraine on 17 July 2025?”. Our recipe for creating training data is entirely automated and scalable, with one language model extracting events from news articles to generate questions, and a different model filtering and rewriting questions to avoid leaking future information. For this work, we use this recipe with 250,000 articles up till April 2025, to create OpenForesight, a dataset of ∼\\sim 50,000 open-ended forecasting questions for training. To grade responses for open-ended questions, we use model-based answer matching (chandak2025answer) consistent with frontier benchmarks like Humanity’s Last Exam (phan2025humanity).\n\n\nEnsuring we truly improve forecasting. We take extensive measures to avoid the leakage of future information during training and evaluation. First, we do not use online search engines for sourcing news, as they have unreliable date cutoffs due to dynamic updates to documents and search ranking (paleka2025pitfalls). Instead, we use the CommonCrawl News corpus, which provides static, monthly snapshots of global news. Second, we only train on events until April 2025, which is when the Qwen3 model weights we train were released. Finally, we do not observe performance on the test set until the very end. Our test set is composed of diverse news sources, different from the ones used in training and validation, to ensure we are not just learning distributional biases of the training data.\n\n\nValidating design choices for LLM Forecasting Systems. We start from Qwen3 (yang2025qwen3technicalreport) 4B and 8B models with thinking enabled. We perform all ablations on a small validation set. We use dense retrieval with the Qwen3-8B Embedding model to provide forecasters relevant chunks from our offline news corpus. Despite a cautious approach of only retrieving articles until one month before the question resolution date to avoid leakage, the retrieved information leads to large improvements. Then, we train language models using RL with GRPO. For the reward function, we propose combining accuracy, and an adaptation of the brier score for open-ended responses (damani2025beyond). Ablations show rewarding accuracy alone hurts calibration, while optimizing only the brier score hurts exploration on hard questions. Our final methodology is illustrated in Figure 1.\n\n\nFigure 1: A summary of our methodology for training language model forecasters.\n\n\nFinal results. In Section˜6, we report results on our held-out test set of open-ended forecasting questions from May to August 2025, and FutureX (zeng2025futurex), an external forecasting benchmark. RL training on OpenForesight makes the predictions of our specialized 8B model competitive with much larger proprietary models in both accuracy and calibration. We also observe large improvements on consistency evaluations for long-term predictions (paleka2025consistency). Finally, we find calibration from our forecasting training generalizes to multiple out of distribution benchmarks.\n\n\nBy providing rigorous probabilistic predictions, open-ended forecasting systems could transform policy making, corporate planning, and financial risk management (tetlock2017expert). To promote forecasting research, we open-source our dataset, code, and models.\n\n\n\n\n2 Related Work\n\nForecasting World Events. Much prior work in Machine Learning and Statistics has focused on forecasting numeric or time-series data (BoxJenkins1976) in diverse domains like weather (Richardson1922), econometrics (Tinbergen1939) or finance (Cowles1933). Instead, our work focuses on the prediction of discrete world events, with both questions and answers described in natural language, also called judgemental forecasting (tetlock2016superforecasting). In the rest of our paper, we refer to this as forecasting for brevity. In prior work on evaluating language models for forecasting (zou2022forecasting; karger2024forecastbench), questions are primarily sourced from prediction markets like Metaculus, Manifold, and Polymarket. Prediction markets provide a platform for online participants to register predictions on questions like “Will Donald Trump win the US Presidential Election in 2024?\", which mostly have binary, yes or no, outcomes and have rapidly grown in popularity over the last few years.\n\n\nEvaluating LLMs for Forecasting. New information (before the event resolves) benefits forecasting. Thus, LLM forecasting work (zou2022forecasting; halawi2024approaching) provides"
  },
  {
    "title": "Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search",
    "url": "https://arxiv.org/abs/2512.25065v1",
    "source": "arxiv",
    "summary": "Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\n  We propose a new alter",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Motivation\n\n2.1 The pursuit of instance-optimality\n2.2 LLMs as heuristic-generators\n2.3 Can LLMs be used to synthesize entire heuristics?\n2.4 Common interface separating policy and mechanism\n\n\n\n3 Design\n\n\n3.1 Interface and Scaffolding\n\n3.1.1 Interface: Value\n3.1.2 Interface: Rank\n\n\n\n3.2 Defining an instance\n\n3.2.1 Automated instance generation\n\n\n\n3.3 Evolutionary search\n\n3.3.1 The template\n3.3.2 The evaluator harness\n\n\n\n\n\n4 Case study: Cache Eviction\n\n\n4.1 Searching for new eviction heuristics\n\n4.1.1 Interface and scaffolding\n4.1.2 An instance generator for caching\n4.1.3 Policy search for cache eviction\n4.1.4 Results\n\n\n\n4.2 Efficiency by design: Queue Topology\n\n4.2.1 Definition of a Queue Topology cache\n4.2.2 Searching the Queue Topology Space\n4.2.3 Experimental Setup\n4.2.4 Results\n\n\n\n\n\n5 Case study: Memory Tiering\n\n\n5.1 Memory Tiering Policies with Vulcan\n\n5.1.1 Interface and scaffolding\n5.1.2 Defining the instance\n5.1.3 Policy search for tiering policies\n\n\n5.2 Results\n\n\n\n6 Related Work\n\nLearning-based specialization.\nAI-driven algorithm and heuristic design.\nSystems use of LLMs.\n\n\n7 Discussion and Conclusion\n\nA LLM-driven survey of resource-management papers\n\nA.1 LLM Assisted Pipeline\n\n\nB Rank-based cache heuristic search: prompt and sample output\nC Queue Topology-based cache heuristic discovered for C7C_{7}\n\n\n\n\n\n\nVulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search\n\n\n\nRohit Dwivedula  Divyanshu Saxena  Sujay Yadalam  Daehyeok Kim  Aditya Akella\nThe University of Texas at Austin\n\n\n\nAbstract\nResource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management.\nDesigning performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.\nWe propose a new alternative: synthesizing instance-optimal heuristics – specialized for the exact workloads and hardware where they will be deployed – using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces.\nWith these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.\nWe use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.\n\n\n\n1 Introduction\n\nSystems research has long treated heuristic design as a manual craft.\nPerformance-critical systems rely on hand-written heuristics – tuned for typical conditions and deployed as static, one-size-fits-all policies.\nHowever, decades of work across caching [cache-ext, darwin], congestion control [mutant-nsdi25-cong-control], kernel queueing disciplines [no-silver-bullet-data-plane], memory tiering [arms], and more – demonstrate the same lesson: there is no universal heuristic.\nDifferent settings demand different heuristics, and specialization is essential for performance.\n\n\nSo, we continually tune and redesign (§2.1): we tweak congestion control algorithms for new network environments [pudica-cc-for-cloud-gaming], tailor prefetching or caching policies for emerging workloads [fetchbpf] or hardware [cachesack-atc22], and evolve queueing disciplines for new performance targets [cake].\nYet despite this retuning, we continue to find instances of sub-optimal performance from these manually-designed heuristics [decade-of-wasted-cores, starvation-sigcomm22, qdisc-scrr].\nThe heuristic design space continually shifts with context-dependent behaviors and changing performance trade-offs, making it increasingly difficult to discover and maintain the ‘right’ (i.e., performant) heuristics for each scenario.\nThis raises a fundamental question: how can we discover “the right heuristics” fast enough, to keep pace with ever-evolving deployment conditions?\n\n\nWe aim to tackle this ever-lasting challenge using Vulcan, a framework in which we recast the heuristic design problem as an an automated search problem using large language models (LLMs): instead of hand-crafting heuristics, we repeatedly invoke generative models to produce instance-optimal heuristics tailored to each deployment context.\nOur framework is inspired by recent advances in LLM-powered algorithm synthesis [funsearch, alphaevolve, evoprompting], which combine generative models with evolutionary search to discover expressive code that maximizes a target quantitative reward function.\n\n\nDirectly applying these search-based techniques to synthesize systems heuristics, however, is non-trivial (§2.3).\nModern systems heuristics tightly couple policy (high-level decision logic) with mechanism (low-level state, data structures, and control paths).\nAs a result, modifying or improving a heuristic requires coordinated reasoning about both intent and implementation.\nIn this setting, simply prompting an LLM to synthesize a complete end-to-end heuristic proves futile (see §2.3).\nOn the other hand, constraining the search to tiny tweaks on top of known algorithms [cc-tweak-bbr] limits the opportunity for genuine innovation.\n\n\nThe central challenge, therefore, is identifying how to leverage generative models where they are strongest today – reasoning over structured signals and expressing high-level decision logic – without requiring them to correctly implement complex mechanisms or manage intricate system state.\nVulcan resolves this tension by placing LLM-driven evolutionary search at the core of heuristic design, while exposing a simple, structured three-step pipeline (Figure˜3 in §3) that cleanly separates policy from mechanism and makes the search tractable.\n\n\nIn the first step of the pipeline, users define the search space by casting the problem into a narrow interface (§3.1).\nOur key observation is that many systems heuristics can be expressed in one of two forms: (i) a function that computes a value from system state (e.g., congestion controllers computing a congestion window), which we term a Value-type heuristic; or (ii) a function that ranks a set of objects and selects among them (e.g., schedulers ranking runnable tasks), which we term a Rank-type heuristic.\nThis interface captures the policy logic cleanly.\nThe corresponding mechanism – the concrete implementation that executes these decisions – is developed separately.\nThis separation exposes a rich mechanism design space for both policy interfaces, which we systematically explore in this paper (§3.1.1 and §3.1.2).\n\n\nSecond, users define the instance for which they seek a specialized heuristic – either by specifying a concrete workload–hardware pair, or by configuring an automated instance generator (§3.2) that algorithmically delineates instances based on observed workload characteristics and system signals.\nToday, heuristics are redesigned only when workloads or objectives change substantially, not because smaller changes are unimportant, but because manual heuristic design is expensive and time-consuming; consequently, even redesigned heuristics are typically made as broadly applicable as possible.\nVulcan fundamentally changes this cost model: by dramatically reducing the human cost of heuristic creation, it becomes practical to synthesize heuristics for much more narrower instances, enabling specialization even for modest shifts such as changes in input parameters, evolving access patterns, or transient workload phases.\nIn effect, Vulcan enables instance-optimal policies: rather than relying on coarse, one-size-fits-all heuristics that underperform across heterogeneous conditions, specialization becomes the default.\n\n\nFinally, users specify how to invoke the evolutionary search process (§3.3).\nThis includes describing the system context, objectives, and available state in natural language, and, critically, defining the evaluation harness used to assess candidate heuristics.\nAs with earlier stages of the pipeline, this step exposes important design trade-offs, such as balancing evaluation speed against fidelity, choosing appropriate metrics of interest, and combining multiple objectives into a single optimization target.\nWe discuss these considerations and illustrate, through concrete examples, how to effectively invoke the search process.\n\n\nIn this paper, we instantiate Vulcan for two policies: cache eviction (§4) and page promotion in tiered-memory systems (§5).\nWe discuss specific choices we made for both of these policies in the design space exposed by the aforementioned three-step process.\nAcross ten different instances, cache eviction policies synthesized by Vulcan either nearly match state-of-the-art baselines or outperform them by 1.94% to 69%. Memory tiering policies designed with Vulcan yield 2.5-7.9% improvement over state-of-the-art baselines, for a variety of workloads.\n\n\nWhile Vulcan automates the search for effective heuristics, it does not obviate the role of human designers.\nOur position is that human expertise is most valuable not in hand-crafting policy minutiae, but in structuring the problem so algorithmic search can succeed.\nWe argue that this structure arises from defining clean interfaces that control the space in which heuristics are discovered, providing the scaffolding that makes LLM-guided design both tractable and meaningful.\n\n\nConversely, Vulcan advocates using LLMs where it is most effective today: generating code that embodies candidate heuristics while respecting the interface and its invariants.\nThis stands in contrast to prior"
  },
  {
    "title": "Many Minds from One Model: Bayesian Transformers for Population Intelligence",
    "url": "https://arxiv.org/abs/2512.25063v1",
    "source": "arxiv",
    "summary": "Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into",
    "comments": [],
    "full_text": null
  },
  {
    "title": "On the geometry and topology of representations: the manifolds of modular addition",
    "url": "https://arxiv.org/abs/2512.25060v1",
    "source": "arxiv",
    "summary": "The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Setup and background\n\n3.1 Previous interpretations of modular addition in neural networks\n3.2 Topological Data Analysis\n\n\n\n4 Canonical Manifolds\n\n4.1 Simple neuron phase distribution dictates representation manifold\n\n4.2 Qualitative analysis of intermediate representations\n\nPrincipal component analysis (PCA).\nDistribution of post-ReLU activations.\nLinking PCA geometry and activation distributions.\n\n\n\n\n\n5 Methodology\n\nPhase Alignment Distributions.\nBetti number distribution.\n\n\n\n6 Results\n\n\n6.1 MLP-Add, Attention 0.0 and 1.0 (pizza and clock) are nearly the same\n\nPAD results.\nPrevious metrics.\nBetti number results.\n\n\n\n\n7 Discussion, Limitations, and Conclusion\n\nA Additional experimental setup details\n\nA.1 Training hyperparameters.\nA.2 Constructing representations\nA.3 Persistent homology\nA.4 Remapping procedure\n\n\nB Proof of theorem 4.1\n\nC Statistical significance of main results\n\nC.1 Figure 5\n\n\nD Torus distance metric\n\nE Previous interpretability metrics (zhong2023the)\n\nE.1 Definitions of gradient symmetricity and distance irrelevance\nE.2 Evaluating gradient symmetricity and distance irrelevance metrics\n\n\n\nF Additional commentary on the results\n\nF.1 The Attention 1.0 model is using attention as a weak non-linearity.\n\n\n\nG GPU-optimized computations\n\nG.1 GPU-optimized center-of-mass in circular coordinates\nG.2 GPU-vectorized distance irrelevance over all n2n^{2} input pairs (a,b)(a,b)\n\nG.3 GPU-optimized gradient symmetricity over all n3n^{3} triplets (a,b,c)(a,b,c)\n\nRuntime.\n\n\n\n\nH Hypothesis: modular addition as a factored map from the torus to the circle\n\n\n\n\n\nOn the geometry and topology of representations: the manifolds of modular addition\n\n\nGabriela Moisescu-Pareja \nMcGill University, Mila\n&amp;Gavin McCracken††footnotemark: \nMcGill University, Mila \n&amp;Harley Wiltzer \nMcGill University, Mila \n\nEqual contribution. {gabriela.moisescu‑pareja, gavin.mccracken}@mail.mcgill.ca\n  \nVincent Létourneau \nUniversité de Montréal, Mila \n&amp;Colin Daniels \nIndependent \n&amp;Doina Precup \nMcGill University, Mila, Google DeepMind \n&amp;Jonathan Love \nLeiden University \n\n\n\n\nAbstract\nThe Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition.\nIn this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations.\nOur methodology goes beyond the interpretation of individual neurons and weights.\nInstead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity.\nThis method reveals that each learned representation is a manifold that we can study utilizing tools from topology.\nBased on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.\n\n\n\n1 Introduction\n\nAs deep neural networks (DNNs) scale and begin to be deployed in increasingly high-stakes settings, it will be imperative to develop a concrete understanding of how these models perform computations and ultimately make decisions.\nTowards this end, research in mechanistic interpretability has focused on identifying sub-structures of these models—referred to as circuits—and understanding the function and formation of these circuits on the subtasks that they are responsible for.\nIn order to extract a generalizable understanding of circuits, researchers have formulated a key hypothesis universality (li2015convergent; olah2020zoom), which suggests that similar networks trained on similar data will form similar circuits. On the other hand, the manifold hypothesis (bengio2013representation; goodfellow2016deep), suggests that representation learning consists of finding a lower-dimensional manifold for the data. If these hypotheses were to be false, the task of interpreting large scale models becomes dire—there would be little hope of identifying common patterns across initializations, architectures, and datasets.\n\n\nYet, recent work claimed totally disparate and disjoint circuits were learned by DNNs trained on the exact same data using modular addition (a+b)modn=c(a+b)\\bmod n=c (zhong2023the). Their results seemingly provide a counter-example to the universality hypothesis, thus suggesting the pursuit of learning interpretable principles that generalize across tasks may be doomed. In fact, this suggests that the identification of simple circuits in larger neural networks could be combinatorially difficult—if different small-scale models trained on one task can learn totally different circuits with no commonality, then large language models (LLMs) may learn many disjoint circuits for a task within their weights simultaneously.\n\n\nOur primary contribution to interpreting modular addition, i.e. the dataset (a+b)modn=c(a+b)\\bmod n=c is the resolution of the seemingly disparate circuits found by zhong2023the.\n\n\n\n\n1.\n\nWe show that, under certain conditions on the structure of learned embeddings, all networks studied by zhong2023the and mccracken2025uncoveringuniversalabstractalgorithm learn preactivations having equivalent geometry and topology, which can be expressed in closed form;\n\n\n\n2.\n\nThese closed-form equations allow us to rigorously claim that all architectures we study universally make use of the same class of manifolds;\n\n\n\n3.\n\nWe introduce new tools, leveraging topological data analysis, to empirically validate these aforementioned conditions, providing extensive empirical evidence of shared representation geometry across networks.\n\n\n\n\n\nAltogether, our results restore the possibility that the universality hypothesis is true, as zhong2023the’s architectures are no longer shown to be a counter-example.\n\n\n\n\n2 Related Work\n\nDeep learning (DL) research increasingly turns to mathematical tasks as controlled settings for investigating learning phenomena and studying the fundamentals of DL (ghosh2025mathematical). Such tasks provide opportunities to (i) derive exact functional forms that yield theoretical insights beyond empirical studies (mccracken2021using), (ii) analyze how agents discover novel algorithms such as matrix multiplication or sorting (fawzi2022discovering; mankowitz2023faster), and (iii) develop methods to better interpret learned policies (raghu2018can). From toy models that capture superposition (elhage2022toy) to formal analyses of in-context learning (lu2024context; lu2025asymptotic), training on math tasks has become a productive way to study DL fundamentals. Within this agenda, mechanistic interpretability has produced especially influential results. By reverse-engineering networks trained on group-theoretic problems–such as modular addition (nanda2023progress; chughtai2023toy; gromov2023grokking; morwani2024feature; mccracken2025uncoveringuniversalabstractalgorithm; yip2024modularadditionblackboxescompressing; he2024learning; tao2025how; doshi2023grok), permutations (standergrokking; wu2024towards), and dihedral multiplication mccracken2025representations–researchers have uncovered mechanisms that speak to core hypotheses about representations (huh2024platonicrepresentationhypothesis), universality (olah2020zoom; li2015convergent), and algorithmic structure in DL (eberle2025position).\n\n\nAs the datasets of group multiplication aren’t linearly separable and modular addition (Cyclic group multiplication) is very well studied, it has become a standard testbed for toy interpretability research. It is ideal for asking: “What exactly do neural networks learn, and how is that computation represented internally?”. Two influential works stand out. First, nanda2023progress reverse-engineered transformers trained on modular addition and described their internal computations to illuminate the grokking phenomenon (power2022grokkinggeneralizationoverfittingsmall), giving progress measures to predict it. Building on this, chughtai2023toy claimed that the algorithm generalized to all group multiplications. Second, zhong2023the modified the transformer from nanda2023progress by interpolating between uniform and learnable attention. They claimed their networks learned two distinct circuits, either a Pizza or a Clock (described by nanda2023progress), proposing metrics to separate the two disjoint circuits.\n\n\nRecently however, mccracken2025uncoveringuniversalabstractalgorithm showed via abstraction, that across both MLPs and transformers, models converge to one unifying divide-and-conquer algorithm that approximates the Chinese Remainder Theorem and matches its logarithmic feature efficiency. They found first-layer neurons are best fit by degree-1 trigonometric polynomials, with later layers requiring degree-2, in contrast to the interpretation of nanda2023progress which modeled neurons as degree-2 trigonometric polynomials in all layers.\n\n\nWhile many works have focused on reverse-engineering specific algorithms in modular addition, relatively little has been done to systematically compare neural representations themselves. Tools from other domains–such as distributional hypothesis testing and topological data analysis (TDA)–offer complementary ways to characterize representations and may enrich mechanistic interpretability.\nFor instance, distributional methods such as maximum mean discrepancy (MMD) gretton2012kernel are rarely used in mechanistic interpretability, though widely applied elsewhere to quantitatively measure similarity, align domains ghifary2014domain; zhao2019learning, and test fairness deka2023mmd; kong2025fair. Topological data analysis (TDA) offers a complementary view: shahidullah2022topologicaldataanalysisneural used persistent homology to track how network layers preserve or "
  },
  {
    "title": "Reliable and Resilient Collective Communication Library for LLM Training and Serving",
    "url": "https://arxiv.org/abs/2512.25059v1",
    "source": "arxiv",
    "summary": "Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication libr",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings",
    "url": "https://arxiv.org/abs/2512.25055v1",
    "source": "arxiv",
    "summary": "This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction),",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Research Background\n\n2.1 Building Energy Management Systems (BEMS) and Interfaces\n2.2 LLM-based AI Agents for Smart Buildings and BEMS\n\n\n\n3 Methodology\n\n\n3.1 Conceptual Framework\n\n3.1.1 Perception Module\n3.1.2 Brain Module\n3.1.3 Action Module\n\n\n\n3.2 AI Agent Prototype Design\n\n3.2.1 Prototype’s perception module\n3.2.2 Brain module\n3.2.3 Action module\n\n\n\n3.3 Prototype Evaluation\n\n3.3.1 Experiment Design\n\n\n\n\n\n4 Results and findings\n\n4.1 Framework’s Generalizability\n4.2 Performance Differences Among Categories of User Queries\n\n4.3 AI Agent Responses to User Queries\n\n4.3.1 Energy Consumption &amp; Analysis\n4.3.2 Cost Management\n4.3.3 Data Visualization\n4.3.4 Device Control and Scheduling\n4.3.5 Memory\n4.3.6 General Information &amp; Support\n\n\n\n\n\n5 Discussion\n\n5.1 Correlation Among Evaluation Metrics\n5.2 Comprehensive and multidimensional BEMS AI Agents assessment\n5.3 Limitations and Future Studies\n\n\n6 Conclusion\n7 Generative AI Use\n\n\n\n\n\nContext-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings\n\n\nTianzhi He\n\n\nFarrokh Jazizadeh\n\n\n\nAbstract\nThis study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype’s performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.\n\n\nkeywords: \nLarge Language Model , Building Energy Management System , AI Agents , Human-Building Interaction\n\n\n\n\\UseRawInputEncoding\n\n\n\\affiliation\n[inst1]organization=School of Civil &amp; Environmental Engineering, and Construction Management, The University of Texas at San Antonio,addressline=BSE 1.310, One UTSA Circle,\ncity=San Antonio,\npostcode=78249,\nstate=TX,\ncountry=U.S.\n\n\n\\affiliation\n[inst2]organization=Department of Civil and Environmental Engineering, Virginia Polytechnic Institute and State University,addressline=200 Patton Hall, 750 Drillfield,\ncity=Blacksburg,\npostcode=24060,\nstate=VA,\ncountry=U.S.\n\n\n\n1 Introduction\n\nBuildings account for approximately 30% of global energy consumption [manic2016building], and are therefore critical in achieving energy efficiency. Targeted mitigation strategies, such as adaptive and human-centered control strategies [jung2019human], smart grid operations [shi2020artificial], Internet of Energy (IoE) adoption [hannan2018review], and fault detection frameworks [gu2024digital], combined with reactive and proactive user interactions, can facilitate efficient energy use in buildings [hannan2018review]. To this end, Building Energy Management Systems (BEMS) have been developed and extensively studied [hannan2018review]. These systems could optimize energy efficiency and support tasks such as human-centered operations, demand response, consumption and cost monitoring, and anomaly detection [hannan2018review, mariano2021review]. Residential BEMS alternatives serve as intelligent control centers in IoT-enabled smart homes to coordinate meters, sensors, actuators, smart appliances, and hubs. BEMS’ design concepts center around offering user-friendly interfaces for end users to manage energy use and achieve comfort and efficiency goals [mahapatra2022home]. In different contexts, users can rely on different interaction modalities. Recent advances in AI-powered technologies, especially those involving natural language processing, have improved user-BEMS interfaces. A prominent example is the use of smart speakers with virtual assistants, which serve as intelligent hubs for both user engagement and smart appliance control [he2022ai].\n\n\nPrevious studies have broadly investigated the applications and architectures of BEMS with various objectives, such as enhancing energy efficiency, reducing operational cost, load profiling, and improving user comfort [han2023home]. However, there have been challenges associated with their practical implementation in meeting such goals due to system complexities that may present a steep learning curve for users [baedeker2020interactive]. These challenges can be observed in both commercial and residential settings, although they may be more pronounced in the latter due to users’ varying levels of technical knowledge. Users may lack awareness of all system features, such as the availability and functionality of smart sensors and devices, or the optimal ways to use those systems, and might have limited ability to use them effectively, given the lack of intuitive interfaces. Moreover, users’ goals may vary widely, from reducing energy cost to enhancing comfort or optimizing the integration of distributed energy resources [mahapatra2022home]. However, users are often unaware of their energy use and generation patterns, and the relationship between those patterns and their behavior and actions, which limits the extent to which BEMS capabilities can be leveraged to improve energy efficiency.\n\n\nIntuitive user interfaces could be leveraged to partially address these complexities by facilitating user-technology interactions and guiding users in leveraging the system’s capabilities toward achieving their objectives. However, given the varied user objectives, the design of such interfaces calls for flexibility. Conventional interfaces have commonly struggled to adapt to these varied objectives, often failing to accommodate open-ended and general user goals or support complex decision-making processes [jin2023human]. In recent years, AI-powered technologies have emerged with the aim of facilitating user interactions through natural language interfaces. However, they often provide only basic support [avdic2020intelligibility]. For instance, as noted, voice-enabled smart speakers have become prevalent but are commonly limited to answering simple queries posed by users [avdic2020intelligibility]. The emergence of large language models (LLMs) has created opportunities to address those limitations. However, even when these interfaces are equipped with more advanced technologies, such as LLMs to understand and respond to user queries, their responses remain generic, whereas effective energy management, particularly when aligned with heterogeneous user objectives, requires awareness of building-specific hardware configurations, operational data patterns, and system attributes [han2023home]. To address these limitations, our research investigates how LLM-based AI agents can be designed and configured to go beyond AI agents with strong conversation capabilities and function as ambient AI agents that are capable of providing context-aware support for users’ goals. Context-awareness pertains to AI agent ability to understand the specifics of their operational environments, including energy consumption and generation patterns, and the operational states of appliances.\n\n\nLLM-based agentic frameworks, exemplified by platforms such as OpenAI’s ChatGPT series and Google’s Gemini series, may offer potential solutions to the aforementioned challenges. Trained on large-scale textual datasets, LLMs learn language patterns and contextual relationships to understand, generate, and interact in human language [brown2020language]. Researchers have successfully integrated LLMs into AI agents to enable autonomous reasoning steps (e.g., Chain-of-Thought prompting) that approximate certain aspects of human problem-solving and decision-making processes [xi2023rise, wang2024survey]. With unified frameworks comprising key modules, such as profile (instructions for AI agents), memory, planning, and action [wang2024survey] or perception, cognition (“brain”), and action [xi2023rise], different LLM-based AI agents have been designed to suit various tasks and applications. These include simulation agents in social science research [kovavc2023socialai, li2023you], tool-augmented agents in natural sciences [boiko2023emergent, bran2023chemcrow], and embodied agents for industrial automation and robotics [mandi2023roco, rana2023sayplan, wu2023tidybot]. The integration of LLM-based ambient AI agents, capable of observing, perceiving, analyzing, and adjusting a BEMS’ environment, has the potential to enable more flexible, convenient, and intuitive user interaction mechanisms between users and BEMS. Such mechanisms could, in turn, be leveraged to enhance user awareness and engagement in reactive and proactive building energy management systems and control applications [king2024sasha]. Our vision centers around a BEMS AI Agent that can communicate with users in natural language, independently and autonomously analyze building data to identify context-aware responses to users’ queries, and provide feedback and "
  },
  {
    "title": "AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG",
    "url": "https://arxiv.org/abs/2512.25052v1",
    "source": "arxiv",
    "summary": "Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-s",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Retrieval-Augmented Generation and Context Selection\n2.2 Retrieval-Augmented Generation and Context Selection\n2.3 Selection Algorithms and Theoretical Guarantees\n\n\n\n3 Method\n\n3.1 Redundancy-Aware Scoring Function\n\n3.2 Greedy Context Selection with Token Budget\n\n3.2.1 Limitation of Greedy Selection: Local Optima\n3.2.2 Theoretical Justification: Approximate Submodularity\n\n\n\n3.3 Adaptive β\\beta for Dynamic Token Constraints\n\n3.3.1 Motivation and Problem Statement\n3.3.2 Theoretical Derivation\n3.3.3 Why Use Expectation?\n3.3.4 Practical Implementation\n\n\n\n\n\n4 Theoretical Analysis\n\n\n4.1 Modularity and Supermodularity of Objective Components\n\n4.1.1 Relevance Term: Modularity\n4.1.2 Redundancy Term: Supermodularity\n4.1.3 Overall Structure: Modular Minus Supermodular\n\n\n\n4.2 Failure of Strict Submodularity\n\n4.2.1 Definition and Intuition of Submodularity\n4.2.2 Analysis of Our Objective\n4.2.3 Counterexample\n4.2.4 Visualization of Marginal Gain\n4.2.5 Practical Implications\n\n\n\n4.3 ε\\varepsilon-Approximate Submodularity and Greedy Guarantee\n\n4.3.1 Definition: ε\\varepsilon-Approximate Submodularity\n4.3.2 Bounding ε\\varepsilon for Our Objective\n4.3.3 Theoretical Role of Adaptive β\\beta\n4.3.4 Greedy Guarantee under ε\\varepsilon-Approximate Submodularity\n4.3.5 Practical Interpretation\n4.3.6 Summary and Theoretical Takeaway\n\n\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n\n5.2 Results and Analysis\n\n5.2.1 Results on the Open Domain (NQ) Dataset\n5.2.2 Quantitative Results on Domain-specific Dataset\n5.2.3 Qualitative/Human Evaluation\n5.2.4 Ablation and Efficiency\n\n\n5.3 Summary\n\n\n6 conclusion\n\n\n\n\n\nAdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG\n\n\n\nChao Peng  Bin Wang  Zhilei Long  Jinfang Sheng \nCentral South University \nYizhi Intelligent (YZInt) \nchao.peng@yzint.cn\n\n\n\nAbstract\nRetrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query–chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance–redundancy trade-off parameter to eliminate manual tuning and to adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits ε\\varepsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.\n\n\n\nAdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG\n\n\n\n\n\nChao Peng   Bin Wang   Zhilei Long   Jinfang Sheng\n\nCentral South University\n\nYizhi Intelligent (YZInt)\n\nchao.peng@yzint.cn\n\n\n\n\n\n\n1 Introduction\n\nRetrieval-augmented generation (RAG), first introduced by Lewis et al. (2020)1, has rapidly developed into a mainstream technique for enabling large language models (LLMs) to incorporate external knowledge and enhance performance on knowledge-intensive tasks. By integrating external documents or knowledge chunks with large models, RAG allows systems to dynamically access up-to-date, domain-specific information without frequent retraining, thereby improving access to up-to-date and domain-specific information without frequent retraining. Dense passage retrievers such as DPR proposed by Karpukhin et al. (2020)2, the ColBERT model by Khattab and Zaharia (2020)3, as well as later architectures like REALM4 and FiD1, have further improved the retrieval, encoding, and fusion mechanisms of RAG in practical applications. Today, RAG is widely applied in open-domain question answering6, scientific literature retrieval7, healthcare, enterprise knowledge management, and other scenarios, becoming a key paradigm for enabling LLMs to efficiently leverage external knowledge.\n\n\nDespite the significant advancements brought by RAG, particularly in enhancing knowledge timeliness, factual consistency, and task adaptability for LLMs, overall performance is still highly dependent on the quality of context chunks returned by the retrieval module. A persistent challenge is how to ensure that the retrieved results are not only highly relevant to the user’s query but also exhibit sufficient diversity in content. Numerous empirical studies have found that systems tend to return overlapping or near-duplicate chunks under top-k retrieval, especially when documents are chunked densely or the corpus is highly redundant89. Such redundancy not only wastes valuable context window (token budget) but can also obscure key information, limiting the model’s capacity for deep reasoning, comparative analysis, or multi-perspective synthesis, ultimately undermining factual accuracy and logical coherence.\n\n\nFor instance, in multi-hop question answering and multi-evidence reasoning tasks, if the retriever mainly returns paraphrased but essentially identical chunks, the model will struggle to acquire complete causal chains or diverse perspectives. This type of pseudo-relevance phenomenon has been shown to be an important contributor to hallucinations in RAG systems: when lacking sufficient heterogeneous evidence, the model may rely on internal priors and produce superficially coherent but externally unsupported and erroneous content10.\n\n\nTo address fragment redundancy and hallucination, Maximal Marginal Relevance (MMR) and its variants have been widely adopted in existing RAG systems as well as in emerging frameworks such as GraphRAG11 and FreshLLM12. By balancing relevance and diversity in the set of retrieved candidates, MMR can reduce redundancy and improve coverage. While effective in practice, these approaches still suffer from notable limitations: (1) their weight parameters are highly dependent on manual tuning and cannot dynamically adapt to the structure of different candidate pools or token budgets; (2) they only support local greedy selection, making it difficult to achieve set-level global optimality and potentially missing the best combination of chunks.\n\n\nTo systematically solve the issues of context redundancy, limited diversity, and cumbersome parameter tuning in RAG, this paper proposes and implements a novel context scoring and selection mechanism based on redundancy-awareness and fully adaptive weighting. Specifically, we design a set-level scoring function that not only measures the relevance between each candidate chunk and the query, but also explicitly penalizes redundancy among the selected fragments. The entire scoring process is mathematically modeled as the weighted difference between a relevance term and a redundancy term, with a tunable parameter β\\beta controlling the trade-off between them. Building on this, we further propose a dynamic and adaptive β\\beta adjustment strategy: by analyzing the average length, mean relevance, and redundancy distribution of the candidate pool, we derive a closed-form solution for the redundancy weight that adapts to different queries and budget constraints. This strategy provides a principled closed-form estimate of β\\beta, eliminating the need for manual parameter tuning or external heuristics. We also provide engineering implementations for instance-level β\\beta, as well as interfaces for fine-tuning on small validation sets and domain-specific customization, improving the method’s robustness and usability in real-world, variable scenarios.\n\n\nTo validate the theoretical foundation and practical effectiveness of the proposed approach, we conduct a rigorous theoretical analysis of the redundancy-aware adaptive selection framework. By proving the ε\\varepsilon-approximate submodularity of the objective function, we establish approximation guarantees for greedy selection under ε\\varepsilon-approximate submodularity. Our analysis further reveals how the adaptive β\\beta mechanism dynamically suppresses excessive redundancy, enhances coverage, and prevents performance degradation, especially in complex data distributions or under tight budget constraints. Experiments demonstrate that the proposed method significantly outperforms traditional baselines on key metrics such as answer quality, coverage, and redundancy control, both in open-domain and biomedical knowledge retrieval tasks.\n\n\nThe main contributions of this work are as follows:\n\n\n(1)We propose a redundancy-aware, fully adaptive context scoring and selection framework that systematically addresses key challenges such as context redundancy and limited diversity in RAG scenarios;\n\n\n(2)We design a set-level relevance-redundancy joint scoring function and derive a closed-form adaptive solution to the β\\beta parameter, enabling dynamic, instance-specific and budget-specific trade-off without manual tuning;\n\n\n(3)We provide a theoretical analysis and proof of ε\\varepsilon-approximate submodularity for our objective, offering theoretical guarantees for the near-global optimality of the greedy selection algorithm.\n\n\n\n\n2 Related Work\n\n\n2.1 Retrieval-Augmented Generation and Context Selection\n\nModern RAG systems typically employ dense or hybrid retrievers, such as DPR2, ColBERT3, or bi-encoder models, for initial passage retrieval, followed by subsequent ranking or selection modules to assemble the final context. This architecture achieves state-of-the-art results on benchmarks like Natural Questions and MS MARCO 6; 14, but practical deployment in real-world domains r"
  },
  {
    "title": "Generative Classifiers Avoid Shortcut Solutions",
    "url": "https://arxiv.org/abs/2512.25034v1",
    "source": "arxiv",
    "summary": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, ins",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Modeling Language as a Sequence of Thoughts",
    "url": "https://arxiv.org/abs/2512.25026v1",
    "source": "arxiv",
    "summary": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficienc",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Other Works\n\nLearning sentence representations.\nSequence compression and gisting.\nRecurrence and memory in Transformers.\n\n\n\n3 Main Design Principles\n\n\n3.1 Data Preparation\n\nSentence splitting.\nSentence tokenization.\nSentence streams as training examples and batch construction.\n\n\n\n3.2 Model\n\nSentence representation and memory.\nCross-attention to sentence memory.\nToken embeddings and self-attention.\nLearnable memory gates.\nContext seeding.\n\n\n\n3.3 Training schedules\n\nSentence-stream curriculum (chunk-size curriculum).\nDown-weighting frequent boundary token (EOS).\n\n\n\n\n\n4 Results\n\nExperimental setup.\n\n4.1 Scaling Efficiency\n\nTraining dataset scaling.\nModel size scaling.\n\n\n\n4.2 Other Baselines\n\nGPT-2 with sentence boundary bias.\nFixed token-span recurrence.\nGPT-2 + gist masking.\n\n\n\n4.3 Reversal curse evaluation (Father–Son completion).\n\nReversed condition.\nNormal condition.\n\n\n\n4.4 Ablations\n\nIncreasing per-layer capacity.\nGradient flow through memory is essential.\nExternal vs. in-context memory.\nOther design choices.\n\n\n\n\n\n5 Conclusion\n\nFuture work.\nLimitations.\n\n\nA Memory Gate Dynamics Over Training\n\nB Numerical Results\n\nB.1 Scaling Efficiency Results\nB.2 GPT-2 with Sentence Boundary Bias Results\nB.3 TG with Fixed Token-Span Recurrence Results\nB.4 GPT-2 + Gist Masking Results\n\n\n\nC Other TG Design Principles\n\nC.1 Batch construction: Uniform Token-Budget Bucketing\nC.2 Scheduled Dropouts\n\n\n\n\n\n\n\nModeling Language as a Sequence of Thoughts\n\n\n\nNasim Borazjanizadeh \nIndependent Researcher \nnasim.borazjanizadeh@gmail.com\n&amp;James L. McClelland11footnotemark: 1\nStanford University \njlmcc@stanford.edu\nAuthors contributed equally to this work.\n\n\nAbstract\nTransformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction—tokens and sentence-level \"thought\" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5–8% more data and ~33–42% more parameters to match TG’s loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.\n\n\n\n1 Introduction\n\nPrior work in cognitive science suggests that in humans, language functions primarily as a transmission system for expressing underlying thoughts, rather than as the constitutive medium of cognition itself [1, 2, 3]. On this view, comprehension involves decoding the serial stream of language to construct mental models that encode temporal sequences, causal relations, and entity properties in a situation [4, 5]. These models organize information into coherent gestalts– latent representations whose properties are not reducible to the sum of their parts [6, 7, 8]. From this perspective, linguistic input is a lossy, serial code for these underlying structures, not the format in which mental models are stored.\n\n\nLarge language models (LLMs) have achieved remarkable capabilities by modeling language as a sequence of tokens and optimizing next-token prediction [9, 10, 11, 12]. However, standard Transformers represent context primarily as token embeddings tied to positional indices; consequently, the model’s internal state is inherently coupled to linear word order [13, 14, 15]. Moreover, because training is driven by surface-level distributional statistics—estimating which token is likely to follow a local pattern of surrounding tokens—models can generate fluent text without constructing a coherent, globally consistent situation model, which leads to systematic failures on compositional, multi-step, and long-context tasks [16, 2, 17, 18].\n\n\nSeveral phenomena highlight the fragility of this token-centric approach. Lepori et al. [19] describe contextualization errors, where lower Transformer layers fail to resolve ambiguities because the representations of relevant prior context are not yet fully contextualized. Similarly, the reversal curse shows that models trained on \"A is B\" often fail to infer \"B is A,\" treating the two directions of a relational fact as distinct statistical patterns rather than a unified semantic relation [20, 21]. Furthermore, LLMs remain strikingly data-inefficient: state-of-the-art models are pretrained on trillions of tokens [9, 12], exceeding the linguistic input of a human child (∼\\sim30 million words) by five orders of magnitude [22]. Yet, humans can often infer the meaning of a new word from a single exposure by linking it to existing conceptual structures [23, 24]. These limitations motivate architectures that build latent representations at a higher level of abstraction than tokens that encode knowledge and causal relations into robust \"thought-level\" states.\n\n\nFigure 1: The Thought Gestalt (TG) model learns token and sentence representations jointly using a single next-token prediction objective; by retaining the computation graph at memory writes, token prediction loss from future sentences backpropagates through cross-attention to refine the parameters that generated earlier sentence representations.\n\n\nIn this work, we introduce the Thought Gestalt (TG) model, a recurrent Transformer architecture designed to model language at two levels of abstraction: tokens and sentence-level thoughts (Figure 1). Our approach is motivated by cognitive evidence that human perceivers segment continuous streams of information into discrete events to organize memory, where the exact verbatim form is short-lived and confined to a narrow span, while earlier content is retained as stable, high-level representations of events and relations [25, 26, 27]. To emulate this, TG predicts the tokens of one sentence at a time while updating and attending to an abstracted memory of sentence representations. Each sentence representation is a d_model vector that captures the sentence’s holistic meaning, or \"gestalt,\" rather than merely summing its tokens. TG uses sentence boundaries as a structural proxy for thought boundaries. While not a one-to-one map to thought boundaries, sentence boundaries serve as natural cognitive junctures where background information is integrated and concepts are updated [28, 29]. Learning sentence representations is thus a first step toward building generative systems that can learn situation models and latent thought representations.\n\n\nTG is\ninspired by the Sentence Gestalt model [30], a recurrent network that incrementally maps a sequence of words onto a single distributed event representation—the “gestalt”—from which role–filler propositions can be decoded. In TG, the holistic vector representation of each sentence is extracted from the contextualized hidden state at the end-of-sentence (&lt;EOS&gt;) position and written into a recurrent memory that conditions subsequent token predictions. Crucially, TG optimizes sentence representations by maintaining their computation graph across memory writes, allowing next-token prediction losses on later sentences to backpropagate through cross-attention into the parameters that produced earlier sentence gestalts. To keep backpropagation tractable, we control the computation depth of memory representations with a simple curriculum over the number of sentences processed continuously without resetting the memory.\n\n\nThis design distinguishes TG from recent architectures that learn sentence embeddings via auxiliary objectives, such as next-sentence prediction, token reconstruction, sentence ordering, or contrastive alignment, on top of a language-modeling loss [31, 32, 33, 34, 35]. Empirical analyses suggest such auxiliary losses can be brittle and even degrade downstream generalization [36, 37]. In contrast, TG employs no separate encoder or auxiliary sentence-level loss: token and sentence representations are generated using the same set of model parameters and are supervised solely via their contribution to the next-token prediction objective. Furthermore, unlike standard Transformers and Gisting [38], where summary tokens are processed in parallel with lexical tokens via attention masks—forcing lower layers to attend to noisy, unrefined representations of prior context—TG enables even early layers to attend to a memory of fully contextualized gestalts, directly targeting the contextualization errors identified by Lepori et al. [19].\n\n\nEmpirically, TG yields systematic gains in both learning efficiency and representational robustness.\nTG is consistently more data and parameter efficient than matched GPT-2 baselines pretrained on WikiText-103.\nIn dataset-scaling experiments (1212–5050M training tokens; N≈85N\\approx 85M non-embedding parameters), TG achieves 22–4%4\\% lower test perplexity at every scale (e.g., 23.223.2 vs. 24.024.0 at 5050M), corresponding to an effective 55–8%8\\% reduction in the training tokens needed to reach the same loss (Figure 2a).\nIn parameter scaling experimen"
  },
  {
    "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
    "url": "https://arxiv.org/abs/2512.25023v1",
    "source": "arxiv",
    "summary": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-an",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Convergence of the generalization error for deep gradient flow methods for PDEs",
    "url": "https://arxiv.org/abs/2512.25017v1",
    "source": "arxiv",
    "summary": "The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approx",
    "comments": [],
    "full_text": null
  },
  {
    "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes",
    "url": "https://arxiv.org/abs/2512.25015v1",
    "source": "arxiv",
    "summary": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a v",
    "comments": [],
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2512.25015v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2512.25015v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 31 Dec 2025]\n    Title:MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes\n    Authors:Siddhant Agarwal, Adya Dhuler, Polly Ruhnke, Melvin Speisman, Md Shad Akhtar, Shweta Yadav            View a PDF of the paper titled MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes, by Siddhant Agarwal and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.\n    \n\n    \n    \n              \n          Comments:\n          Accepted by AAAI 2026\n        \n\n          Subjects:\n          \n            Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2512.25015 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2512.25015v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2512.25015\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Shweta Yadav [view email]          [v1]\n        Wed, 31 Dec 2025 18:06:21 UTC (2,134 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes, by Siddhant Agarwal and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2025-12\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to "
  },
  {
    "title": "Diffusion Language Models are Provably Optimal Parallel Samplers",
    "url": "https://arxiv.org/abs/2512.25014v1",
    "source": "arxiv",
    "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequ",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis",
    "url": "https://arxiv.org/abs/2512.24999v1",
    "source": "arxiv",
    "summary": "We introduce \\textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Gi",
    "comments": [],
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; math &gt; arXiv:2512.24999v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Mathematics  Statistics Theory\n    \n\n    \n      arXiv:2512.24999v1 (math)\n    \n\n\n  \n    \n  [Submitted on 31 Dec 2025]\n    Title:Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis\n    Authors:Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani            View a PDF of the paper titled Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis, by Seunghoon Paik and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:We introduce \\textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $\\theta_0$ with current iterate $\\theta_T$, the basic inequality upper bounds $f(\\theta_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $\\theta_0$, $\\theta_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.\n    \n\n    \n    \n              \n          Comments:\n          47 pages, 3 figures (7 subfigures)\n        \n\n          Subjects:\n          \n            Statistics Theory (math.ST); Machine Learning (cs.LG); Numerical Analysis (math.NA); Optimization and Control (math.OC); Machine Learning (stat.ML)\n        \n          Cite as:\n          arXiv:2512.24999 [math.ST]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2512.24999v1 [math.ST] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2512.24999\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Seunghoon Paik [view email]          [v1]\n        Wed, 31 Dec 2025 17:49:37 UTC (466 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis, by Seunghoon Paik and 3 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: math.ST\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2025-12\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n        cs.NA\n        math\n        math.NA\n        math.OC\n        stat\n        stat.ML\n        stat.TH\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn mor"
  },
  {
    "title": "Classifying long legal documents using short random chunks",
    "url": "https://arxiv.org/abs/2512.24997v1",
    "source": "arxiv",
    "summary": "Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 t",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Efficiently Estimating Data Efficiency for Language Model Fine-tuning",
    "url": "https://arxiv.org/abs/2512.24991v1",
    "source": "arxiv",
    "summary": "While large language models (LLMs) demonstrate reasonable zero-shot capability across many downstream tasks, fine-tuning is a common practice to improve their performance. However, a task's data efficiency--i.e., the number of fine-tuning examples needed to achieve a desired level of performance--is often unknown, resulting in costly cycles of incremental annotation and retraining. Indeed, we demo",
    "comments": [],
    "full_text": null
  },
  {
    "title": "DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments",
    "url": "https://arxiv.org/abs/2512.24985v1",
    "source": "arxiv",
    "summary": "Vision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments--a core necessity that has been largely overlooked. To addre",
    "comments": [],
    "full_text": "\n\n\n\nI INTRODUCTION\n\nII RELATED WORK\n\nII-A Embodied Question Answering Benchmark\nII-B Handling Low-Light Images\n\n\n\nIII DarkEQA: Dataset Construction  and QA Pair Generation\n\n\nIII-A Low-Light Image Synthesis for Benchmark Inputs\n\n\nIII-A1 Noise-free low-light image synthesis\n\nDecoding to linear RGB\nExposure scaling\nRe-encoding to sRGB\n\n\n\nIII-A2 Physics-motivated low-light image synthesis\n\nUnprocessing (sRGB →\\rightarrow RAW)\nNoise formation in RAW\n(1) Photon shot noise\n(2) Read noise\n(3) Row noise\n(4) Quantization noise\nSimplified ISP (RAW →\\rightarrow sRGB)\n\n\n\n\nIII-B Dataset Construction\nIII-C Dataset Statistics\n\n\n\nIV EXPERIMENTS\n\nIV-A Experimental Setup\n\nIV-B Baseline Models\n\nBlind LLMs\nVLMs\nLLIE model\n\n\n\nIV-C Results and Analysis\n\nImpact of illumination drop and sensor noise\nEffectiveness of low-light image enhancement (LLIE) pre-processing\nModel-specific accuracy\nQuestion-wise accuracy\n\n\n\n\nV CONCLUSION\n\n\n\n\n\nDarkEQA: Benchmarking Vision-Language Models for \nEmbodied Question Answering in Low-Light Indoor Environments\n\n\n\nYohan Park1, Hyunwoo Ha2, Wonjun Jo2, and Tae-Hyun Oh1\nCorresponding author: Tae-Hyun Oh.\nThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.1Yohan Park and Tae-Hyun Oh are with the School of Computing, Korea Advanced Institute of Science and Technology (KAIST), Daejeon 34141, South Korea\n(e-mail: john.a.park@kaist.ac.kr, taehyun.oh@kaist.ac.kr).2Hyunwoo Ha and Wonjun Jo are with the Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang 37673, South Korea\n(e-mail: hyunwooha@postech.ac.kr, jo1jun@postech.ac.kr).\n\n\nAbstract\nVision Language Models (VLMs) are increasingly adopted as central reasoning modules for embodied agents. Existing benchmarks evaluate their capabilities under ideal, well-lit conditions, yet robust 24/7 operation demands performance under a wide range of visual degradations, including low-light conditions at night or in dark environments–a core necessity that has been largely overlooked. To address this underexplored challenge, we present DarkEQA, an open-source benchmark for evaluating EQA-relevant perceptual primitives under multi-level low-light conditions. DarkEQA isolates the perception bottleneck by evaluating question answering from egocentric observations under controlled degradations, enabling attributable robustness analysis. A key design feature of DarkEQA is its physical fidelity: visual degradations are modeled in linear RAW space, simulating physics-based illumination drop and sensor noise followed by an ISP-inspired rendering pipeline.\nWe demonstrate the utility of DarkEQA by evaluating a wide range of state-of-the-art VLMs and Low-Light Image Enhancement (LLIE) models. Our analysis systematically reveals VLMs’ limitations when operating under these challenging visual conditions. Our code and benchmark dataset will be released upon acceptance.\n\n\n\nI INTRODUCTION\n\n\nAdvances in vision-language models (VLMs) have significantly enhanced robotic capabilities, improving semantic scene understanding [peng2023openscene, liang2023open], spatial reasoning [dorbala2022clip, yang2025thinking], and vision-language-action (VLA) policies [zitkovich2023rt, driess2023palm, ahn2022can]. Numerous Embodied Question Answering (EQA) benchmarks have been proposed to assess this commonsense reasoning for embodied agents, largely assuming well-lit, ideal visual conditions [das2018embodied, majumdar2024openeqa]. However, household robots are often intended for 24/7 operation, which means they will frequently encounter low-light scenarios, such as nighttime, entering dark rooms or power blackouts. As robot deployment in varied environments grows, robust perception under these conditions is not an edge case but a core necessity [keller2020illumination]. Accordingly, benchmarks that explicitly stress-test embodied VLM reasoning under low illumination are essential to quantify real-world robustness. Nevertheless, acquiring large-scale, real-world low-light images with clean, paired annotations—ideally with corresponding well-lit reference views—is challenging and costly, which has hindered the construction of such benchmarks. As a result, existing benchmarks have largely overlooked systematic evaluation of VLM-based reasoning and perception under degraded illumination, limiting their ability to predict real-world robustness.\n\n\nTo fill this evaluation void, we present DarkEQA, an open-source benchmark to systematically measure the perceptual primitives for embodied tasks under low-light conditions. The design of DarkEQA is primarily grounded in a physically based formulation, where all visual degradations are modeled at the RAW sensor data level (or in linear RGB space). This follows the physics of illumination and sensor noise to realistically simulate real-world Image Signal Processing (ISP) scenarios. Moreover, to ensure benchmark integrity and prevent potential data contamination [shumailov2024ai], all Question Answering (QA) pairs are deterministically generated via rule-based procedure, rather than depending on commodity VLM services. QA generation results in a family of queries targeting perceptual primitives, including from simple object recognition (e.g., “Is there a cushion in the image?”) to affordance reasoning (e.g., “I want to sleep, is this room suitable for this?”).\n\n\nDarkEQA provides 9.4k question–image pairs, a standardized evaluation protocol, and a public codebase to reproduce our low-light degradation pipeline. Our DarkEQA benchmarks a diverse set of vision–language models (VLMs), including both open- and closed-source systems [liu2024improved, li2024llava, wang2025internvl3, yang2025qwen3, hurst2024gpt]. We also evaluate a state-of-the-art low-light image enhancement (LLIE) model [feijoo2025darkir] as a preprocessing baseline. Our evaluation yields two observations. First, while humans can recognize structural scene information of input images from intensity contrast, all tested VLMs show a clear performance decline as the images degrade. Second, while LLIE preprocessing can improve performance under certain degradation levels, its effects are not consistently positive; in some cases, it yields limited gains or even leads to performance degradation, highlighting its practical limitations. Together, these results show that current VLM-based EQA pipelines remain brittle under low-light corruption, and that perceptual enhancement alone is insufficient as a general solution, motivating robustness-oriented evaluation and method development.\n\n\n\n\n\n\n\n\n\nFigure 2: \nLow-light synthesis pipeline with disentangled illumination and noise factors.\nTo generate controlled low-light inputs for our benchmark, we adopt an ISP-inspired unprocessing and noise formulation from prior work [brooks2019unprocessing, wei2021physics]. Crucially, we produce paired variants for each original image to disentangle failure sources in VLM-based EQA: (a) a physics-based branch (top) that unprocesses sRGB to Bayer RAW, injects four noise components in RAW, and then applies EV drop and gamma compression; and (b) a noise-free branch (bottom) that applies the same EV drop in linear RGB without noise injection. This paired design enables separate evaluation of performance degradation due to illumination reduction versus sensor noise. The bottom-left panel summarizes the sRGB→\\rightarrowRAW unprocessing steps, and the bottom-right panel visualizes the four noise components (shot, read, row-pattern, and quantization noise) as independent signals. The small red boxes in the read and row noise examples indicate zoomed-in crops for visualization.\n\n\n\n\n\nII RELATED WORK\n\n\n\nII-A Embodied Question Answering Benchmark\n\n\nEmbodied Question Answering (EQA), first introduced by Das et al. [das2018embodied], requires an agent to navigate and interact with an environment to answer a question. Early benchmarks primarily centered on static 3D scenes, such as ScanQA [azuma2022scanqa], to evaluate tasks like object identification and basic spatial relationships. OpenEQA [majumdar2024openeqa] is introduced to assess an agent’s exploration capabilities, posing diverse questions related to scene state, agent knowledge, and object attributes. Concurrently, a substantial body of research has focused on benchmarking deep spatial reasoning [zhu2025cospace, dang2025ecbench, zhang2025open3dvqa, song2025robospatial], evaluating complex object relationships [zhai2025memory]. Other works have pushed towards dynamic and procedural understanding, utilizing 3D scene graphs [armeni20193d, saxena2024GraphEQA, ali2025graphpad] or focusing on multimodal reasoning [hasegawa2025promqa, kamath2023gpr].\n\n\nHowever, those existing EQA benchmarks often overlook real-world robustness. While NoisyEQA [wu2024noisyeqa] addresses query noise, robustness to adverse environmental conditions remains a significant gap. Notably, no current benchmark evaluates EQA in dark or low-light situations, which are common in the real world. We therefore introduce the first benchmark for indoor embodied question answering in dark environments to assess robustness under poor visibility.\n\n\n\n\nII-B Handling Low-Light Images\n\n\nRecent research has explored two main directions for addressing the challenges of low-light visual perception.\nThe first line of work targets robust recognition under low-light conditions, aiming to improve performance on specific vision tasks such as depth estimation, object detection, or pose estimation [gasperini_morbitzer2023md4all, wang2021regularizing, lee2023human, sasagawa2020yolo, spencer2020defeat]. Although these approaches demonstrate impressive robustness, they are typically constrained to single-task, highlighting a gap between low-light robustness in isolated perception and the embodied reasoning required in EQA.\nThe second research stream focuses on low-light image enhancement (LLIE), wh"
  },
  {
    "title": "A Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts",
    "url": "https://arxiv.org/abs/2512.24980v1",
    "source": "arxiv",
    "summary": "We introduce a two-sort weighted modal logic for possibilistic reasoning with fuzzy formal contexts. The syntax of the logic includes two types of weighted modal operators corresponding to classical necessity ($\\Box$) and sufficiency ($\\boxminus$) modalities and its formulas are interpreted in fuzzy formal contexts based on possibility theory. We present its axiomatization that is \\emph{sound} wit",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Formal concept analysis\n2.2 Fuzzy set, possibility theory, and possibilistic reasoning\n\n\n3 Fuzzy Formal Context and Cut-based Concepts\n\n4 Two-sorted Weighted Modal Logic\n\n4.1 Application\n\n\n\n5 Necessity and Sufficiency Fragments of 2WML\n\n5.1 The system 2WKB\n5.2 The system 2WKF\n\n\n6 Extension to Multi-Relational Fuzzy Contexts\n\n7 Concluding Remarks\n\n7.0.1 Acknowledgements\n\n\n0.A Properties of Fuzzy Contexts and Cut-Based Concepts\n\n0.B Proof of Theorem 5.1\n\n0.B.1 Proof of Lemma 2\n0.B.2 Proof of Model Existence Lemma\n0.B.3 Proof of Truth Lemma\n\n\n\n\n\n\n11institutetext: Institute of Information Science, Academia Sinica, Taipei 115, Taiwan \n11email: {prosen,liaucj}@iis.sinica.edu.tw\nA Modal Logic for Possibilistic Reasoning with Fuzzy Formal Contexts\n\n\nProsenjit Howlader\n\n  \nChurn-Jung Liau\nThe corresponding author\n\n\nAbstract\nWe introduce a two-sort weighted modal logic for possibilistic reasoning with fuzzy formal contexts. The syntax of the logic includes two types of weighted modal operators corresponding to classical necessity (□\\Box) and sufficiency (⊟\\boxminus) modalities and its formulas are interpreted in fuzzy formal contexts based on possibility theory. We present its axiomatization that is sound with respect to the class of all fuzzy context models. In addition, both the necessity and sufficiency fragments of the logic are also individually complete with respect to the class of all fuzzy context models. We highlight the expressive power of the logic with some illustrative examples. As a formal context is the basic construct of formal concept analysis (FCA), we generalize three main notions in FCA, i.e., formal concepts, object oriented concepts, and property oriented concepts, to their corresponding cc-cut concepts in fuzzy formal contexts. Then, we show that our logical language can represent all three of these generalized notions. Finally, we demonstrate the possibility of extending our logic to reasoning with multi-relational fuzzy contexts, in which the Boolean combinations of different fuzzy relations are allowed.\n\n\n\n1 Introduction\n\nFormal concept analysis (FCA) is a mathematical theory for studying structures of concepts [FCA]. In FCA, a formal context (or simply context), aka polarity, is a triple (G,M,I)(G,M,I), where GG and MM are sets of objects and attributes, respectively, and I⊆G×MI\\subseteq G\\times M is a binary incidence relation connecting them. Given a context, a formal concept is a pair of subsets of objects and attributes (A,B)(A,B) such that AA is the set of objects possessing all attributes in BB and BB is the set of attributes shared by all objects in AA. For the formal concept, AA and BB are called its extent and intent, respectively. Furthermore, inspired by rough set theory [pawlak2012rough], notions of rough concepts including object and property oriented concepts, have been also defined [duntsch2002modal, yao2004comparative, yao2004concept].\n\n\nThe theoretical framework of FCA has evolved into an indispensable tool for knowledge discovery from complex datasets which are usually presented as formal contexts . Traditionally, most data mining methods represent the discovered knowledge as simple if-then rules. However, this is not expressive enough for the representation of more complicated knowledge. Moreover, when applying to decision-making, simple rules can facilitate only shallow reasoning by matching their antecedents with the decision situation and taking consequents of matched rules. By employing a logical approach, we can address the expressivity of knowledge representation formalisms for data mining from formal contexts.\n\n\nFor the knowledge representation and reasoning with formal contexts, several logical formalisms have received much attention [dunn, CWFSPAPMTAWN, Conradie2017167, HCgame, CH, HOWLADER2023115, ijcrs23]. In a recent work, it is shown that two-sorted Boolean modal logic can serve as a reliable base for the representation of both formal and rough concepts [bmlpl]. The logic is a synergy of two-sorted modal logic and Boolean modal logic.\n\n\nOn one hand, two-sorted modal logic is an instance of many-sorted polyadic modal logic, which was studied in [mspml] and then extended to a many-sorted hybrid logic in [LEUSTEAN]. Because a formal context has two domains and an incidence relation, it is an appropriate semantic model for two-sorted modal logic. On the other hand, Boolean modal logic combines two dual branches of modal logic: one with the box modality (□)(\\square) representing necessity, and the other with the window modality (⊟)(\\boxminus) representing sufficiency  [Gargov1987]. These two kinds of modalities correspond to basic operators in rough approximations and FCA, and thus facilitate the representation of different notions of concepts in formal contexts.\n\n\nWhile FCA has been an effective tool for data analysis, because of the pervasive uncertainty and vagueness of information in complex datasets, the binary relation in formal contexts may be not two-valued in many applications. In particular, to deal with vague information in formal contexts, various fuzzy FCA methods have been proposed [Antoni2018, Belohlavek04, Belohlavek11, BelohlavekV05, BritoBEBC18], in which a formal context is extended to a fuzzy context by replacing the crisp incidence relation with a fuzzy one. The derivation operators map fuzzy sets on GG to fuzzy sets on MM and vice versa, forming a Galois connection. Consequently, fuzzy concepts are pairs of fuzzy sets mutually determined by these operators. Typically, membership functions of these fuzzy sets measure the degree to which an attribute (object) connects with all objects (attributes). However, unlike the existing approaches, we further define the derivation operators using (strict) cc-cuts for c∈[0,1]c\\in[0,1] on these fuzzy sets, mapping a set of objects to the attributes whose degree of connecting with all these objects meets or exceeds cc, and vice versa.\n\n\nAs logics for fuzzy formal contexts are under-explored yet, there exist only a few works on this topic. Among them, two approaches to connecting logic with fuzzy formal contexts have been proposed in [CodaraEGV18]. One is to follow an idea in [CodaraV15] by interpreting fuzzy logic formulas as formal concepts, and the other is to consider semantic evaluations and formulas of a fuzzy logic as objects and attributes of a fuzzy context, respectively, and use the fuzzy relation between them to denote the satisfaction degree of a formula in an evaluation.\n\n\nIn this paper, we present yet another approach based on the weighted extension of the above-mentioned two-sorted Boolean modal logic. By considering the fuzzy incidence relation as a class of possibility distributions, we use weighted modalities to represent uncertainty measures corresponding to different operators in FCA as explicated in [DuboisSP07]. In addition, as these weighted modalities correspond to our cut-based derivation operators, we define cc-cut concepts as the generalization of formal and rough concepts in fuzzy contexts and show that our logic can represent and reason with these generalized concepts.\n\n\nThe remainder of the paper is organized as follows. The next section contains background knowledge on FCA, fuzzy sets, and possibility theory. In Section 3, we introduce basic operators for constructing generalized concepts in fuzzy formal contexts. In addition, we define the cut-based notions of formal and rough concepts. In Section 4, we present the two-sorted weighted modal logic and show its application to the representation of different cc-cut concepts. While the completeness of the proposed logic remains an open issue, we prove that its necessity and sufficiency fragments are individually complete in Section 5. In Section 6, we discuss the possible extension of the logic to reasoning with multi-relational fuzzy contexts. We conclude the paper with some directions of future work in Section 7.\n\n\n\n\n2 Preliminaries\n\nIn this section, we introduce some preliminary notions about FCA and possibility theory.\n\n\n\n2.1 Formal concept analysis\n\nLet us recall the definition of a context (G,M,I)(G,M,I) introduced in the previous section.\nA given context induces two derivation operators +:(𝒫(G),⊆)→(𝒫(M),⊇)+:(\\mathcal{P}(G),\\subseteq)\\rightarrow(\\mathcal{P}(M),\\supseteq) and −:(𝒫(M),⊇)→(𝒫(G),⊆)-:(\\mathcal{P}(M),\\supseteq)\\rightarrow(\\mathcal{P}(G),\\subseteq), where for all A∈𝒫​(G)A\\in\\mathcal{P}(G) and B∈𝒫​(M)B\\in\\mathcal{P}(M):\n\n\nA+={m∈M∣∀g​(g∈A⇒g​I​m)},B−={g∈G∣∀m​(m∈B⇒g​I​m)}.A^{+}=\\{m\\in M\\mid\\forall g(g\\in A\\Rightarrow gIm)\\},\\penalty 10000\\ \\penalty 10000\\ \\penalty 10000\\ B^{-}=\\{g\\in G\\mid\\forall m(m\\in B\\Rightarrow gIm)\\}.\n\n\nIt can be shown that the pair of maps forms a Galois connection. Moreover, every Galois connection is associated with some formal context, and this correspondence is bijective. For details, we refer the reader to [textfca].\n\n\nA (formal) concept is a pair of sets (A,B)(A,B) such that A+=BA^{+}=B and A=B−A=B^{-}, where AA and BB are called its extent  and intent, respectively. We denote the set of all concepts by 𝐁​(𝕂)\\mathbf{B}(\\mathbb{K}). It forms a complete lattice and is denoted by 𝐁¯​(𝕂)\\underline{\\mathbf{B}}(\\mathbb{K}).\n\n\nDüntsch and Gediga [duntsch2002modal] defined sufficiency, dual sufficiency, possibility and necessity operators based on a context. In particular, for a context 𝕂:=(G,M,I)\\mathbb{K}:=(G,M,I), A⊆GA\\subseteq G, and B⊆MB\\subseteq M, the pairs of possibility and necessity operators are defined as follows:\n\n\nA◇:={m∈M∣∃g​(g​I​m∧g∈A)},A□:={m∈M∣∀g​(g​I​m⇒g∈A)}A^{\\Diamond}:=\\{m\\in M\\mid\\exists g(gIm\\wedge g\\in A)\\},\\penalty 10000\\ \\penalty 10000\\ \\penalty 10000\\ A^{\\Box}:=\\{m\\in M\\mid\\forall g(gIm\\Rightarrow g\\in A)\\}.\n\n\nB◇:={g∈G∣∃m​(g​I​m∧m∈B)},B□:={g∈G∣∀m​(g​I​m⇒m∈B)}B^{\\Diamond}:=\\{g\\in G\\mid\\exists m(gIm\\wedge m\\in B)\\},\\penalty 10000\\ \\penalty 10000\\ \\penalty 10000\\ \\penalty 10000\\ B^{\\Box}:=\\{g\\in G\\mid\\forall m(gIm\\Rightarrow m\\in B)\\}."
  },
  {
    "title": "SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets",
    "url": "https://arxiv.org/abs/2512.24977v1",
    "source": "arxiv",
    "summary": "Sequential structure is a key feature of multiple domains of natural cognition and behavior, such as language, movement and decision-making. Likewise, it is also a central property of tasks to which we would like to apply artificial intelligence. It is therefore of great importance to develop frameworks that allow us to evaluate sequence learning and processing in a domain agnostic fashion, whilst",
    "comments": [],
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Theoretical Foundations\n\n\n\n2 Architecture, design and implementation\n\n\n2.1 SymSeq: Symbolic sequence generation and analysis\n\n\n2.1.1 Main components\n\nshort\nshort\nshort\nshort\nshort\n\n\n2.1.2 Complexity-Guided Grammar Synthesis\n\n\n\n2.2 SeqBench: Token embeddings and datasets\n\n\n2.2.1 Main components\n\nshort\nshort\nshort\n\n\n\n\n\n\n\n3 Applications and use-cases\n\n\n3.1 Psycholinguistics and cognitive psychology: designing experimental paradigms and datasets\n\n3.1.1 Non-adjacent dependencies and temporal binding\n3.1.2 AGL dataset generation\n\n\n\n3.2 Cognitive computing benchmarks: evaluating biological, neuromorphic and machine learning architectures on psychologically meaningful tasks\n\n3.2.1 Biological neural networks\n3.2.2 Artificial and neuromorphic neural networks\n\n\n3.3 Syntactic structure of animal behavior: analyzing latent dynamics from the perspective of generative linguistics\n\n\n\n4 Discussion and Outlook\n\n4.1 Grammar inference and the decidability constraint\n4.2 Future directions\n\n\n\n5 Methods\n\n\n5.1 Network architectures\n\nBiological neural networks.\nArtificial and neuromorphic neural networks.\n\n\n\n5.2 Metrics\n\nToken-level metrics\nString-level metrics\nString-set level metrics\nGrammar-level metrics\n\n\n\n\n\n\n\n\n\n\nSymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets\n\n\nZajzon, Barna\nb.zajzon@fz-juelich.de\nInstitute for Advanced Simulation (IAS-6), Jülich Research Centre, Germany\n\n\nBouhadjar, Younes\n\nPeter-Grunberg Institute, Neuromorphic Software Ecosystems (PGI-15), Jülich Research Centre, Germany\n\n\nFabre, Maxime\n\nPeter-Grunberg Institute, Neuromorphic Software Ecosystems (PGI-15), Jülich Research Centre, Germany\n\nGroningen Cognitive Systems and Materials Center (CogniGron), University of Groningen\n\n\nSchmidt, Felix\n\nPeter-Grunberg Institute, Neuromorphic Software Ecosystems (PGI-15), Jülich Research Centre, Germany\n\nRWTH Aachen University, Aachen, Germany\n\n\nOstendorf, Noah\n\nInstitute for Advanced Simulation (IAS-6), Jülich Research Centre, Germany\n\nRWTH Aachen University, Aachen, Germany\n\n\nNeftci, Emre\n\nPeter-Grunberg Institute, Neuromorphic Software Ecosystems (PGI-15), Jülich Research Centre, Germany\n\nDepartment of Electrical Engineering and Information Technology, RWTH Aachen University, Aachen, Germany\n\n\nMorrison, Abigail\n\nInstitute for Advanced Simulation (IAS-6), Jülich Research Centre, Germany\n\nDepartment of Computer Science 3 - Software Engineering, RWTH Aachen University, Aachen, Germany\n\n\nDuarte, Renato\n\nCenter for Neuroscience and Cell Biology (CNC-UC), University of Coimbra, Portugal\n\nCenter for Innovative Biomedicine and Biotechnology (CIBB-UC), University of Coimbra, Portugal\n\n\n\nAbstract\nSequential structure is a key feature of multiple domains of natural cognition and behavior, such as language, movement and decision-making. Likewise, it is also a central property of tasks to which we would like to apply artificial intelligence. It is therefore of great importance to develop frameworks that allow us to evaluate sequence learning and processing in a domain agnostic fashion, whilst simultaneously providing a link to formal theories of computation and computability.\nTo address this need, we introduce two complementary software tools: SymSeq, designed to rigorously generate and analyze structured symbolic sequences, and SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks to evaluate the performance of artificial learning systems in cognitively relevant domains.\nIn combination, SymSeqBench offers versatility in investigating sequential structure across diverse knowledge domains, including experimental psycholinguistics, cognitive psychology, behavioral analysis, neuromorphic computing and artificial intelligence.\nDue to its basis in Formal Language Theory (FLT), SymSeqBench provides researchers in multiple domains with a convenient and practical way to apply the concepts of FLT to conceptualize and standardize their experiments, thus advancing our understanding of cognition and behavior through shared computational frameworks and formalisms. The tool is modular, openly available and accessible to the research community.\n\n\n\n1 Introduction\n\nSymbols and symbol systems are central constructs in both cognitive sciences and theoretical computer science, providing a unique formal link between cognition and computation [stephen_jose__hanson_ea1b50cb]. This relation, manifested as a branch of the mathematical theory of computation known as Formal Language Theory (FLT), grounds the investigation of complex cognitive, psychological and behavioral processes onto a systematic terminology and set of conventions for describing generative rule systems and the structures they generate [gerhard_ja_ger_bbb90244, w_tecumseh_fitch_ece3a959]. From simple pattern perception to language and sequential decision-making, such formal systems allow the algorithmic specification of complex cognition and can be used to understand, model and analyze cognition and behavior [roma_patel_9b004cc0, christopher_i__petkov_3a05f34b, steven_t__piantadosi_50d37955].\n\n\nEvaluating sequence learning and processing has long been central to advancing our understanding of both artificial and natural intelligence, from Lashley’s seminal work on serial order and the syntax of action [lashley_problem_1951] to contemporary large-scale neural sequence models. Accounting for the full scope of sequence processing is nevertheless challenging: system-specific constraints and the many dimensions along which task characteristics and model demands vary mean that most tasks probe only a limited portion of the problem space. Canonical experimental paradigms and principled benchmarks make it possible to address this challenge by providing shared tasks and evaluation protocols that allow systematic comparison and reveal key limitations in learning and generalization [hurlstone_memory_2014, sculley2018winners, Geirhos2020, Raji2021]. The rise of linguistically-competent and computationally proficient artificial intelligence [nova_spivack_8421a206] has led to renewed interest in tasks and metrics that can evaluate systems’ capacity to learn and generalize complex structured sequences [aarohi_srivastava_7ed2343a, james_fodor_5915e049, ankur_sikarwar_ace45230]. Ideally, and given that human intelligence remains the ultimate reference point, such tasks should be grounded in cognitive theory and capture relevant aspects of human reasoning, behavior and cognition [brenden_m__lake_2ea4879c, jos__hern_ndez_orallo_93f4b461, kriegeskorteCognitiveComputationalNeuroscience2018, palmeriModelbasedCognitiveNeuroscience2017]. We argue that moving beyond monolithic approaches or discrete language-theoretic classes toward a more fluid understanding of temporal processing requires tools that capture its multifaceted nature and complexity at multiple scales, enabling the principled design of tasks with controllable sequence complexity. These allow researchers to scrutinize the properties of sequential structure, generative capacity and the requirements it imposes on the computational machinery.\n\n\nStrongly inspired by decades of human psycholinguistic research, the tools we present here address these needs by providing a comprehensive framework for the specification, generation, manipulation, and analysis of symbolic sequences and corresponding embeddings, as well as a benchmark suite of datasets and tasks that can be used to evaluate the performance of artificial cognitive systems. With applications spanning experimental psychology, behavioral analysis, neuromorphic computing, and artificial intelligence, among others, our framework aims to facilitate interdisciplinary research and the adoption of formal constructs to multiple scientific domains that investigate structured sequential data [daniel_silver_e3edf12e, gerard_o_regan_0ab1b793]. We illustrate the scope and applicability with concrete use-cases in areas such as dataset generation for artificial grammar learning experiments, the analysis and comparison of behavioral ethograms in different experimental conditions, the evaluation of neuromorphic, artificial neural network architectures, and the mechanistic dissection of neural sequence models.\n\n\nWhile other tools and approaches have been proposed over the years that address specific aspects of symbolic sequence processing, cognitive modeling and computational benchmarking, they largely do so with a narrow focus. For example, specialized software exists to automatically generate and select string sets for artificial grammar learning (AGL) experiments [Bailey2008], to evaluate large language models’ meta-learning and generalization from minimal exposure [ekin_aky_rek_b020c7d7], to systematically deploy standardized neuroscience experiments and tasks [manuel_molano_maz_n_c01c84dc] or to investigate neuromorphic embodied agents’ performance in standardized environments and tasks [philipp_weidel_a8eab012, jakob_jordan_07d4b27f], among others. An integrated, broadly applicable tool requires a higher level of abstraction and generalization. The adoption of FLT concepts, as we propose here, has proven insightful for predicting the generalization limits of artificial neural networks [Deletang22_chomsky], proposing architectural augmentations that could increase their expressiveness [Deletang22_chomsky, Sima2020] or to expose computational similarities and differences between biological and artificial intelligence [Fitch2014].\n\n\nThese studies provide powerful tools and valuable theoretical insights into the algorithmic and computational properties and limitations of neural architectures designed to cope with sequential structural regularities. They demonstrate the importance of appropriate computational formalisms and benchmark tasks, but are not easily generalizable across research domains. The ubiquity of temporal structure in animal cognition and behavior, as well as the power of adopting forma"
  },
  {
    "title": "Attribution-Guided Distillation of Matryoshka Sparse Autoencoders",
    "url": "https://arxiv.org/abs/2512.24975v1",
    "source": "arxiv",
    "summary": "Sparse autoencoders (SAEs) aim to disentangle model activations into monosemantic, human-interpretable features. In practice, learned features are often redundant and vary across training runs and sparsity levels, which makes interpretations difficult to transfer and reuse. We introduce Distilled Matryoshka Sparse Autoencoders (DMSAEs), a training pipeline that distills a compact core of consisten",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Evaluating the Impact of Compression Techniques on the Robustness of CNNs under Natural Corruptions",
    "url": "https://arxiv.org/abs/2512.24971v1",
    "source": "arxiv",
    "summary": "Compressed deep learning models are crucial for deploying computer vision systems on resource-constrained devices. However, model compression may affect robustness, especially under natural corruption. Therefore, it is important to consider robustness evaluation while validating computer vision systems. This paper presents a comprehensive evaluation of compression techniques - quantization, prunin",
    "comments": [],
    "full_text": null
  },
  {
    "title": "Large language models and the entropy of English",
    "url": "https://arxiv.org/abs/2512.24969v1",
    "source": "arxiv",
    "summary": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations betwee",
    "comments": [],
    "full_text": "\n\n\n\nA Corpora\nB More about models\nC Data summary\n\n\n\n\n\nLarge language models and the entropy of English\n\n\nColin Scheibnera,b,†\n\n  \nLindsay M. Smitha,d,†\n\n  \nWilliam Bialeka,c,d\n\naJoseph Henry Laboratories of Physics, bPrinceton Center for Theoretical Science, and cLewis–Sigler Institute for Integrative Genomics, Princeton University, Princeton NJ 08544 USA\ndInitiative for the Theoretical Sciences, The CUNY Graduate Center, 365 Fifth Avenue, New York NY 10016 USA\n\n\n(December 31, 2025)\n\nAbstract\nWe use large language models (LLMs) to uncover long–ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to N∼104N\\sim 10^{4} characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large NN. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long–ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.\n\n$\\dagger$$\\dagger$footnotetext: These authors contributed equally to this work.\n\nEntropy helps us to describe phenomena ranging from steam engines to black holes. Entropy also quantifies our intuitive notion of information, and this measure is unique in satisfying plausible constraints Shannon_1948. Shannon used language as an accessible example of these ideas, and played a “guessing game” with human subjects to estimate the entropy of written English Shannon_1951. The results of this experiment gave bounds on the conditional entropy for a single character in the text given knowledge of the previous NN characters; tighter bounds can be obtained by asking subjects not just to guess but to bet Cover+King_1978. Decades later, performance at the guessing game is essentially the objective function for training large language models.\n\n\nShannon interpreted his results as suggesting that the conditional entropy per character approached a plateau for N∼100N\\sim 100. Hilberg argued that the data are actually consistent with a decay ∼1/N\\sim 1/\\sqrt{N} Hilberg_1990; if this continues to larger NN, then the entropy of texts would be sub–extensive Ebeling+Nicolis_1992; Debowksi_2011. There are hints that the mutual information between characters decays as a power of their separation and that the number of distinct words in long texts grows as a fractional power of their length Ebeling+Poschel_1994. These could all be signs of long range, scale-invariant correlations, evoking connections to statistical physics.\nBut these data were limited to modest NN, and there is a long history of skepticism about whether statistical structure tells us anything interesting about language Chomsky_1956.\n\n\nLarge language models (LLMs) give us a new tool to explore the entropy of text. In particular, we can play Shannon’s guessing game, asking the model to generate the next character (or, more precisely, the next token) given a sequence of KK tokens from real text. Since the model returns the full distribution rather than a single guess, we can compute directly the model’s estimate of the conditional entropy. We also can use the model to encode the actual next token, and this code length bounds the conditional entropy of the real text. Importantly we can do these computations out to K∼103−104K\\sim 10^{3}-10^{4}. Along the path to these models the community has assembled enormous data sets from which we can estimate the decay of correlations out to similarly large separations.\n\n\nHere we show that across many classes of text the code lengths generated by well-trained LLMs agree with one another and continue to decrease at scales N&gt;103N&gt;10^{3}, in some cases with no sign of a plateau. This is possible only if there are effective interactions that reach across these long distances. A corollary is that there is a small but significant mutual information between characters at these large separations. Going beyond the mean, we see structure in the distribution of code lengths including an approximately power–law divergence in the distribution at small code lengths and large NN, pointing toward an emergence of near certainty about the next character in a long sequence. Finally we explore how these large NN behaviors develop as the models learn. We present a brief overview of these results and discuss their implications. A fuller account will be given elsewhere SSB2.\n\n\nLLMs are trained on ensembles of text obtained by scraping the internet. These corpora can be used for the study of the structure of language itself.\nWe use a publicly available example, the English variant of the Colossal Clean Crawled Corpus (C4), a collection curated from &gt;3.65×108&gt;3.65\\times 10^{8} internet documents using automated filters raffel2019exploring; dodge2021documenting.\nMore focused data sets include a collection of pages from English Wikipedia and Simple English Wikipedia wikidump, as well as news articles from the BBC li2024latesteval; the last are especially useful since they are time stamped, so we can test models with text that could not have contributed to their training. We also study a corpus of narratives paired with their summaries and analyses kryscinski2021booksum.\nFinally, we analyze poetry drawn from two sources: the Gutenberg Poetry Corpus, a collection of poetry mined from the Project Gutenberg book collection parrish_gutenberg_poetry_corpus, and a more carefully curated collection from the Poetry Foundation suayptalha_poetry_foundation_poems_hf. For details see Appendix A.\n\n\nWe use four models for our analysis: OLMo 2 1B olmo20242olmo2furious, Llama 3.2 1B grattafiori2024llama3herdmodels, Qwen3 8B qwen3technicalreport, and a 1.7B parameter model we train on a subset of the DCLM dataset li2024datacomplm. Each model uses a different tokenizer to convert text at the character level to discrete tokens. All models are open-source in their model weights (parameters); OLMo 2 and our 1.7B model have the advantage of being trained with open datasets as well. All models consist of a decoder-only Transformer vaswani2017attention. We note that we trained DCLM 1.7B on at least 2 orders of magnitude less data than the models released by large labs, and the maximum context length that it accommodates is 2048 tokens; this model also has no mid- or post-training. Despite this, we still see similar behaviors in the code length across context length. For details see Appendix B.\n\n\nConsider a segment of text, defined as a sequence of characters {c1,c2,⋯,cN}\\{c_{1},\\,c_{2},\\,\\cdots,\\,c_{N}\\}. LLMs start by converting this into a sequence of tokens {t1,t2,⋯,tK}≡t1​⋯​K\\{t_{1},\\,t_{2},\\,\\cdots,\\,t_{K}\\}\\equiv t_{1\\cdots K}, where the average ratio of characters per token N/K∼4−5N/K\\sim 4-5 is specific to each model and text corpus (Table 1, Appendix C). Given this input LLMs return the probability distribution of the next token, PK​(tK+1|t1​⋯​K)P_{K}(t_{K+1}|t_{1\\cdots K}); we recall that this distribution provides a basis for encoding the next token with a code length Shannon_1948; Mezard+Montanari_2009\n\n\n\nℓ​(t1​⋯​K)=−log⁡PK​(tK+1|t1​⋯​K).\\ell(t_{1\\cdots K})=-\\log P_{K}(t_{K+1}|t_{1\\cdots K}).\n\n(1)\n\n\nThe mean code length\n\n\n\nL​(K)=⟨ℓ​(t1​⋯​K)⟩data=−⟨log⁡PK​(tK+1|t1​⋯​K)⟩dataL(K)=\\langle\\ell(t_{1\\cdots K})\\rangle_{\\rm data}=-\\langle\\log P_{K}(t_{K+1}|t_{1\\cdots K})\\rangle_{\\rm data}\n\n(2)\n\n\nis an upper bound on the conditional entropy of the real distribution out of which the text is drawn. The sum over sequence length bounds the total entropy\n\n\n\nS​(K)≤∑k=1KL​(k),S(K)\\leq\\sum_{k=1}^{K}L(k),\n\n(3)\n\n\nand this is proportional to the loss function typically used for training the model, e.g. with K=4096K=4096 for the OLMo 2 1B pre-training.\n\n\n\nFigure 1: Code length vs. context length across models. We evaluate L​(N)L(N) from Eq (2) for the C4 corpus using the OLMo 2 1B olmo20242olmo2furious, Llama 3.2 1B grattafiori2024llama3herdmodels, Qwen3 8B qwen3technicalreport, and DCLM 1.7B li2024datacomplm models. While there are differences of detail, all of these well trained models yield remarkably similar results. Error bars computed from the variance across random subsets of the data are smaller than the “hash” from point–to–point variability. \n\n\nFigure 1 shows the code length L​(N)L(N) for the C4 corpus in the OLMo 2 1B olmo20242olmo2furious, Llama 3.2 1B grattafiori2024llama3herdmodels, Qwen3 8B qwen3technicalreport, and DCLM 1.7B li2024datacomplm models. To make comparisons across models meaningful, we use characters rather than tokens as the unit of length, converting via the mean N/KN/K for each model. We note that these results are within Shannon’s bounds at N=100N=100 Shannon_1951, but the code length continues to fall slowly out to the largest N∼104N\\sim 10^{4}.\n\n\nThree of the four models agree almost perfectly for L​(N&gt;103)L(N&gt;10^{3}), and none show signs of a plateau at large NN. This is consistent with the conjecture that L​(N→∞)L(N\\rightarrow\\infty) might actually vanish Hilberg_1990, though the decay is much slower than one would estimate from data at smaller NN. It is perhaps surprising that the models disagree so much for N&lt;100N&lt;100. The objective function used in training is the total code length for strings of thousands of tokens, which certainly emphasizes large NN. But the models apparently can succeed in compressing long texts even while missing some of the small NN structure that we expect arises from rules of spelling and grammar. Training of the smaller DCLM 1.7B model was entirely in our hands, and performance is not as good as for the models trained by larger groups, but the qualitative behavior is ver"
  },
  {
    "title": "The Impact of LLMs on Online News Consumption and Production",
    "url": "https://arxiv.org/abs/2512.24968v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) change how consumers acquire information online; their bots also crawl news publishers' websites for training data and to answer consumer queries; and they provide tools that can lower the cost of content creation. These changes lead to predictions of adverse impact on news publishers in the form of lowered consumer demand, reduced demand for newsroom employees, and an",
    "comments": [],
    "full_text": "\n\n\n\n\n1 Introduction\n\nPublisher traffic declines, but the decline emerges only after August 2024.\nSome publishers decide to block LLM traffic, but blocking reduces both total and human traffic for large publishers.\nNo near-term contraction in newsroom hiring relative to other roles.\nPublishers do not scale up textual production; they shift toward richer pages and embedded components.\n\n\n2 Contribution and Related Work\n\n3 Data\n\n\n3.1 Website traffic\n\nSimilarWeb\nComscore Web-Behavior Panel\n\n\n\n3.2 Publisher characteristics and content quantity\n\nRobots.txt and page structure\nContent quantity proxies.\n\n\n3.3 Job postings and employment\n3.4 Sample construction and merge across sources\n\n\n\n4 Results\n\n4.1 Effect 1: Publisher Traffic Declines Emerge Only After August 2024\n4.2 Effect 2: Blocking GenAI Bots Reduces Both Total and Human Traffic for Large Publishers\n4.3 Effect 3: No Near-Term Contraction in Newsroom Hiring Relative to Other Roles\n4.4 Effect 4: Publishers Shift Toward Media-Rich Content Rather Than Scaling Text\n\n\n5 Discussion and Conclusion\n\n\n\n\n\nThe Impact of LLMs on Online News Consumption and Production\n\n\n\nHangcheng Zhao\nRutgers Business School. hangcheng.zhao@rutgers.edu\n  \nRon Berman\nThe Wharton School of the University of Pennsylvania. ronber@wharton.upenn.edu\n\n(December 2025\n )\n\nAbstract\nLarge language models (LLMs) change how consumers acquire information online; their bots also crawl news publishers’ websites for training data and to answer consumer queries; and they provide tools that can lower the cost of content creation. These changes lead to predictions of adverse impact on news publishers in the form of lowered consumer demand, reduced demand for newsroom employees, and an increase in news “slop.” Consequently, some publishers strategically responded by blocking LLM access to their websites using the robots.txt file standard.\nUsing high-frequency granular data, we document four effects related to the predicted shifts in news publishing following the introduction of generative AI (GenAI).\nFirst, we find a consistent and moderate decline in traffic to news publishers occurring after August 2024. Second, using a difference-in-differences approach, we find that blocking GenAI bots can have adverse effects on large publishers by reducing total website traffic by 23% and real consumer traffic by 14% compared to not blocking. Third, on the hiring side, we do not find evidence that LLMs are replacing editorial or content-production jobs yet. The share of new editorial and content-production job listings increases over time. Fourth, regarding content production, we find no evidence that large publishers increased text volume; instead, they significantly increased rich content and use more advertising and targeting technologies.\nTogether, these findings provide early evidence of some unforeseen impacts of the introduction of LLMs on news production and consumption.\n\n\nKeywords: Large language models, Generative AI, News production, News consumption, Online News Publishers, Staggered Difference-in-Differences, Synthetic Difference-in-Differences, Labor demand, AI Slop, Digital Media\n\n\n\n\n1 Introduction\n\nGenerative AI (GenAI) and large language models (LLMs) are reshaping how consumers discover and consume information online. Unlike search-based discovery intermediaries (e.g., Google) that primarily redirect users to publishers through links, LLM-mediated interfaces can directly provide answers and summaries, potentially substituting away from click-through visits.111The Economist, “AI is killing the web. Can anything save it?” https://www.economist.com/business/2025/07/14/ai-is-killing-the-web-can-anything-save-it, accessed Dec 18, 2025. Recent evidence shows that LLM adoption reduces traditional search activity and downstream browsing to smaller sites, and users click external links less often when AI summaries appear in search results.222Pew Research Center 2025, “Google users are less likely to click on links when an AI summary appears in the results,” https://www.pewresearch.org/short-reads/2025/07/22/google-users-are-less-likely-to-click-on-links-when-an-ai-summary-appears-in-the-results/, accessed Dec 18, 2025. These developments raise a few interesting questions: (1) to what extent does generative AI function as a substitute for publisher traffic and monetization? (2) how do publishers respond? and (3) is this response effective?\n\n\nNews publishers have been at the center of the debate about the impact of LLMs because LLMs use news content to improve their model training and also to answer user queries. Since the business model of news publishers depends heavily on discovery and referral from intermediaries, some publishers have been quick to respond by blocking LLM access to their websites using the robots.txt standard.333RFC 9309 https://www.rfc-editor.org/rfc/rfc9309.html, accessed Dec 18, 2025. The impact of this blocking can be three-fold: it can reduce bot traffic from LLMs used to scan the websites to train models; it can reduce bot traffic by LLMs used to answer user queries; and it can reduce referral traffic by LLMs sending users to visit the news publisher. Publishers can also try to respond by lowering headcount which would save costs, or by increasing content production using LLMs to attract more consumers.\n\n\nGiven these different potential effects, it remains less understood (1) what the overall impact of LLMs on publisher traffic is; (2) whether blocking access is an effective strategic response; and (3) how LLM tools affect the supply of news content by publishers through hiring decisions and changes in the quantity and format of content they produce.\n\n\nWe address these questions by constructing a high-frequency, publisher panel that links daily traffic to strategic blocking rules for LLM crawlers, publishers’ page structure, and publishers’ hiring. We combine daily domain-level visits from SimilarWeb with historical robots.txt files and HTML snapshots from the HTTP Archive and employer-linked job postings from Revelio. To separate total visits to publishers (which may include bot and crawler traffic) from real audience demand, we also use household browsing data from the Comscore Web-Behavior Panel.444https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/comscore/ Using these data, we document four empirical facts characterizing early shifts in the news publishing industry following the diffusion of generative AI, which are summarized below.\n\n\nPublisher traffic declines, but the decline emerges only after August 2024.\n\nFollowing the introduction of ChatGPT in November 2022 and Google AI Overview in May 2024, there were widespread predictions and anecdotal reporting of a substantial drop in publisher traffic. Our analysis using SimilarWeb visit data for news domains shows that traffic is broadly stable through mid-2023 and does not exhibit an immediate post-ChatGPT and post-Google AI Overview collapse. We employ a multiple change-point detection approach [Killick et al., 2012] and identify persistent breaks in traffic patterns, most prominently in November 2023 and August 2024, after which traffic levels shift to a lower level. We estimate a synthetic difference-in-differences (SDID) [Arkhangelsky et al., 2021] model using log traffic in the six months before and six months after each detected breakpoint, with the top 100 retail websites as the control group. Following the August 2024 break, traffic to news publishing websites decreased by approximately 13.2% relative to the control group. The point estimates for the November 2023 break are also negative but statistically insignificant.\n\n\n\nSome publishers decide to block LLM traffic, but blocking reduces both total and human traffic for large publishers.\n\nOne way that publishers could respond to their content being crawled by LLMs and to an increase in bot traffic is to declare that crawling is not allowed in their robots.txt file. The robots.txt file of a website (part of the RFC 9309 standard) instructs web crawlers on what they are allowed and not allowed to access. We identify when each publisher first disallowed GPT-related crawlers using the HTTP Archive.555https://httparchive.org/ News publishers choose to block LLM access more often than non-news websites. About 80% of the top publishers block LLM crawlers in 2023, in a staggered pattern of blocking starting mid-2023. We use the staggered adoption pattern in a difference-in-differences analysis [Callaway and Sant’Anna, 2021] that compares blocking publishers to not-yet-blocking publishers to estimate the effect of blocking LLM crawlers on total traffic. We find a persistent post-blocking decline of 23.1% in log-monthly visits measured by SimilarWeb. To assess whether this effect reflects reduced automated (e.g., bot) traffic rather than changes in real audience demand, we replicate the analysis using Comscore Web-Behavior panel data. We again find a sizable and statistically significant post-blocking decline in monthly publisher visits of 13.9%. These results imply that blocking LLMs can have significant negative effects on publishers: it is followed by lower total traffic and lower human traffic, and not merely the mechanical removal of bot visits for large websites. Indeed, some publishers removed the Disallow rule in 2024. However, when we extend the analysis to lower-traffic websites in the Comscore data, we find heterogeneous results that vary with website size.\n\n\n\nNo near-term contraction in newsroom hiring relative to other roles.\n\nAnother possible response by publishers to a decline in traffic and reduced cost of content production is to reduce their newsroom headcount. We use employer-linked job postings to track publishers’ hiring by occupation. Motivated by evidence that LLM exposure is heterogeneous across tasks and that measured labor-market impacts have been modest in the near term [Eloundou et al., 2024, Noy and Zhang, 2023, Brynjolfsson et al., 2025, Humlum a"
  },
  {
    "title": "ShowUI-$π$: Flow-based Generative Models as GUI Dexterous Hands",
    "url": "https://arxiv.org/abs/2512.24965v1",
    "source": "arxiv",
    "summary": "Building intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments. However, existing GUI agents rely on discrete click predictions (x,y), which prohibits free-form, closed-loop trajectories (e.g. dragging a progress bar) that require continuous, on-the-fly perception and adjustment. In this work, we develop Show",
    "comments": [],
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Digital GUI Automation.\n2.2 Physical Robotic Manipulation.\n\n\n\n3 ScreenDrag Dataset\n\n3.1 Problem Definitions\n3.2 Dataset Construction\n3.3 Evaluation Metrics\n\n\n\n4 ShowUI-π\\piModel\n\n4.1 Unified Continuous and Discrete Actions\n4.2 Flow-based Continuous Trajectory\n4.3 Directional Regularization\n\n\n\n5 Experiments\n\n5.1 Main Results\n5.2 Key Ablations\n5.3 Qualitative Analysis\n\n\n6 Conclusion\n\nA More Results\n\nA.1 Grounding Performance of ShowUI-π\\pi\nA.2 Drag Performance of ShowUI-π\\pi on public benchmark\nA.3 Effects of Co-training with Drag Data and Grounding Data\n\n\n\nB Setup\n\nB.1 Training Details\nB.2 Training Data\n\n\n\nC Dataset Construction\n\nC.1 How we collect raw data\nC.2 Data visualization\nC.3 Data-driven Closed-loop Online Evaluation\n\n\nD Failure Cases of Baseline Models\nE Limitations and Future Work\n\n\n\n\n\n\nShowUI-π\\pi: Flow-based Generative Models as GUI Dexterous Hands\n\n\nSiyuan Hu111Equal contribution. Kevin Qinghong Lin111Equal contribution. Mike Zheng Shou222Corresponding author.\nShow Lab, National University of Singapore\n\nhttps://showlab.github.io/showui-pi\n\n\n\n\nAbstract\nBuilding intelligent agents capable of dexterous manipulation is essential for achieving human-like automation in both robotics and digital environments.\nHowever, existing GUI agents rely on discrete click predictions (x,y)(x,y), which prohibits free-form, closed-loop trajectories (e.g., dragging a progress bar) that require continuous, on-the-fly perception and adjustment.\nIn this work, we develop ShowUI-π\\pi, the first flow-based generative model as GUI dexterous hand,\nfeaturing the following designs:\n(i) Unified Discrete–Continuous Actions, integrating discrete clicks and continuous drags within a shared model, enabling flexible adaptation across diverse interaction modes;\n(ii) Flow-based Action Generation for drag modeling, which predicts incremental cursor adjustments from continuous visual observations via a lightweight action expert, ensuring smooth and stable trajectories;\n(iii) Drag Training data and Benchmark, where we manually collect and synthesize 20K drag trajectories across five domains (e.g., PowerPoint, Adobe Premiere Pro), and introduce ScreenDrag, a benchmark with comprehensive online and offline evaluation protocols for assessing GUI agents’ drag capabilities.\nOur experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g., Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π\\pi achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach.\nWe hope this work advances GUI agents toward human-like dexterous control in digital world. The code is available at https://github.com/showlab/showui-pi.\n\n\n\n1 Introduction\n\nBuilding intelligent assistants capable of dexterous manipulation is essential for achieving human-like automation in both physical and digital environments [showui, openvla, magma]. For example, in the physical world, robotic dexterous hands are deployed for manipulation tasks e.g.,, sorting objects on physical desktops [rt1, rt2, pi_0, pi_0_5].\nAnalogously, in the digital world, Graphical User Interfaces (GUI) serve as the primary medium through which people interact with computers, and automating GUI interactions holds substantial promise for enhancing productivity [workarena], reducing workload [webshop], and improving accessibility [mind2web].\n\n\nRecent advances in large language models (LLMs) [gpt5, claude4.5, webagentplan, webarena, multimodalweb] and vision-language models (VLMs) [qwen2vl, cogagent, seeclick, uground] have accelerated progress in GUI agents [showui, uitars1.5]. These emerging visual-centric agents directly perceive screen observations and output actions such as clicking or typing.\nHowever, most existing GUI agents [aguvis, opencua] are obtained by fine-tuning foundation VLMs without architecture adaptation, which represent action’s coordinate in a discrete, tokenized form.\nThis representation fundamentally restricts agents from executing complex high-degree-of-freedom dragging, such as creative drawing or Captcha-solving by rotation i.e., tasks inherently demanding continuous, real-time visual observation and responsible for incremental trajectory adjustment,\nBy contrast, Vision-Language-Action models [diffusion_policy, pi_0, pi_0_5] in robotics leverage flow-based generative methods (e.g., diffusion policy, flow matching) to enable achieving such continuous, fine-grained control on the fly.\n\n\nMotivated by how humans control the mouse for fine-grained cursor movement, i.e., continuously perceive and adjust actions, we wonder: can we construct such a digital dexterous hand in GUI?\nWe propose ShowUI-π\\pi, the first flow-based GUI model designed for continuous trajectory.\nSpecifically, ShowUI-π\\pi highlights the following architecture:\n(i) Unified Discrete-Continuous Actions: ShowUI-π\\pi casts discrete clicks as drags with negligible movements, and integrates them with continuous drags into a unified modeling. Under this formulation, both action types are represented by a sequence of (x,y,m)(x,y,m) triplets, where (x,y)(x,y) are cursor coordinates and m∈{down,up}m\\in\\{\\texttt{down},\\texttt{up}\\} is the mouse button state. This unified design allows ShowUI-π\\pi to handle both drag and click tasks with a single shared model, adapting without task-specific head selection.\n(ii) Flow-based Action Generation: Different from existing GUI agents predicting discrete, tokenized actions from language decoding, e.g.,, click(x,y) and drag(start, end), ShowUI-π\\pi is a flow-based generative model, and employs a lightweight action expert to incrementally predict cursor adjustments from continuous visual observations. Built on a transformer backbone, the action expert is trained with flow matching to generate stable and precise action trajectories;\n(iii) ScreenDrag benchmark: We construct a benchmark specially for continuous GUI tasks, including 505 real-world drag tasks across five domains covering both professional control and daily usage, such as PowerPoint, OS Desktop and file manager, Handwriting on canvas, Adobe Premiere Pro, and Captcha solving, with 101 tasks for each domain.\nMoreover, to fully evaluate continuous trajectories, we introduce two complementary modes: an offline open-loop evaluation using average trajectory error and endpoint accuracy, and an online closed-loop evaluation using task success rate.\n(iv) Continuous Trajectory Training Data: To advance the model training, we construct a training dataset of 20K manually collected and synthesized drag trajectories across the five aforementioned domains and 11 categories of tasks, and all trajectories have recorded UI states and dense coordinates.\n\n\nOur experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g., Operator scores 13.27, and the best Gemini-2.5-CUA reaches 22.18). In contrast, ShowUI-π\\pi achieves 26.98 with only 450M parameters, underscoring both the difficulty of the task and the effectiveness of our approach. Moreover, our ablation studies further reveal the superficial nature of standard flow-matching training and highlight the substantial impact of individual designs. Our contributions are three-fold:\n\n\n•\n\nThe first flow-based GUI agent for continuous trajectories. To the best of our knowledge, we are the first work to tackle continuous trajectory-based drags in GUI automation, revealing core limitations of discrete, tokenized actions. We propose ShowUI-π\\pi, a lightweight 450M flow-based VLA that unifies discrete clicks and continuous drags within a shared modeling.\n\n\n\n•\n\nScreenDrag training dataset and benchmark suite. We introduce ScreenDrag, a benchmark tailored for continuous GUI manipulation, with 505 real-world drag tasks across five domains and 20K manually collected and synthesized drag trajectories over 11 task categories. ScreenDrag supports both offline open-loop metrics and online closed-loop success evaluation.\n\n\n\n•\n\nComprehensive evaluation and key insights. Experiments show that proprietary GUI agents still struggle on ScreenDrag (e.g.,, Operator 13.27, best Gemini-2.5-CUA 22.18), while ShowUI-π\\pi reaches 26.98 with only 450M parameters, highlighting both task difficulty and model effectiveness. Ablations further demonstrate the impact of our individual design choices for achieving robust continuous control.\n\n\n\n\n\n\n\n2 Related Work\n\nFigure 2: ScreenDrag Automated Data Collection Pipeline. ScreenDrag automated data generation pipeline for continuous trajectory-based GUI interaction data. The pipeline includes three stages: (i) Element Parsing: The software application UI is parsed with UI Automation of Windows SDK in order to retrieve the UI element metadata. (ii) Task Proposal: Given the UI element metadata, an LLM will be prompted to generate a drag instruction, the expected metadata change and the drag code with dense trajectory. (iii) Trajectory Synthesis: The drag code will be executed in the software environment. A rule-based verifier will check the parsed metadata from UI states before and after the drag to ensure that the metadata change satisfies the expectation.\n\n\n\n2.1 Digital GUI Automation.\n\nLLM-based agents in digital environments have evolved from reasoning and tool-use paradigms, such as Chain-of-Thought [chain_of_thought], ReAct [react], and related prompting frameworks [socratic, gpt4tools, assistgpt], toward task-oriented GUI automation. Existing methods include training-free pipelines which plan with a VLM and execute via external tools [assistgui, seeact], and training-based models jointly learning perception and action from screenshots and instructions [seeclick, cogagent, ferretui].\nExisting GUI agents, including ShowUI, decode actions into discrete text tokens, simplifying integration with VLM planners but limiting control to simple clicks or short drags. ShowUI-π\\pi departs from these methods by directly modeling continuo"
  },
  {
    "title": "Semi-overlapping Multi-bandit Best Arm Identification for Sequential Support Network Learning",
    "url": "https://arxiv.org/abs/2512.24959v1",
    "source": "arxiv",
    "summary": "Many modern AI and ML problems require evaluating partners' contributions through shared yet asymmetric, computationally intensive processes and the simultaneous selection of the most beneficial candidates. Sequential approaches to these problems can be unified under a new framework, Sequential Support Network Learning (SSNL), in which the goal is to select the most beneficial candidate set of par",
    "comments": [],
    "full_text": "\n\n\n\n\n1 Introduction\n\nDuality and the support network learning problem\nTowards multi-bandits\nFrom duality to the semi-overlapping property\n\n\n\n2 Related work\n\nMulti-armed bandits and best-arm identification\nMulti-bandits, overlapping and semi-overlapping (structured) MMABs\nCombinatorial bandits and Monte Carlo Tree Search\nStructure learning and sparse candidate methods\nComputational efficiency of joint evaluations\nTask selection in multi-task and auxiliary task learning\nClient selection in federated learning\nPartner selection in multi-agent systems and multi-agent RL\nDataset selection\n\n\n3 Multi-Bandit Problem Setup\n4 The GapE Algorithm\n5 Error probability bounds\n\n6 Applications of sequential support network learning\n\n6.1 Multi-task learning and auxiliary task learning\n6.2 Federated learning\n6.3 Coalition Learning in Multi-agent systems\n\n\n7 Discussion and Conclusion\n8 Future Work\nA Proofs of Lemmata in Theorem 3\n\n\n\n\n\nSemi-overlapping multi-bandit best arm identification\nfor sequential support network learning\n\n\n\\nameAndrás Antos \\emailantos@mit.bme.hu \n\\addrDepartment of Artificial Intelligence and Systems Engineering\nBudapest University of Technology and Economics\nBudapest, Hungary\n\n  \n\\nameAndrás Millinghoffer \\emailmilli@mit.bme.hu \n\\addrDepartment of Artificial Intelligence and Systems Engineering\nBudapest University of Technology and Economics\nBudapest, Hungary\n\\addrE-Group ICT Software Zrt.\nBudapest, Hungary\n\n  \n\\namePéter Antal \\emailantal@mit.bme.hu \n\\addrDepartment of Artificial Intelligence and Systems Engineering\nBudapest University of Technology and Economics\nBudapest, Hungary\n\\addrE-Group ICT Software Zrt.\nBudapest, Hungary\n\n\n\nAbstract\nMany modern AI and ML problems require evaluating partners’ contributions through shared yet asymmetric, computationally intensive processes and the simultaneous selection of the most beneficial candidates.\nSequential approaches to these problems can be unified under a new framework, Sequential Support Network Learning (SSNL), in which the goal is to select the most beneficial candidate set of partners for all participants using trials;\nthat is, to learn a directed graph that represents the highest-performing contributions.\nWe demonstrate that a new pure-exploration model, the semi-overlapping multi-(multi-armed) bandit (SOMMAB), in which a single evaluation provides distinct feedback to multiple bandits due to structural overlap among their arms, can be used to learn a support network from sparse candidate lists efficiently.\nWe develop a generalized GapE algorithm for SOMMABs and derive new exponential error bounds that improve the best known constant in the exponent for multi-bandit best-arm identification.\nThe bounds scale linearly with the degree of overlap, revealing significant sample-complexity gains arising from shared evaluations.\nFrom an application point of view, this work provides a theoretical foundation and improved performance guarantees for sequential learning tools for identifying support networks from sparse candidates in multiple learning problems, such as in multi-task learning (MTL), auxiliary task learning (ATL), federated learning (FL), and in multi-agent systems (MAS).\n\n\nKeywords: \nmulti-armed bandit, overlapping multi-bandit, best arm identification, multi-task learning, federated learning, multi-agent systems\n\n\n\n1 Introduction\n\nMulti-Armed Bandits (MAB) is a widely used model today.\nIts best arm identification (BAI) setting (Audibert et al., 2010; Bubeck et al., 2011) is a pure exploration problem, distinct from the original MAB problem to maximize the cumulative reward (see e.g., Robbins, 1952; Auer et al., 2002).\n\n\nThe core objective of BAI is to formulate a strategy that recommends the most beneficial one from KK possible options.\nThe well-established MAB/BAI model has been motivated by the fact\nthat the efficient allocation of trial resources is a critical aspect,\ngiven the significant computational, statistical, ethical, financial, and security/privacy-related budgetary limitations of trials in numerous domains.\nThe uniform exploration of options could result in the inefficient utilization of trial resources,\npotentially leading to suboptimal option selection.\nTherefore, it is imperative to employ effective strategies for the distribution of trials across options,\nthereby ensuring the efficacy of the developed strategy.\nTo evaluate a MAB strategy, usually either the reward of the recommended arm or the probability of error (i.e., not selecting the best arm) is used.\n\n\nTo address the best arm identification problem, two algorithms were given by Audibert et al. (2010):\n1) the Upper Confidence Bounds (UCB-E) method with a parameter,\nwhose optimal value depends on the complexity of the problem, and\n2) the parameter-free Successive Rejects (SR) method.\nBoth these algorithms are shown to be nearly optimal,\nthat is, their error probability decreases exponentially in the number of pulls.\n\n\nBAI has been utilized, for example, in the context of function learning (Maron and Moore, 1993; Madani et al., 2004; Krueger et al., 2015; Mohr and van Rijn, 2021); in the feature subset selection problem Gaudel and Sebag (2010); Chaudhry and Lee (2018), and it is intensively used in hyper-parameter learning based on the results from random(ized) trials corresponding to train-test-validation splits Li et al. (2018).\nNew class of applications of BAI in artificial intelligence are the selection of the best set of (1) jointly learnable tasks in multi-task learning (MTL) (Caruana, 1997; Standley et al., 2020; Fifty et al., 2021), (2) beneficial tasks for a target task in auxiliary task learning (ATL) (Guo et al., 2019; Millinghoffer et al., 2024), (3) contributing clients in federated learning (FL) (Zhu et al., 2024), and (4) agents forming the most capable auxiliary coalition for an agent to assist in complex problem solving (Larsson et al., 2021; Zhang and et al., 2024; Cohen and Agmon, 2024, 2025) (for a summary of application domains, see Table 1).\nIn these problems, the optimal option does not necessarily comprise all other entities (tasks, clients, or agents, respectively),\nbecause some of the entities may exert a deleterious effect on another, for instance, due to disparities in the sample distributions available for the entities.\n\n\nDuality and the support network learning problem\n\nIn these applications, another important property is the donor-recipient duality,\nin which entities serving as recipients (targets) may also act as donors (contributors) for others. Indeed, in many real-world problems, we may observe (complete) entity duality denoting a setting in which all entities in the problem simultaneously play the role of both a donor and a recipient.\nA further practically important property arises from the mandatory joint evaluation of participating entities, which implies computational and even implementation coupling.\nThe condition relational duality formalizes the complete form of this property, which requires that if a set SS is considered as a candidate donor set for an entity aa, then for any b∈Sb\\in S the corresponding variant set S∖{b}∪{a}S\\setminus\\{b\\}\\cup\\{a\\} is available as a candidate donor set for bb,\nensuring structural coherence of candidate relations across entities. By strong duality, we mean the combination of entity and relational duality.\nThis donor-recipient duality naturally leads to a joint selection problem, where we must determine the most beneficial set of donors for each (recipient) entity.\nWe refer to this problem class as sequential support network learning (SSNL),\nwhere each entity aims to identify the most beneficial set of contributors using optionally computationally coupled trials in a sequential learning framework.\nThe solution for a SSNL problem can be represented by a directed graph with edges pointing to recipients from donors in the optimal support sets.\n\n\n\nTowards multi-bandits\n\nThe support network learning problem can be viewed as a weakly coupled joint selection problem, which suggests using multi-bandits, a generalization of MABs, as follows.\nWe assume the existence of MM entities,\neach characterized by distinct joint learning properties\nand for entity mm, KmK_{m} candidates (options, arms) of beneficial entity subsets.\nThe objective is to identify the optimal option for each entity.\nCertainly, if the number of options were allowed to be exponential in MM the problem easily becomes intractable;\nthus, in practice, that must be assumed to be limited.\nWe refer to this assumption as the candidate lists are sparse.\nSubsequently, it is necessary to solve MM MAB problems in parallel,\nwhile constrained by the total available resources.\nIt is possible that a greater quantity of resources will be required to identify the best option for one node than for another.\nTherefore, employing a uniform or other arbitrary strategy generally will not result in optimal performance.\nThis problem structure is formulated by Gabillon et al. (2011b, a)\nas the multi-bandit/multi-MAB (MMAB) best arm identification / pure exploration over MM MABs.\n\n\nThere are multiple ways to evaluate a MMAB strategy.\nThree of them are the ones based on\n\n\n1.\n\nthe average of the rewards of the recommended arms over the bandits,\n\n\n\n2.\n\nthe average error probability over the bandits,\n\n\n\n3.\n\nthe maximum error probability over the bandits.\n\n\n\n\n\nThe UCB-E and Successive Rejects algorithms above, being designed for a single MAB,\nare not obvious to extend to MMAB problems.\nBy Gabillon et al. (2011b), studying the MMAB problem in the fixed budget setting,\nthe Gap-based Exploration (GapE) algorithm has been proposed,\nwhich focuses on the gap of the arm,\nthat is, the difference between the mean value of the arm and that of the best arm (in the same bandit).\nThey prove an upper-bound on the error probability for GapE,\nwhich decreases exponentially with the budget (see Proposition 2),\nand they also report numerical simulations.\n\n\n\nFrom duality to the semi-overlapping pr"
  },
  {
    "title": "AMAP Agentic Planning Technical Report",
    "url": "https://arxiv.org/abs/2512.24957v1",
    "source": "arxiv",
    "summary": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. ",
    "comments": [],
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2512.24957v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2512.24957v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 31 Dec 2025]\n    Title:AMAP Agentic Planning Technical Report\n    Authors:Yulan Hu, Xiangwen Zhang, Sheng Ouyang, Hao Yi, Lu Xu, Qinglin Lang, Lide Tan, Xiang Cheng, Tianchen Ye, Zhicong Li, Ge Chen, Wenjin Yang, Zheng Pan, Shaopan Xiong, Siran Yang, Ju Huang, Yan Zhang, Jiamang Wang, Yong Liu, Yinfeng Huang, Tucheng Lin, Xin Li, Ning Guo            View a PDF of the paper titled AMAP Agentic Planning Technical Report, by Yulan Hu and 22 other authors\n    View PDF\n\n\n\n    \n            Abstract:We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries with a filter ratio of 1:10,000, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2512.24957 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2512.24957v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2512.24957\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Hu Yulan [view email]          [v1]\n        Wed, 31 Dec 2025 16:39:09 UTC (1,773 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled AMAP Agentic Planning Technical Report, by Yulan Hu and 22 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2025-12\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors "
  },
  {
    "title": "MSACL: Multi-Step Actor-Critic Learning with Lyapunov Certificates for Exponentially Stabilizing Control",
    "url": "https://arxiv.org/abs/2512.24955v1",
    "source": "arxiv",
    "summary": "Achieving provable stability in model-free reinforcement learning (RL) remains a challenge, particularly in balancing exploration with rigorous safety. This article introduces MSACL, a framework that integrates exponential stability theory with maximum entropy RL through multi-step Lyapunov certificate learning. Unlike methods relying on complex reward engineering, MSACL utilizes off-policy multi-",
    "comments": [],
    "full_text": null
  },
  {
    "title": "ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT",
    "url": "https://arxiv.org/abs/2512.24948v1",
    "source": "arxiv",
    "summary": "Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in populatio",
    "comments": [],
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2512.24948v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2512.24948v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 31 Dec 2025]\n    Title:ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT\n    Authors:Xinran Gong, Gorkem Durak, Halil Ertugrul Aktas, Vedat Cicek, Jinkui Hao, Ulas Bagci, Nilay S. Shah, Bo Zhou            View a PDF of the paper titled ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT, by Xinran Gong and 7 other authors\n    View PDF\n\n\n\n    \n            Abstract:Coronary artery calcium (CAC) scoring from chest CT is a well-established tool to stratify and refine clinical cardiovascular disease risk estimation. CAC quantification relies on the accurate delineation of calcified lesions, but is oftentimes affected by artifacts introduced by cardiac and respiratory motion. ECG-gated cardiac CTs substantially reduce motion artifacts, but their use in population screening and routine imaging remains limited due to gating requirements and lack of insurance coverage. Although identification of incidental CAC from non-gated chest CT is increasingly considered for it offers an accessible and widely available alternative, this modality is limited by more severe motion artifacts. We present ProDM (Property-aware Progressive Correction Diffusion Model), a generative diffusion framework that restores motion-free calcified lesions from non-gated CTs. ProDM introduces three key components: (1) a CAC motion simulation data engine that synthesizes realistic non-gated acquisitions with diverse motion trajectories directly from cardiac-gated CTs, enabling supervised training without paired data; (2) a property-aware learning strategy incorporating calcium-specific priors through a differentiable calcium consistency loss to preserve lesion integrity; and (3) a progressive correction scheme that reduces artifacts gradually across diffusion steps to enhance stability and calcium fidelity. Experiments on real patient datasets show that ProDM significantly improves CAC scoring accuracy, spatial lesion fidelity, and risk stratification performance compared with several baselines. A reader study on real non-gated scans further confirms that ProDM suppresses motion artifacts and improves clinical usability. These findings highlight the potential of progressive, property-aware frameworks for reliable CAC quantification from routine chest CT imaging.\n    \n\n    \n    \n              \n          Comments:\n          21 pages, 8 figures\n        \n\n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2512.24948 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2512.24948v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2512.24948\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Xinran Gong [view email]          [v1]\n        Wed, 31 Dec 2025 16:29:05 UTC (4,562 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled ProDM: Synthetic Reality-driven Property-aware Progressive Diffusion Model for Coronary Calcium Motion Correction in Non-gated Chest CT, by Xinran Gong and 7 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2025-12\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n     "
  }
]