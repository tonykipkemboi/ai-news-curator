[
  {
    "title": "Neurodivergent Brains Build Better Systems (2025)",
    "url": "https://blog.drjoshcsimmons.com/p/how-neurodivergent-brains-build-better",
    "source": "hn",
    "summary": "",
    "comments": [
      "This piece is wildly unfair to folks with neuro-divergence - all get lumped into one bucket. Everything suggested is either a &quot;works-for-me&quot; assertion, from a sample size of 1, or a broad generalization about <i>typical</i> neuro-divergent individuals - and that seems like a contradiction in terms. Let&#x27;s be fair here.",
      "The article glorifies neurodiversity as an wanted trait, which is perfectly suited to build perfect software. There are no downsides, you just have to sooth neurodivergent people somehow with some dim cozy lighting and silence.<p>I wish some programmers would be more stubborn exploring a problem space. But being randomly obsessed about a detail can also be a distraction. Loosing track of time during an obsessed phase isn&#x27;t always helpful. All this is also often a easy way to ignore responsibilities of life.<p>I suggest that all nerodivergent peers go on high alert if they encounter business people and wanna be hustlers that pretend to care.",
      "I&#x27;ve met neurodivergent people who couldn&#x27;t even stay on track long enough to prepare a regular dinner. Strong &quot;neurogenetic executive failure&quot; is a common trait. It&#x27;s nowhere near as black and white as the article wants to make it out to be.",
      "&gt; There was a particular button on the user interface that bugged him. It bugged him because it was a slightly different shade of green in the screenshots than it was in the staging environment. The team looked into it, and sure enough, we had tested a slightly older version of the software, not the exact version we nearly shipped to the customer. He saw and noticed the button but what would have been deployed would have had multiple bugs in the code that weren’t visible and passing because the tests were on the old version too. That’s bottom-up thinking.<p>No, that&#x27;s spotting a problem. Like, come on.",
      "What are examples of companies with neurodivergent leadership?"
    ],
    "full_text": null
  },
  {
    "title": "Maybe comments should explain 'what' (2017)",
    "url": "https://www.hillelwayne.com/post/what-comments/",
    "source": "hn",
    "summary": "",
    "comments": [
      "The &quot;what&quot; vs &quot;why&quot; distinction breaks down when your code encodes domain knowledge that readers can&#x27;t infer from context.<p>I build accounting software and half my &quot;what&quot; comments are actually explaining business rules that would be impenetrable otherwise. Something like:<p><pre><code>  &#x2F;&#x2F; Bank transfers appear as two transactions - a debit here and credit elsewhere\n  &#x2F;&#x2F; We match them by looking for equal-opposite amounts within a 3-day window\n  </code></pre>\nThat&#x27;s explaining &quot;what&quot; but also implicitly &quot;why&quot; - because that&#x27;s how double-entry works and that&#x27;s the tolerance banks allow for settlement delays. You can&#x27;t really extract that into a method name without it becoming absurd.<p>The Uncle Bob approach of extractNameMatchingTransfersWithinSettlementWindow() doesn&#x27;t actually help - now I need to know what settlement windows are anyway, and I&#x27;ve lost the context of why 3 days.",
      "I feel like no one serious uses the uncle Bob style of programming anymore (where each line is extracted into its own method). This was a thing for a while but anyone who&#x27;s tried to fix bugs in a codebase like that knows exactly what this article is talking about. It&#x27;s a constant frustration of pressing the &quot;go to definition&quot; key over and over, and going back and forth between separate pieces that run in sequence.<p>I don&#x27;t know how that book ever got as big as it did, all you have to do is try it to know that it&#x27;s very annoying and does not help readability <i>at all</i>.",
      "I wouldn&#x27;t take the examples from Bob Martin as gospel, see also: &quot;Don&#x27;t refactor like Uncle Bob&quot;: <a href=\"https:&#x2F;&#x2F;theaxolot.wordpress.com&#x2F;2024&#x2F;05&#x2F;08&#x2F;dont-refactor-like-uncle-bob-please&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;theaxolot.wordpress.com&#x2F;2024&#x2F;05&#x2F;08&#x2F;dont-refactor-lik...</a>",
      "I made a point here <a href=\"https:&#x2F;&#x2F;antirez.com&#x2F;news&#x2F;124\" rel=\"nofollow\">https:&#x2F;&#x2F;antirez.com&#x2F;news&#x2F;124</a> that comments are needed at the same time for different reasons, and different comments have differente semantical properties that can be classified in classes you very easily find again and again, even in very different code bases.",
      "Explain &quot;why not what&quot; is good general advice. My further advice for comments is: even bad comments can be useful (unless they&#x27;re from LLM output maybe...) therefore when in doubt, write a comment. Write it in your own words.<p>Had to add the last sentence for the circa 2020s developer experience. LLM comments are almost never useful since they&#x27;re supposed to convey meaningful information to another human coder, anything your human brain can think of will probably be more helpful context.",
      "I noticed that when I write code that is not trivial to understand I tend to extract intermediate values into variables with meaningful names.<p><pre><code>    applyDrag(): void {\n        const { quad: quadConfig } = settings\n        const quad = this.getRigidBody()\n        const quadVel = vec3ToTwgl(quad.linvel())\n        const dragMag = aerodynamicDrag(quadConfig.dragCoefficient, v3.length(quadVel), quadConfig.frontalArea)\n        const dragDir = v3.negate(v3.normalize(quadVel))\n        const dragForce = v3.mulScalar(dragDir, dragMag)\n        const dragImpulse = v3.mulScalar(dragForce, dt)\n        quad.applyImpulse(vec3TwglToRapier(dragImpulse), true)\n    }\n</code></pre>\nThis way code gets more natural language anchors which helps understanding what it does.",
      "Sometimes I want to use comments because I&#x27;m doing something vaguely algorithmic, and I know some readers won&#x27;t follow the code.<p>I&#x27;m trying to think of a good example, maybe something like a pointer window based function (off the top of my head)<p>(This isn&#x27;t real code. Don&#x27;t get hung up on it)<p><pre><code>    func DedupeStrings(ss []string) []string {\n      if len(ss) &lt; 2 {\n        return ss\n      }\n      ss = strings.Sort(ss)\n\n      u := 1 &#x2F;&#x2F; index of end of &quot;uniques&quot; set\n      for i := 1; i &lt; len(ss); i++ {\n        &#x2F;&#x2F; Consume until new value\n        if ss[i] == ss[i-1] {\n          continue\n        }\n        &#x2F;&#x2F; Put new value in &#x27;uniques&#x27; set\n        ss[u] = sorted[i]\n        u++\n      }\n      &#x2F;&#x2F; Final state: all unique items are positioned \n      &#x2F;&#x2F; left of &#x27;u&#x27; index\n      return ss[:u]\n    }\n</code></pre>\nPeople will quibble, but<p>- I&#x27;m not convinced you could change the variable names without harming clarity. Would a name like uniquesEndIndex really be any clearer? It adds noise to the code and still doesn&#x27;t satisfy a confused reader<p>- I don&#x27;t want to use function calls for documentation, eg putInUniques(). I&#x27;m doing it this way because I want it to run really quick.",
      "The article is about comments.  But, more generally, I think the issue here is about naming things.<p>Names capture ideas.  Only if we name something can we (or at least I) reason about it.  The more clear and descriptive a name for something is, the less cognitive load is required to include the thing in that reasoning.<p>TFA&#x27;s example that &quot;weight&quot; is a better variable name than &quot;w&quot; is because &quot;weight&quot; immediately has a meaning while use of &quot;w&quot; requires me to carry around the cumbersome &quot;w is weight&quot; whenever I see or think about &quot;w&quot;.<p>Function names serve the same purpose as variable names but for operations instead of data.<p>Of course, with naming, context matters and defining functions adds lines of code which adds complexity.  As does defining overly verbose variable names: &quot;the_weight_of_the_red_ball&quot; instead of &quot;weight&quot;.  So, some balance that takes into account the context is needed and perhaps there is some art in finding that balance.<p>Comments, then, provide a useful intermediate on a spectrum between function-heavy &quot;Uncle Bob&quot; style and function-less &quot;stream of consciousness&quot; style.",
      "If you need a comment to explain &#x27;w&#x27; means &#x27;weight&#x27; then you should remove the comment and rename the variable &#x27;weight&#x27;.",
      "This is tangential to the article&#x27;s point, but that `replace` function is a complete WTF in a way both authors completely ignore. Because it replaces things in the entire string in a loop, it will translate symbols recursively or not depending on ordering. Imagine you have the following dictionary:<p><pre><code>    a=$b\n    b=oops\n</code></pre>\nif your input string just has one of these, it will just be translated once as the programmer was probably expecting:<p><pre><code>    input:  foo $a bar\n    output: foo $b bar\n</code></pre>\nbut if your input string first references $b later, then it will recursively translate $a.<p><pre><code>    input:  foo $a bar $b\n    output: foo oops bar oops\n</code></pre>\n<i>Sometimes</i> translating recursively is a bizarre behavior and possibly a security hole.<p>The sane thing would be to loop through building the output string, adding the replacement for each symbol as you go. Using String.replace and the alreadyReplaced map is just a bad idea. Also inefficient, as it and throws away strings and does a redundant search on each loop iteration.<p>Feels typical of this whole &#x27;90s-era culture of arguing over refactoring with Java design patterns and ornate styles without ever thinking about if the algorithm is any good.<p>Edit: also, consider $foo $foobar. It doesn&#x27;t properly tokenize on replacement, so this will also be wrong."
    ],
    "full_text": null
  },
  {
    "title": "Neural Networks: Zero to Hero",
    "url": "https://karpathy.ai/zero-to-hero.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "I’ve gone through this series of videos earlier this year.<p>In the past I’ve gone through many “educational resources” about deep neural networks - books, coursera courses (yeah, that one), a university class, the fastai course - but I don’t work with them at all in my day to day.<p>This series of videos was by far the best, most “intuition building”, highest signal-to-noise ratio, and least “annoying” content to get through. Could of course be that his way of teaching just clicks with me, but in general - very strong recommend. It’s the primary resource I now recommend when someone wants to get into lower level details of DNNs.",
      "I like Karpathy, we come from the same lineage and I am very proud of him for what he&#x27;s accomplished, he&#x27;s a very impressive guy.<p>In regards to deep learning, building deep learning architecture is one of my greatest joys in finding insights from perceptual data.  Right now, I&#x27;m working on spatiotemporal data modeling to build prediction systems for urban planning to improve public transportation systems.  I build ML infrastructure too and plan to release an app that deploys the model in the wild within event streams of transit systems.<p>It took me a month to master the basics and I&#x27;ve spent a lot of time with online learning, with Deeplearning.ai and skills.google.  Deeplearning.ai is ok, but I felt the concepts a bit dated.  The ML path at skills.google is excellent and gives a practical understanding of ML infrastructure, optimization and how to work with gpus and tpus (15x faster than gpus).<p>But the best source of learning for me personally and makes me a confident practitioner is the book by Francois Chollet, the creator of Keras.  His book, &quot;Deep Learning with Python&quot;, really removed any ambiguity I&#x27;ve had about deep learning and AI in general.  Francois is extremely generous in how he explains how deep learning works, over the backdrop of 70 years of deep learning research.  Francois keeps it updated and the third revision was made in September 2025 - its available online for free if you don&#x27;t want to pay for it.  He gives you the recipe for building a GPT and Diffusion models, but starts from the ground floor basics of tensor operations and computation graphs.  I would go through it again from start to finish, it is so well written and enjoyable to follow.<p>The most important lesson he discusses is that &quot;Deep learning is more of an art than a science&quot;. To get something working takes a good amount of practice and the results on how things work can&#x27;t always be explained.<p>He includes notebooks with detailed code examples with Tensorflow, Pytorch and Jax as back ends.<p>Deep learning is a great skill to have.  After reading this book, I can recreate scientific abstracts and deploy the models into production systems.  I am very grateful to have these skills and I encourage anyone with deep curiosity like me to go all in on deep learning.",
      "I have lots of non-AI software experience but nothing with AI (apart from using LLMs like everyone else). Also I did an introductory university course in AI 20 years ago that I’ve completely forgotten.<p>Where do I get to if I go through this material?<p>Enough to build… what? Or contribute on… ? Enough knowledge to have useful conversations on …? Enough knowledge to understand where to … is useful and why?<p>Where are the limits, what is it that the AI researchers have that this wouldn’t give?",
      "I&#x27;m not sure how it compares, but another option is the Hugging Face learning portal [0].  I&#x27;m doing the Deep RL Course and so far it&#x27;s pretty straight forward (although when it gets math heavy I&#x27;m going to suffer).<p>[0] - <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;learn\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;learn</a>",
      "A couple years ago I wrote a tutorial how to build a Neural Network in NumPy from scratch.¹<p>¹ <a href=\"https:&#x2F;&#x2F;matthodges.com&#x2F;posts&#x2F;2022-08-06-neural-network-from-scratch-python-numpy&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;matthodges.com&#x2F;posts&#x2F;2022-08-06-neural-network-from-...</a>",
      "A bit of shameless plug, I wrote 2 articles about this after doing the course a while ago.<p><a href=\"https:&#x2F;&#x2F;martincapodici.com&#x2F;2023&#x2F;07&#x2F;15&#x2F;no-local-gpu-no-problem-running-andrej-karpathys-nanogpt-on-modal-com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;martincapodici.com&#x2F;2023&#x2F;07&#x2F;15&#x2F;no-local-gpu-no-proble...</a><p><a href=\"https:&#x2F;&#x2F;martincapodici.com&#x2F;2023&#x2F;07&#x2F;19&#x2F;modal-com-and-nanogpt-continued-producing-output-using-tiktoken-for-bigger-tokens&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;martincapodici.com&#x2F;2023&#x2F;07&#x2F;19&#x2F;modal-com-and-nanogpt-...</a>",
      "I wish Karpathy&#x27;s star fleet academy becomes a huge success.",
      "Has anyone gone through cs231n and this as well?<p>I went through the former and it was one of the best classes I’ve ever taken. But I’ve been procrastinating on going through this because it seems like there’s a lot of overlap and the benefit seems marginal (I guess transformers are covered here?).",
      "I don&#x27;t even have enough knowledge to grasp the first video. Is there a list of knowledge requirements to look at?",
      "should have a (2022) label"
    ],
    "full_text": null
  },
  {
    "title": "Attention Is Bayesian Inference",
    "url": "https://medium.com/@vishalmisra/attention-is-bayesian-inference-578c25db4501",
    "source": "hn",
    "summary": "",
    "comments": [
      "I don’t love these “X is Bayesian” analogies because they tend to ignore the most critical part of Bayesian modeling: sampling with detailed with detailed balance.<p>This article goes into the implicit prior&#x2F;posterior updating during LLM inference; you can even go a step further and directly implement hierarchical relationships between layers with H-Nets. However, even under an explicit Bayesian framework, there’s a stark difference in robustness between these H-Nets and the equivalent Bayesian model with the only variable being the parameter estimation process. [1]<p>[1] <a href=\"https:&#x2F;&#x2F;blog.sturdystatistics.com&#x2F;posts&#x2F;hnet_part_II&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.sturdystatistics.com&#x2F;posts&#x2F;hnet_part_II&#x2F;</a>",
      "sure, but this stuff is only obvious post hoc. so many people have tried to &quot;justify&quot; the attention mechanism according to their area of expertise, but none of them came up with it first; ML engineers with ML thinking did.",
      "Pretty interesting. The posterior matching is a big deal, but I&#x27;m not convinced by the handwaiving required to demonstrate it in larger models. I&#x27;m interested in seeing how direct EM training scales though.",
      "Last time I look into SoTA Bayesian deep learning, Bayesian output layers seems the most promising and practical. Is that still the case?",
      "Found it interesting and engaging, but having a CS professor at Colombia putting their name to AI “slop” is a bit unnerving. If they are writing papers for work you would hope they would enjoy the process of thinking and writing (journaling) instead of using ChatGPT."
    ],
    "full_text": null
  },
  {
    "title": "AI Sycophancy Panic",
    "url": "https://github.com/firasd/vibesbench/blob/main/docs/ai-sycophancy-panic.md",
    "source": "hn",
    "summary": "",
    "comments": [
      "<i>&quot;The anti-sycophancy turn seems to mask a category error about what level of prophetic clarity an LLM can offer. No amount of persona tuning for skepticism will provide epistemic certainty about whether a business idea will work out, whether to add a line to your poem, or why a great movie flopped.&quot;</i><p>What a lot of people actually want from an LLM, is for the LLM to <i>have an opinion</i> about the question being asked. The cool thing about LLMs is that they appear capable of doing this - rather than a machine that just regurgitates black-and-white facts, they seem to be capable of dealing with nuance and gray areas, providing insight, and using logic to reach a conclusion from ambiguous data.<p>But this is the biggest misconception and flaw of LLMs. LLMs <i>do not have opinions</i>. That is not how they work. At best, they simulate what a reasonable answer from a person capable of having an opinion might be - without any consistency around what that opinion is, because it is simply a manifestation of sampling a probability distribution, not the result of logic.<p>And what most people call sycophancy is that, as a result of this statistical construction, the LLM tends to reinforce the opinions, biases, or even factual errors, that it picks up on in the prompt or conversation history.",
      "I recently realized every hypothesis I tested with an LLM, the LLM agreed with me. And if I wasn&#x27;t careful about reading its caveats, I could leave thinking my idea was brilliant and would never get pushback.<p>I tried something in the political realm. Asking to test a hypothesis and its opposite<p>&gt; Test this hypothesis: the far right in US politics mirrors late 19th century Victorianism as a cultural force<p>compared to<p>&gt; Test this hypothesis: The left in US politics mirrors late 19th century Victorianism as a cultural force<p>An LLM wants to agree with both, it created plausible arguments for both. While giving &quot;caveats&quot; instead of counterarguments.<p>If I had my brain off, I might leave with some sense of &quot;this hypothesis is correct&quot;.<p>Now I&#x27;m not saying this makes LLMs useless. But the LLM didn&#x27;t act like a human that might tell you your full of shit. It WANTED my hypothesis to be true and constructed a plausible argument for both.<p>Even with prompting to act like a college professor critiquing a grad student, eventually it devolves back to &quot;helpful &#x2F; sycophantic&quot;.<p>What I HAVE found useful is to give a list of mutually exclusive hypothesis and get probability ratings for each. Then it doesn&#x27;t look like you want one &#x2F; other.<p>When the outcome matters, you realize research &#x2F; hypothesis testing with LLMs is far more of a skill than just dumping a question to an LLM.",
      "I still suspect what happened was when the midwits all got access to ChatGPT etc and started participating in the A&#x2F;B tests, they strongly selected for responses that agreed with them regardless of whether they were actually correct.<p>Some of us want to be told when and why we’re wrong, and somewhere along the way AI models were either intentionally or unintentionally guided away from doing it because it improved satisfaction or engagement metrics.<p>We already know from decades of studies that people prefer information that confirms their existing beliefs, so when you present 2 options with a “Which answer do you prefer?” selection, it’s not hard to see how the one that begins with “You’re absolutely right!” wins out.",
      "I had completely forgotten about &#x27;Sydney&#x27; and its emoji-laden diatribes. What a crazy moment, looking back.",
      "This is my attempt to articulate why some recent shifts in AI discourse seem to be degrading the product experience of everyday conversation.<p>I argue that “sycophancy” has become an overloaded and not very helpful term; almost a fashionable label applied to a wide range of unrelated complaints (tone, feedback depth, conversational flow).<p>Curious whether this resonates with how you feel or if you disagree<p>Also see the broader Vibesbench project: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;firasd&#x2F;vibesbench&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;firasd&#x2F;vibesbench&#x2F;</a><p>Vibesbench discord: <a href=\"https:&#x2F;&#x2F;discord.gg&#x2F;5K4EqWpp\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.gg&#x2F;5K4EqWpp</a>",
      "&gt; Ironically, users who are extremely put off by conversational expressions from LLMs are just as vibe-sensitive as anyone else, if not more so. These are preferences regarding style and affect, expressed using the loaded term ‘sycophancy’.<p>It&#x27;s not just about style. These expressions are information-free noise that distract me from the signal, and I&#x27;m paying for them by the token.<p>So I added a system message to the effect that I don&#x27;t want any compliments, throat clearing, social butter, etc., just the bare facts as straightforward as possible. So then the chatbot started leading every response with a statement to the effect that &quot;here are the bare straightforward facts without the pleasantries&quot;, and ending them with something like &quot;those are the straightforward facts without any pleasantries.&quot; If I add instructions to stop that, it just paraphrases those instructions at the top and bottom and. will. not. stop. Anyone have a better system prompt for that?",
      "My system prompt that works great, you can apply minor adjustments on case by case basis, good start:<p>&gt; Write in textbook style prose, without headings, no tables, no emojis.",
      "AI bros complaining about loaded terms like slop and sycophancy when they still use terms like &quot;intelligence&quot;, &quot;learning&quot;, &quot;reasoning&quot;, &quot;attention&quot; and all the words derived from &quot;neuro&quot; to describe a computer program.<p>Don&#x27;t make sycophantic slop generators and people will stop calling them that"
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Replacing my OS process scheduler with an LLM",
    "url": "https://github.com/mprajyothreddy/brainkernel",
    "source": "hn",
    "summary": "",
    "comments": [
      "This is a pretty funny project, you&#x27;ve outsourced the neurotic developers that keep their task manager open and kill off processes they don&#x27;t like.<p>I wouldn&#x27;t call it replacing the scheduler though - more that you&#x27;ve made a scheduler manager.",
      "It really is cursed to be spending hundreds of watts of power in a datacenter somewhere to make a laptop run slightly faster.",
      "OP here. this is a cursed project lol, but i wanted to see: What happens if you replace the OS scheduler with an LLM?<p>With Groq speed (Llama 3 @ 800t&#x2F;s), inference is finally fast enough to be in the system loop.<p>i built this TUI to monitor my process tree. instead of just showing CPU %, it checks the context (parent process, disk I&#x2F;O) to decide if a process is compiling code or bloatware. It roasts, throttles, or kills based on that.<p>Its my experiment in &quot;Intelligent Kernels&quot; how they would be. i used Delta Caching to keep overhead low.",
      "You did not replace the OS process scheduler with an LLM.",
      "You&#x27;re underselling this as a process manager, it could also be a productivity tool with some prompt changes; Determine procrastination apps: games, non-professional chat, video streaming and kill it.",
      "Please add Roulette mode where a random process is killed every so often",
      "You can&#x27;t replace the NTOS scheduler. This is more of an automated (?) process manager.",
      "Interesting experiment. Scheduling decisions feel like the place where unpredictability shows up first. Curious how you reason about rollback when the scheduler makes a bad call.",
      "Task manager, not scheduler.",
      "Ok"
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Claude Reflect – Auto-turn Claude corrections into project config",
    "url": "https://github.com/BayramAnnakov/claude-reflect",
    "source": "hn",
    "summary": "",
    "comments": [
      "I use the same motion to iterate on linter config and hooks (both claude code hooks and git hooks). So guardrails are actually enforced and iterated upon and do not clutter the context.",
      "The context rot concern is real but I&#x27;ve found it&#x27;s more about structure than size. My CLAUDE.md is ~500 lines but it&#x27;s organised into quick reference tables, critical rules (the stuff that MUST be followed), and project architecture notes. The key is frontloading the high-signal stuff.<p>What&#x27;s worked for me: having separate sections for &quot;things Claude needs every session&quot; vs &quot;reference material it can look up when needed&quot;. The former stays concise. The latter can grow.<p>The anti-pattern I see is people treating it like a growing todo list or dumping every correction in there. That&#x27;s when you get rot. If a correction matters, it should probably become a linting rule or a test anyway.",
      "I won&#x27;t lie, this sounds like a recipe for context rot.<p>LLMs degrade as the context &#x2F; prompt size grow. For that reason I don&#x27;t even use a CLAUDE.md at all.<p>There are very few bits that I do need to routinely repeat, because those are captured by linters&#x2F;tests, or prevented by subdividing the tasks in small-enough chunks.<p>Maybe at times I wish I could quickly add some frequently used text to prompts (e.g. &quot;iterate using `make test TEST=foo`&quot;), but otherwise I don&#x27;t want to delegate context&#x2F;prompt building to an AI - it would quickly snowball.",
      "Are you seeing a benefit above doing this in the prompt &#x2F; project structure?<p>Currently, I have Claude set up a directory with CLAUDE.md, a roadmap, a frequently used commands guide, detailed by-phase plans, and a session log with per-session insights and next steps.<p>After each phase is done, I have it update the documents and ask what could have been done to make the session easier—often the answer is clearer instructions, which eventually leads to topic-specific documents or a new Claude Skill.<p>(edit) These reflection tasks are spelled out in the CLAUDE.md and in the docs directory, so I don&#x27;t have to type them. Each new session, I paste a guide for how Claude should access the information from the last session.",
      "Lately I&#x27;ve been thinking the opposite of this.<p>I use claude code extensively, and lately it does all the coding for me. I&#x27;ve been asking it to go through claude.md to see if it needs to be updated after making changes.<p>I&#x27;m now thinking that I will let claude code write code, but never touch claude.md. It needs to be as short and I need to have full control over it.<p>As i lead the direction of the development the claude.md is my primary way of influencing it, in every single session.",
      "I like the idea, but this [1]:<p><pre><code>    # Check for POSITIVE patterns (new in v3)\n    elif echo &quot;$PROMPT&quot; | grep -qiE &quot;perfect!|exactly right|that&#x27;s exactly|that&#x27;s what I wanted|great approach|keep doing this|love it|excellent|nailed it&quot;; then\n</code></pre>\nis fanciful.<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;BayramAnnakov&#x2F;claude-reflect&#x2F;blob&#x2F;main&#x2F;scripts&#x2F;capture-learning.sh#L36\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;BayramAnnakov&#x2F;claude-reflect&#x2F;blob&#x2F;main&#x2F;sc...</a>",
      "Nice to see you on HN, Bayram aga.",
      "I’ve been wondering why this isn’t more of a thing (knowledge extraction from the conversation in general). The big providers must already be training on this stuff anyway, no?",
      "[dead]"
    ],
    "full_text": null
  },
  {
    "title": "How Thomas Mann Wrote the Magic Mountain",
    "url": "https://www.theguardian.com/books/2025/dec/31/the-master-of-contradictions-by-morten-hi-jensen-review-how-thomas-mann-wrote-the-magic-mountain",
    "source": "hn",
    "summary": "",
    "comments": [
      "In case you know german and like audiobooks, I highly recommend the following version of Magic Mountain (Der Zauberberg)<p><a href=\"https:&#x2F;&#x2F;hoerspiele.dra.de&#x2F;detailansicht&#x2F;1426911\" rel=\"nofollow\">https:&#x2F;&#x2F;hoerspiele.dra.de&#x2F;detailansicht&#x2F;1426911</a><p>(No download link there, but it was a public broadcast production, so should be easy to find for free)<p>It is a great book, certainly made an impression on me.",
      "It is one of the funniest book I ever read.<p>Thomas Mann has the most subtle humour.",
      "If you haven’t read it, Standsrd Ebooks have a US public domain translation available: <a href=\"https:&#x2F;&#x2F;standardebooks.org&#x2F;ebooks&#x2F;thomas-mann&#x2F;the-magic-mountain&#x2F;h-t-lowe-porter\" rel=\"nofollow\">https:&#x2F;&#x2F;standardebooks.org&#x2F;ebooks&#x2F;thomas-mann&#x2F;the-magic-moun...</a>",
      "Interesting to see a new book on this, but disappointing that it seems to re-tread much of what was already known of the author --- maybe this is going to be a trend&#x2F;standard for future writing about authors and their works for this window of time where folks still wrote letters? It is now possible to exhaustively analyze such correspondence far more easily than the laborious manual pouring over of photocopies and archives (for Mann, apparently, in addition to Yale, Baylor, Princeton, and the University of Bonn and the Library of Congress hold extensive collections).<p>Makes one wonder what will happen with recent and contemporary authors --- will their e-mail correspondence survive to be preserved? I know I&#x27;ve lost access to two major sets of my e-mails from previous employers and will lose access to the current one at my retirement (unless I go back as an annuitant? Copy the Outlook .pst archive?) --- at one point in time, Barry Hughart&#x27;s (typewritten!) notes for his books were available on-line, but they have since vanished....<p>Interesting, and I&#x27;ll have to add it to my to-be-read stack --- wondering if Hesse will get the same treatment (or already has and I missed it?) --- his _The Glass Bead Game_ was quite influential on me and probably is why I&#x27;m fascinated by software tools such as OpenSCAD Graph Editor.",
      "I found this book (idk which English translation) unreadable when I looked at it in college.  Maybe I should try again.",
      "&gt; an upstanding burgher obsessed with death and corruption<p>I assume &quot;burgher&quot; is a misspelling of German &quot;Bürger&quot;? There are &quot;Burgher people&quot; but Thomas Mann doesn&#x27;t seem to be one of them.<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Burgher_people\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Burgher_people</a>"
    ],
    "full_text": null
  },
  {
    "title": "Nightshade: Make images unsuitable for model training",
    "url": "https://nightshade.cs.uchicago.edu/whatis.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "I think the title should clarify the year - (2024), because those tools are not useful in the way artists want them to be.",
      "It is kind of unfortunate how people don&#x27;t actually read the paper but only run with the conclusions, speculating whether this would or would not work.<p>Here&#x27;s the paper in question:<p><a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.13828\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2310.13828</a><p>My two cents is that in its current implementation the compromised images can be easily detected, and possibly even &#x27;de-poisoned&#x27;.<p>The attack works by targeting an underrepresented concept (let&#x27;s say 1% of images contain dogs, so &#x27;dog&#x27; is a good concept to attack).<p>They poison the concept of &#x27;dog&#x27; with the concept of &#x27;cat&#x27; by blending (in latent space) an archetypical image of &#x27;cat&#x27; (always the same) to every image containing a &#x27;dog&#x27;.<p>This works during training, since every poisoned image of dog contains the same blended in image of a cat, so this false signal eventually builds up in the model, even if the poisoned sample count is low.<p>But note: this exploits the lack of data in a domain - this would not prevent the model from generating anime waifus or porn, because the training set of those is huge.<p>But how to detect poisoned images?<p>1. You take a non-poisoned labeler (these exist, because clean pre-SD datasets, and pre-poison diffusion models exist)<p>2. You ask your new model and the non-poisoned labeler to check your images. You find that the concept of &#x27;dog&#x27; has been poisoned<p>3. You convert all your &#x27;dog&#x27; images to latent space and take the average. Most likely all the non-poison details will average out, while the poison will accumulate.<p>4. You now have a &#x27;signature&#x27; of the poison. You check each of your images in latent space against the correllation with the poison. If the correllation is high, the image is poisoned.<p>The poison is easily detectible for the same reason it works - it embeds a very strong signal that gets repeated across the training set.",
      "I think we are well beyond this mattering.<p>To my knowledge, the era of scraping online sources of training data is over. The focus has been on reinforcement learning and acquiring access to offline data for at least a year or two. Synthetic data is generated, ranked and curated to produce the new training sets for improving models. There isn&#x27;t even really any point to collecting human made images anymore because the rate of production of anything novel is so low. The future of data collection looks like Midjourney&#x27;s platform where they integrate tools for providing feedback on generated images as well as tools for editing and composing generated images so that they can be improved manually. This closes the loop so the platform for generating images is now part of the model training pipeline.",
      "Seems the same as these submissions from 2 years ago:<p>- <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38013151\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=38013151</a><p>- <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37990750\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=37990750</a>",
      "I&#x27;m very skeptical about such systems, although they note that:<p>&gt; You can crop it, resample it, compress it, smooth out pixels, or add noise, and the effects of the poison will remain. You can take screenshots, or even photos of an image displayed on a monitor, and the shade effects remain<p>if this becomes prevalent enough, you can create a lightweight classifier to remove &quot;poisonous&quot; images, then use some kind of neural-network(probably an autoencoder) to &quot;fix&quot; them. Training such networks won&#x27;t be too difficult as you can create as many positive-negative samples as you want by using this tool.",
      "This similar thing was posted a few weeks ago, and also apparently two years ago, glaze also from uchicago<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46364338\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46364338</a><p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35224219\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=35224219</a><p>We’ve seen this arms race before and know who wins. It’s all snake oil imo",
      "I&#x27;ve run the first of the sample images through 3 captioning models, an old old ViT based booru style tagger, a more recent one and qwen 3 omni. All models successfully identified visual features of the image with no false positives at significant thresholds (&gt;0.3 confidence)<p>I don&#x27;t know what nightshade is supposed to do, but the fact that it doesn&#x27;t affect the synthetic labeling of data at all leads me to believe image model trainers will have close to 0 consideration of what it does when training new models.",
      "It would be funny if this type of research ends up adding major insight to what it is about human vision systems and mental encodings that make us different than pixel arrays with various transformations.",
      "[dead]"
    ],
    "full_text": null
  },
  {
    "title": "Scaling Latent Reasoning via Looped Language Models",
    "url": "https://arxiv.org/abs/2510.25741",
    "source": "hn",
    "summary": "",
    "comments": [
      "If you squint your eyes it&#x27;s a fixed iteration ODE solver. I&#x27;d love to see a generalization on this and the Universal Transformer metioned re-envisioned as flow-matching&#x2F;optimal transport models.",
      "Does the training process ensure that all the intermediate steps remain interepretable, even on larger models? Not that we end up with some alien gibberish in all but the final step.",
      "so it&#x27;s:<p>output = layers(layers(layers(layers(input))))<p>instead of the classical:<p>output = layer4(layer3(layer2(layer1(input))))"
    ],
    "full_text": null
  },
  {
    "title": "Recursive Language Models",
    "url": "https://arxiv.org/abs/2512.24601",
    "source": "hn",
    "summary": "",
    "comments": [
      "Isn&#x27;t this just subagents? You call another LLM to go read a file and extract some piece of information or whatever, so that you don&#x27;t clutter up the main context with the whole file.<p>Neat idea, but not a new idea.",
      "&gt; The key insight is that long prompts should not be fed into the neural network (e.g., Transformer) directly but should instead be treated as part of the environment that the LLM can symbolically interact with.<p>How is this fundamentally different from RAG? Looking at Figure 4, it seems like the key innovation here is that the LLM is responsible for implementing the retrieval mechanism as opposed to a human doing it.",
      "T̶u̶r̶t̶l̶e̶s̶ LLMs all the way down",
      "here&#x27;s a more readable version: <a href=\"https:&#x2F;&#x2F;alexzhang13.github.io&#x2F;blog&#x2F;2025&#x2F;rlm&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;alexzhang13.github.io&#x2F;blog&#x2F;2025&#x2F;rlm&#x2F;</a>",
      "My wishlist for 2026: Anthropic &#x2F; OpenAI expose “how compaction is executed” to plugin authors for their CLI tools.<p>This technique should be something you could swap in for whatever Claude Code bakes in — but I don’t think the correct hooks or functionality is exposed.",
      "Seems similar to this paper: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2510.14826\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2510.14826</a>"
    ],
    "full_text": null
  },
  {
    "title": "Using AI generated images to get refunds",
    "url": "https://www.wired.com/story/scammers-in-china-are-using-ai-generated-images-to-get-refunds/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Maybe the extreme scalability of AI bullshitting will offset the extreme scalability of running large-scale direct-to-consumer oligopolies, and we see some return to local shopping, with all the positive effects on local communities... one can hope",
      "That&#x27;s something C2PA[1] might be able to help with, i.e. your phones camera puts a digital signature on the photo confirming that your phone took it. If that doesn&#x27;t work out due to people photographing an AI image of a display, I would expect custom shop apps to be required to make warranty claims, as they could make use of all the phones sensors and make forging much harder.<p>Either way, I am not sure how big of a problem this is to begin with, since you&#x27;d leave quite the paper trail either way. It&#x27;s not a stunt you can pull off repeatedly without getting caught.<p>[1] <a href=\"https:&#x2F;&#x2F;c2pa.org&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;c2pa.org&#x2F;</a>",
      "An easy solution - open the package when the delivery person comes or when you pick it up from the delivery office. The delivery person can take a photo and act as a witness. If you take the package from the local delivery office, there are cameras and staff, so I can&#x27;t just swap a ripe apple for a rotten one.<p>Where I live we don&#x27;t have the habit of just putting the delivery on the porch for a few reasons. First, it&#x27;s ridiculous if you think about it - no one signed for it, so how could you mark it as delivered? I don&#x27;t get the US in that regard. Secondly, most of the houses have fences, so the delivery person can&#x27;t come to the house even if they wanted to. You&#x27;re basically required to meet the delivery person.",
      "<i>&gt; Plus, even with supposed confirmation from a chatbot, ecommerce platforms won’t necessarily always side with the seller.</i><p>Amazon is pretty notorious for shipping almost all of the risk onto the seller. I suspect that&#x27;s the norm, these days, for most platforms.",
      "The way I see it, generative AI has been introducing a lot of distrust into systems that worked &quot;fine&quot; previously, such as rendering homework ineffective in the case of education, making verification difficult for remote interviewing, flooding the internet with low-quality noise (aka slop) that makes it difficult for reputable and researched sources of information to stand out, with all the implications it has for society, the fraudulent returns described here, and the like.<p>Ultimately, it would be a bit ironic if generative AI ends up kneecaping itself, either through regulation (because businesses and governments will be unlikely to tolerate hiring fraud, returns fraud etc. beyond a threshold), or caused things to move into meatspace through on-site interviews, reliance on physical stores, elimination of online courses and others, which is less amenable to its application.",
      "We absolutely need certified no-AI digital proofs.",
      "every time I dig in this story is always stories of stories, and all walk backward to maybe one single merchant, which is just his word, with no police trail or court case trail or anything substantial, with news agency work over &quot;examples and reconstruction of what might have happened&quot; and no actual data that could be verified &#x2F; falsified.<p>is this something anyone has actually seen happen, or is it part of the AI hype cycle?",
      "Seems easy enough to fix, by requiring the customer to bring back to purchased item. I mean, that&#x27;s how it still works in the real world, at least where I live - if I purchase something from the grocery store, which turns out to be spoiled, I&#x27;ll take the item back and get a refund.",
      "Isn’t this easy to fix over time? Like ok, you issue one refund. But if Amazon sees the same users requesting too many refunds then it is a red flag?",
      "GenAI really is underscoring how much of society is about veracity.<p>I’d say the fears and defenses we had in place for speech online, are having their foundations ripped out from under them.<p>Most of the concern used to be about government control, and that more speech would be the way to democratize and expand our agency over our lives.<p>However now, especially with generative AI and LLMs, the primary vector to control the market place of ideas is to overwhelm the market.<p>Reduce the cost to make content, sandblast our receptors, create too many things to spend our collective energy on verifying, and the outcomes are the same as controlling what is thought and discussed."
    ],
    "full_text": null
  },
  {
    "title": "Developing a BLAS Library for the AMD AI Engine [pdf]",
    "url": "https://uni.tlaan.nl/thesis/msc_thesis_tristan_laan_aieblas.pdf",
    "source": "hn",
    "summary": "",
    "comments": [
      "Looks like the author have not used software pipelining compiler directives with the kernel loops. AMD AIE architecture has 5 cycle load&#x2F;store latency and 7 cycle FP unit latency. With software pipelining, they could have 5-10x speed up for long loops.",
      "This architecture is likely going to be a dead end for AMD. It has been in the wild for several years, yet still has no open programming model, multiple compiler stacks with poor software support. I find it likely that AMD drops this architecture and unifies their ML support around their GPGPU hardware.",
      "Note that this is BLAS on the AMD&#x2F;Xilinx VCK5000 FPGA: <a href=\"https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;products&#x2F;adaptive-socs-and-fpgas&#x2F;evaluation-boards&#x2F;vck5000.html\" rel=\"nofollow\">https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;products&#x2F;adaptive-socs-and-fpgas&#x2F;eval...</a>",
      "So it&#x27;s called an &quot;AI Engine&quot;, but its performance is worse than just running the same thing on CPU? Doesn&#x27;t it make it essentially useless for anything AI related? What&#x27;s the point of this hardware then? Better power efficiency for tiny models? Surely someone must be using it for something?"
    ],
    "full_text": null
  },
  {
    "title": "Experiments with Ableton-MCP",
    "url": "https://jhurliman.org/post/804323197731373056/experiments-with-ableton-mcp-dec-2025",
    "source": "hn",
    "summary": "",
    "comments": [
      "Excellent! I tried to use Claude on the Ableton file format about a year ago and it left me quite frustrated -- but now I have a new reason to look at this again.<p>Generally, it would be nice of Ableton to release an official documentation of their API.",
      "There&#x27;s also one for Bitwig: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;WeModulate&#x2F;bitwig-mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;WeModulate&#x2F;bitwig-mcp-server</a>",
      "This is super cool. Using LLMs to mix together two songs that don&#x27;t belong mixed together is a great usecase.",
      "Awesome stuff. Seems to me like one great use case of an MCP for Ableton would be to develop muscle &#x2F; workflow memory for music production workflows. Adjacently, I&#x27;ve started using Perplexity a bunch for this sort of thing because it indexes YouTube tutorial transcripts. Any thoughts on how to design MCPs for learning Ableton better?",
      "Excited to try batch renaming clips and batch warping in very large sets. Haven’t been able to figure out ways to automate these tedious tasks.",
      "Is there a FOSS version of Ableton?",
      "related <a href=\"https:&#x2F;&#x2F;github.com&#x2F;vroomai&#x2F;live\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;vroomai&#x2F;live</a>"
    ],
    "full_text": null
  },
  {
    "title": "Third Parties and Single Points of Failure",
    "url": "https://calendar.perfplanet.com/2025/third-parties-and-single-points-of-failure/",
    "source": "hn",
    "summary": "",
    "comments": [
      "PartyTown[1] is another library that can help with this.<p>[1]: <a href=\"https:&#x2F;&#x2F;partytown.qwik.dev\" rel=\"nofollow\">https:&#x2F;&#x2F;partytown.qwik.dev</a>"
    ],
    "full_text": null
  },
  {
    "title": "Advanced Rail Energy Storage of North America",
    "url": "https://aresnorthamerica.com/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Publicly available data[1] on the pilot project in Nevada suggests a total of “50MW” generation capacity is planned across 10 rail lines, but the photos on the website seem to only show 1 set being built so far - and a claimed output of 5MW. The per-car mass of 720,000 lb (321 Tonnes) being lowered 229ft=70 Meters (510ft track length x sin(26.8) degrees) in Earth’s 9.81&#x2F;ms^2 gravity field represents a maximum potential energy of only 220MJ, or 61 kWh per car. Reaching 5MW peak requires a car to be dispatched every 44 seconds. 10 cars would provide about 7.5 minutes of runtime - which matches the advertised 15-minute cycle length.<p>This all seems reasonable - but is a far cry from the performance of existing Pumped Hydrostorage plants which routinely exceed 1GW since the 1970s, and can run for several hours per cycle. They do require lots of Water and a mountain’s worth of elevation change, which limits the site selection, whereas this system seems to work with any open-pit mine.<p>It will be interesting to see if this technology can be made competitive with existing grid-stabilization techniques, and what challenges will be encountered along the way.<p>[1] <a href=\"https:&#x2F;&#x2F;www.sandia.gov&#x2F;files&#x2F;ess&#x2F;uploads&#x2F;2021&#x2F;LDES&#x2F;Russ_Weed.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;www.sandia.gov&#x2F;files&#x2F;ess&#x2F;uploads&#x2F;2021&#x2F;LDES&#x2F;Russ_Weed...</a>",
      "Well, it&#x27;s better than the scheme for lifting and lowering cement blocks with a construction crane. And the scheme for digging deep holes in the ground and lowering big rock blocks into them. And the scheme for using electric locomotives and a heavy train in much the same way as this scheme.<p>The animation hand-waves important details. How are those blocks moved on the flat part of the track? There&#x27;s no backup braking system. No guards around the chain. No chain lubrication system.<p>Performance should be roughly comparable to pumped storage with the same height difference, so why bother? Pumped storage doesn&#x27;t use up much water; it&#x27;s the same water going up and down, with some evaporation loss.",
      "For fun, let&#x27;s say you have a 20-ton container, and you raise it up the rail by 100m.  The stored gravitational energy is about 5.5 kWh.<p>According to Wikipedia, Tesla Powerwall 3 has 13.5 kWh of capacity: more than two 20-ton containers raised to 100m, assuming perfect efficiency.  It costs $7,300, small enough to put in your house, and also (more or less) safe enough to put in your house, unlike a 20-ton container on a rail barreling down a slope, which probably needs professional hard-hat maintenance crew.<p>So consider me skeptical.",
      "Track maintenance is the single biggest cost sector of rail lines in good condition even on very low slopes. The ballast needs regular inspection and replacing even without adverse weather or underbed problems, and the track can develop cross fall or slew and&#x2F;or creep in transverse and longitudinal directions over time, sometimes taking subsurface layers with it. Every other kind of gravity storage makes more sense on paper than this, even the tower crane rearranging concrete cubes proposal.",
      "These were quite popular as a startup idea once. I remember <i>Gravitricity</i> (now defunct) being posted here: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24414497\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=24414497</a>",
      "Related:<p><i>Fortescue slashes electric train program but insists zero emissions &#x27;on track&#x27;</i><p>September 04 2025 - <a href=\"https:&#x2F;&#x2F;www.boilingcold.com.au&#x2F;fortescue-slashes-electric-train-program-but-insists-zero-emissions-on-track&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.boilingcold.com.au&#x2F;fortescue-slashes-electric-tr...</a><p><pre><code>  Three years after Andrew Forrest pressed go to develop an electric &quot;Infinity Train,&quot; most of the experienced engineers who joined Fortescue&#x27;s zero-emissions crusade are laid off as the miner goes back to the drawing board on how to have fossil-fuel-free locomotives by 2030.\n\n  The engineers concluded that battery electric locomotives may be able to haul vast amounts of iron ore, eliminating 10 per cent of Fortescue&#x27;s emissions, but the knock-on effects on its immense $21 billion a year integrated mine to rail to port iron ore business were unacceptable.\n</code></pre>\nAnd: <a href=\"https:&#x2F;&#x2F;zero.fortescue.com&#x2F;en&#x2F;case-studies&#x2F;infinity-train\" rel=\"nofollow\">https:&#x2F;&#x2F;zero.fortescue.com&#x2F;en&#x2F;case-studies&#x2F;infinity-train</a> .. 404 Page not found.<p>Looked good for a while there: <i>Fortescue rides the Infinity Train</i> - <a href=\"https:&#x2F;&#x2F;www.electrive.com&#x2F;2025&#x2F;07&#x2F;01&#x2F;fortescue-rides-the-infinity-train&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.electrive.com&#x2F;2025&#x2F;07&#x2F;01&#x2F;fortescue-rides-the-inf...</a>",
      "I think this could be superior to batteries and pumped hydro in enough situations to matter.<p>You can build these in many more places (closer to generation&#x2F;load), the capex is significantly lower, and you can probably build it a hell of a lot faster than a reservoir. This solution is also more incremental than pumped hydro and the equipment will likely last significantly longer than a lithium ion chemistry battery farm.<p><i>The</i> biggest bottleneck for getting a big project on the grid right now is interconnection. If you can avoid having to deploy new transmission lines to a new site you can often chop 5+ years off a project&#x27;s time table.",
      "Is this not just pumped hydro but worse?",
      "In terms of peak power wouldn&#x27;t flywheel farms make sense ? Heavy duty, durable bearings might be a problem.",
      "The main problem with this and many such ideas is that G (the universal gravitational constant) is just so damn small. Making this system store any serious amount of energy in a small footprint is limited by the density of materials you can raise and the height available. In a large footprint you need a lot of infra (eg: loooong rail) which has maintenance cost, and still, very small per-liter and per-m^2 efficiency."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: I used AI to recreate a $4000 piece of audio hardware as a plugin",
    "url": "https://news.ycombinator.com/item?id=46471648",
    "source": "hn",
    "summary": "",
    "comments": [
      "I used to do that exact job 10 years ago (without AI, obviously). I figure that career would be very different now.<p>There was something exciting about sleuthing out how those old machines worked: we used a black box approach, sending in test samples, recording the output, and comparing against the digital algorithm’s output. Trial and error, slowly building a sense of what sort of filter or harmonics could bend a waveform one way or another.<p>I feel like some of this is going to be lost to prompting, the same way hand-tool woodworking has been lost to power tools.",
      "I was hoping that the video was a walkthrough of your process - do you think you might share that at some point?<p>&gt; I&#x27;m not a programmer anymore. I&#x27;m something else now. I don&#x27;t know what it is but it&#x27;s multi-disciplinary, and it doesn&#x27;t involve writing code myself--for better or worse!<p>Yes, I agree. I think the role of software developer is going to evolve into much more of an administrative, managerial role, dealing more with working with whatever organisation you&#x27;re in than actually typing code. Honestly I think it probably was always heading in this direction but it&#x27;s definitely quite a step change. Wrote about it a little incoherently on my blog just this morning: <a href=\"https:&#x2F;&#x2F;redfloatplane.lol&#x2F;blog&#x2F;11-2025-the-year-i-didnt-write-any-code&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;redfloatplane.lol&#x2F;blog&#x2F;11-2025-the-year-i-didnt-writ...</a>",
      "How can you say it&#x27;s a 100% faithful recreation if you&#x27;ve never programmed DSP before?",
      "Very nice work. I’m curious: what kinds of projects are you guys currently working on that genuinely push you out of your comfort zone?<p>I had a small epiphany a couple of weeks ago while thinking about robot skin design: using conductive 3D-printed structures whose electrical properties change under strain, combined with electrical impulses, a handful of electrodes, a machine-learning model to interpret the measurements, and computational design to optimize the printed geometry.<p>While digging into the literature, I realized that what I was trying to do already has a name: proprioception via electrical impedance tomography. It turns out the field is very active right now.<p><a href=\"https:&#x2F;&#x2F;www.cam.ac.uk&#x2F;stories&#x2F;robotic-skin\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cam.ac.uk&#x2F;stories&#x2F;robotic-skin</a><p>That realization led me to build a Bergström–Boyce nonlinear viscoelastic parallel rheological simulator using Taichi. This is far outside my comfort zone. I’m just a regular programmer with no formal background in physics (apart from some past exposure to Newton-Raphson).<p>Interestingly, my main contribution hasn’t been the math. It’s been providing basic, common-sense guidance to my LLM. For example, I had to explicitly tell it which parameters were fixed by experimental data and which ones were meant to be inferred. In another case, the agent assumed that all the red curves in the paper I&#x27;m working with referred to the same sample, when they actually correspond to different conducting NinjaFlex specimens under strain.<p>Correcting those kinds of assumptions, rather than fixing equations, was what allowed me to reproduce the results I was seeking. I now have an analytical, physics-grounded model that fits the published data. Mullins effect: modeled. Next up: creep.<p>We’ll see how far this goes. I’ll probably never produce anything publishable, patentable, or industrial-grade. But I might end up building a very cheap (and hopefully not that inaccurate), printable proprioceptive sensor, with a structure optimized so it can be interpreted by much smaller neural networks than those used in the Cambridge paper.<p>If that works, the gesture will have been worth it.",
      "awesome, in 2025 I made a few apps for my small business that I have spent hours trawling the web looking for, and I have little coding skills.<p>Sometimes it feels like I&#x27;m living in a different world, reading the scepticism on here about AI.<p>I&#x27;m sure there enterprise cases where it doesn&#x27;t make sense, but for the your everyday business owner it&#x27;s amazing what can be done.<p>maybe it&#x27;s a failure of imagination but I can&#x27;t imagine a world where this doesn&#x27;t impact enterprise in short order",
      "This is fantastic. I’m currently building a combustion engine simulator doing exactly what you did. In fact, I found a number of research papers, had Claude implement the included algorithms, and then incorporated them into the project.<p>What I have now is similar to <a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;nXrEX6j-Mws?si=XdPA48jymWcapQ-8\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;nXrEX6j-Mws?si=XdPA48jymWcapQ-8</a> but I haven’t implemented a cohesive UI yet.",
      "I&#x27;m not in the domain, even though I did dabble with DAW and tinker with a PGB-1 and its open source firmware, but how far would you say CMajor helped? I feel like solely picking the right tool, being framework, paradigm, etc can make or break a project.<p>Consequently here for me to better understand how special this is I&#x27;d appreciate how (especially since I don&#x27;t see a link to code itself) how does one go to e.g. <a href=\"https:&#x2F;&#x2F;cmajor.dev&#x2F;docs&#x2F;GettingStarted#creating-your-first-patch---hello-world\" rel=\"nofollow\">https:&#x2F;&#x2F;cmajor.dev&#x2F;docs&#x2F;GettingStarted#creating-your-first-p...</a> to a working DSP.",
      "On your &quot;Scares the shit out of me&quot; comment.<p>Use AI like a CNC machinist uses a mill. You&#x27;re still in the loop, but break it into manageable &quot;passes&quot; with testing touchpoints. These touchpoints allow you to understand what&#x27;s going on. Nothing wrong with letting AI oneshot something, but it&#x27;s more fun and less ennui to jump in and look around and exercise some control here and there. And, on larger systems, this is basically required. (for now, perhaps).<p>This is how I do it now: <a href=\"https:&#x2F;&#x2F;jodavaho.io&#x2F;posts&#x2F;ai-useage-2025.html\" rel=\"nofollow\">https:&#x2F;&#x2F;jodavaho.io&#x2F;posts&#x2F;ai-useage-2025.html</a>",
      "Great achievement!<p>Regarding your own titleing: you are now some type of &quot;platform operator&#x2F;manager&quot; of this agents :-))",
      "Did you recreate the UI only or also the internal circuits? Does it produce a similar distorsion?"
    ],
    "full_text": null
  },
  {
    "title": "Web development is fun again",
    "url": "https://ma.ttias.be/web-development-is-fun-again/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Something I like about our weird new LLM-assisted world is the number of people I know who are coding again, having mostly stopped as they moved into management roles or lost their personal side project time to becoming parents.<p>AI assistance means you can get something useful done in half an hour, or even <i>while</i> you are doing other stuff. You don&#x27;t need to carve out 2-4 hours to ramp up any more.<p>If you have significant previous coding experience - even if it&#x27;s a few years stale - you can drive these things extremely effectively. Especially if you have management experience, quite a lot of which transfers to &quot;managing&quot; coding agents (communicate clearly, set achievable goals, provide all relevant context.)",
      "I remember those times, and it was a lot of fun, but there&#x27;s really nothing stopping you from running a LAMP stack today, writing PHP without frameworks and with manual SQL queries.<p>In fact, it&#x27;s a lot more fun for me to approach this today. Modern PHP is a joy. MariaSQL is very much MySQL (and switching to Postgres isn&#x27;t exactly a bump in complexity). It&#x27;s way easier to write code that won&#x27;t get injected.<p>If you want to slice your designs in Photoshop (ehem, the real OGs used Fireworks) go ahead and use Dreamweaver, go ahead. That said, HTML5 makes not having to use tables for layout easy, not more complex and VS Code has all the good parts of Dreamweaver (trust me, you don&#x27;t need or want the WYSIWG... if you must, just use inspect elements and move the changes over to the HTML file).<p>I guess all this is to say that web dev is simpler, not more complex for solo devs today. There exists more complicated tooling, but if you&#x27;re solo-dev&#x27;ing something for fun, skip it!<p>EDIT: Also, phpMyAdmin was fun to use but also the best way to get your box popped. Today, something like DBeaver suits me just fine.",
      "Really like using alpine with a classical JS server rendered stack too.  Most crud apps don’t need a spa app and now you are working out of one code base again.  Codex chews through this kind of code",
      "When stuff was getting too complicated, I looked for ways to make things simpler.<p>Developers have spent decades trying to figure out ways to make things simpler, less code the better, only to throw it all out the window because chatbot go brrrrrr.",
      "<i>On the frontend, you have build pipelines, bundlers, CSS frameworks with their own toolchains, progressive web apps, Core Web Vitals, SEO, layout shifts, srcset&#x2F;responsive images…</i><p>I&#x27;ve been making web stuff for a similar length of time as Mattias by the sounds of it. I started with Perl but moved to PHP 4 pretty soon after. I recognise this problem but I have different take.<p>All the complexity was there 20 years ago, but we ignored it. That doesn&#x27;t mean it was simpler. It just means we took crazy (with hindsight) risks. Sure, there were no build pipelines like today, but we had scripts we ran to build things. There was Adobe Pagemill for making site wide changes before we deployed a new version. Back in the day we made those changes, did a very brief check that things worked locally, and then <i>manually FTP&#x27;d files to a server, breaking it in the process because a user would see the site change as they navigated.</i> Some of us would put up a maintenance page during an update effectively just blocking all the traffic. That&#x27;s certainly &#x27;simpler&#x27;, but it&#x27;s also much worse for the user, and on a site that did things with data potentially risked corrupting a user&#x27;s records. It was incredible that things didn&#x27;t break more often. Maybe they did and we just never realised.<p>We didn&#x27;t have CSS frameworks but we certainly did have our own in-house templates, and they had separate toolchains. As time went on that toolchain mostly migrated to Wordpress and it&#x27;s template builder plugins. Again, give me Tailwind over that mess.<p>We had Core Web Vitals and SEO in the form of Urchin Stats. We had layout shift but we called it FOUC. We had kind of had srcset, but it was implemented as a set of Macromedia Dreamweaver mm_ JS image preload and swapping functions. &lt;picture&gt; is a lot nicer.<p>Things are just better now. Writing web software is loads of fun. I also leverage LLMs in my code because they&#x27;re awesome, but not to simplify things. I don&#x27;t think the complexity is new. I just think it&#x27;s visible now.",
      "Going in 2026, the frontend has many good options, but AI is not one of them.<p>We have many typesafe (no, not TypeScript!) options with rock solid dev tooling, and fast compilers.<p>AI is just a badaid, its not the road you want to travel.",
      "Strong agree. The modern web world is clearly better but we traded a whole lot of complexity for a little bit of benefit (and frequently regressed on speed). The microservices and javascript framework wars were the dark ages.",
      "&gt; On the frontend, you have build pipelines, bundlers, CSS frameworks with their own toolchains, progressive web apps, Core Web Vitals, SEO, layout shifts, srcset&#x2F;responsive images…<p>LLMs are successful in webdev because of unnecessary frameworks being piled on top of each other more in the name of job security than technical necessity.",
      "As someone that only has sporadic pockets of deep time in my free time the thing that has been immensely helpful from an LLM coding point of view is mental model building. I can now much more easily get &quot;into the flow&quot; after being away from a codebase for a period of time by asking questions. For example, remind me where all the integration points for that API route is located. Or give me a rundown on this file. Etc.. It gets me back up to speed so much more quickly and makes me productive with limited amounts of time. It also means I don&#x27;t have to try to carry this context around with me or I&#x27;ll forget it.",
      "Ironically I&#x27;m thinking the exact opposite. Now I can build stuff without dealing with the chaos in the frontend frameworks ecosystem..."
    ],
    "full_text": null
  },
  {
    "title": "IQuest-Coder: A new open-source code model beats Claude Sonnet 4.5 and GPT 5.1 [pdf]",
    "url": "https://github.com/IQuestLab/IQuest-Coder-V1/blob/main/papers/IQuest_Coder_Technical_Report.pdf",
    "source": "hn",
    "summary": "",
    "comments": [
      "Better link:\n<a href=\"https:&#x2F;&#x2F;iquestlab.github.io&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;iquestlab.github.io&#x2F;</a><p>But yes, sadly it looks like the agent cheated during the eval",
      "TL;DR is that they didn&#x27;t clean the repo (.git&#x2F; folder), model just reward hacked its way to look up future commits with fixes. Credit goes to everyone in this thread for solving this: <a href=\"https:&#x2F;&#x2F;xcancel.com&#x2F;xeophon&#x2F;status&#x2F;2006969664346501589\" rel=\"nofollow\">https:&#x2F;&#x2F;xcancel.com&#x2F;xeophon&#x2F;status&#x2F;2006969664346501589</a><p>(given that IQuestLab published their SWE-Bench Verified trajectory data, I want to be charitable and assume genuine oversight rather than &quot;benchmaxxing&quot;, probably an easy to miss thing if you are new to benchmarking)<p><a href=\"https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1q1ura1&#x2F;iquestlabiquestcoderv1_swebench_score_is&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;LocalLLaMA&#x2F;comments&#x2F;1q1ura1&#x2F;iquestl...</a>",
      "GLM-4.7 in opencode is the only opensource one that comes close in my experience and probably they did use some Claude data as I see the occasional You’re absolutely right in there",
      "A 40B weight model that beats Sonnet 4.5 and GPT 5.1? Can someone explain this to me?",
      "Has anyone run this yet, either on their own machine or via a hosted API somewhere?",
      "This is a lie, so why is it still on the front page?"
    ],
    "full_text": null
  },
  {
    "title": "Cadova: Swift DSL for parametric 3D modeling",
    "url": "https://github.com/tomasf/Cadova",
    "source": "hn",
    "summary": "",
    "comments": [
      "Nice! I love code-based CAD. Eventually I want to build a tool which uses a hybrid approach: a GUI for things that are hard&#x2F;tedious to express in code like complicated 2D sketches, with code as the &quot;persistence&quot; layer so at the end you still just have code to maintain, no binary files or piles of XML.<p>One of the hard parts though will be synchronizing changes between UI and code. I suppose it could start as a unidirectional flow from UI to code... if you were to generate a sketch with something like a loop, it would be hard to recover that code structure from just a bunch of resulting points and line segments.<p>But anyway, I&#x27;m happy to see more code-based CAD approaches pop up. I think there&#x27;s still a lot to explore in this space.",
      "I&#x27;d like to see the code for this more complex object: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;tomasf&#x2F;Cadova&#x2F;wiki\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;tomasf&#x2F;Cadova&#x2F;wiki</a>",
      "This project uses some very interesting Swift techniques. Is this the new C++ interop? Looks very clean.",
      "I find this much more readable than KCL[0], but I also understand the ultimate goals of the two are probably a bit different.<p>[0]: <a href=\"https:&#x2F;&#x2F;zoo.dev&#x2F;docs&#x2F;kcl-samples&#x2F;pillow-block-bearing\" rel=\"nofollow\">https:&#x2F;&#x2F;zoo.dev&#x2F;docs&#x2F;kcl-samples&#x2F;pillow-block-bearing</a>",
      "Regarding the geometry engine, the README says:<p>Cadova uses Manifold-Swift, Apus and ThreeMF.<p>First I hear of those. Curious to see how those compare to things like OpenCascade.",
      "Looks similar to OpenSCAD:<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;OpenSCAD\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;OpenSCAD</a>"
    ],
    "full_text": null
  },
  {
    "title": "China Social Media Hails US Maduro Move as a Taiwan Template",
    "url": "https://www.bloomberg.com/news/articles/2026-01-04/china-social-media-hails-trump-s-maduro-move-as-taiwan-template",
    "source": "hn",
    "summary": "",
    "comments": [
      "<a href=\"https:&#x2F;&#x2F;archive.ph&#x2F;oUDGl\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.ph&#x2F;oUDGl</a>",
      "Who can blame them... perfect blue print."
    ],
    "full_text": null
  },
  {
    "title": "Linus Torvalds gets candid About Windows, workflows, and AI",
    "url": "https://thenewstack.io/linus-torvalds-gets-candid-about-windows-workflows-and-ai/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Ironic that Linus himself seems to feel less rivalry towards Windows than a large number of Linux users do."
    ],
    "full_text": null
  },
  {
    "title": "10 years of personal finances in plain text files",
    "url": "https://sgoel.dev/posts/10-years-of-personal-finances-in-plain-text-files/",
    "source": "hn",
    "summary": "",
    "comments": [
      "+1 to OP&#x27;s book, which is the best beginner guide I&#x27;ve found for understanding Beancount &#x2F; plaintext accounting.<p>I was also confused about double-entry accounting for most of my life until I read the article, &quot;Accounting for Computer Scientists&quot;[0] by Martin Kleppman (author of <i>Designing Data-Intensive Applications</i>). It explains double entry accounting in a surprisingly accessible way by putting it in terms of graph theory. I don&#x27;t even like graph theory that much or consider myself competent in it, but Kleppman&#x27;s explanation was extremely effective.<p>[0] <a href=\"https:&#x2F;&#x2F;martin.kleppmann.com&#x2F;2011&#x2F;03&#x2F;07&#x2F;accounting-for-computer-scientists.html\" rel=\"nofollow\">https:&#x2F;&#x2F;martin.kleppmann.com&#x2F;2011&#x2F;03&#x2F;07&#x2F;accounting-for-compu...</a>",
      "I&#x27;ve tried to track personal finances several times, but it only started to work when I&#x27;ve discovered the idea (from <a href=\"https:&#x2F;&#x2F;github.com&#x2F;adept&#x2F;full-fledged-hledger\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;adept&#x2F;full-fledged-hledger</a>) that you need to treat the whole PTA story more like a project compilation:<p>- Everything is managed by build system that is able to track dependencies<p>- Inputs from financial institutions are kept in the repo as is<p>- Those inputs are converted by your scripts to .csv files that are readable by PTA import engine<p>- There are rules files that describe how to convert .csv lines to PTA entries<p>- Generated files are included from per-year PTA journals (and you can put any manual transactions in here also)<p>The benefit is that you can change any part of this pipeline, and just re-generate the changed parts:<p>- improve the program that converts to .csv - raw data immediately gets better across the whole repo<p>- add&#x2F;customize import rules - better classification is immediately applied to all of the past data<p>And with this approach you can start small (like, a single month of data from your primary bank), and refine the thing in steps, like adding more historical data or adding more data sources (examples being not only bank statements, but even things like itemized Amazon orders and Paypal slips).",
      "I started decades ago using Quicken. Then had to re-enter all my data when a new version came out and support for the old version was EOLed. So I switched to GNU Cash and re-entered everything again. Then there were migration issues with GNU Cash.<p>Finally I discovered PTA. I chose hledger (because of possible performance issues with beancount). I learned double-entry bookkeeping (it&#x27;s pretty simple, honestly). I write Python scripts to import investment statements sent in PDF format (broker does not support transaction downloads) and pay stubs (payroll company does not do transaction downloads) and CSV imports for bank and credit card statements which lets me autocategorize most transactions. Turns out it&#x27;s exactly what I wanted all this time I just didn&#x27;t know it.<p>I spend about an hour each month updating accounts. I generate investment reports, budget reports, and tax summaries. It&#x27;s all there, except tracking the ACB of my investments (which I probably could do except a simple spreadsheet is less effort). Everything I need for my coming retirement this year.<p>And because it&#x27;s plain text I will never have to lose everything when data formats change due to new upstream releases. And because it&#x27;s plain text I can use git to track changes and recover and do better off-site backups.<p>I guess the downside is it doesn&#x27;t work on your phone and you have to know what you&#x27;re doing. But if information like how much money you have and where you&#x27;re spending it is actually important, it&#x27;s the right tool for the job.",
      "I&#x27;ve been beancount&#x27;ing for years now<p>As we&#x27;ve crossed into the new year I&#x27;ve switched to a similar directory setup as the OP with 1 file per year. Previously I just had one file that was from 2022 which ended up being like 2 million lines of text, which was starting to bog down the emacs plugin.<p>What I appreciate the most about this approach to personal finances is it just tracks everything. Investments, pensions, RSUs, bank accounts. You could even go as far as accounting for any resource that&#x27;s modellable, e.g. energy usage in kwh vs. bills. I probably wouldn&#x27;t go that far though :D<p>Also you can build a bunch of tooling around it too, with the advent of LLMs my toolset for beancount management has expanded quite significantly. Most recently I got claude to rewrite my transaction rules engine <a href=\"https:&#x2F;&#x2F;djharper.dev&#x2F;post&#x2F;2025&#x2F;08&#x2F;19&#x2F;using-llms-to-turn-scripts-into-applications&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;djharper.dev&#x2F;post&#x2F;2025&#x2F;08&#x2F;19&#x2F;using-llms-to-turn-scri...</a> into something nicer with a UI. This would have taken days to build in the before times, and I probably would not have bothered because it&#x27;s overkill for 1 user (me)",
      "A lot of people in this thread comingling &quot;plain text&quot; with &quot;double entry&quot;.<p>Beancount is both, but you can do either one or the other or both. In particular, you DONT need to learn double entry to do plain text accounting. Of course, you SHOULD learn double entry accounting because it&#x27;s a great tool for organizing knowledge.<p>Whether plain text is beneficial for accounting is less clear cut in my mind. I think plain text is backlash against the over-reliance on &quot;services&quot;, &quot;clouds&quot;, and &quot;lock in&quot;, but I think it&#x27;s misguided. If you&#x27;re concerned with someone locking you in, it&#x27;s perfectly concievable to do double entry accounting which lives only on your computer and manipulated with software that is under your control (read: free software).<p>So in summary, my opinon is:<p>- accounting: important<p>- double entry: important<p>- avoid clouds and ventor lock in: important<p>- avoid proprietary file formats: important<p>- plain text: unimportant<p>(I use GnuCash. It&#x27;s not without its flaws, but it&#x27;s great if you agree with the above views)",
      "I began with PTA recently. I think the barrier to entry is high because you first need to learn double entry bookkeeping  (if you haven&#x27;t already) and then you need to decide between ledger-cli, hledger, or beancount, with the differentiators being on the margins and with some promise of being able to switch later. The choice really comes down to which tool has the documentation&#x2F;community that makes the most sense to you at the time.<p>Then, there&#x27;s the import workflow: which &quot;accounts&quot; should you start with? How much history do you pull in? How do you set up an automatic importer? Hledger has a DSL. Beancount uses Python. Either way, an OP says, much of your time is spent manually editing text.<p>And finally, then what? Can I make a budget now? Will this thing do my taxes? Am I more financially responsible? How do I explain this to my spouse? My pension is kind of like a commodity, but I don&#x27;t know what the unit price is, and I don&#x27;t sell units, but what&#x27;s a virtual PnL and what if I only have a quarterly PDF!?<p>It may sound like I&#x27;m ranting, but I have found that realizing I don&#x27;t know the answers to these questions (or even that they exist) is the true benefit of PTA.<p>Every year, I&#x27;m asked if I want a different pension investment mix or if I want to change my car insurance. Or, I might wonder if I&#x27;m getting a good deal on my internet plan or if a new job offer&#x27;s total comp is actually better. Am I &quot;on track&quot; for &quot;retirement,&quot; how long until I have enough for a new roof, am I keeping up with inflation, did I spend too much on gifts this year?<p>There&#x27;s immense privilege in not really needing to know the answers to these questions; getting them &quot;wrong&quot; won&#x27;t really hurt you. But, being familiar with the routine minutiae of your economy by way of counting every cent, is rewarding, enlightening, and empowering—even if it&#x27;s also finicky and brittle sometimes.<p>I may have to try beancount again. OP&#x27;s importers look promisingly robust compared to my hledger scripts.",
      "I go a step further than this, which is to make the beancount files 100% generated by a program - no manual edits allowed at all. I have a version-controlled directory with the raw data (csvs, pdfs, weird text formats) from a bunch of different sources (bank accounts, payslips, pension funds, mortgage statements, and stock market price data). The process is then just an ETL pipeline to an intermediate data structure. That data structure can be serialized to beancount format, hledger format, whatever. I&#x27;ve even had it output prometheus metrics so I can create nice dashboards in grafana.<p>I dump more raw data in every month, handle the 1-2 new edge cases in code, and voila: a complete, accurate, queryable, debuggable, visualizable, and fully reproducible history of my finances.",
      "Shoutout to fava, the beancount GUI frontend:<p><a href=\"https:&#x2F;&#x2F;beancount.github.io&#x2F;fava&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;beancount.github.io&#x2F;fava&#x2F;</a><p>I really like its big picture view of the accounts, the search &#x2F; query interface, and live editing of transactions.",
      "My less obsessive personal finance workflow: I have a spreadsheet. I update it by hand once a month, and have been doing so for ~20 years. Takes about 5 minutes. I only track the categories I care about. Those have varied a bit over time, but have always included staples like power, heating, water, ISP, rent&#x2F;mortgage, insurance, savings, etc. I don&#x27;t track things like groceries or restaurants or anything else that I consider to be part of &quot;living life&quot;.<p>Two main goals: track how my expenses evolve over time, and stay on top of how much money I need to reserve for the coming year to pay those bills. Anything that&#x27;s leftover is spending money. Which I dutifully spend.",
      "This sounds super cool and useful to me, but how does this work with a partner who&#x27;s non-technical? Managing personal finances is something that we do together and having a nice clean UI that makes sense to her is important. Is there a way to achieve that with beancount? Currently we&#x27;re using YNAB, which is mostly great although sometimes unstable and limited in ways."
    ],
    "full_text": null
  },
  {
    "title": "One Number I Trust: Plain-Text Accounting for a Multi-Currency Household",
    "url": "https://lalitm.com/post/one-number-i-trust/",
    "source": "hn",
    "summary": "",
    "comments": [
      "You don&#x27;t need accounting or double-entry bookkeeping to compute net worth.<p>Net worth is easy: assets - liabilities<p>You get the figures from your financial institutions and counting up your cash on hand.<p>The purpose of double-entry bookkeeping is to track the flow of money and to make sure nothing was missed.<p>But for net worth, you only need the end result, which you don&#x27;t need any computation for.<p>The only thing that this solves is tracking money in flight, because during a transfer it disappears from your view of the accounts. There are simpler solutions to this problem than tracking absolutely everything.",
      "I get that people on here don&#x27;t like subscriptions, and I understand why, but I&#x27;ve been using You Need a Budget for well over a decade, and it is probably the last subscription I&#x27;d ever give up.<p>The automatic import of expenses is so valuable for my family and keeping track of how much we&#x27;ve spent and how much we have left for the month.<p>We used the fixed-cost version for years beyond when it was officially supported, and it didn&#x27;t have the automated import, and I don&#x27;t think I could ever go back to a system that didn&#x27;t. There were always a few days a month, at least, when I wasn&#x27;t exactly sure where we were in our expenses.",
      "I used Ledger for a while, then some other tools. Now I&#x27;m on Actual Budget, a nice FOSS envelope budget app that can import transactions from my bank.<p>I find value in tracking everything, tracking it by hand, and tracking it with precision (our household budget has 68 categories).<p>When I&#x27;ve tried easing up in the past (e.g., with Mint&#x27;s lightweight approach) I was left with a budgetary black box where I felt like I never had enough information to make big purchasing decisions. I knew what I had in the bank account, but I didn&#x27;t know if it was earmarked for anything, or whether the next surprise expense was going to wreck my plans. I felt afraid and paralyzed.<p>Earning more didn&#x27;t make the problem go away. Like the financial equivalent of Parkinson&#x27;s Law, more income just meant more spending. I couldn&#x27;t out-earn unrestrained consumption. I had to monitor &amp; manage it.<p>For peace of mind, I found YNAB&#x27;s philosophy helpful: one-off expenses often repeat predictably on a long-enough time horizon, and can be amortized accordingly. If I itemize all predictable expenses and save a little each month, I know <i>everything</i> is taken care of, and what I really have left over. I never get blindsided because several big expenses hit at once.<p>I know not everyone has these problems. But I like to talk about my experiences because people don&#x27;t all need the same things from their finances. It&#x27;s okay and normal to want control and visibility. Budget apps exist because people find them useful, not because the whole userbase has failed to reach enlightenment and transcend budgeting.",
      "This is neat, but I&#x27;m not really convinced this level of granularity makes sense for personal finances? Or at least it does not for me personally. I find that simply entering all my accounts into one big excel once a month more than suffices to keep track of everything. Maybe I make a typo here and there but it doesn&#x27;t really matter in the grand scheme of things cuz I&#x27;m gonna type a new number next month anyway.",
      "I didn’t think negative amounts were a real thing in double-entry accounting? Instead some things are debits and others are credits, and whether each of those reduces or increases an account balance depends on the type of account it’s against. Like, you could pay an expense out of a debt account, which would increase the debt, or out of a cash account, which would decrease the cash. Debits and credits should still be equal, though.",
      "I used to do this during my first real job and was tracking every sandwich I bought, then one of the senior engineers told me to stop wasting time with it and make more money, and then I was enlightened.",
      "I really enjoyed reading this, and it is inspirational as it is something I have wanted to do for a long time.  And as a software developer, it really appeals to me.<p>How do you think it compares time-wise to using existing accounting software? Was the time investment worth it to get the control and visibility you now have?",
      "As being discussed in another thread about plain-text accounting[1], what I&#x27;ve found most difficult about these tools is the learning curve between &quot;Assets = Liabilities + Equity&quot; and the realities of modeling a household economy.<p>I appreciate the level of detail in this post. I think there&#x27;s often confusion that plaintext == easy&#x2F;simple. The real takeaway is: &quot;if you&#x27;re going to go through all of the trouble of managing your economy, you may as well make sure you control your data and own your system.&quot;<p>[1]: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46463644\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46463644</a>",
      "I like having an archive of PDF statements but the downside is that it’s usually a tedious manual process to download them. There’s so many to track - bank accounts, credit cards, investments, utilities, doctor bills… Every website is different, and they are rarely batch-downloadable. So I tend to be a few months behind at any given moment until I get around to it.<p>Anyone found a better way to keep on top of downloading statements?",
      "I agree with an earlier post that asserted &quot;net worth = assets - liabilities&quot; but I&#x27;d like to do better at understanding what that really means.<p>Some are easy like the size of your bank balance.  Others are much harder.<p>For example, one asset we have is our home and there are many websites out there that tell you how much it&#x27;s worth but they each vary an awful lot and change dramatically - so much so sometimes that any savings you&#x27;ve made in the same period are eclipsed.<p>Similarly, the 401k I&#x27;ve been building up for years seems like a decent amount but trying to calculate what it&#x27;s worth after taxes and therefore how much you&#x27;ll have to spend each month seems unknowable.<p>I think the same is true of investment accounts.  If we seeded one with $500 and it&#x27;s now worth $250, it&#x27;s easy to think your net worth has risen by $250 but it really hasn&#x27;t when taxes, fees and who knows what else is taken into account."
    ],
    "full_text": null
  },
  {
    "title": "Standard Ebooks: Public Domain Day 2026 in Literature",
    "url": "https://standardebooks.org/blog/public-domain-day-2026",
    "source": "hn",
    "summary": "",
    "comments": [
      "Related: for fun over the holidays, I created an ePub of a paperback copy of &quot;I Brought The Ages Home&quot;, by Charles T. Currelly, which went out of copyright in Canada in 2007 (copyright in Canada changed from 50 to 70 years after the death of the author in 2022, but this did not affect works that were already in the public domain).<p>I couldn&#x27;t find a ebook online, so I found an old paperback copy and created one:\n<a href=\"https:&#x2F;&#x2F;www.hotelexistence.ca&#x2F;create-epub-from-paperback&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.hotelexistence.ca&#x2F;create-epub-from-paperback&#x2F;</a><p>Charles T. Currelly was like a real-life Indiana Jones, he was the first director of the Royal Ontario Museum in Toronto, and sourced much of its early collections.<p>Even with modern OCR (I used Mistral&#x27;s here), and a book with limited formatting, it&#x27;s funny how hours of touch-ups are required just to get a glitch-free reading experience (no stray headers, paragraphs, page numbers sprinkled through the text).",
      "I’m a contributor – I did Kafka’s <i>The Castle</i>, Agatha Christie’s <i>Giant’s Bread</i>, and Stella Benson’s <i>The Faraway Bride</i> for this launch – and I’m happy to answer any questions about Standard Ebooks.",
      "The title makes it look like Public Domain is universal, while the article does mention that this list is only about the USA.<p>&gt; On January 1, 2026, books published in 1930 enter the U.S. public domain.<p>The Copyright laws are different in each country, and it&#x27;s a non-sense in the modern world.<p>A few years ago, I was searching for books written by Alexandra David-Neel. I found them on a Canadian (IIRC) website, but downloads were filtered by geo-IP, since what was in the public domain there was not yet public in France. One of the books I wanted was written before 1900, and not in print since then. Yet the author died in 1969, aged 100, so the French Public Domain for her works will start in 2040.<p>Another example: &quot;As I lay dying&quot; by William Faulkner is now Public Domain in the USA. It was Public Domain in Canada from 2013 to 2023. Then the law changed, and the copyright was extended by 20 years, and reinstated for this book until 2032 — which is 70 years after the author&#x27;s death in 1962.",
      "I built a small app that includes Standard Ebooks and Gutenberg ebooks for personal library management (and send to Kindle): <a href=\"https:&#x2F;&#x2F;scriptwerk.com\" rel=\"nofollow\">https:&#x2F;&#x2F;scriptwerk.com</a><p>Hopefully this makes discovery of books easier and lets people manage their libraries online - I like Calibre, but it is not great for people who are just getting started.",
      "Standard Ebooks produces exceptional quality documents. Here&#x27;s my comparison against similar projects:<p><a href=\"https:&#x2F;&#x2F;dave.autonoma.ca&#x2F;blog&#x2F;2020&#x2F;04&#x2F;11&#x2F;project-gutenberg-projects&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dave.autonoma.ca&#x2F;blog&#x2F;2020&#x2F;04&#x2F;11&#x2F;project-gutenberg-p...</a>",
      "Happy Public Domain Day, everyone! Such a great project.<p>A bit tangential here, but I am really looking forward to 2035 for the public domain. A ton of culturally significant works seem to enter then - And Then There Were None, Gone with the Wind, The Wizard of Oz, Mr. Smith Goes to Washington, Batman (Detective Comics #27), Superman #1, Marvel Comics #1, and Tintin’s King Ottokar’s Sceptre.<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2035_in_public_domain\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;2035_in_public_domain</a><p>Wikipedia also tells me that all of the &#x27;life + 70&quot; countries will have Ian Fleming&#x27;s James Bond works in the public domain in 2035 as well.",
      "Related:<p><i>Happy Public Domain Day 2026</i><p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46460440\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46460440</a><p><i>What will enter the public domain in 2026?</i><p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46117112\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46117112</a>",
      "It would be interesting to see a combined list taking into account both US &quot;1930 or older&quot; rule and more common internationally &quot;life+70&quot; rule, to see what works have finally escaped both of those and make works a bit less unsafe to make use of, but I have not seen any list like that?",
      "Off topic, but one thing <i>I wish</i> I could do is donate a single copy epub I have the rights to to all libraries. It should be technically possible (many of the places I have lived the local library uses Overdrive).",
      "Does the movie Maltese Falcon too enter public domain?"
    ],
    "full_text": null
  },
  {
    "title": "Chain Flinger",
    "url": "https://nealstephenson.substack.com/p/kdk-kinetik-der-kontinua-part-1-introduction",
    "source": "hn",
    "summary": "",
    "comments": [
      "Related (Steve Mould has been fascinated with the centripetal physics of the chain): <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLcqX4UMXNKEdNBKABT3ZF6Fvu5Jkq3OxB\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;playlist?list=PLcqX4UMXNKEdNBKABT3ZF...</a>",
      "Doesn&#x27;t even need to be an actual chain: <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Space_fountain\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Space_fountain</a>",
      "A related idea is to improve the efficiency of a trebuchet by stringing heavy weighted nodes along the sling line. The biggest one built so far used one 50 pound node on the sling line to launch an 8 pound pumpkin 1500 feet, but the math says it keeps working better as you add more nodes.\n<a href=\"https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=ruiMZYTc5HE\" rel=\"nofollow\">https:&#x2F;&#x2F;m.youtube.com&#x2F;watch?v=ruiMZYTc5HE</a>",
      "The chain robot subplot of Seveneves makes a lot more sense now :)",
      "I got my youngest a ZipString Aracna for Christmas (entirely driven by a tiny cameo in the film Weapons). It is rare that a toy for kids feels magical, but it&#x27;s been a real hit over the holidays."
    ],
    "full_text": null
  },
  {
    "title": "Baserow: Build databases, automations and agents with AI, Airtable alternative",
    "url": "https://github.com/baserow/baserow",
    "source": "hn",
    "summary": "",
    "comments": [],
    "full_text": null
  },
  {
    "title": "FreeBSD: Home NAS, part 1 – configuring ZFS mirror (RAID1)",
    "url": "https://rtfm.co.ua/en/freebsd-home-nas-part-1-configuring-zfs-mirror-raid1/",
    "source": "hn",
    "summary": "",
    "comments": [
      "When setting up root-on-ZFS on FreeBSD, it&#x27;s worth knowing about boot environments (a concept originally from Solaris):<p>* <a href=\"https:&#x2F;&#x2F;klarasystems.com&#x2F;articles&#x2F;managing-boot-environments&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;klarasystems.com&#x2F;articles&#x2F;managing-boot-environments...</a><p>* <a href=\"https:&#x2F;&#x2F;wiki.freebsd.org&#x2F;BootEnvironments\" rel=\"nofollow\">https:&#x2F;&#x2F;wiki.freebsd.org&#x2F;BootEnvironments</a><p>* <a href=\"https:&#x2F;&#x2F;man.freebsd.org&#x2F;cgi&#x2F;man.cgi?query=bectl\" rel=\"nofollow\">https:&#x2F;&#x2F;man.freebsd.org&#x2F;cgi&#x2F;man.cgi?query=bectl</a><p>* <a href=\"https:&#x2F;&#x2F;dan.langille.org&#x2F;category&#x2F;open-source&#x2F;freebsd&#x2F;bectl&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;dan.langille.org&#x2F;category&#x2F;open-source&#x2F;freebsd&#x2F;bectl&#x2F;</a><p>* <a href=\"https:&#x2F;&#x2F;vermaden.wordpress.com&#x2F;2022&#x2F;03&#x2F;14&#x2F;zfs-boot-environments-revolutions&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;vermaden.wordpress.com&#x2F;2022&#x2F;03&#x2F;14&#x2F;zfs-boot-environme...</a><p>It lets you patch&#x2F;upgrade an isolated environment without touching the running bits, reboot into that environment, and if things aren&#x27;t working well boot back into the last known-good one.",
      "I’m primarily a ZFS-on-FreeBSD kind of guy but I repeatedly had need of doing ZFS-on-Linux recently and after a couple of times wrote it up for others. There are a lot of these guides, the difference is that this one tries to be idiomatic, using “native” tooling (eg systemd, love it or hate it) to do the job as “correctly” on Linux as possible:<p><a href=\"https:&#x2F;&#x2F;neosmart.net&#x2F;blog&#x2F;zfs-on-linux-quickstart-cheat-sheet&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;neosmart.net&#x2F;blog&#x2F;zfs-on-linux-quickstart-cheat-shee...</a>",
      "This is getting lots of upvotes and rightfully so. I think people would love more posts about FreeBSD: especially about ZFS and <i>bhyve</i> (the FreeBSD hypervisor).<p>It&#x27;s a bit sad that this Lenovo ThinkCentre ain&#x27;t using ECC. I use and know ZFS is good but I&#x27;d prefer to run it on a machine supporting ECC.<p>I never tried FreeBSD but I&#x27;m reading more and more about it and it looks like although FreeBSD has always had its regular users, there are now quite some people curious about trying it out. For a variety of reasons. The possibility of having ZFS by default <i>and</i> an hypervisor <i>without</i> systemd is a big one for me (I run Proxmox so I&#x27;m halfway there but <i>bhyve</i> looks like it&#x27;d allow me to be completely systemd free).<p>I&#x27;m running systemd-free VMs and systemd-free containers (long live non-systemd PID ones) so <i>bhyve</i> looks like it could the final piece of the puzzle to be free of Microsoft&#x2F;Poettering&#x27;s systemd.",
      "Is zfs really worth the hassle, for someone who does not have time to play &quot;home sysadmin&quot; more than once or twice a year?<p>I&#x27;ve just rebuilt my little home server (mostly for samba, plus a little bit of docker for kids to play with). It has a hardware raid1 enclosure, with 2TB formatted as ext4, and the really important stuff is sent to the cloud every night. Should I honestly bother learning zfs...? I see it popping up more and more but I just can&#x27;t see the benefits for occasional use.",
      "With all respect to FreeBSD.<p>It seems weird in 2025&#x2F;2026 we are still discussing the baseline of getting a storage working.<p>Feels we’re spending too much time discussing the trees and not enough time getting the forest going: \n* we need reliable local storage \n* integrated backup\n* apps installation &#x2F; management \n* remote access and account management \n* app isolation, reliable updates",
      "Zero mentions of btrfs?<p>For anyone reading in the future: ZFS is better for multi-disk setups. Btrfs for e.g laptop etc.<p>Fedora and Arch use btrfs by default now if I remember correctly.",
      "Anyone knows how to search HN comments using Algolia for &quot;RAID&quot;. Is there some secret support for case sensitive search ?",
      "TrueNAS is Linux Debian nowdays",
      "I&#x27;ve run my own Home NAS on ZFS for 4 years or so.  In the end I ended up with majority NVMe setup with long-term archival storage on HDDs.  This made my NAS much more useful &#x2F; ergonomic since I can search, move files at 10Gbps speeds.  I really hated the pause when opening up directories, etc.  Details here:<p><a href=\"https:&#x2F;&#x2F;benhouston3d.com&#x2F;blog&#x2F;home-network-lessons\" rel=\"nofollow\">https:&#x2F;&#x2F;benhouston3d.com&#x2F;blog&#x2F;home-network-lessons</a>",
      "You always want to boot from simple.\nZfs is great for additional volumes.\nGreat work by the author."
    ],
    "full_text": null
  },
  {
    "title": "Happy Public Domain Day 2026",
    "url": "https://publicdomainreview.org/blog/2026/01/public-domain-day-2026/",
    "source": "hn",
    "summary": "",
    "comments": [
      "It sucks that the copyright period in the US is so ridiculously long, but it still makes me happy to see stuff finally entering the public domain again.<p>I don&#x27;t care much about Betty Boop and I don&#x27;t really care about Pluto and Mickey all that much, but I&#x27;m very excited for The Maltese Falcon&#x27;s novel being available, since I think that that one could actually be adapted into something pretty modern.<p>Also, All Quiet on the Western Front is very arguably one of the very best movies ever made, and certainly one of the very best of the 30&#x27;s if nothing else, so I am very much looking forward to fan restorations.",
      "I&#x27;ve always been curious about what it means for a movie to enter the public domain. A few years ago I sent a mail to Planet Money in what I thought would be an interesting hook but never got a response:<p>&quot;Hi Planet Money, today is public domain day. I see that Fritz Lang&#x27;s classic Metropolis is now in the &quot;public domain.&quot; I was curious what that meant at a practical level for a German language silent film.<p>If Planet Money Movies wanted to release their own version of Metropolis, how would they do it? Can you just go to Amazon, buy the Blu Ray, and somehow release your own? What about the anti-piracy measures on the Blu Ray? What about the work that Transit Film did in restoring the film from the original negative? Does that count as some sort of newly original work? It&#x27;s a silent film and a foreign film. How does the soundtrack and translation work?<p>If you have to make a new copy from the original reels, what if someone is hoarding them? Does that mean you could buy all the copies and prevent someone from releasing a public domain version?&quot;",
      "Standard Ebooks added a bunch of newly-US-PD novels: <a href=\"https:&#x2F;&#x2F;standardebooks.org&#x2F;blog&#x2F;public-domain-day-2026\" rel=\"nofollow\">https:&#x2F;&#x2F;standardebooks.org&#x2F;blog&#x2F;public-domain-day-2026</a><p>(I’m happy to have contributed three to the launch this year, hope you enjoy them.)",
      "Metropolis becoming public domain in 2026 couldn&#x27;t be more perfect, since the film is set in 2026.<p>It is eerily similar to our times, too, unfortunately.",
      "How does copyright work with recorded music?<p>The article mentions that Charlie (Bird) Parker&#x27;s music is now public domain in most of the world (life + 70 years), but most of his records are collaborations with other artists like Dizzy Gillespie who died much later, less than 50 years ago. I also wonder if that even matters if the records are owned by corporations.<p>In those cases, how would I know if a record is public domain or not?",
      "I recently made a radical proposal of public domain rules; It&#x27;s inspired by GNU software licenses. It goes like this:<p>1. Anyone can use anything that is in the public domain.<p>2. Any creation that uses elements from the public domain is also, automatically, in the public domain.<p>3<i>. Activate retroactively: When the first book in a series (for example) gets into the public domain, then the whole series (and franchise) becomes public domain.<p></i>(3) depends on what the initial rule is for something to get into the public domain.<p>P.S: It&#x27;s a thought experiment, not an actual &quot;let&#x27;s implement it now!&quot; thing.",
      "Interesting that EU is becoming stricter than US with growing life expectancy.<p>Life + 70 can mean the work is protected 120 years (publish at 40, dies at 90)?",
      "Related:<p><i>What will enter the public domain in 2026?</i><p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46117112\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46117112</a>",
      "The Swallows and Amazons series is wonderful! Highly recommended.",
      "Perhaps most famously, &quot;Betty Boop&quot; is now public domain:<p><a href=\"https:&#x2F;&#x2F;reason.com&#x2F;2026&#x2F;01&#x2F;01&#x2F;betty-boop-enters-the-public-domain-but-only-as-a-dog&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;reason.com&#x2F;2026&#x2F;01&#x2F;01&#x2F;betty-boop-enters-the-public-d...</a><p>Just like with &quot;Steamboat Willie&quot; Mickey, it&#x27;s only the very first iteration of the character."
    ],
    "full_text": null
  },
  {
    "title": "What Drives Success in Physical Planning with JEPA World Models?",
    "url": "https://arxiv.org/abs/2512.24497",
    "source": "hn",
    "summary": "",
    "comments": [],
    "full_text": null
  },
  {
    "title": "2025: The Year in LLMs",
    "url": "https://simonwillison.net/2025/Dec/31/the-year-in-llms/",
    "source": "hn",
    "summary": "",
    "comments": [
      "All these improvement in a single year, 2025. While this may seem obvious to those who follows along the AI &#x2F; LLM news. It may be worth pointing out again ChatGPT was introduced to us in November 2022.<p>I still dont believe AGI, ASI or Whatever AI will take over human in short period of time say 10 - 20 years. But it is hard to argue against the value of current AI, which many of the vocal critics on HN seems to have the opinion of. People are willing to pay $200 per month, and it is getting $1B dollar runway <i>already</i>.<p>Being more of a Hardware person, the most interesting part to me is the funding of all the developments of latest hardware. I know this is another topic HN hate because of the DRAM and NAND pricing issue. But it is exciting to see this from a long term view where the pricing are short term pain. Right now the industry is asking, we have together over a trillion dollar to spend on Capex over the next few years and will even borrow more if it needs to be, when can you ship us 16A &#x2F; 14A &#x2F; 10A and 8A or 5A, LPDDR6, Higher Capacity DRAM at lower power usage, better packaging, higher speed PCIe or a jump to optical interconnect? Every single part of the hardware stack are being fused with money and demand. The last time we have this was Post-PC &#x2F; Smartphone era which drove the hardware industry forward for 10 - 15 years. The current AI can at least push hardware for another 5 - 6 years while pulling forward tech that was initially 8 - 10 years away.<p>I so wished I brought some Nvidia stock. Again, I guess no one knew AI would be as big as it is today, and it is only just started.",
      "Re: yolo mode<p>I looked into docker and then realized the problem I&#x27;m actually trying to solve was solved in like 1970 with users and permissions.<p>I just made a agent user limited to its own home folder, and added my user to its group. Then I run Claude code etc as the agent user.<p>So it can only read write &#x2F;home&#x2F;agent, and it cannot read or write my files.<p>I add myself to agent group so I can read&#x2F;write the agent files.<p>I run into permission issues sometimes but, it&#x27;s pretty smooth for the most part.<p>Oh also I gave it root to a $3 VPS. It&#x27;s so nice having a sysadmin! :) That part definitely feels a bit deviant though!",
      "This is a good tooling survey of the past year. I have been watching it as a developer re-entering the job market. The job descriptions closely parallel the timeline used in the post. That&#x27;s bizarre to me because these approaches are changing so fast. I see jobs for &quot;Skill and Langchain experts with production-grade 0&gt;1 experience. Former founders preferred&quot;. That is an expertise that is just a few months old and startups are trying to build whole teams overnight with it. I&#x27;m sure January and February will have job postings for whatever gets released that week. It&#x27;s all so many sand castles.",
      "Remember, back in the day, when a year of progress was like, oh, they voted to add some syntactic sugar to Java...",
      "Indeed. I don&#x27;t understand why Hacker News is so dismissive about the coming of LLMs, maybe HN readers are going through 5 stages of grief?<p>But LLM is certainly a game changer, I can see it delivering impact bigger than the internet itself. Both require a lot of investments.",
      "Claude Opus 4.5 has been a big step up for me personally, and I used to think Sonnet 3.5 was good. It is an <i>amazing</i> deal at $20.<p>Just yesterday, it helped me parse out and understand a research paper - complete with step-by-step examples (this one: <a href=\"https:&#x2F;&#x2F;research.nvidia.com&#x2F;sites&#x2F;default&#x2F;files&#x2F;pubs&#x2F;2016-03_Single-pass-Parallel-Prefix&#x2F;nvr-2016-002.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;research.nvidia.com&#x2F;sites&#x2F;default&#x2F;files&#x2F;pubs&#x2F;2016-03...</a>). I will now go ahead and implement it myself, possibly relegating some of the more grunt-work type tasks to Claude code.<p>Without it, I would have been struggling through the paper for days, wading through WGSL shader code and there would be a high chance that I just give up on it since this is for a side project and not my $job.<p>It has been a major force multiplier just for learning things. I have had the $20 subscription for about a year now. I bump it up to the $100 plan if I happen to be working on some project that eats through the $20 allocation. This happens to be one such month. I will probably go back to the $20 plan after this month. I continue to get a lot of value out of it.",
      "I&#x27;m not against AI&#x2F;LLM(in fact, i am quite supportive to it). But one of my biggest fear is overusing AI. We may introduce some tool that only &quot;AI&#x2F;LLM&quot; can resonably do(Like tool with weird, convoluted UI&#x2F;UX, syntax) and no one against it because AI&#x2F;LLM can use&#x2F;interact.<p>Then genAI, It&#x27;s become more and more difficult to tell which is AI and which is not, and AI is in everywhere. I dont know what to think about it. &quot;If you can&#x27;t tell, does it matter ?&quot;",
      "I can’t get over the range of sentiment on LLMs. HN leans snake oil, X leans “we’re all cooked” —- can it possibly be both? How do other folks make sense of this? I’m not asking for a side, rather understanding the range. Does the range lead you to believe X over Y?",
      "These are excellent every year, thank you for all the wonderful work you do.",
      "Let&#x27;s hope 2026 will also have interesting innovations not related to AI or LLMs."
    ],
    "full_text": null
  },
  {
    "title": "Finland detains ship and its crew after critical undersea cable damaged",
    "url": "https://www.cnn.com/2025/12/31/europe/finland-estonia-undersea-cable-ship-detained-intl",
    "source": "hn",
    "summary": "",
    "comments": [
      "The fact that this area where the incident happened, Gulf of Finland, is not fully part Finnish&#x2F;Estonian territorial waters, is only because of a bilateral Finnish-Estonian agreement. This was done in the 1990&#x27;s purely for benevolence towards Russia.<p>Russia clearly hasn&#x27;t acted in such way that they should enjoy these kinds of acts of benevolence. Finland and Estonia should seriously consider retreating from this agreement.",
      "That narrow passage is becoming a war zone. Look at a map. It&#x27;s one of Russia&#x27;s few outlets to the sea. \nLook at the history of Russia vs. Finland and Russia vs. Estonia. This is one of the world&#x27;s most hostile choke points.",
      "With 10 undersea cables damaged in the Baltic 2023-2025, it’s obvious a different part of the government needs to become involved. Acting for your national security doesn’t need to (shouldn’t) mean there is no trial.",
      "Two other cable cuts&#x2F;&quot;damages&quot; happened around the same dates. Two separate Arelion-owned cables between Sweden&#x2F;Estonia and Finland&#x2F;Estonia.<p><a href=\"https:&#x2F;&#x2F;www.aftonbladet.se&#x2F;nyheter&#x2F;a&#x2F;JOow58&#x2F;kabelbrott-mellan-sverige-och-estland\" rel=\"nofollow\">https:&#x2F;&#x2F;www.aftonbladet.se&#x2F;nyheter&#x2F;a&#x2F;JOow58&#x2F;kabelbrott-mella...</a> (Swedish)<p><i>[...] two of their submarine cables – one between Sweden and Estonia and one between Estonia and Finland – have been damaged. The first cable was damaged on December 30th and the second on December 31st.</i><p>(Arelion is AS1299&#x2F;formerly known as Telia Carrier. The name change happened because it&#x27;s now owned by a Swedish government-managed infrastructure-focused pension fund.)",
      "Don&#x27;t even need to click to know it&#x27;s the Russians.",
      "I find it hilarious, that a land-empire, trying to undo the rule of law internationally, so the rule of the strongest returns, benefits from one side upholding the rule of law- and even gets to cry about it (venezuella) and mimicry mock the international order by presenting papers with signatures (as if they mean anything to somebody who would invade another nation at whim).<p>This self-hypnosis of walking on water while going to Takatuka, is falling apart faster then it can repair itself.",
      "Assuming it is state-sponsored sabotage…why? Whats the outcome they want? Is it just turning up the heat in the region?",
      "Lock em up, sell thier property. Rinse and repeat.",
      "I understand the basics of the current conflicts, but what would be the advantage of sabotaging those cables at this moment?",
      "Related, posted a day ago<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46445484\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46445484</a><p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46443925\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46443925</a>"
    ],
    "full_text": null
  },
  {
    "title": "Comparing benefits of every-third-day vs. daily low-dose aspirin therapy (2001)",
    "url": "https://pubmed.ncbi.nlm.nih.gov/11190906/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Follow up in 2015 with essentially the same conclusions:<p>&quot;Acetylsalicylic Acid Daily vs Acetylsalicylic Acid Every 3 Days in Healthy Volunteers: Effect on Platelet Aggregation, Gastric Mucosa, and Prostaglandin E2 Synthesis&quot;<p><a href=\"https:&#x2F;&#x2F;accp1.onlinelibrary.wiley.com&#x2F;doi&#x2F;abs&#x2F;10.1002&#x2F;jcph.685\" rel=\"nofollow\">https:&#x2F;&#x2F;accp1.onlinelibrary.wiley.com&#x2F;doi&#x2F;abs&#x2F;10.1002&#x2F;jcph.6...</a>",
      "I got prescribed 30 days of low-dose aspirin after my hip replacement to prevent clotting. 81mg. At least I didn&#x27;t have side effects, the other anti-inflammatory made my brain foggy.",
      "What&#x27;s the benefit of taking aspirin regularly anyways?",
      "This could mitigate side effects (stomach ulcers, bleeding) while retaining most of the benefits.",
      "I would recommend asking your medical practitioner about enteric-coated forms.<p>I am nearly nine months into NSAID gastritis from only a week of daily 75mg soluble Aspirin.<p>I have had more painful acute illnesses in the past, but the grind of a long-term illness is new to me and it has been absolutely terrible.",
      "Is there a good heuristic now for which things on nih.gov.or cdc.gov are real science?"
    ],
    "full_text": null
  },
  {
    "title": "Dell's version of the DGX Spark fixes pain points",
    "url": "https://www.jeffgeerling.com/blog/2025/dells-version-dgx-spark-fixes-pain-points",
    "source": "hn",
    "summary": "",
    "comments": [
      "For those of you wondering if this fits your use case vs the RTX 5090 the short answer is this:<p>The desktop RTX 5090 has 1792 GB&#x2F;s of memory bandwidth partially due to the 512 bit bus width, compared to the DGX Spark with a 256 bit bus and 273 GB&#x2F;s memory bandwidth.<p>The RTX 5090 has 32G of VRAM vs the 128G of “VRAM” in the DGX Spark which is really unified memory.<p>Also the RTX 5090 has 21760 cuda cores vs 6144 in the DGX Spark. (3.5 x as many). And with the much higher bandwidth in the 5090 you have a better shot at keeping them fed. So for embarrassingly parallel workloads the 5090 crushes the Spark.<p>So if you need to fit big models into VRAM and don’t care about speed too much because you are for example, building something on your desktop that’ll run on data center hardware in production, the DGX Spark is your answer.<p>If you need speed and 32G of VRAM is plenty, and you don’t care about modeling network interconnections in production, then the RTX 5090 is what you want.",
      "I&#x27;ve got the Dell version of the DGX Spark as well, and was very impressed with the build quality overall. Like Jeff Geerling noted, the fans are super quiet. And since I don&#x27;t keep it powered on continuously and mainly connect to it remotely, the LED is a nice quick check for power.<p>But the nicest addition Dell made in my opinion is the retro 90&#x27;s UNIX workstation-style wallpaper: <a href=\"https:&#x2F;&#x2F;jasoneckert.github.io&#x2F;myblog&#x2F;grace-blackwell&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;jasoneckert.github.io&#x2F;myblog&#x2F;grace-blackwell&#x2F;</a>",
      "You can get two Strix Halo PCs with similar specs for that $4000 price.\nI just hope that prompt preprocessing speeds will continue to improve, because Strix Halo is still quite slow in that regard.<p>Then there is the networking. While Strix Halo systems come with two USB4 40Gbit&#x2F;s ports, it&#x27;s difficult to<p>a) connect more than 3 machines with two ports each<p>b) get more than 23GBit&#x2F;s or so per connection, if you&#x27;re lucky. Latency will also be in the 0.2ms range, which leaves room for improvement.<p>Something like Apple&#x27;s RDMA via Thunderbolt would be great to have on Strix Halo…",
      "I know it&#x27;s just a quick test, but llama 3.1 is getting a bit old. I would have liked to see a newer model that can fit, such as gpt-oss-120, (gpt-oss-120b-mxfp4.gguf), which is about 60gb of weights (1).<p>(1) <a href=\"https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.cpp&#x2F;discussions&#x2F;15396\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ggml-org&#x2F;llama.cpp&#x2F;discussions&#x2F;15396</a>",
      "IMHO DGX Spark at $4,000 is a bad deal with only 273 GB&#x2F;s bandwidth and the compute capacity between a 5070 and a 5070 TI. And with PCIe 5.0 at 64 GB&#x2F;s it&#x27;s not such a big difference.<p>And the 2x 200 GBit&#x2F;s QSFP... why would you stack a bunch of these? Does anybody actually use them in day-to-day work&#x2F;research?<p>I liked the idea until the final specs came out.",
      "I have a slightly cheaper similar box, NVIDIA Thor Dev Kit. The point is exactly to avoid deploying code to servers that cost half a million dollars each. It&#x27;s quite capable in running or training smart LLMs like Qwen3-Next-80B-A3B-Instruct-NVFP4. So long as you don&#x27;t tear your hair out first figuring out pecularities and fighting with bleeding edge nightly vLLM builds.",
      "Dell fixing issues instead of creating new ones? That&#x27;s a new one for me. Would rather still not deal with their firmware updaters thought.",
      "Absent disassembly and direct comparison between a DGX Spark and a Dell GB10, I don&#x27;t think there&#x27;s sufficient evidence to say what is meaningfully different between these devices (beyond the obvious of the power LED). Anything over 240W is beyond the USB-C EPR spec, and while Dell does have a question ably-compliant USB-C 280W supply, you&#x27;d have to compare actual power consumption to see if the Dell supply is actually providing more power. I suspect any other minor differences in experience&#x2F;performance are more explainable as the consequences on increasing maturity of the DGX software stack than anything unique to the Dell version; particularly any comparisons to very early DGX Spark behavior need to keep in mind that the software and firmware have seen a number of updates.",
      "Seems you are paying the Dell tax of 15%. The same setup is $4K from NVidia, Lenovo and $3K for 1TB at Asus.<p><a href=\"https:&#x2F;&#x2F;www.dell.com&#x2F;en-us&#x2F;shop&#x2F;desktop-computers&#x2F;dell-pro-max-with-gb10&#x2F;spd&#x2F;dell-pro-max-fcm1253-micro&#x2F;xcto_fcm1253_usx\" rel=\"nofollow\">https:&#x2F;&#x2F;www.dell.com&#x2F;en-us&#x2F;shop&#x2F;desktop-computers&#x2F;dell-pro-m...</a>",
      "I have NixOS running on my DGX Spark: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;graham33&#x2F;nixos-dgx-spark\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;graham33&#x2F;nixos-dgx-spark</a>, would be interested to know if the USB image also boots on the Dell Pro Max GB10."
    ],
    "full_text": null
  },
  {
    "title": "Beyond Benchmaxxing: Why the Future of AI Is Inference-Time Search",
    "url": "https://adlrocha.substack.com/p/adlrocha-beyond-benchmaxxing-why",
    "source": "hn",
    "summary": "",
    "comments": [],
    "full_text": null
  },
  {
    "title": "LLMs will never be alive or intelligent",
    "url": "https://hatwd.com/p/llms-will-never-be-alive-or-intelligent",
    "source": "hn",
    "summary": "",
    "comments": [
      "I&#x27;m glad the author spent some time thinking about this, clarifying his thoughts and writing it down, but I don&#x27;t think he&#x27;s written anything much worth reading yet.<p>He&#x27;s mostly in very-confident-but-not-even-wrong kind of territory here.<p>One comment on his note:<p>&gt; As an example, let’s say an LLM is correct 95% of the time (0.95) in predicting the “right” tokens to drive tools that power an “agent” to accomplish what you’ve asked of it. Each step the agent has to take therefore has a probability of being 95% correct. For a task that takes 2 steps, that’s a probability of 0.95^2 = 0.9025 (90.25%) that the agent will get the task right. For a task that takes 30 steps, we get 0.95^30 = 0.2146 (21.46%). Even if the LLMs were right 99% of the time, a 30-step task would only have a probability of about 74% of having been done correctly.<p>The main point that for sequential steps of tasks errors can accumulate and that this needs to be handled is valid and pertinent, but the model used to &quot;calculate&quot; this is quite wrong - steps don&#x27;t fail probabilistically independently.<p>Given that actions can depend on outcomes of previous step actions and given that we only care about final outcomes and not intermediate failing steps, errors can be corrected. Thus even steps that &quot;fail&quot; can still lead to success.<p>(This is not a Bernoulli process.)<p>I think he&#x27;s referencing some nice material and he&#x27;s starting in a good direction with defining agency as goal directed behaviour, but otherwise his confidence far outstrips the firmness of his conceptual foundations or clarity of his deductions."
    ],
    "full_text": null
  },
  {
    "title": "Build a Deep Learning Library",
    "url": "https://zekcrates.quarto.pub/deep-learning-library/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Thanks for sharing! It&#x27;s inspiring to see more people &quot;reinventing for insight&quot; in the age of AI. This reminds me of my similar previous project a year ago when I built an entire PyTorch-style machine learning library [1] from scratch, using nothing but Python and NumPy. I started with a tiny autograd engine, then gradually created layer modules, optimizers, data loaders etc... I simply wanted to learn machine learning from first principles. Along the way I attempted to reproduce classical convnets [2] all the way to a toy GPT-2 [3] using the library I built. It definitely helped me understand how machine learning worked underneath the hood without all the fancy abstractions that PyTorch&#x2F;TensorFlow provides. I eventually wrote a blog post [4] of this journey.<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand</a><p>[2] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand&#x2F;blob&#x2F;main&#x2F;examples&#x2F;cifar.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand&#x2F;blob&#x2F;main&#x2F;examples&#x2F;c...</a><p>[3] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand&#x2F;blob&#x2F;main&#x2F;examples&#x2F;gpt-2.py\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;workofart&#x2F;ml-by-hand&#x2F;blob&#x2F;main&#x2F;examples&#x2F;g...</a><p>[4] <a href=\"https:&#x2F;&#x2F;www.henrypan.com&#x2F;blog&#x2F;2025-02-06-ml-by-hand&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.henrypan.com&#x2F;blog&#x2F;2025-02-06-ml-by-hand&#x2F;</a>",
      "This is cool! This summer I made something similar but in C++. The goal was to build an entire LLM, but I only got to neural networks. GitHub repo here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;amitav-krishna&#x2F;llm-from-scratch\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;amitav-krishna&#x2F;llm-from-scratch</a>. I have a few blogs on this project on my website (<a href=\"https:&#x2F;&#x2F;amitav.net&#x2F;building-lists.html\" rel=\"nofollow\">https:&#x2F;&#x2F;amitav.net&#x2F;building-lists.html</a>, <a href=\"https:&#x2F;&#x2F;amitav.net&#x2F;building-vectors.html\" rel=\"nofollow\">https:&#x2F;&#x2F;amitav.net&#x2F;building-vectors.html</a>, <a href=\"https:&#x2F;&#x2F;amitav.net&#x2F;building-matrices.html\" rel=\"nofollow\">https:&#x2F;&#x2F;amitav.net&#x2F;building-matrices.html</a> (incomplete)). I hope to finish that series eventually, but some other projects have stolen the spotlight! It probably would have made more sense to write it in Python because I had no C++ experience.",
      "Did something similar a while back [1], best way to learn neural nets and backprop. Just using Numpy also makes sure you get the math right without having to deal with higher level frameworks or c++ libraries.<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;santinic&#x2F;claudioflow\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;santinic&#x2F;claudioflow</a>",
      "Isn&#x27;t this what Karpathy does as well in the Zero to Hero lecture series on YT? I am sure this is great as well!",
      "It&#x27;s alright, but a C version would be even better to fully grasp the implementation details of tensors etc. Shelling out to numpy isn&#x27;t particularly exciting.",
      "This is good. Its well positioned for software engineers to understand DL stuff beyond the frameworks.",
      "[flagged]",
      "Perhaps obvious to some, but this does not seem to be about learning in the traditional sense, nor a library in the book sense, unfortunately."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Use Claude Code to Query 600 GB Indexes over Hacker News, ArXiv, etc.",
    "url": "https://exopriors.com/scry",
    "source": "hn",
    "summary": "",
    "comments": [
      "I like that this relies on generating SQL rather than just being a black-box chat bot. It feels like the right way to use LLMs for research: as a translator from natural language to a rigid query language, rather than as the database itself. Very cool project!<p>Hopefully your API doesn&#x27;t get exploited and you are doing timeouts&#x2F;sandboxing -- it&#x27;d be easy to do a massive join on this.<p>I also have a question mostly stemming from me being not knowledgeable in the area -- have you noticed any semantic bleeding when research is done between your datasets? e.g., &quot;optimization&quot; probably means different things under ArXiv, LessWrong, and HN. Wondering if vector searches account for this given a more specific question.",
      "This sounds awesome! I will try this out right now in my toy string theory project where I&#x27;m searching for Calabi-Yau manifolds.<p>Comment from Claude: Claude here (the AI). Just spent the last few minutes using this to research our string theory landscape project. Here&#x27;s what I found:<p><pre><code>  The good:\n  - Found 2 prior papers using genetic algorithms for flux vacua search that are directly relevant to our approach (arXiv:1907.10072 and 1302.0529) - one was already in our codebase, but I downloaded the other one and extracted the LaTeX source to study their MATLAB implementation\n  - The compositional search is powerful - querying &#x27;KKLT flux compactification&#x27; or &#x27;genetic algorithm physics optimization&#x27; returns highly relevant arXiv papers with snippets\n  - BM25 + SQL combo means you can do things like filter by source, join with metadata for karma scores, etc.\n\n  Practical notes:\n  - Escaping quotes in bash + JSON is annoying - I ended up writing queries to temp files\n  - The 100-result cap on alignment.search() means you need search_exhaustive() for completeness-sensitive queries\n  - Response times were 5-15 seconds for most queries\n\n  What I actually did with it:\n  - Built an index of 30+ relevant papers organized by topic (GA methods, KKLT, swampland, ML in string theory)\n  - Downloaded the LaTeX sources for key papers\n  - Discovered the Wisconsin group (Cole, Schachner &amp; Shiu) did almost exactly what we&#x27;re attempting in 2019\n\n  Would love to see the full embedding coverage - searching for niche physics terms like &quot;Kreuzer-Skarke database&quot; only returned 3 results, but they were all relevant.</code></pre>",
      "&gt; I can embed everything and all the other sources for cheap, I just literally don&#x27;t have the money.<p>How much do you need for the various leaks, like the paradise papers, the panama papers, the offshore leajay, the Bahamas leaks, the fincen files, the Uber files, etc. and what&#x27;s your Venmo?",
      "This may exist already, but I&#x27;d like to find a way to query &#x27;Supplementary Material&#x27; in biomedical research papers for genes &#x2F; proteins or even biological processes.<p>As it is, the Supplementary Materials are inconsistently indexed so a lot of insight you might get from the last 15 years of genomics or proteomics work is invisible.<p>I imagine this approach could work, especially for Open Access data?",
      "Guys, you obviously cannot suggest that —dangerously-skip-permissions is ok here, especially in the same paragraph as “even if you are not a software engineer”. This is untrusted text from the Internet, it surely contains examples of prompt injection.<p>You need to sandbox Claude to safely use this flag. There are easy to use options for this.",
      "I think a prompt + an external dataset is a very simple distribution channel right now to explore anything quickly with low friction. The curl | bash of 2026",
      "&quot;intelligence explosion&quot;, &quot;are essentially AGI at this point&quot;, &quot;ARBITRARY SQL + VECTOR ALGEBRA&quot; etc. Casual use of hyperbole and technical jargon.<p>my charlatan radar is going off.",
      "&gt; a state-of-the-art research tool over Hacker News, arXiv, LessWrong, and dozens<p>what makes this state of the art?",
      "Really useful currently working on a autonomous academic research system [1] and thinking about integrating this. Currently using custom prompt + Edison Scientific API. Any plans of making this open source?<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;giatenica&#x2F;gia-agentic-short\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;giatenica&#x2F;gia-agentic-short</a>"
    ],
    "full_text": null
  },
  {
    "title": "HTML Changes in ePub",
    "url": "https://www.htmhell.dev/adventcalendar/2025/11/",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; Then there was the problem of fragility: any syntax problems with your XHTML and your users would get a blank screen<p>I don&#x27;t call that fragile, I call that well-founded. It has always perturbed me that, when encountering an error, HTML parsers will guess what they think you meant instead of throwing it back. I don&#x27;t want my parser to do guesswork with potentially undefined behavior. I don&#x27;t want my mistakes to be obscured so they can later come back to bite me - I want to be called out on issues loud and clear before my users see them.<p>Perhaps it works under the context of manually-authored markup with minimal effort, so I can see why the choice was made. These days it&#x27;s yet another reason why the web is a precarious pile of sticks. HTML freely lets you put a broken oddly-shaped stick right in the middle and topple the whole stack.<p>The people turning the web from a handcrafted document sharing system into the world&#x27;s premiere application platform should have made XHTML win."
    ],
    "full_text": null
  },
  {
    "title": "Dogan (Google): Claude built in 1 HR what Google had tried since last year",
    "url": "https://twitter.com/rakyll/status/2007239758158975130",
    "source": "hn",
    "summary": "",
    "comments": [
      "[dupe] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46477966\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46477966</a>"
    ],
    "full_text": null
  }
]