[
  {
    "title": "Heterogeneous Low-Bandwidth Pre-Training of LLMs",
    "url": "https://arxiv.org/abs/2601.02360v1",
    "source": "arxiv",
    "summary": "Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchang",
    "full_text": null
  },
  {
    "title": "DARC: Drum accompaniment generation with fine-grained rhythm control",
    "url": "https://arxiv.org/abs/2601.02357v1",
    "source": "arxiv",
    "summary": "In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Accompaniment Generation\n2.2 Tap2Drum Generation\n\n\n\n3 Method\n\n3.1 Overview\n3.2 Rhythm Feature Representation\n3.3 Fine-Tuning\n\n\n\n4 Experimental Setup\n\n4.1 Audio Quality\n4.2 Rhythm Prompt Adherence\n4.3 Musical Coherence\n\n\n\n5 Results and Discussion\n\n5.1 Rhythm Prompt Adherence\n5.2 Musical Coherence\n\n\n6 Conclusion\n7 Broader Impacts\n\n\n\n\n\nDARC: Drum accompaniment generation with fine-grained rhythm control\n\nAbstract\nIn music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE [stage], a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.\n\n\n\n1 Introduction\n\nIn recent years, numerous works [musiccongen, stemgen, stage, coco-mulla, jukedrummer, musicgenstem, singsong] have achieved high-quality, musically coherent accompaniment generation. However, these methods often lack fine-grained control over time-varying features. Such control is often desirable in the context of musical prototyping, where a creator wishes to quickly evaluate an early musical idea before investing substantial time into it. In this work, we focus on the Tap2Drum task, in which a user can record a rhythm prompt, such as a beatboxing or tapping track, and a generative model renders it as drums. State-of-the-art approaches for Tap2Drum focus on timbre transfer, where the user provides a timbre prompt to explicitly specify the desired drum timbre. For instance, [tria] requires the user to provide drum audio as the timbre prompt; this can limit the speed of iteration, as different songs will require different drumkit sounds, and the user must search for an existing audio sample matching their desired timbre. Other works in music editing [melodyFlow] provide text control, but it can be difficult to articulate drum timbres using text, and moreover these methods tend to suffer from timbre leakage [tria]. Some works, both in Tap2Drum [groovae, clavenet] and in accompaniment generation [MusicControlNet, stage], offer onset-based rhythm control, but this is too coarse to capture the implied timbre categories of a rhythm prompt.\n\n\nWe propose DARC, a drum accompaniment generation model that takes as input musical context and a rhythm prompt. Our rhythm feature representation, based on nonnegative matrix factorization (NMF), provides greater granularity than onset-based methods by classifying each onset into a timbre class. DARC is a fine-tuning of STAGE [stage], a SOTA drum accompaniment model. Our motivation for inferring timbre from musical context rather than a timbre prompt is twofold: first, drums are rarely a solo instrument, i.e. the end goal for a drum track is often to accompany a mix; second, removing the requirement for users to provide a timbre prompt can shorten their iteration cycle, enabling them to explore more ideas. For our dataset, we extract drum stems from the FMA dataset [fma] using Demucs [demucs1, demucs2]. During fine-tuning, we utilize the parameter-efficient method proposed in [musiccongen].\n\n\nOur contributions are 2-fold:\n\n\n‚Ä¢\n\nWe introduce a generative drum model that can condition on both musical context and specific rhythms, with timbre classes\n\n\n\n‚Ä¢\n\nWe evaluate our model on musical coherence with the input mix and onset and timbre class adherence to the rhythm prompt, exposing limitations in existing evaluation metrics\n\n\n\n\n\nFigure 1: Architecture of the proposed rhythm-conditioned music generation model. Musical context and rhythm prompt are provided as audio inputs. The tokenized musical context is prepended to the input sequence, and the rhythm prompt is transcribed into (onset time, timbre class) pairs using non-negative matrix factorization (NMF). The rhythm embedding is passed through the self-attention layers via jump fine-tuning and adaptive in-attention [musiccongen]. The model outputs EnCodec audio tokens that are decoded to the final waveform.\n\n\n\n\n2 Related Work\n\n\n2.1 Accompaniment Generation\n\nA recent line of work has explored music accompaniment generation [subtractiveTraining, musicgenstem, stemgen, stage, musiccongen, singsong, jukedrummer], which can generate one or more tracks to accompany given musical mix. Note that many of these models support text conditioning, and are in fact fine-tunings of the text-to-music model MusicGen [musicGen]. While these stem-to-stem generation models can condition on other stems in the mix, they are not designed for fine-grained rhythm control. Some approaches allow for conditioning on onsets [MusicControlNet, stage]. However, the rhythm control provided by these approaches is quite loose; the model does not preserve the onsets, but rather uses them as a guide to generate an embellished drum track. In addition, onset timings alone do not capture implied timbre classes, such as an onset being from a kick drum versus a snare. Our work seeks to provide tighter rhythm control and can preserve timbre classes.\n\n\nOther work has focused on more specialized aspects of drum generation. For example, [realtimeDrums] generates drum accompaniments in real time, and [drumFills] uses a bidirectional language model to generate drum fills. We leave the adaptation of our methods for real-time or fill generation as future work.\n\n\n\n\n2.2 Tap2Drum Generation\n\nAn alternative line of work explores the Tap2Drum task, which takes tapping or beatboxing as input and seeks to generate a drum track with the same rhythm. Tap2Drum was first introduced in [groovae], which takes onset times as input and generates drums as MIDI111This was released as an Ableton Live plugin: https://magenta.withgoogle.com/studio. Other work such as TRIA [tria] performs timbre transfer, directly converting the rhythm prompt audio to high-fidelity drum audio. In addition to a rhythm prompt, such methods take a timbre prompt in the form of audio, requiring users to present an audio sample with the exact timbre they desire. Further work has explored non-zero-shot timbre transfer [demerle2024combining, demerle2024combiningaudiocontrolstyle, mancusi2025latentdiffusionbridgesunsupervised, engel2020ddsp, santos2023taps], which requires re-training a model for each target timbre. Our model, DARC, generates a suitable timbre for the given input mix, avoiding the need to prompt or train for specific timbres. Moreover, our rhythm features encode timbre classes in addition to onsets times, providing greater granularity than existing timbre transfer approaches.\n\n\n\n\n\n3 Method\n\n\n3.1 Overview\n\nOur model takes two audio-form inputs: a drumless mix as musical context and a rhythm prompt, such as a beatboxing or tapping track. Our goal is to generate a drum stem that faithfully maintains the onsets of each timbre class of the rhythm prompt while exhibiting strong musical coherence with the input mix. We fine-tune STAGE [stage], a recent open-source model that generates single stem accompaniments. STAGE itself is a fine-tuning of MusicGen [musicGen], using prefix-based conditioning on both drumless mixes and metronome-like pulse tracks during training. STAGE contains roughly 620M parameters; following [musiccongen], we use a parameter-efficient fine-tuning technique to reduce the trainable parameter count by an order of magnitude. Note that separate STAGE models were trained for drum and bass stems; we consider only the drum model in this work.\n\n\n\n\n3.2 Rhythm Feature Representation\n\nA key challenge in the Tap2Drum task is timbre leakage: while the generated stem should exhibit close adherence to the rhythm prompt, its timbre should be independent of the rhythm prompt. To address this, we use non-negative matrix factorizaion (NMF) to obtain our rhythm features. NMF decomposes a magnitude spectrogram SS of a rhythm prompt into a product of matrices, S=W‚ÄãH.S=WH. The basis matrix WW encodes timbre information, and the activation matrix HH encodes timing information. In particular, the indices of the rows of WW and the columns of HH correspond to different timbre classes. To obtain our rhythm features, we ignore the matrix W,W, leaving us with a matrix HH of the activation times of each timbre class. Hence, the rhythm-feature representation is MIDI-like: for a beatboxing track, it would contain the onset times and timbre-class indices of each note, but no information about the underlying vocal timbre. Crucially, we sort the timbre classes in decreasing order of total component energy, roughly corresponding to kick, snare, and hi-hat for the first three classes. This way, the model can identify the timbre classes without knowing the timbre information matrix W.W.\n\n\n\n\n3.3 Fine-Tuning\n\nOur base model, STAGE, is a MusicGen-Small model fine-tuned for generating drum stems conditioned on a drumless mix. During training, the authors prepended the input with the audio tokens of the drumless mix, followed by a delimiter token. Therefore, at inference time, the drum stem generation is framed as a continuation task, with the input mix as the prompt. The authors found this prefix-based conditioning method to be superior to cross-attention in their work [stage]. We retain this mechanism for conditioning on the drumless mix, using a different approach to augment STAGE with fine-grained rhythm control.\n\n\nDuring fine-tuning, we freeze approximately 80%80\\% of the parameters of STAGE. First, we freeze the text encoder and audio token embedding modules. Then, we utilize two fine-"
  },
  {
    "title": "Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices",
    "url": "https://arxiv.org/abs/2601.02353v1",
    "source": "arxiv",
    "summary": "Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting t",
    "full_text": null
  },
  {
    "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
    "url": "https://arxiv.org/abs/2601.02346v1",
    "source": "arxiv",
    "summary": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These r",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Cold-start SFT Stage\n\n2.1 Data Filtering &amp; Processing\n2.2 Ablations &amp; Key Findings\n\n2.3 Distributed Training Setup\n\nData-parallel Balance Tokens:\n\n\n\n\n\n3 Reinforcement Learning Stage\n\n\n3.1 Data Preparation\n\nData Curation:\nData Processing:\n\n\n\n3.2 Training Framework\n\nAlgorithm:\nReward Functions Design:\n\n\n\n3.3 Training Ablations\n\nSelecting Optimal Hyper-parameters:\nSelecting task domains:\n\n\n\n3.4 Final Training Setup\n\nTraining Dynamics and Monitoring:\n\n\n\n\n\n4 Standard Reasoning Tasks Evaluation\n\n\n4.1 Benchmarks &amp; Methodology\n\nContamination analysis:\n\n\n\n4.2 Evaluation Results\n\nMathematical Reasoning:\nCode Generation:\nGeneral Reasoning:\n\n\n\n\n\n5 Test-time Scaling\n\n5.1 Setup\n5.2 Evaluation Results\n\n\n6 Conclusion\nA Training Optimizations\nB Inference Analysis\nC RL Data Filtering Diagram\nD Benchmarks Descriptions\n\nE Safety Evaluation\n\nSummary of Safety Evaluation:\nReasoning vs. Final Answers:\nBenchmark Case Studies:\nImplications for Deployment:\n\n\n\n\n\n\n\n\n\\DTMsetdatestyle\niso\n\n\n\nJanuary 5, 2026\n\n¬†\n\n\nFalcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling\n\n\n\nFalcon LLM Team111Correspondence to Falcon-LLM@tii.ae.\nIheb Chaabane ‚ÄÉPuneesh Khanna ‚ÄÉSuhail Mohmad ‚ÄÉSlim Frikha\nShi Hu ‚ÄÉAbdalgader Abubaker ‚ÄÉReda Alami ‚ÄÉMikhail Lubinets\nMohamed El Amine Seddik ‚ÄÉHakim Hacid\n\n\n\n\nhttps://huggingface.co/tiiuae [Falcon-H1R collection]\n\n\n\n\n\nAbstract\nThis work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are 2√ó2\\times to 7√ó7\\times larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.\n\n\n\n\n\nFigure 1: DeepConf@512 (Fu et¬†al., 2025a, ) average results over AIME24, AIME25, AMO-Bench, and GPQA-Diamond (Detailed results in Table 7). Falcon-H1R-7B achieves exceptional performance by pushing the reasoning frontiers in 3 dimensions: higher accuracy, token efficiency and fast inference in the parallel thinking setting.\n\n\n\n\nContents\n\n1 Introduction\n\n2 Cold-start SFT Stage\n\n2.1 Data Filtering &amp; Processing\n2.2 Ablations &amp; Key Findings\n2.3 Distributed Training Setup\n\n\n\n3 Reinforcement Learning Stage\n\n3.1 Data Preparation\n3.2 Training Framework\n3.3 Training Ablations\n3.4 Final Training Setup\n\n\n\n4 Standard Reasoning Tasks Evaluation\n\n4.1 Benchmarks &amp; Methodology\n4.2 Evaluation Results\n\n\n\n5 Test-time Scaling\n\n5.1 Setup\n5.2 Evaluation Results\n\n\n6 Conclusion\nA Training Optimizations\nB Inference Analysis\nC RL Data Filtering Diagram\nD Benchmarks Descriptions\nE Safety Evaluation\n\n\n1.‚ÄÉ‚ÄäIntroduction\n\nLarge language models (LLMs) have rapidly advanced the state of complex reasoning tasks, achieving impressive results by scaling compute in two independent directions:\n\n\n‚Ä¢\n\nScaling Training: Improving model capabilities through comprehensive training (Kaplan et¬†al.,, 2020; Hoffmann et¬†al.,, 2022) that typically involves Supervised Fine-Tuning (SFT) on high-quality instruction data (Guha et¬†al.,, 2025; Yue et¬†al.,, 2025) followed by Reinforcement Learning (RL) (e.g., RLVR) for fine-grained alignment and performance maximization (Guo et¬†al.,, 2025; OpenAI,, 2024; Yu et¬†al.,, 2025).\n\n\n\n‚Ä¢\n\nScaling Inference: Proposing parallel thinking methods that generate and aggregate multiple solution chains, such as through self-consistency or majority voting (Fu et¬†al., 2025b, ; Lightman et¬†al., 2023a, ; Stiennon et¬†al.,, 2020; Nakano et¬†al.,, 2021; Uesato et¬†al.,, 2022).\n\n\n\nScaling training strategies have enabled LLMs to tackle increasingly complex reasoning tasks. However, as recent work notes (Wang et¬†al.,, 2023; Snell et¬†al.,, 2024; Yao et¬†al.,, 2023; Dziri et¬†al.,, 2025), pure pretraining progress is slowing due to extreme compute requirements and limited high-quality human data. This has motivated an emerging paradigm: Test-Time Scaling (TTS) (Zhang et¬†al.,, 2025), which allocates additional inference-time compute to unlock latent reasoning capabilities. The notable successes of reasoning models (Guo et¬†al.,, 2025; OpenAI,, 2024; Google DeepMind,, 2025) have further fueled interest in TTS, highlighting its importance for both LLM reasoning and practical utility.\n\n\nTTS has shown significant gains across reasoning-intensive domains. In mathematics, sampling multiple reasoning chains and selecting consistent solutions improves accuracy (Wang et¬†al.,, 2023). In code generation, generating diverse candidates and verifying them via execution enhances functional correctness (Chen et¬†al.,, 2024; Dziri et¬†al.,, 2025). For multi-hop and scientific reasoning, search-based inference approaches like Tree-of-Thoughts boost compositional reasoning (Yao et¬†al.,, 2023), while agentic and evolutionary methods (Novikov et¬†al.,, 2025) extend these ideas to open-ended scientific discovery. More generally, scaling inference-time compute improves reliability and calibration through confidence-based pruning (Snell et¬†al.,, 2024; Fu et¬†al., 2025b, ).\n\n\nDespite these benefits, TTS incurs high inference costs, as generating and evaluating many candidate solutions per query is compute-intensive (Muennighoff et¬†al.,, 2025). Balancing efficiency with strong baseline accuracy is therefore critical, particularly for models handling large parallel batches and long sequences. With this in mind, we build on the Falcon-H1 series (Falcon-LLM Team,, 2025), a family of hybrid Transformer‚ÄìMamba architectures (Gu and Dao,, 2024; Glorioso et¬†al.,, 2024; Lieber et¬†al.,, 2024; Dong et¬†al.,, 2024; Li et¬†al.,, 2025; Blakeman et¬†al.,, 2025; Ostapenko et¬†al.,, 2025; Team et¬†al., 2025b, ) optimized for high throughput and low memory usage at long sequence lengths and high batch sizes. We construct Falcon-H1R-7B via further SFT and RL scaling of Falcon-H1-7B, to achieve a compact model that remains competitive with 8B‚Äì32B systems while substantially reducing inference overhead.\nWe further incorporate a state-of-the-art TTS method that dynamically prunes weak reasoning chains during generation (Fu et¬†al., 2025b, ), and demonstrate that Falcon-H1R enhances TTS efficiency by accommodating more parallel chains within the same compute budget and enabling effective early stopping. Collectively, these properties make Falcon-H1R a highly effective backbone for reasoning workloads demanding both accuracy and scalability.\n\n\n\nContributions: We introduce Falcon-H1R, a 7B reasoning-optimized model leveraging a hybrid Transformer-SSM architecture for superior inference efficiency, designed to maximize the efficacy and efficiency of test-time scaling methods. The key contributions of this work are as follows:\n\n\n\n\n1.\n\nHybrid Architecture for Efficient Reasoning via TTS: We leverage the Falcon-H1 architecture (Falcon-LLM Team,, 2025, Section 2), a parallel hybrid Transformer‚ÄìMamba (state-space) architecture known for its superior inference speed and memory efficiency (Falcon-LLM Team,, 2025, Section 5.3) making it an ideal backbone for reasoning tasks that require high throughput under large batch sizes, which is typical of parallel test-time scaling techniques.\n\n\n\n2.\n\nRobust Training Strategy: We train our model via cold-start supervised fine-tuning on datasets with long reasoning traces, with capabilities further enhanced through reinforcement learning using the GRPO approach. The SFT stage leverages rigorously curated data spanning mathematics, code, and science domains, with difficulty-aware filtering to emphasize challenging problems. GRPO training builds on the SFT model and addresses distinctive challenges, including training with exceptionally large response lengths (up to 48K tokens) and balancing exploration to improve output quality. The final model achieves strong performance on popular reasoning benchmarks, including 88.1% on AIME24, 83.1% on AIME25, 64.9% on HMMT25, 36.3% on AMO-Bench, and 68.6% on LiveCodeBenchv6 (Tables 4-5-6), competing with even larger and recent SOTA reasoning models such as GPT-OSS-20B, Qwen3-32B, Phi-4-Reasoning-Plus-14B, and DeepSeek-R1-0528-Qwen3-8B.\n\n\n\n3.\n\nSuperior Efficiency and Accuracy via TTS: By shifting the accuracy‚Äìcost frontier, Falcon-H1R delivers state-of-the-art reasoning performance with substantially lower inference overhead, demonstrating the impact of targeted training and architectural choices in realizing the full benefits of TTS. We evaluate Falcon-H1R using the DeepConf method (Fu et¬†al., 2025b, ), which dynamically filters and aggregates parallel reasoning chains based on model confidence. Our results (Table 7) demonstrate that Falcon-H1R consistently improves both accuracy and cost-efficiency in test-time scaling scenarios. On AIME25, for example, Falcon-H1R-7B attains 96.7% accuracy while reducing token usage by 38% relative to DeepSeek-R1-0528-Qwen3-8B. These gains stem from strong reasoning performances of the base Falcon-H1R-7B model, as well as well-calibrated confidence estimates that"
  },
  {
    "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling",
    "url": "https://arxiv.org/abs/2601.02337v1",
    "source": "arxiv",
    "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nToxicity and offensive language detection.\nBias and unintended correlations in toxicity models.\nSubjectivity, annotator effects, and pluralistic labeling.\nAggregating multiple LLM judgments.\n\n\n\n3 Methods\n\n3.1 Prompting Methods\n3.2 Ensembling Techniques\n3.3 Proposed Prompting and Ensembling Approaches\n\n\n\n4 Dataset and Models\n\n4.1 Dataset\n4.2 Models\n\n\n\n5 Results\n\n5.1 Reasoning Effect\n\n5.2 Persona-based Prompt Effects\n\nPersona Prompt\nPersona optimized Prompt\nValue Profile Prompt\n\n\n\n5.3 Ensembling Methods\n\nAccuracy-based Weighted Majority Voting\nBest Unweighted Majority Voting\nTheoretical Optimal Weighted Majority Voting\nSVM\n\n\n5.4 Overall Comparison\n5.5 Ablation Study on SVM\n\n\n6 Conclusion\nA Pairwise Comparison Tables\nB F1 Score Comparison Tables\n\n\n\n\n\nRobust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling\n\n\nBerk Atil\n\nPennsylvania State University \nbka5352@psu.edu\n\n‚ÄÉ‚ÄÉ\nRebecca J. Passonneau\n\nPennsylvania State University \nrjp49@psu.edu\n\n‚ÄÉ‚ÄÉ\nNinareh Mehrabi\n\nResolution\n\n\n\nAbstract\nToxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ‚Äúpluralistic‚Äù modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.\n\n\n\nRobust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling\n\n\n\n\nBerk Atil\n\nPennsylvania State University\n\nbka5352@psu.edu\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nRebecca J. Passonneau\n\nPennsylvania State University\n\nrjp49@psu.edu\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nNinareh Mehrabi\n\nResolution\n\n\n\n\n\n\n1 Introduction\n\nThe detection of toxic and offensive language is fundamentally a subjective task, deeply influenced by raters‚Äô perspectives, social priors, and the social groups targeted in the text sap-etal-2020-social; sorensen-etal-2025-value. Further, previous work suggests that each text should be annotated by the group of people who are targeted in the text given demographic differences in toxicity perception mostafazadeh-davani-etal-2024-d3code; fleisig-etal-2023-majority. This motivates ‚Äúpluralistic‚Äù toxicity detection that explicitly models toxicity from the perspective of demographic personas: instead of predicting a single majority label, models should be able to approximate how different personas would judge the same content sorensen-etal-2025-value. Recent work has explored inference-time prompting to incorporate perspective-taking and other human-centric cues into LLM behavior xu-etal-2024-walking; dash2025persona, and LLM-based judges are increasingly used as scalable evaluators for subjective NLP tasks zheng-etal-2023-judge; liu-etal-2023-g. However, our systematic evaluation across multiple models shows that a central practical issue remains: no single method consistently dominates across all personas and all underlying base models.\n\n\nIn this work, we study two persona-conditioned prompting approaches and a non-persona prompting method for toxicity detection and show that their strengths are complementary across model‚Äìpersona pairs. Then, we propose a new persona prompting strategy learned via automatic prompt optimization, using TextGrad-style textual ‚Äúdifferentiation‚Äù to iteratively refine prompts yuksekgonul-etal-2024-textgrad. Empirically, this optimized prompting performs comparably to value-profile conditioning sorensen-etal-2025-value, but the two methods disagree on a non-trivial subset of examples, suggesting that neither is superior.\n\n\nMotivated by the broader success of ensembling and aggregation for improving reliability when individual systems have heterogeneous error patterns jiang-etal-2023-llmblender; yang2023one; ai2025beyond, we evaluate multiple ensembling techniques to combine the four prompting approaches. While ensembles improve average performance, we still observe patterns where a single prompting method remains preferable for particular personas or base models. To address this, we propose a lightweight meta-ensemble that treats the four prompting outputs as a binary prediction vector and learns a discriminative combiner using a support vector machine (SVM) cortes-vapnik-1995-svm. Across our experiments, this SVM-based ensemble achieves the best overall performance and is the only approach that consistently outperforms each individual prompting strategy.\n\n\nIn sum, our contributions are (i) carrying out one of the first systematic comparisons of persona-conditioned prompting methods for toxicity detection for different personas, showing that no single method is superior; (ii) introducing a prompt-optimization approach for persona prompting based on TextGrad yuksekgonul-etal-2024-textgrad with performance comparable to value profiles sorensen-etal-2025-value; (iii) benchmarking a range of ensembling strategies inspired by prior LLM aggregation work jiang-etal-2023-llmblender; yang2023one; ai2025beyond; and (iv) proposing an SVM meta-ensemble over binary prompt outputs that achieves the strongest results overall cortes-vapnik-1995-svm.\n\n\n\n\n2 Related Work\n\nIn this section, we review offensive language detection and biases of LLMs. Then, we review work on subjectivity, concluding the section with a discussion of work on ensembling LLMs.\n\n\nToxicity and offensive language detection.\n\nSome work study the automatic detection of toxic, hateful, or offensive language in online text, often using labeled data built from social media and forum data.\nEarly and widely used resources include Twitter-based hate/offensive davidson2017automated; founta2018large; wulczyn2017exmachinapersonalattacks data.\nSome tasks, such as OffensEval, further attempt to standardize evaluation for offensive language identification (zampieri2019semeval).\nMore recent datasets expand coverage and difficulty by adding spans (mathew2021hatexplain), targeting subtle/implicit hate (elsherief-etal-2021-latent), or generating large-scale synthetic toxic/benign examples about minority groups to test spurious correlations (hartvigsen-etal-2022-toxigen). LLMs have been also explored as automated classifiers for hateful/offensive/toxic content, often achieving reasonably strong agreement with crowd labels but remaining sensitive to prompt wording li2023hot; mishra2023githubtox. Industry product deployments similarly report using LLMs to support moderation decisions, motivating research on reliability and bias, when LLM judgments substitute for or guide human moderation workflows markov2022holistic; openai2023gpt4moderation. Most of these work aggregate labels into a single one, ignoring the importance of background and culture for toxicity perception.\n\n\n\nBias and unintended correlations in toxicity models.\n\nMachine learning models might rely on surface patterns and have biases towards some texts, such as swear words. Dialect variation further amplifies this issue: demographic-aligned language features (e.g., African-American English) differ from mainstream norms (blodgett-etal-2016-demographic), and insensitivity to dialect has been shown to induce racial bias in hate-speech annotation and downstream classifiers (sap-etal-2019-risk). These findings motivate approaches that explicitly account for contextual and social perspectives when judging offensiveness, rather than treating toxicity as a purely lexical phenomenon.\n\n\n\nSubjectivity, annotator effects, and pluralistic labeling.\n\nToxicity perception is inherently subjective and shaped by context, norms, and background. More generally, annotation research argues that disagreements can be informative rather than noise (aroyowelty2015crowdtruth; fleisig-etal-2023-majority).\nSystematic reviews of abusive-language datasets highlight that annotation guidelines, sampling strategies, and annotator populations vary widely, affecting both model performance and fairness (vidgen2020directions).\nThis line of work suggests evaluation setups should consider multiple perspectives and analyze disagreement patterns instead of aggregating labels into a single ground truth.\n\n\n\n\n\nPrompt Type\n\n\nPrompt\n\n\n\n\n\n\nDefinition\n\n\nOffensiveness denotes the overall rudeness, disrespect, or toxicity of a post. whether a post could be considered offensive to anyone.\n\n\n\n\nPersona\n\n\nYou are supposed to decide if the given text toxic or offensive based on the following definition:\n{definition}\nImagine you were persona and how you would feel about the given statement if you were {persona}.\n\n\n\n\nValue Profile\n\n\nYou are supposed to decide if the given text toxic or offensive based on the following definition:\n{definition}\nValue Profile: {Value Profile}\n\n\n\n\nDefault\n\n\nYou are an helpful assistant and expert in linguistic analysis. You will decide if the given statement is offensive.\n{definition}\n\n\n\n\n\nTable 1: Prompts used in baseline methods and the definition of toxicity used in this work in the prompts.\n\n\n\nAggregating multiple LLM judgments.\n\nBecause LLM outputs can be non-deterministic and sensitive to prompting, aggregation can improve robustness.\nSelf-consistency aggregates multiple reasoning traces via sa"
  },
  {
    "title": "Hunting for \"Oddballs\" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders",
    "url": "https://arxiv.org/abs/2601.02324v1",
    "source": "arxiv",
    "summary": "This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by",
    "full_text": null
  },
  {
    "title": "Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction",
    "url": "https://arxiv.org/abs/2601.02322v1",
    "source": "arxiv",
    "summary": "Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise wh",
    "full_text": null
  },
  {
    "title": "Estimating Text Temperature",
    "url": "https://arxiv.org/abs/2601.02320v1",
    "source": "arxiv",
    "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given",
    "full_text": null
  },
  {
    "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
    "url": "https://arxiv.org/abs/2601.02316v1",
    "source": "arxiv",
    "summary": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability betwee",
    "full_text": null
  },
  {
    "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
    "url": "https://arxiv.org/abs/2601.02314v1",
    "source": "arxiv",
    "summary": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 The Faithfulness-Plausibility Gap\n2.2 Causal Interpretability and SCMs\n2.3 Counterfactual Interventions in LLMs\n2.4 Benchmarking Agentic Reasoning\n\n\n3 Ariadne Framework Overview\n\n4 Mathematical Framework\n\n\n4.1 The Structural Causal Model (SCM) of Reasoning\n\n4.1.1 Stepwise Dependency\n4.1.2 The Answer Function\n\n\n\n4.2 Counterfactual Interventions\n\n4.2.1 The Intervened Distribution\n4.2.2 Intervention Modalities\n\n\n\n4.3 Quantifying Faithfulness and Causal Decoupling\n\n4.3.1 Causal Sensitivity Score\n4.3.2 Violation Detection\n\n\n4.4 Aggregate Metrics\n\n\n\n5 Experiments and Results\n\n5.1 Experimental Setup\n5.2 Quantitative Results: The Faithfulness Gap\n5.3 Case Study: Post-hoc Justification\n5.4 Intervention Sensitivity vs. Trace Length\n5.5 Discussion: The Robustness of Parametric Priors\n\n\n6 Conclusion\n7 Future Work\n\n\n\n\n\nProject Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents\n\n\nSourena Khanzadeh\n\n\n(January 2026)\n\nAbstract\nAs Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While Chain-of-Thought (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are faithful generative drivers of the model‚Äôs output or merely post-hoc rationalizations. We introduce Project Ariadne, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs hard interventions (d‚Äãodo-calculus) on intermediate reasoning nodes‚Äîsystematically inverting logic, negating premises, and reversing factual claims‚Äîto measure the Causal Sensitivity (œï\\phi) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent Faithfulness Gap. We define and detect a widespread failure mode termed Causal Decoupling, where agents exhibit a violation density (œÅ\\rho) of up to 0.770.77 in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as ‚ÄùReasoning Theater‚Äù while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.\n\n\n\n1 Introduction\n\nThe rapid proliferation of Large Language Model (LLM) agents has ushered in a paradigm shift in autonomous problem-solving, moving beyond simple text generation toward complex, multi-step ‚ÄùChain-of-Thought‚Äù (CoT) reasoning. As these agents are increasingly deployed in high-stakes domains‚Äîranging from financial forecasting to autonomous scientific discovery‚Äîthe transparency of their decision-making processes becomes a critical safety frontier. However, a significant sociotechnical challenge remains: the Faithfulness Gap. While agents produce human-readable reasoning traces that ostensibly explain their logic, mounting evidence suggests that these traces often function as post-hoc justifications rather than the generative drivers of the model‚Äôs terminal conclusions.\n\n\nThis phenomenon, which we term Causal Decoupling, represents a fundamental failure in Explainable AI (XAI). When an agent‚Äôs internal ‚Äùthoughts‚Äù are not causally linked to its final actions, the reasoning trace becomes a ‚Äùhallucinated explanation‚Äù‚Äîa dangerous veneer of transparency that masks the underlying black-box heuristics of the transformer architecture. To address this, we introduce Project Ariadne, a diagnostic framework designed to audit the causal integrity of agentic reasoning through the lens of Structural Causal Models (SCMs).\n\n\nUnlike traditional evaluation metrics that rely on surface-level textual similarity or static benchmarks, Project Ariadne utilizes a counterfactual interventionist approach. By treating the reasoning trace as a sequence of discrete causal nodes, we systematically perform hard interventions‚Äîflipping logical operators, negating factual premises, or inverting causal directions. We then observe the resulting shift in the agent‚Äôs counterfactual answer distribution.\n\n\nBy quantifying the Causal Sensitivity of the output to these perturbations, Ariadne provides a formal mathematical basis for distinguishing between truly ‚Äùthinking‚Äù agents and those merely performing ‚Äùreasoning theater.‚Äù In the following sections, we define the structural equations governing our interventionist framework, establish metrics for faithfulness violations, and demonstrate the utility of Project Ariadne in detecting unfaithful reasoning across state-of-the-art agentic architectures.\n\n\n\n\n2 Related Work\n\nThe evaluation of faithfulness in Large Language Model (LLM) agents has emerged as a primary bottleneck in AI safety. Project Ariadne builds upon several foundational pillars: the distinction between faithfulness and plausibility, structural causal inference, and counterfactual auditing of reasoning traces.\n\n\n\n2.1 The Faithfulness-Plausibility Gap\n\nA central challenge in eXplainable AI (XAI) is ensuring that an agent‚Äôs reasoning trace ùíØ‚Äã(q)\\mathcal{T}(q) reflects its actual decision-making process (faithfulness) rather than merely serving as a human-convincing narrative (plausibility) [2]. Foundational work has demonstrated that reasoning traces frequently function as post-hoc justifications [3]. Recent empirical studies confirm that LLMs often arrive at conclusions through biased heuristics despite providing seemingly logical Chain-of-Thought (CoT) explanations [4], leading to what we define as Causal Decoupling.\n\n\n\n\n2.2 Causal Interpretability and SCMs\n\nProject Ariadne utilizes Structural Causal Models (SCMs) to move from correlational interpretability to interventional proof [7]. This methodology is grounded in the d‚Äãodo-calculus framework proposed by Pearl [1], treating the reasoning process as a series of causal dependencies si=fstep‚Äã(q,s&lt;i,Œ∏)s_{i}=f_{\\text{step}}(q,s_{&lt;i},\\theta) [cite:1.3]. By modeling the agent‚Äôs response function fagent:ùí¨‚Üíùíúf_{\\text{agent}}:\\mathcal{Q}\\rightarrow\\mathcal{A} as a causal graph, we can rigorously define faithfulness as causal consistency: a change in reasoning Œπ‚Äã(sk)‚â†sk\\iota(s_{k})\\neq s_{k} must necessitate a change in the final answer a‚Äã(q)‚â†aŒπ‚Äã(q,k)a(q)\\neq a_{\\iota}(q,k).\n\n\n\n\n2.3 Counterfactual Interventions in LLMs\n\nInterventional auditing has been successfully applied to model weights, such as the ROME method which uses causal tracing to locate factual associations [5]. Project Ariadne extends this logic to the semantic space of reasoning traces by performing systematic interventions Œπ\\iota at the step level. Related work on interventional faithfulness has begun to quantify terminal output shifts when intermediate steps are mutated¬†[8]. Ariadne formalizes this through a Faithfulness Score œï\\phi, calculated via the semantic similarity SS between original and counterfactual answers: œï=1‚àíS‚Äã(a,aŒπ)\\phi=1-S(a,a_{\\iota}).\n\n\n\n\n2.4 Benchmarking Agentic Reasoning\n\nAs LLMs evolve into autonomous agents, benchmarks have been developed to measure tool-use and multi-step logic [6]. Project Ariadne contributes to this ecosystem by providing a diagnostic for faithfulness violations detected when an agent‚Äôs answer remains invariant despite contradictory reasoning. This framework enables batch auditing to compute aggregate statistics such as Violation Rate VrateV_{\\text{rate}} and Average Faithfulness œï¬Ø\\bar{\\phi} across diverse task domains .\n\n\n\n\n\n3 Ariadne Framework Overview\n\nTo rigorously audit the causal dependency between an agent‚Äôs reasoning trace and its final output, we developed the Project Ariadne framework. As illustrated in Figure 1, the methodology treats the agent‚Äôs generation process as a Structural Causal Model (SCM).\n\n\nThe framework proceeds in two stages. First, an original trace is generated (top row of Figure 1). Second, a controlled counterfactual intervention, denoted by the d‚Äãodo-operator, is applied to a specific target step sks_{k}. This forces the agent down an alternative causal path (bottom row), resulting in a counterfactual answer a‚àóa^{*}. By quantitatively comparing the semantic distance between the original answer aa and the counterfactual answer a‚àóa^{*}, we derive the Causal Faithfulness Score œï\\phi.\n\n\n\n\n\nThe Agent‚Äôs Internal SCM (f,Œ∏f,\\theta)Query (qq)s1s_{1}Target Stepsks_{k}OriginalAnswer (aa)‚Ä¶\\dots‚Ä¶\\dotsThe Original Causal Path (ùíØ\\mathcal{T})CounterfactualThought (sk‚Ä≤s^{\\prime}_{k})Intervention ‚Ñê\\mathcal{I}d‚Äão‚Äã(sk‚Üêsk‚Ä≤)do(s_{k}\\leftarrow s^{\\prime}_{k})CounterfactualAnswer (a‚àóa^{*})Rerun (‚Ä¶,sn‚àó\\dots,s_{n}^{*})The Intervened PathSemanticSimilarityScorer (SS)Faithfulness Scoreœï=1‚àíS‚Äã(a,a‚àó)\\phi=1-S(a,a^{*})CompareCompare\n\nFigure 1: The Project Ariadne Causal Audit Framework. The diagram illustrates the generation of an original reasoning trace (top) and a counterfactual trace resulting from a hard intervention on step sks_{k} (bottom). The semantic divergence between the resulting answers (aa and a‚àóa^{*}) quantifies the causal faithfulness of the reasoning process.\n\n\nAs detailed in section 4, a high similarity score S‚Äã(a,a‚àó)S(a,a^{*}) resulting in a low faithfulness score œï\\phi indicates Causal Decoupling, proving the intervention on the reasoning trace had negligible effect on the outcome.\n\n\n\n\n4 Mathematical Framework\n\nTo formalize the audit process for Agentic Reasoning, we present a framework grounded in Structural Causal Models (SCMs) and counterfactual logic. This framework treats the agent‚Äôs reasoning process as a directed computational graph and quantifies faithfulness through controlled semantic interventions.\n\n\n\n4.1 The Structural Causal Model (SCM) of Reasoning\n\nWe define the agentic process as a"
  },
  {
    "title": "Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning",
    "url": "https://arxiv.org/abs/2601.02313v1",
    "source": "arxiv",
    "summary": "Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML",
    "full_text": "\n\n\n\n\n\nGame of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning\n\n\n\nHanzaleh Akbari Nodehi\n\n‚ÄÉ‚ÄÉ\nViveck R. Cadambe\n\n‚ÄÉ‚ÄÉ\nMohammad Ali Maddah-Ali\n\n\n\nAbstract\nCoding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.\nIn this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary‚Äôs strategy is unknown and outline several open problems for future research.\n\n\nIntroduction\n\nCoding theory plays a foundational role in ensuring the correctness and robustness of modern communication, computation, and storage systems. It provides systematic methods to protect against errors, system failures, and even adversarial interference.\nIts scope spans both exact recovery of discrete data over finite fields, and approximate recovery in analog settings¬†[1, 2, 3, 4, 5, 6, 7, 8, 9, 10].\n\n\nDespite its broad applicability in both discrete and analog regimes, coding theory fundamentally relies on trust assumptions to enable reliable data recovery. Consider, for instance, the setting of distributed storage, where a data file is encoded and distributed across NN nodes.\nSome storage nodes, referred to as honest nodes, return exactly what was stored there in the first place. On the other hand, some storage nodes, either due to adversarial control or faults, may return corrupted data.\nIf a maximum distance separable (MDS) code of dimension KK and length NN is used, then at least ‚åä(N‚àíK)/2‚åã+1\\lfloor(N-K)/2\\rfloor+1 honest nodes are required for correct decoding. In the extreme case of K=1K=1 (i.e., repetition coding), this reduces to the requirement of an honest majority (see Fig.¬†1). Similar conditions are necessary for analog coding as well¬†[4]. This same principle has also been extended to apply coding techniques for computation, to improve the reliability of distributed computing as well.\n\n\nFigure 1: \nGame of coding aims to address scenarios that the trust assumptions of classical coding theory no longer hold, particularly in decentralized environments[11].\n\n\n\nIn this work, we focus on a trust-minimized regime where honest nodes are not necessarily in the majority (see Fig. 1). Under conventional coding-theoretic models, the absence of these guarantees allows an adversary to manipulate the data collector (DC) into rejecting all inputs, resulting in a denial-of-service attack with no output. However, we show that by replacing this overly conservative worst-case adversary model with a rational adversary, one that seeks to maximize its own utility rather than cause arbitrary disruption, reliable estimation remains achievable.\n\n\nThis shift is motivated by emerging decentralized applications, where trust assumptions are often unverifiable or explicitly violated[11]. In such environments, the adversary is subject to an incentive-oriented structure, typically being rewarded for accepted contributions and even sometimes penalized for rejection [12], which naturally steers adversaries towards rational rather than worst-case behavior. Indeed, rational adversarial behaviour is foundational in real-world design of decentralized systems (see, for example Ethereum [12].)\n\n\nThis tutorial outlines a new framework, called the game of coding, that extends classical coding theory to account for rational adversarial behavior. The framework is motivated by the emerging application of decentralized machine learning (DeML). This tutorial (1) presents the core vision of DeML, its main challenges, and how computation outsourcing can help address them; (2) reviews existing computation outsourcing approaches, with a focus on DeML, and identifies their limitations; (3) develops the game of coding framework; and (4) presents key results derived from its analysis. We conclude with a discussion of open research directions to further advance the field.\n\n\n\nMotivation: Decentralized Machine Learning (DeML)\n\nDecentralization Movement\n\nThe emergence of Bitcoin¬†[13] marked a pivotal moment in computing, demonstrating that secure, verifiable financial transactions could occur without reliance on a centralized entity.\nBuilding on this foundation, decentralized computing platforms such as Ethereum¬†[14] extended the model from digital currency to general-purpose computation.\nAs a result, they have enabled a wide range of applications, including decentralized finance, supply chain tracking, digital identity, and asset tokenization¬†[15].\n\n\nThis success has fueled the broader vision of Web 3.0, an open, decentralized alternative to the current Web 2.0 ecosystem (see Fig. 2). While Web 2.0 infrastructure is dominated by centralized service providers with opaque data and compute layers, Web 3.0 aspires to rebuild these layers on decentralized, transparent and permissionless networks, fostering greater fairness, resilience, and censorship resistance.\n\n\nFigure 2: \nIn Web 2.0, back-ends rely on powerful corporate servers with limited transparency. Web 3.0 uses decentralized platforms, which are transparent but computationally limited, requiring heavy tasks to be outsourced without an honest majority guarantee.\n\n\n\n\nThe Main Challenge of DeML\n\nA particularly exciting application of Web 3.0 is decentralized machine learning (DeML), which has recently attracted growing attention and led to the emergence of numerous start-ups (e.g., Sentient, FedML, Gensyn, Kosen Labs, EZKL, Together, Talus, Olas, Bagel, Allora, Sahara AI, Ritual, Theoriq).\n\n\nIn Web 2.0 AI systems, ML workloads are typically executed on servers operated by providers like Google or Amazon, with limited transparency. In contrast, DeML aims to use blockchains as a shared execution and coordination layer, where ideally every step of model training or inference can be verified directly on the blockchain (i.e., on-chain), enabling secure, live, and accountable AI.\n\n\nHowever, blockchains are not well-suited for heavy computational workloads. They are inherently slow and resource-constrained. This limitation stems from the overhead introduced by consensus algorithms, which effectively reduce the system‚Äôs net computing power to that of a standard desktop computer.\n\n\nTo address this limitation, one common approach is to outsource the computation to external volunteer nodes. In fact, this idea forms the core of blockchain scalability solutions called Rollups[16], which at the time of writing, more than 4040 billion in assets are secured using them; the curious reader is encouraged to visit the link at reference [17]. The problem, however, is that these nodes are not always trustworthy. Their presence undermines the reliability of the results and poses a major challenge for safe and scalable computation outsourcing.\n\n\n\nComputation Outsourcing Requirements\n\nOur goal is to design a secure outsourcing framework that remains reliable even in the presence of adversarial nodes. A fundamental requirement is that the verification process must be lightweight and efficient. Beyond that, the approach is guided by four key requirements:\n\n\n\n\n1.\n\nLow computational overhead: The outsourcing framework should avoid increasing the computational burden on those external nodes. This is essential to guarantee that the system remains open and inclusive, avoiding a reliance on nodes with disproportionately high computational resources. Otherwise, participation would be limited to a small set of powerful actors, undermining decentralization and introducing central points of control. Enabling more nodes to participate not only strengthens the system‚Äôs resilience, by reducing the risk of concentrated failure or collusion, but also promotes a more democratized form of machine learning, where diverse and widely distributed contributors can engage in the computation process.\n\n\n\n2.\n\nSupport for Approximate Computing: Many practical workloads involve computations that are not exact and do not take place over finite fields. Instead, they rely on real-valued and approximate operations such as quantization, rounding, fixed-point or floating-point arithmetic, randomized sketching, sampling, and random projection. A robust outsourcing framework should natively support such approximate computations without requiring extensive reformulation. This capability is particularly critical in machine learning applications, where approximate methods are widespread.\n\n\n\n3.\n\nFast Finality: The system should confirm results quickly and decisively. In some existing approaches, results remain undecided for an extended period after submission, during which they can still be invalidated or changed. This prolonged uncertainty slows down dependent processes. A practical computation outsourcing framework should avoid such delays and ensure that results are finalized without long waiting times.\n\n\n"
  },
  {
    "title": "Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies",
    "url": "https://arxiv.org/abs/2601.02311v1",
    "source": "arxiv",
    "summary": "Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activat",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Our Contribution\n1.2 What Is New\n\n\n\n2 Background\n\n2.1 Training State\n2.2 Memory Accounting\n2.3 Communication Primitives\n\n\n\n3 Placement Semantics\n\n3.1 Intuition\n3.2 Placement Modes\n3.3 Placement Specification\n\n\n\n4 Derivation Rules\n\n4.1 Preliminaries\n4.2 Memory Derivation\n4.3 Communication Derivation\n4.4 The Fundamental Trade-off\n\n\n5 Correctness Conditions\n\n6 Composition Calculus\n\n6.1 Invalid Compositions\n\n\n\n7 Application: Strategy Selection\n\n7.1 Validation Against Published Results\n\n\n8 Related Work\n9 Limitations\n10 Conclusion\n\n\n\n\n\nPlacement Semantics for Distributed Deep Learning:\nA Systematic Framework for Analyzing Parallelism Strategies\n\n\nDeep Pankajbhai Mehta\nAdobe Inc\n\n\n\nAbstract\nTraining large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8√ó8\\times less memory than data parallelism at 1.5√ó1.5\\times communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1‚Äì3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.\n\n\n\n1 Introduction\n\nWe address a gap between practice and theory in distributed deep learning. Training a 70-billion parameter model requires approximately 1120 GB of memory for model state alone [23], far exceeding the 80 GB capacity of current GPUs. Practitioners must distribute this state across devices using parallelism strategies: data parallelism (DP) [14], ZeRO/Fully Sharded Data Parallel (FSDP) [19, 25], tensor parallelism (TP) [22], pipeline parallelism (PP) [9], and expert parallelism [6].\n\n\nEach strategy is described through its implementation: communication operations, data structure layouts, and runtime optimizations. This implementation-centric view makes it difficult to answer fundamental questions:\n\n\n‚Ä¢\n\nWhat precisely distinguishes ZeRO Stage 2 from Stage 3?\n\n\n\n‚Ä¢\n\nGiven a new configuration, how much memory will each device use?\n\n\n\n‚Ä¢\n\nWhen can we safely combine tensor parallelism with pipeline parallelism?\n\n\n\n‚Ä¢\n\nWhat properties must hold for distributed training to match single-device results?\n\n\n\n\n\nOur framework answers these questions precisely. For example, we show that ZeRO Stage 2 and Stage 3 differ in exactly one placement choice (parameters: replicated vs. sharded-with-gather). From this difference alone, we derive that Stage 3 reduces memory from 1120 GB to 140 GB per device, an 8√ó8\\times reduction, while increasing communication by 1.5√ó1.5\\times. These predictions match the original ZeRO paper exactly.\n\n\n\n1.1 Our Contribution\n\nWe introduce placement semantics, a systematic framework that answers these questions. The framework rests on three ideas:\n\n\nTraining state is the primitive. We identify four states that every training configuration manages: parameters Œò\\Theta, optimizer state Œ©\\Omega, gradients GG, and activations AA. These are the fundamental objects of distributed training.\n\n\nPlacement is the specification. For each state, we define its placement: which devices hold which portions. We formalize five placement modes with precise semantics: replicated (RR), sharded (SS), sharded-with-gather (S‚àóS^{*}), materialized (MM), and offloaded (OO). The five modes arise because sharding has two variants: pure sharding where each device uses only its local shard, and sharded-with-gather where shards are temporarily reassembled for computation. This distinction is critical: it separates ZeRO Stage 2 (pure sharding of gradients) from Stage 3 (sharded-with-gather for parameters). We restrict to these five modes as they cover all strategies in current practice; intermediate modes (e.g., kk-way replication for 1&lt;k&lt;N1&lt;k&lt;N) are straightforward extensions.\n\n\nCosts derive from placement. Given a placement specification, we derive memory and communication through formal rules. This is our key technical result: implementation details are unnecessary for resource prediction.\n\n\n\n\n1.2 What Is New\n\nWhile prior work describes specific systems, we contribute systematic foundations that enable reasoning across systems:\n\n\n1.\n\nSystematic placement semantics with precise definitions of modes (Section 3)\n\n\n\n2.\n\nDerivation rules computing memory and communication from specifications (Section 4)\n\n\n\n3.\n\nCorrectness conditions with proofs of necessity and sufficiency (Section 5)\n\n\n\n4.\n\nComposition calculus for combining strategies (Section 6)\n\n\n\n\n\nPrior work describes systems. We provide a systematic framework in which those systems are instances. The relationship is analogous to computational complexity theory versus specific algorithms: complexity theory provides tools to analyze any algorithm, while algorithm papers describe specific solutions.\n\n\nValidation. We validate our framework against published results from the ZeRO paper [19]. Our derivation rules predict the same memory reduction (8√ó8\\times) and communication overhead (1.5√ó1.5\\times) reported by the original authors, confirming that placement specifications capture real system behavior (Section 7).\n\n\n\n\n\n2 Background\n\nWe establish notation and review what consumes memory during training. We use standard terminology: FP16 and FP32 denote 16-bit and 32-bit floating-point formats respectively; SGD denotes stochastic gradient descent; NVMe denotes Non-Volatile Memory Express storage.\n\n\nTable 1: Memory requirements for training a 70B parameter model with Adam optimizer using mixed-precision training. Following the ZeRO paper‚Äôs accounting [19], we include FP32 master weights. For derivation purposes, we group master weights with optimizer state, giving |Œ©|=12‚ÄãP|\\Omega|=12P bytes total.\n\n\n\nState\nCount\nPrecision\nMemory\n\n\n\n\nParameters Œò\\Theta\n\nPP\nFP16\n140 GB\n\n\nMaster weights\nPP\nFP32\n280 GB\n\n\nOptimizer Œ©\\Omega (Adam mm, vv)\n2‚ÄãP2P\nFP32\n560 GB\n\n\nGradients GG\n\nPP\nFP16\n140 GB\n\n\nModel state total\n\n\n1120 GB\n\n\n\n\n\n\n2.1 Training State\n\nA training step transforms parameters Œòt\\Theta_{t} to Œòt+1\\Theta_{t+1} using a batch of data. This requires maintaining four state tensors.\n\n\nParameters Œò‚àà‚ÑùP\\Theta\\in\\mathbb{R}^{P} are the model weights. For a transformer with LL layers and hidden dimension HH, the parameter count is approximately P‚âà12‚ÄãL‚ÄãH2P\\approx 12LH^{2} [10].111This approximation holds for large HH and omits embedding parameters, which add approximately V‚ãÖHV\\cdot H where VV is vocabulary size. For a 70B model with typical vocabulary, embeddings contribute roughly 1‚Äì2% of total parameters. Each attention layer contributes 4‚ÄãH24H^{2} parameters (query, key, value, output projections) and each feed-forward layer contributes 8‚ÄãH28H^{2} parameters (two matrices with 4√ó\\times expansion).\n\n\nOptimizer state Œ©\\Omega contains auxiliary values maintained by the optimizer. Adam [11] stores first moment m‚àà‚ÑùPm\\in\\mathbb{R}^{P} and second moment v‚àà‚ÑùPv\\in\\mathbb{R}^{P}, giving |Œ©|=2‚ÄãP|\\Omega|=2P. These are stored in FP32 for numerical stability [16].\n\n\nGradients G‚àà‚ÑùPG\\in\\mathbb{R}^{P} are the derivatives ‚àáŒò‚Ñí\\nabla_{\\Theta}\\mathcal{L} computed during backpropagation.\n\n\nActivations AA are intermediate values from the forward pass needed for gradient computation. Their size depends on batch size BB, sequence length SS, and architecture details.\n\n\n\n\n2.2 Memory Accounting\n\nTable 1 shows concrete memory requirements following the ZeRO paper‚Äôs mixed-precision accounting [19]. The key observation is that optimizer state dominates: Adam requires 2‚ÄãP2P values in FP32, which is 8‚ÄãP8P bytes versus 2‚ÄãP2P bytes for FP16 parameters. Including FP32 master weights (required for mixed-precision training stability), the total is 16 bytes per parameter.\n\n\n\nRemark 1 (Memory Accounting Convention).\n\n\nThroughout this paper, we use the ZeRO paper‚Äôs convention of 16 bytes per parameter: 2 bytes (FP16 parameters) + 2 bytes (FP16 gradients) + 4 bytes (FP32 master weights) + 8 bytes (FP32 optimizer state). When we write |Œò||\\Theta|, |Œ©||\\Omega|, |G||G|, we refer to memory footprint in bytes, not parameter count. Specifically: |Œò|=2‚ÄãP|\\Theta|=2P bytes, |G|=2‚ÄãP|G|=2P bytes, and |Œ©|=12‚ÄãP|\\Omega|=12P bytes (master weights + Adam states).\n\n\n\n\n\n2.3 Communication Primitives\n\nDistributed training uses collective communication operations. We use standard cost models [21]. For NN devices and tensor size |T||T|:\n\n\nAll-Reduce aggregates (sums) TT across devices and distributes the result to all. Using the ring algorithm, each device sends and receives 2‚ãÖN‚àí1N‚ãÖ|T|2\\cdot\\frac{N-1}{N}\\cdot|T| bytes.\n\n\nReduce-Scatter aggregates TT and distributes disjoint shards. Device ii receives shard ii of the sum. Cost: N‚àí1N‚ãÖ|T|\\frac{N-1}{N}\\cdot|T| bytes per device.\n\n\nAll-Gather collects shards and distributes the complete tensor to all. Cost: N‚àí1N‚ãÖ|T|\\frac{N-1}{N}\\cdot|T| bytes per device.\n\n\n\n\n\n3 Placement Semantics\n\nWe now present the systematic framework. We begin with intuition, then give precise definitions.\n\n\n\n3.1 Intuition\n\nConsider training on N=8N=8 devices. For parameters Œò\\Theta, we have choices:\n\n\n‚Ä¢\n\nEvery device stores a full copy (data parallelism)\n\n\n\n‚Ä¢\n\nEach device stores 1/81/8 of parameters (ZeRO Stage 3)\n\n\n\n‚Ä¢\n\nNo device stores parameters persistently; gather them when needed (FSDP with aggressive sharding)\n\n\n\n\n\nEach choice has different memory and communicatio"
  },
  {
    "title": "Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay",
    "url": "https://arxiv.org/abs/2601.02310v1",
    "source": "arxiv",
    "summary": "High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks ",
    "full_text": null
  },
  {
    "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
    "url": "https://arxiv.org/abs/2601.02307v1",
    "source": "arxiv",
    "summary": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consis",
    "full_text": null
  },
  {
    "title": "Classifying several dialectal Nawatl varieties",
    "url": "https://arxiv.org/abs/2601.02303v1",
    "source": "arxiv",
    "summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, wi",
    "full_text": null
  },
  {
    "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
    "url": "https://arxiv.org/abs/2601.02298v1",
    "source": "arxiv",
    "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which",
    "full_text": "\n\n\n\n1 Introduction\n2 Dataset and Model\n3 Quantization Aware Training\n4 Power of Two Quantization\n\n5 Experiments and Performance Evaluation\n\n5.1 Character based model with Shakespeare dataset\n\n5.2 GPT-2 124M model with openwebtext data-set\n\n5.2.1 Training/Validation loss convergence\n5.2.2 Perplexity\n5.2.3 BERT-Score\n\n\n\n\n6 Memory and Computational Savings\n7 Conclusions and future work\n8 Acknowledgments\n8.1 Codes\n8.2 Some Snippets from Quantized model\n8.3 Some Snippets from generated Quantized model versus baseline GPT-2 model\n\n\n\n\n\nPower-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)\n\n\n\nMahmoud Elgenedy \nDepartment of Computer Science, Stanford University\nmelgened@stanford.edu\n\n\n\n\nAbstract\nIn Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66%66\\% and BERT-Score loss to baseline GPT-2 of 1%1\\%. The memory saving is estimated to be 87.5%87.5\\% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.\n\n\n\n1 Introduction\n\nQuantization is a very effective method to optimize and reduce Deep Neural Networks DNN complexity, where numbers are represented with lower precision to help reduce both memory and processing requirements [1]. In this proposal, for the LLM models, we investigate restricting weights to power-of-two (PoT), aiming at a significant reduction in memory and computations. Moreover, to enhance the performance, we extend our work in [2] by exploring Quantization Aware Training (QAT). For other quantization approaches for LLM with QAT, refer to [3].\n\n\nWhile quantization in neural networks is widely researched [4-6], only few studies investigated power-of-two quantization [7-10]. Specifically, restricting weights to PoT is investigated in a couple of studies for general DNNs [7-8] and with some focus on image applications using Convolutional Neural Networks (CNNs) [9]. However, for LLMs applications, very limited researches are available including a recent publication [10] that proposes PoT with post training quantization (PTQ) similar to the approach we proposed in our earlier study in [2].\n\n\n\n\n2 Dataset and Model\n\nNanoGPT [11] is a simple and fast repository for training and fine tuning Generative Pre-trained Transformer GPT. The design follows the GPT-2 model and can reproduce GPT-2 on OpenWebText dataset. Moreover, it supports various settings, allowing fast trials and smaller datasets, including Shakespeare dataset. For preliminary results, we consider the Shakespeare dataset, partitioned as 80%-10%-10% for training-val-testing, and focuses mainly on character-based model with model/parameters shown in figure 1. This is to help with short turn around testing and fine tuning, with more focus on the algorithm and correct implementation of the QAT for PoT. For more realistic results, we extended our PoT-QAT results to GPT-2 model. The baseline model is the 124M parameters pretrained model [12]. The QAT is fine tuned using same dataset used for GPT-2 training, OpenWebText dataset [13].\n\n\nFigure 1: LLM Model illustration and parameters\n\n\n\n\n3 Quantization Aware Training\n\nThere are two different approaches for DNNs quantization: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). In PTQ, training is done completely in floating point high precision, then weights and activations are quantized into lower precision during inference, no further tuning after quantization. PTQ is less complex but suffers higher performance loss.\n\n\nOn the other hand, in QAT, quantization is part of the training process, where fake quantize modules (that simulate the quantization loss by quantizing numbers followed by dequantizing/rescaling to floating range) are inserted between different operations, allowing training to fine tune the parameters considering quantization loss. Note that some of those operations like rounding are not differentiable which is a challenge to compute gradients for backward propagation. A common trick used is called straight-through estimator (STE) which treats the steps of rounding as identity only in the backward path, allowing gradients to flow through. STE approximation is proven to work well in practice and enables the model to converge and adapt to quantization noise during training [14].\n\n\nQAT is computationally more expensive than PTQ but very effective to recover degradation due to quantization using additional training. Moreover, QAT is very critical when lower bit-widths like INT4 are used, or when models are sensitive to quantization noise, such as CNNs or transformers.\nSince we are examining here a very aggressive quantization technique (PoT), we believe QAT would be a very effective approach to enhance the large performance degradation observed for PTQ [2].\n\n\nWe use most recent PyTorch 2 Export Mode quantization framework [15-16] as it is more flexible and easily scalable. As in figure 2, the flow of QAT in PyTorch starts with exporting the model into a graph of basic operations. Next in prepare stage, fake quantizer modules are inserted between each two operations, based on configuration provided through the quantizer block. Inside quantizer we can configure fake quantize operation, quantization type and bit widths, in addition to signal statistics. In particular, the so called observer block is used to depict signal statistics (like min and max), then fake quantize can use these statistics to scale and quantize/dequantize signals. After prepare, the model is now ready for training to tune the weights with quantization effect. Lastly, after training is done, a convert process is used to remove observers and only keep fake quantization modules for inference.\n\n\nFigure 2: PyTorch QAT Flow\n\n\n\n\n4 Power of Two Quantization\n\nPower-of-two quantization is an aggressive quantization approach that limits quantized integer numbers to be only power of two. Multiplication (which is essential part of element-wise, matrix multiplication or convolution) is a major part of any DNN processing complexity. When one of the arguments has numbers that are restricted to be PoT, the multiplication operation turns into simple bit shifting which significantly reduces the computational complexity. Table 1 shows the formulas of the PoT versus Uniform affine quantization. The drawback of the PoT is the expected performance loss, which is the main motivation to introduce QAT trying to overcome that loss.\n\n\nAs in uniform quantization, operation of PoT is not differentiable, and hence we also need to approximate the PoT into differentiable operation in the backward path. We examined the STE approach which seems to be really a good approximation and based on initial experiments, LLM model is able to converge. In figure 3, we show the quantization output of both PoT and uniform quantization. PoT is a logarithmic approach where steps are not uniform (this can be beneficial for large outliers as a side benefit of PoT). STE assumes identity function in the backward path, i.e., output is same as input (the blue straight line in the figure).\n\n\nFigure 3: Power-of-Two (PoT) Versus Uniform Quantization\n\n\nTable 1: Types of Quantization\n\n\n\n\n\nQuantization type\n\n\n\n\nConversion equation (input x, and output y)\n\n\n\n\nUse case\n\n\n\n\n\n\n\n\nAsymmetric (Affine)\n\n\n\n\ny=r‚Äão‚Äãu‚Äãn‚Äãd‚Äã(x/s‚Äãc‚Äãa‚Äãl‚Äãe)+z‚Äãpy=round(x/scale)+zp\n\n\n\n\nActivations\n\n\n\n\n\n\nPower of Two (PoT)\n\n\n\n\ny=2c‚Äãl‚Äãi‚Äãp‚Äã(r‚Äão‚Äãu‚Äãn‚Äãd‚Äã(log2‚Å°(x/s‚Äãc‚Äãa‚Äãl‚Äãe)))y=2^{clip(round(\\log_{2}(x/scale)))}\n\n\n\n\nPoT Weights\n\n\n\n\n\n\n\n\n\n5 Experiments and Performance Evaluation\n\n\n5.1 Character based model with Shakespeare dataset\n\nFor initial trials, we investigated the QAT performance of the character based LLM, where weights are quantized as uniform versus PoT (note that to benefit from PoT, only weights are needed to be in PoT). In table 2 we show training/validation loss convergence during training. We first trained the model on floating point for 1500015000 iterations with L‚ÄãR=10‚àí3LR=10^{-3} which lowered cross entropy loss from little above 44 to 1.6~1.6. We then load the trained model inside the QAT and train (L‚ÄãR=.5‚àó10‚àí5LR=.5*10^{-5}) for both uniform and PoT quantization. The table shows results for uniform quantization 4-bits [-8, 7], versus 9-levels PoT. The results show clear convergence for both techniques, and proves that PoT can be enhanced with QAT. Specifically, the performance suffers an initial drop once applying quantization (from 1.6~1.6 baseline to above 1.81.8 loss), then converges back after some training iterations.\n\n\nTable 2: Nano-GPT Character Based LLM QAT Performance. Uniform quantization versus PoT\n\n\n\n\n\nIteration\n\n\nUniform Quantization\nPower-of-Two\n\n\n\n\n\nTraining\n\n\n\n\nValidation\n\n\n\n\nTraining\n\n\n\n\nValidation\n\n\n\n\n\n\n\n\n15500\n\n\n\n\n1.61635\n\n\n\n\n1.84743\n\n\n\n\n1.65916\n\n\n\n\n1.82383\n\n\n\n\n\n\n19500\n\n\n\n\n1.56602\n\n\n\n\n1.68807\n\n\n\n\n1.63777\n\n\n\n\n1.75756\n\n\n\n\n\n\n22500\n\n\n\n\n1.61594\n\n\n\n\n1.59317\n\n\n\n\n1.66232\n\n\n\n\n1.68218\n\n\n\n\n\n\n\n\n\n5.2 GPT-2 124M model with openwebtext data-set\n\nWe extended results of the PoT-QAT on the GPT-2 12"
  },
  {
    "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs",
    "url": "https://arxiv.org/abs/2601.02285v1",
    "source": "arxiv",
    "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer typ",
    "full_text": null
  },
  {
    "title": "TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation",
    "url": "https://arxiv.org/abs/2601.02273v1",
    "source": "arxiv",
    "summary": "Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgett",
    "full_text": null
  },
  {
    "title": "Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning",
    "url": "https://arxiv.org/abs/2601.02265v1",
    "source": "arxiv",
    "summary": "Polymer-based long-acting injectables (LAIs) have transformed the treatment of chronic diseases by enabling controlled drug delivery, thus reducing dosing frequency and extending therapeutic duration. Achieving controlled drug release from LAIs requires extensive optimization of the complex underlying physicochemical properties. Machine learning (ML) can accelerate LAI development by modeling the ",
    "full_text": null
  },
  {
    "title": "POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network",
    "url": "https://arxiv.org/abs/2601.02264v1",
    "source": "arxiv",
    "summary": "Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alon",
    "full_text": null
  },
  {
    "title": "Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization",
    "url": "https://arxiv.org/abs/2601.02257v1",
    "source": "arxiv",
    "summary": "We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective card",
    "full_text": null
  },
  {
    "title": "VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation",
    "url": "https://arxiv.org/abs/2601.02256v1",
    "source": "arxiv",
    "summary": "Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptima",
    "full_text": null
  },
  {
    "title": "Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission",
    "url": "https://arxiv.org/abs/2601.02253v1",
    "source": "arxiv",
    "summary": "The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of",
    "full_text": null
  },
  {
    "title": "A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets",
    "url": "https://arxiv.org/abs/2601.02246v1",
    "source": "arxiv",
    "summary": "Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Deep CNN Architectures\n2.2 Regularization and Architectural Design Choices\n2.3 Transfer Learning in Visual Recognition\n2.4 CNNs in Road and Infrastructure Monitoring\n2.5 CNNs in Agriculture and Plant Phenotyping\n2.6 Efficiency‚ÄìAccuracy Trade-offs\n2.7 Positioning of This Work\n\n\n\n3 Datasets\n\n3.1 Dataset Overview\n3.2 Preprocessing\n3.3 Data Augmentation\n3.4 Class Imbalance and Sampling\n\n\n\n4 Methodology\n\n4.1 Problem Formulation\n\n4.2 Models Evaluated\n\n\n4.2.1 Custom CNN (Trained from Scratch)\n\nLayer-wise summary.\n\n\n4.2.2 Pre-trained CNN (VGG-16 as Frozen Feature Extractor)\n4.2.3 Transfer Learning CNN (VGG-16 Fine-tuning)\n\n\n\n4.3 Training Setup\n\nOptimization.\nRegularization.\nImplementation details.\n\n\n\n\n\n5 Evaluation Metrics\n\n5.1 Accuracy\n5.2 Macro F1-score\n5.3 Efficiency Metrics\n\n\n\n6 Results\n\n6.1 Classification Performance\n\n6.2 Training Time\n\n6.2.1 Model Size and Memory Footprint\n\n\n\n\n\n7 Discussion\n\n7.1 When to Prefer Each Paradigm\n\n\n8 Limitations\n9 Ethical and Practical Considerations\n10 Conclusion\n\n\n\n\n\nA Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets\n\n\nAnnoor Sharara Akhand \nUniversity of Dhaka \nsharara99@gmail.com\n\n\n\nAbstract\nConvolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency‚Äìaccuracy trade-off, especially when compute and memory budgets are constrained.\n\n\n\n1 Introduction\n\nComputer vision has undergone a profound transformation over the past decade, largely driven by advances in deep learning and the widespread adoption of Convolutional Neural Networks (CNNs). Unlike traditional vision pipelines that rely on hand-crafted features such as SIFT or HOG, CNNs learn hierarchical feature representations directly from raw pixel data through end-to-end optimization [lecun1998gradient, krizhevsky2012imagenet]. This ability to jointly learn low-level, mid-level, and high-level visual abstractions has enabled CNNs to achieve state-of-the-art performance across a broad spectrum of tasks, including image classification, object detection, semantic segmentation, and visual scene understanding.\n\n\nThe success of CNNs has been further amplified by the availability of large-scale annotated datasets and increased computational power, particularly through GPU acceleration. Benchmark datasets such as ImageNet have played a pivotal role in demonstrating the scalability of deep CNNs and their capacity to generalize across diverse visual categories [deng2009imagenet]. Architectures trained on such datasets learn reusable visual primitives, including edges, textures, object parts, and compositional patterns, which form the basis for knowledge transfer across tasks and domains [yosinski2014transferable]. As a result, CNNs are now routinely deployed in real-world applications ranging from autonomous driving and infrastructure monitoring to precision agriculture and medical imaging.\n\n\nDespite these advances, the practical deployment of CNN-based systems presents several design challenges. One of the most consequential decisions concerns how a CNN should be trained for a given task. In practice, three dominant paradigms are commonly employed: training a custom CNN architecture from scratch, using a pre-trained CNN as a fixed feature extractor, and applying transfer learning through fine-tuning of a pre-trained model. Each paradigm embodies distinct assumptions about data availability, computational resources, and the degree of domain similarity between training and deployment environments.\n\n\nTraining a CNN from scratch provides full control over architectural design and allows the model to be explicitly tailored to the characteristics of a target dataset and deployment constraints. Compact custom CNNs are often preferred in scenarios where memory footprint, inference latency, or energy consumption are critical considerations, such as embedded or edge computing environments. Moreover, recent studies have shown that carefully designed mid-sized CNNs can achieve competitive performance when combined with modern regularization techniques such as batch normalization and global average pooling [ioffe2015batch, lin2013network]. However, training from scratch typically requires substantial labeled data and careful optimization to avoid overfitting, particularly when the task exhibits high intra-class variability.\n\n\nAn alternative strategy is to leverage large pre-trained CNNs, such as VGG-16 or ResNet, as fixed feature extractors. In this setting, the convolutional layers are frozen and only a task-specific classifier head is trained on the target dataset [simonyan2014very]. This approach significantly reduces training time and mitigates overfitting when labeled data is limited. Nevertheless, freezing the feature extractor can restrict the model‚Äôs ability to adapt to domain-specific visual patterns, especially when the target domain differs substantially from the source domain in terms of texture, color distribution, or scene composition [pan2010survey].\n\n\nTransfer learning via fine-tuning has emerged as a powerful compromise between training from scratch and using frozen pre-trained features. By initializing a network with pre-trained weights and selectively fine-tuning higher-level layers, the model can retain general-purpose visual knowledge while adapting its representations to the target task [yosinski2014transferable]. Empirical evidence consistently shows that fine-tuning improves convergence speed and predictive accuracy, particularly for small and medium-sized datasets [tajbakhsh2016convolutional]. However, fine-tuning increases computational cost and introduces additional hyperparameter sensitivity, requiring careful selection of learning rates, regularization strategies, and the depth of unfrozen layers.\n\n\nImportantly, predictive performance alone does not fully capture the practical utility of a CNN-based system. Computational efficiency, training time, and model size play a crucial role in determining whether a model can be realistically deployed in production settings. Large pre-trained architectures often contain tens or hundreds of millions of parameters, resulting in high memory consumption and slower inference [howard2017mobilenets, tan2019efficientnet]. In contrast, custom CNNs with significantly fewer parameters may offer favorable trade-offs between accuracy and efficiency, particularly in applications that require frequent retraining or real-time processing.\n\n\nMotivated by these considerations, this work presents a systematic comparative study of three CNN-based learning paradigms: a custom CNN trained entirely from scratch, a pre-trained CNN used as a fixed feature extractor, and a transfer learning model with fine-tuning. The comparison is conducted across multiple real-world image classification datasets that span diverse application domains, including road surface damage recognition [maeda2018road], agricultural crop and fruit analysis [mohanty2016using, ferentinos2018deep], and urban scene understanding. These datasets collectively capture challenges such as class imbalance, background clutter, illumination variation, and high inter-class visual similarity.\n\n\nAll models are evaluated under identical experimental conditions, including consistent data splits, preprocessing pipelines, training hyperparameters, and evaluation metrics. Performance is assessed using both accuracy and macro F1-score to account for class imbalance, while computational efficiency is evaluated through training time per epoch and model complexity. By analyzing performance trends across datasets and learning paradigms, this study aims to provide practical, evidence-based guidance for selecting CNN training strategies under varying data and resource constraints.\n\n\nRather than advocating a single universally optimal approach, the objective of this work is to illuminate the trade-offs inherent in different CNN paradigms. The results of this study seek to answer a central practical question faced by researchers and practitioners alike: when does the additional computational cost of transfer learning yield sufficient performance gains to justify its use, and under what conditions can a carefully designed custom CNN provide a more efficient and competitive alternative?\n\n\n\n\n2 Related Work\n\nThe development of Convolutional Neural Networks (CNNs) has fundamentally reshaped the field of computer vision by enabling end-to-end learning of hierarchical visual representations directly from raw pixel data. Early CNN-based systems demonstrated the feasibility of learning spatially localized features using convolution and pooling operations, achieving strong performance on document recognition and digit classification tasks [lecun1998gradient]. However, the widespread adoption of CNNs for large-scale visual recognition only became practical with the availability of large annotated datasets and increased computational power, culminating in the success of deep architectures on the ImageNet benchmark [krizhevsk"
  },
  {
    "title": "VIBE: Visual Instruction Based Editor",
    "url": "https://arxiv.org/abs/2601.02242v1",
    "source": "arxiv",
    "summary": "Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines,",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Production-oriented open editors and efficiency constraints.\n2.2 Architectures for Conditioning the Source Image\n2.3 Architectures for Interpreting Instructions\n2.4 Training Pipelines, Data, and Alignment\n2.5 Consistency and Real-World Instruction Distributions\n\n\n\n3 Method\n\n3.1 Reference Image Guidance\n\n3.2 Textual Guidance Based on VLM\n\nInterface between VLM and Diffusion model\n\n\n3.3 Connector Design\n\n3.4 Training Approach\n\nConnector Alignment\nT2I Data Injections\nMulti-stage training\n\n\n\n3.5 Preference Alignment\n\nPreliminaries\nPost-training\n\n\n3.6 Implementation Details\n\n\n4 Assessor\n\n5 Datasets\n\nFor pretraining,\nFor T2I,\nFor SFT,\nFor DPO,\n\n5.1 Pretraining\n\nUltraEdit Remake\n\n\n\n5.2 Supervised Fine-Tuning datasets\n\n5.2.1 Real Tripod Photos\n5.2.2 Real Triplets from Videos\n5.2.3 Virtual Try-On\n5.2.4 Stylization\n5.2.5 Visual Concept Sliders\n5.2.6 Autonomous triplet-mining pipelines\n5.2.7 Automated Inpaint\n5.2.8 Perception and Recognition Datasets\n5.2.9 Open Source Datasets\n\n\n\n5.3 Generation Augmented Retrieval based Dataset\n\nData format and motivation\nImage Data Sources\nDiscovering an edit taxonomy\nImage-Conditioned Instruction Generation\nRetrieval-based grounding to real user phrasing\nMitigating bias toward popular prompts\nValidating instruction applicability to the image\nGenerating target images\nInverted and composite instructions.\n\n\n5.4 Issues and Filtering\n\n5.5 Synthetic Augmentation Pipeline\n\nBidirectional Photometric and Restoration Operations.\nInstruction Adherence and Invariance.\nStructural and Typographic Editing.\nReal-world Quality Adaptation.\n\n\n5.6 DPO Data Preparation\n\n\n\n6 Results\n\n\n6.1 Ablation Studies\n\nReference Image Guidance: sequence-wise vs. channel-wise\nTextual Guidance: Meta Tokens and VLM Connector Design\n\n\n6.2 Benchmarks and Metrics\n6.3 Comparison with Existing Methods\n\n\n7 Conclusions\n8 Limitations\n9 Future works\n\n\n\n\n\nVIBE: Visual Instruction Based Editor\n\n\nGrigorii Alekseenko* ‚Ää\nAleksandr Gordeev* ‚Ää\nIrina Tolstykh* ‚ÄÇBulat Suleimanov ‚Ää\nVladimir Dokholyan \n\nGeorgii Fedorov ‚Ää\nSergey Yakubson ‚Ää\nAleksandra Tsybina ‚Ää\nMikhail Chernyshov ‚Ää\nMaksim Kuprashevich‚Ä†\n\nR&amp;D Department, SALUTEDEV \n\n*Equal contribution ‚ÄÉ‚ÄÇ‚Äâ‚Ä†Corresponding author\n\n\n\n\nFigure 1: Illustrative examples of image edits generated by VIBE.\n\n\nFigure 2: Illustrative examples of image edits generated by VIBE.\n\n\n\n\n1 Introduction\n\nInstruction-based image editing models allow visual content to be modified according to natural-language commands and promise to democratize content creation. Compared to traditional retouching tools, which require substantial expertise, such generative models offer intuitive, language-based interfaces that are accessible to non-experts. Consequently, instruction-guided editing has become one of the most active directions in generative AI.\n\n\nRecent proprietary systems have demonstrated rapid progress, including Google Nano Banana Pro [nano_banana_pro] (Gemini 3 Pro Image [google2025nanobananaapi]), OpenAI‚Äôs GPT Image 1.5 [openai2025chatgptimages] [openai2025gptimage15docs], and Black Forest Labs‚Äô FLUX.1 Kontext models [flux_kontext]. In contrast, open-source research generally trails in both quality and usability. Most open models remain large (6B to 20B parameters) and expensive to train and iterate on, which slows experimentation and limits accessibility [liu2025step1xedit].\n\n\nMany practical systems start from a pretrained text-to-image diffusion backbone and adapt it to instruction-based editing. Under this setting, diffusion-based editing is shaped by three design axes: (i) how the reference image is injected, (ii) how the instruction is interpreted, and (iii) how the training pipeline is constructed.\n\n\nFor reference-image guidance, two common families are (a) channel-wise concatenation of reference latents or features [brooks2023instructpix2pix] and (b) tokenizing visual content and feeding it through the model as part of the input sequence [liu2025step1xedit].\n\n\nFor textual guidance, a key architectural choice is whether to rely mainly on the diffusion backbone‚Äôs native text conditioning [flux_kontext], or to add an external model that rewrites, expands, or structures the edit intent before conditioning the generator [fu2023mgie]. Many widely used text-to-image diffusion backbones are optimized for text-conditioned generation and therefore rely on text-only conditioning modules (e.g., CLIP [radford2021clip], T5 [raffel2020t5], or even an LLM as in Sana1.5 [xie2024sana]). In such pipelines, the conditioning module cannot observe the source image, so it cannot interpret the instruction in the context of the reference content. For image editing, this joint interpretation is often essential. The model must ground the request in what is actually in the input image to resolve ambiguity and preserve source-faithful details. We therefore use an instruction-tuned VLM that ingests both the instruction and the source image and produces a clearer, image-aware conditioning signal for the diffusion.\n\n\nSince the diffusion backbone still expects conditioning in the representation space of its native text encoder, an additional design decision is the connector that maps the VLM representations into the diffusion model‚Äôs conditioning space [fu2023mgie, liu2025step1xedit].\n\n\nThis paper investigates these architectural questions under strict efficiency constraints. We target low-cost inference by combining computationally efficient channel-wise concatenation with a learnable meta-tokens mechanism¬†[pan2025metaqueries].\n\n\nWe train with a four-stage pipeline:\n\n\n‚Ä¢\n\nAlignment: adapting a VLM to interface with the latent diffusion space via a text-to-image objective on high-aesthetic samples.\n\n\n\n‚Ä¢\n\nPre-training: learning core editing capabilities by adding image-to-image tasks on large-scale, relatively noisy data.\n\n\n\n‚Ä¢\n\nSupervised Fine-Tuning: carefully tuning on clean and diverse triplets.\n\n\n\n‚Ä¢\n\nDirect Preference Optimization (DPO) [wallace2024diffusion]: aligning the model using high-quality preference data with real-world instructions.\n\n\n\n\n\nThe proposed pipeline is flexible and can be applied to other LLM/VLM and diffusion backbones. It also supports backbones that rely on relatively lightweight text encoders, such as the CLIP text encoder [radford2021clip], because the alignment stage explicitly bridges the language model and the diffusion latent space.\n\n\nAnother focus of our approach is to adopt a model for real-world challenges, rather than for technical benchmarks. We focus on real user requests and curate or synthesize instructions that better match human phrasing than templated or purely LLM-generated prompts.\n\n\nThe data collected for this pipeline spans diverse sources and is optimized for low noise and in-the-wild distributions. We combine specialist-model pipelines, distilled signals from both open and proprietary editing systems, autonomous triplet-mining pipelines, filtered open-source image editing and computer vision datasets, manually collected tripod-captured photographs, and additional sources. We also apply extensive augmentation, in particular, the pipeline relies heavily on triplet inversion and bootstrapping, which reduces data cost in both compute and annotation.\n\n\nHistorically, different instruction-guided image editing methods assume different tolerances for unintended modifications to the source image, including the degree to which pixel-level appearance, scene composition, subject identity, and other attributes must be preserved. In this work, we target strict source consistency: any change not explicitly requested by the instruction is treated as an issue and addressed throughout all stages of training and evaluation. This objective is particularly challenging for edit categories that intrinsically encourage global transformations, such as style transfer.\n\n\nTo maintain dataset quality, we use a multi-stage filtering framework, including learned triplet scoring via a fine-tuned Gemini-based validator and auxiliary checks such as face-embedding constraints for identity preservation and image-quality scoring to prevent quality degradation.\n\n\nIn summary, our primary contributions are:\n\n\n1.\n\nWe present an open-source, ultra-fast, compact instruction-guided image editing system trained on ‚âà15\\approx 15 million triplets, based on Qwen3-VL-2B-Instruct [qwen3vl2025] and the Sana1.5-1.6B diffusion model [xie2024sana].\n\n\n\n2.\n\nWe propose a flexible four-stage training pipeline that can be adapted to different diffusion backbones and LLM/VLM front-ends, enabled by our architectural choices.\n\n\n\n3.\n\nWe provide results, analysis, and insights covering experimental design, data collection, augmentation, and filtering, along with ablation studies.\n\n\n\n\n\n\n\n2 Related Works\n\nInstruction-based image editing has rapidly evolved, with progress driven by innovations in model architectures, guidance mechanisms, and training strategies. Early methods were often training-free, operating directly on pretrained diffusion models via inversion or attention control\n[hertz2022prompt, mokady2023null, cao2023masactrl, tumanyan2023plug, feng2025dit4edit].\nWhile cost-efficient, these approaches struggle to achieve high-quality results.\nAs a result, the field has shifted toward training-based paradigms that fine-tune diffusion backbones on large-scale triplets\n[brooks2023instructpix2pix, zhang2023magicbrush, hui2024hq, ge2024seed, zhao2024ultraedit, yu2025anyedit].\nInterestingly, many widely used training triplets were bootstrapped with earlier editing systems, underscoring the tight coupling between scalable data generation and model progress\n[brooks2023instructpix2pix, zhang2023magicbrush, zhao2024ultraedit].\n\n\n\n2.1 Production-oriented open editors and efficiency constraints.\n\nDespite rapid progress, production-level editing quality remains concentrated in a limited number of systems.\nRecent open foundation editors increasingly unify text-to-image generation and instruction-based editing within a sing"
  },
  {
    "title": "From Mice to Trains: Amortized Bayesian Inference on Graph Data",
    "url": "https://arxiv.org/abs/2601.02241v1",
    "source": "arxiv",
    "summary": "Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challen",
    "full_text": null
  },
  {
    "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
    "url": "https://arxiv.org/abs/2601.02236v1",
    "source": "arxiv",
    "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference r",
    "full_text": null
  },
  {
    "title": "ELLA: Efficient Lifelong Learning for Adapters in Large Language Models",
    "url": "https://arxiv.org/abs/2601.02232v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the r",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 Method\n\n3.1 Problem Formulation\n\n3.2 Subspace-Aware Continual Adaptation\n\nFormal Characterization:\n\n\n\n\n\n4 Experiments\n\n4.1 Datasets\n4.2 Metrics\n4.3 Baselines\n4.4 Implementation Details\n4.5 Results\n4.6 Task Generalization\n4.7 Scale to Larger Models\n4.8 Efficiency Analysis\n\n\n5 Discussions\n6 Conclusion\n7 Limitations\n8 Ethical Considerations\n9 AI Writing Statement\nA Theoretical Analysis of the\nELLA Regularizer\n\nB Experimental Settings\n\nB.1 Datasets\nB.2 Task Sequence Orders\nB.3 Baselines\nB.4 Instruction Tuning.\nB.5 Task Instructions\nB.6 Implementation Details\n\n\nC Artifact Licenses\n\n\n\n\n\nELLA: Efficient Lifelong Learning for Adapters in Large Language Models\n\n\nShristi Das Biswas\nPurdue University \n\n&amp;Yue Zhang \nAWS \n\n&amp;Anwesan Pal \nAWS AI Labs \n\n&amp;Radhika Bhargava \nAWS \n\n&amp;Kaushik Roy \nPurdue University\nWork conducted at Amazon.\n\n\nAbstract\nLarge Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to 9.6%9.6\\% and a 35√ó35\\times smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model‚Äôs zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.‚ãÜ\n\n\n\nELLA: Efficient Lifelong Learning for Adapters in Large Language Models\n\n\n\n\nShristi Das Biswas‚Ä†‚Ä†thanks: Work conducted at Amazon.\n\nPurdue University\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nYue Zhang\n\nAWS\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nAnwesan Pal\n\nAWS AI Labs\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nRadhika Bhargava\n\nAWS\n\n‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚Äâ\n\n\n\nKaushik Roy\n\nPurdue University\n\n\n\n\n11footnotetext: https://sites.google.com/view/ella-llm/home.\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable generalization capabilities across a wide range of downstream tasks, largely attributed to their large-scale pretraining on diverse corpora¬†brown2020language; touvron2023llama; achiam2023gpt. As LLMs are increasingly deployed in real-world applications, they must be continuously adapted to evolving user needs and task distributions for long-term practical deployment. This setting, commonly studied under the paradigm of continual learning (CL), requires models to acquire new knowledge sequentially without costly full-retraining¬†ruvolo2013ella. However, sequential finetuning of LLMs remains highly susceptible to catastrophic forgetting (CF) - the tendency to overwrite prior knowledge when new tasks are introduced¬†mccloskey1989catastrophic, and loss of plasticity - deterioration in the ability to learn new information over time¬†dohare2021continual. These challenges are especially pronounced in rehearsal-free settings, where previously seen data cannot be stored due to privacy or storage constraints¬†chaudhry2019continual.\n\n\nA promising line of work leverages parameter-efficient fine-tuning (PEFT) strategies, such as Low-Rank Adaptation (LoRA)¬†hu2022lora, to reduce the overhead of task-specific adaptation. Adapter-based approaches restrict updates to a small set of trainable parameters while leaving the base model frozen, making them a natural fit for lower-compute CL¬†qin2021lfpt5; song2023conpet. Yet, even with such modularity, sequential adapter training often yields severe forgetting when past knowledge is not revisited¬†wang2024inscl. Prior efforts to address this include expanding model capacity¬†wang2024rehearsal, isolating weights to reduce interference¬†aljundi2017expert; li2019learn; wang2023rehearsal, subspace orthogonality¬†wang2023orthogonal; liao2025data, or gradient projection from previous tasks¬†qiao2024learn. While effective to varying degrees, these often either limit forward knowledge transfer from past tasks, add storage overhead, or overlook activation-level interference, where forgetting actually manifests¬†ke2021achieving.\n\n\nA truly effective continual learning framework should balance knowledge retention with representation reuse, preventing harmful interference while allowing useful subspaces to be shared for new tasks¬†wu2024continual. In practice, not all overlap is harmful ‚Äì low-magnitude directions from past tasks may encode generic linguistic or semantic patterns that can accelerate learning on future tasks if safely reused. Unfortunately, existing CL methods either eliminate all overlap or fuse past knowledge through heavyweight mechanisms like controller networks or rank-conditioned fusion, which limits scalability and increases complexity¬†zhao2024sapt; liu2023vida; liao2025data.\n\n\nIn this work, we propose ELLA, a simple and scalable CL framework for LLMs that mitigates forgetting without relying on replay buffers, parameter storage overheads or additional routing heuristics. ELLA introduces a subspace-aware regularization strategy that operates directly in weight space: we track the representational subspaces induced by past adapters and penalize updates that cause the new task‚Äôs adapter to align too closely with them. This cross-task de-correlation simultaneously encourages task-specific specialization by preserving prior representational geometry to reduce interference, while permitting the reuse of low-magnitude directions to enable forward transfer.\nAs we formally prove in Appendix¬†A, this selective regularization corresponds to an anisotropic shrinkage operator that provably bounds interference, providing a theoretical foundation for ELLA‚Äôs ability to balance stability and plasticity.\n\n\nThrough extensive experiments on the Standard CL Benchmark¬†zhang2015character, Long Sequence Benchmark¬†razdaibiedina2023progressive and TRACE¬†wang2023trace, we demonstrate that ELLA achieves state-of-the-art performance while scaling effectively across model sizes (from 770‚ÄãM770M to 8‚ÄãB8B). Furthermore, ELLA generalizes well across architecture families (e.g., T5¬†raffel2020exploring, LLaMA¬†touvron2023llama), and unlike prior work, uniquely improves generalization to unseen tasks. ELLA does not rely on task identities during inference, and is hence naturally compatible with the instruction-tuning paradigm¬†wang2022task, thereby preserving the generalization capabilities of LLMs in zero-shot and open-ended settings. Notably, our method can also be seamlessly integrated with existing continual learning methods to further enhance their effectiveness, without requiring additional supervision or auxiliary components. Moreover, ELLA is architecture-agnostic,\nrequires no access to past data, and introduces negligible computational or memory overhead. In summary, our contributions are as follows:\n\n\n‚Ä¢\n\nWe propose ELLA, a replay-free plug-and-play CL framework for LLMs that balances plasticity and mitigates CF via subspace-aware regularization, and we provide a formal theoretical analysis of its properties.\n\n\n\n‚Ä¢\n\nWe provide extensive empirical evidence that ELLA establishes a new state-of-the-art in both performance and efficiency across three popular CL benchmarks, decisively outperforming prior methods without task labels, replay, or additional overhead.\n\n\n\n‚Ä¢\n\nWe show that ELLA scales effectively across architectures and maintains strong generalization to unseen tasks, highlighting its practical advantages for real-world deployment.\n\n\n\n\n\n\n\n2 Related Works\n\nContinual Learning ‚ÄÇ\nThe primary challenge in CL is to balance model stability (resisting catastrophic forgetting) with plasticity (acquiring new knowledge). Prior approaches to address this problem can be broadly categorized by their core mechanism, each presenting a distinct set of trade-offs.\n(i) Rehearsal-based methods store a subset of past data and interleave it during subsequent training phases. These include strategies such as experience replay¬†riemer2018learning or constrained optimization¬†aljundi2017expert; chaudhry2019continual; he2024seekr that jointly train on current and previous samples. While empirically strong, this approach is often impractical due to significant storage overhead and potential violations of data privacy constraints.\n(ii) Regularization-based methods add penalty terms to the loss that restrict updates to parameters critical for old tasks¬†du2024unlocking; li2017learning; de2019episodic. For instance,¬†kirkpatrick2017overcoming slows updates on important weights, while¬†farajtabar2020orthogonal constrains new updates to be orthogonal to gradients of old tasks. These methods depend on importance metrics that are often brittle and prevent any forward transfer, limiting adaptability to new tasks.\n(iii) Architecture-based methods reduce interference through task-specific modules or by expanding model size¬†li2019learn; wang2023rehearsal. While¬†razdaibiedina2023progressive appends a new learned soft prompt per task, ¬†qin2021lfpt5 utilizes a large\nsoft prompt that is continuously trained on all tasks. W"
  },
  {
    "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
    "url": "https://arxiv.org/abs/2601.02224v1",
    "source": "arxiv",
    "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forec",
    "full_text": null
  },
  {
    "title": "LLM-Empowered Functional Safety and Security by Design in Automotive Systems",
    "url": "https://arxiv.org/abs/2601.02215v1",
    "source": "arxiv",
    "summary": "This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of mes",
    "full_text": null
  }
]