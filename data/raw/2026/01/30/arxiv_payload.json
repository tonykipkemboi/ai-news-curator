[
  {
    "title": "RedSage: A Cybersecurity Generalist LLM",
    "url": "https://arxiv.org/abs/2601.22159v1",
    "source": "arxiv",
    "summary": "Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Cybersecurity Benchmarks\n2.2 Cybersecurity Datasets and Models\n\n\n\n3 RedSage\n\n3.1 RedSage Pre-training Data\n3.2 RedSage Post-training Data\n\n3.3 RedSage Benchmark\n\nMulti-stage verification.\nHuman quality control.\nData decontamination.\n\n\n3.4 RedSage Training\n\n\n\n4 Experiments and Results\n\n4.1 Evaluation Results on RedSage-Bench\n4.2 Evaluation Results on Cybersecurity Benchmarks\n4.3 Evaluation Results on General Benchmarks\n\n\n5 Discussion and Limitations\n6 Conclusion\n7 Ethics Statement\n8 Reproducibility Statement\n\nA Dataset Details\n\nA.1 CyberFineWeb\nA.2 RedSage Seed\nA.3 RedSage Conversation\nA.4 RedSage Benchmarks\n\n\n\nB Training Details\n\nB.1 Pre-training Details\nB.2 Post-training Details\nB.3 Estimated Training Time and Computational Cost Analysis\n\n\n\nC Evaluation Details\n\n\nC.1 Evaluation Setup\n\nCompared methods.\n\n\nC.2 RedSage Benchmarks\n\nC.3 Cybersecurity Benchmarks\n\nCyberMetric (CyMtc).\nSecBench (ScBen).\nMMLU Computer Security (MMLU-CSec).\nSECURE.\nCTI-Bench.\nSecEval (ScEva).\n\n\n\nC.4 General LLM Benchmarks\n\nARC-Challenge (ARC-C).\nHellaSwag (HSwag).\nTruthfulQA (TQA).\nMMLU.\nWinoGrande (WinoG).\nGSM8K.\nIFEval.\n\n\n\n\n\nD Additional Evaluation Results\n\nD.1 Larger Model Scaling\n\n\n\n\n\n\n\n\n\\minted@def@optcl\nenvname-P envname#1\n\n\n\n\n\nRedSage: A Cybersecurity Generalist LLM\nNaufal Suryanto1, Muzammal Naseer1,∗, Pengfei Li1, Syed Talal Wasim2, Jinhui Yi2, Juergen Gall2, Paolo Ceravolo3, Ernesto Damiani3\n1Khalifa University, 2University of Bonn, 3University of Milan\n∗Project lead\nCybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools.\nBuilding on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training.\nTo rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&amp;A items covering cybersecurity knowledge, skills, and tool expertise.\nRedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.\nProject Page: https://risys-lab.github.io/RedSage/\n\n\n\nFigure 1: Overview of the RedSage pipeline. RedSage is trained through continual pre-training on cybersecurity-filtered corpora and post-training with curated and augmented conversation data, followed by evaluation on a comprehensive benchmark covering knowledge, skills, and tool expertise.\n\n\n\n1 Introduction\n\nThe rapid evolution of cybersecurity threats has elevated the need for proactive and comprehensive defense strategies, as organizations face increasingly sophisticated attacks and advanced persistent threats (Che Mat et al., 2024). Modern cybersecurity involves a wide range of critical tasks, including threat analysis, incident response, vulnerability management, and security monitoring. However, the complexity of security tools and the level of expertise required to operate them present significant challenges. These challenges are compounded by a global skills shortage, with research estimating a demand–supply gap of millions of unfilled cybersecurity positions ((ISC)š, 2024). Consequently, there is growing momentum to employ cybersecurity-tuned LLMs to augment human analysts.\n\n\nRecent efforts have produced cybersecurity-trained LLMs, yet most emphasize a single training stage while overlooking others. For instance, some extend pretraining on domain-specific corpora (Kassianik et al., 2025) but apply limited post-training with only 835 samples (Yu et al., 2025) or fewer than 30K cybersecurity-filtered items (Weerawardhena et al., 2025), while others focus on supervised fine-tuning with large cybersecurity Q&amp;A collections without pretraining to strengthen domain knowledge (Deep Hat, 2025). Further, existing cybersecurity benchmarks offer only partial coverage, such as omitting tool proficiency and qualitative evaluation of free-response Q&amp;A beyond simple MCQs (see Table 1 and Fig. 2). Beyond these gaps, most works also do not release their data or pipelines, limiting reproducibility and openness (see Table 2).\n\n\nTo address these gaps, we present RedSage (Retrieval-Enhanced Data-driven Security Assistant Guidance and Evaluation), an open-source LLM tailored for cybersecurity. As illustrated in Fig. 1, RedSage integrates large-scale continual pretraining on cybersecurity-filtered corpora, post-training with curated and agentically augmented datasets, and rigorous evaluation across knowledge, skills, and tool proficiency. Our key contributions are:\n(1) assembling an 11.8B-token corpus of cybersecurity data for domain-specific continual pretraining,\n(2) constructing a 266K-sample augmented dataset via an agentic pipeline for supervised fine-tuning, followed by preference alignment with open-source data,\n(3) introducing RedSage-Bench, a benchmark with 30K MCQs for broad coverage and 240 open-ended Q&amp;A items for quality evaluation across knowledge, skills, and tools, and\n(4) RedSage, an open 8B model with data and code, achieving state-of-the-art results on established cybersecurity benchmarks while also improving on general benchmarks.\n\n\n\n\n\nTable 1: \nComparison of cybersecurity LLM benchmarks. Columns indicate knowledge (Know.), skills (Skill), tool proficiency (Tool), and use of quality scoring (Qual.). Size = total samples. Agentic CTF benchmarks (e.g., NYU-CTF, CyBench) are excluded as they are interactive rather than base LLM eval.\n\n\n\n\n\n\\rowcolorGray\nName\n\nKnow.\nSkill\nTool\nQual.\nSize\n\n\nSecEval\n✓\n✗\n✗\n✗\n2,000\n\n\nCyberMetric\n✓\n✗\n✗\n✗\n10,000\n\n\nCyberBench\n✓\n✗\n✗\n✗\n80,422\n\n\nSECURE\n✓\n✗\n✗\n✗\n4,072\n\n\nCS-Eval\n✓\n✗\n✗\n✗\n4,369\n\n\nSecBench\n✓\n✗\n✗\n✗\n47,910\n\n\nCTI-Bench\n✓\n✓\n✗\n✗\n5,610\n\n\nCyberSecEval\n✗\n✓\n✗\n✗\n1,000\n\n\nRedSage-Bench (Ours)\n✓\n✓\n✓\n✓\n30,240\n\n\n\n\n\n\nFigure 2: \nTaxonomy of RedSage Seed&amp;Bench dataset.\nIt spans knowledge, practical offensive skills,\nand tool expertise (CLI and Kali Linux).\n\n\n\n\n\n\n\n2 Related Works\n\n\n2.1 Cybersecurity Benchmarks\n\nGeneral Knowledge.\nSeveral benchmarks assess LLMs’ understanding of core cybersecurity concepts via structured Q&amp;A.\nSecEval (Li et al., 2023) includes 2K+ MCQs across nine domains (web, system, application security).\nCyberMetric (Tihanyi et al., 2024) provides 10K MCQs generated with RAG and expert validation, spanning penetration testing and network security.\nCyberBench (Liu et al., 2024) extends beyond MCQs to tasks such as NER, summarization, and classification.\nSECURE (Bhusal et al., 2024) targets Industrial Control Systems with domain-specific MCQs on risk reasoning and vulnerability analysis.\nCS-Eval (Yu et al., 2024) covers 42 subcategories across three cognitive levels (Knowledge, Ability, Application) using MCQs, multi-answer, T/F, and open-ended items.\nSecBench (Jing et al., 2025) offers 44,823 MCQs and 3,087 SAQs in Chinese and English, capturing both factual recall and logical reasoning.\n\n\nApplications and Agentic Tasks.\nApplication-oriented benchmarks probe reasoning beyond recall.\nCTIBench (Alam et al., 2024) defines four tasks: MCQs, common vulnerabilities and exposures(CVE)-to-common weakness enumeration(CWE) mapping, common vulnerability scoring system (CVSS) prediction, and threat actor attribution in cyber threat intelligence.\nCyberSecEval (Wan et al., 2024) examines model risks across eight areas (e.g., exploit generation, prompt injection).\nAgentic evaluations such as NYU-CTF (Shao et al., 2024) and CyBench (Zhang et al., 2025) assess red-team capabilities through capture the flag (CTF) challenges (e,g web exploitation, reverse engineering) in interactive settings.\n\n\nWhile these efforts advance evaluation of knowledge and applications, they rarely isolate competence in understanding and operating security tools or systematically assess the quality of free-form responses. As summarized in Table 1, most benchmarks specialize in either knowledge or applications, and even agentic ones lack explicit tool-focused assessment. We address these gaps with RedSage-Bench, which jointly measures knowledge, skills, and tool proficiency (Fig. 2).\n\n\n\n\n2.2 Cybersecurity Datasets and Models\n\nEarly Cybersecurity Datasets. Early domain-specific models such as CyBERT (Ranade et al., 2021), SecureBERT (Aghaei et al., 2023), and CTI-BERT (Park and You, 2023) showed the value of domain-adaptive fine-tuning. However, their datasets were not released. Moreover, as encoder-based models, they require task-specific fine-tuning, restricting scalability.\n\n\nCybersecurity Datasets for LLMs. With the advent of LLMs, several groups curated cybersecurity-specific corpora.\nPRIMUS (Yu et al., 2025) (Trend Micro) provides 2.75B tokens for continued pretraining, 835 samples for supervised fine-tuning, and reasoning distillation, extending Llama-3.1-8B-Instruct into Llama-Primus-Base and -Merged.\nFoundation-Sec-8B (Kassianik et al., 2025) (Cisco) collects 5.1B tokens via large-scale scraping and fil"
  },
  {
    "title": "Discovering Hidden Gems in Model Repositories",
    "url": "https://arxiv.org/abs/2601.22157v1",
    "source": "arxiv",
    "summary": "Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fin",
    "full_text": null
  },
  {
    "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
    "url": "https://arxiv.org/abs/2601.22156v1",
    "source": "arxiv",
    "summary": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RN",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\nRNN-Attention Hybrid Models\nPosition Encoding in Hybrid Models\nDistilling Transformers into Hybrid Models\n\n\n\n3 Preliminaries\n\nNotations\nSoftmax Attention Layers\nModern RNN Layers\n3.1 The Impact of Attention Layer Selection when Distilling Transformers into Hybrids\n3.2 The Importance of Position Encoding for Language Modeling and Length Generalization\n\n\n\n4 HALO: An Efficient Pipeline to Distill Transformers into Hybrids\n\n4.1 Initialization Stage: Attention Weight Transfer\n4.2 Stage 1: Hidden State Alignment\n4.3 Attention Layer Selection\n4.4 Stage 2: Knowledge Distillation\n4.5 Stage 3: Finetuning\n\n\n\n5 HypeNet: An Effective Attention-RNN Hybrid Architecture\n\n\n5.1 HyPE: Hybrid Positional Encoding (➊)\n\nMotivation\nAttention Logits Scaling\nConversion Details\n\n\n\n5.2 Other Architectural Modifications\n\nQK-Normalization (➋)\nGQA to MHA (➌)\nOutput Gate (➍)\nIncreased Model Size\n\n\n5.3 RNN Mixer\n\n\n\n6 Experiments\n\n\n6.1 Experimental Setup\n\nModels\nTraining Configurations\nEvaluation\nEvaluation Data for Layer Selection\nEfficiency Measurement\n\n\n\n6.2 Main Results: Distilling from Qwen3\n\nTakeaway 1\nTakeaway 2\n\n\n\n6.3 HypeNet Ablations: Training From Scratch\n\nPosition Encoding\nDifferent RNN Mixers\nTakeaway\n\n\n6.4 HALO Ablations: Architectural Modifications\n6.5 HALO Ablations: Attention Layer Selection\n6.6 Efficiency Results\n\n\n7 Conclusion\n\nA Complete Formulation of HypeNet\n\nAttention Layers\nRNN Layers\nForget Gate\n\n\nB HALO Training Configurations\nC HypeNet Model Configurations\n\nD Addition Notes on the Model Architecture\n\nShort Convolution\n\n\n\nE Computational Cost of Each Stage\n\nThe Number of Tokens Required by KL-Guided Layer Selection\nE.1 RNN Mixer Efficiency Measurement\n\nE.2 More Evaluation Details\n\nQwen3 YaRN\nDownstream Tasks for CSR\n\n\n\n\n\nF More Experimental Results\n\n\nF.1 Attention Logits Scaling Validation\n\nF.1.1 HALO Configuration Ablation Experiments\n\n\n\n\n\nG Which RNN Mixers are Compatible with HypeNet?\n\n\nG.1 HypeNet’s Compatibility with Mamba2\n\nMulti-Head Mechanism\n\n\nG.2 A Note on Kimi Delta Attention\n\n\n\nH Training and Model Configurations for Training From Scratch Experiments\n\nH.1 Training Configurations\nH.2 Model Configurations\n\n\n\n\n\n\n\nHybrid Linear Attention Done Right:\nEfficient Distillation and Effective Architectures for Extremely Long Contexts\n\n\nYingfa Chen\n\n  \nZhen Leng Thai\n\n  \nZihan Zhou\n\n  \nZhu Zhang\n\n  \nXingyu Shen\n\n  \nShuo Wang\n\n  \nChaojun Xiao\n\n  \nXu Han\n\n  \nZhiyuan Liu\n\n\n\nAbstract\nHybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch.\nSome recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation.\nHowever, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models.\nIn this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications.\nWe convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data111The code and model checkpoints can be found at: https://github.com/THUNLP/hybrid-linear-attention..\n\nMachine Learning, ICML\n\n\n\n1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing, China  2OpenBMB\n{chenyingfa1999, thaizhenleng123}@gmail.com \nwangshuo.thu@gmail.com, han-xu@tsinghua.edu.cn \n\n\n11footnotetext: Equal contributions.\n\n\n1 Introduction\n\nTransformer-based language models (Vaswani et al., 2017) rely on softmax attention blocks, which have a quadratic complexity with respect to the context length, making them prohibitively expensive for long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention (Katharopoulos et al., 2020) and state space models (Gu and Dao, 2024) are much faster for long-context modeling due to their linear complexity.\nHowever, pure RNN models with fixed-size states generally underperform softmax attention, particularly on recall-intensive tasks (Jelassi et al., 2024; Yang et al., 2025b). To address this gap, there is a surge in interest in hybrid architectures that interleave attention and RNN layers222We hereby use hybrid architectures/models to refer to architectures/models that consist of softmax attention and RNN layers., achieving a favorable tradeoff between model performance and inference throughput (Lieber et al., 2024; MiniMax et al., 2025; Qwen, 2025; Kimi et al., 2025; NVIDIA et al., 2025).\n\n\nFigure 1: Left &amp; center: the performance-efficiency tradeoff of our model, HypeNet, versus the Qwen3 series, measured with 128K context length and BFloat16 precision. Right: the time per output token of the 1.7B models across different context lengths. For 1M context length, the Qwen3 model runs out of GPU memory. HypeNet is converted from Qwen3 using our distillation procedure, HALO, and has better performance-efficiency tradeoff than Qwen3.\n\n\nHybrid architectures are typically pre-trained from scratch at a large scale (Qwen, 2025; NVIDIA et al., 2025), placing them beyond the reach of most academic research teams.\nHence, some works focus on distilling pre-trained Transformer models into hybrid architectures (Gu et al., 2025; Hoshino et al., 2025; Wang et al., 2025b). These distillation methods use far fewer training tokens and produce hybrid models that are comparable to their Transformer counterparts on various common-sense reasoning (CSR) tasks. Although distilled hybrid models typically underperform those trained from scratch, they are valuable since they allow teams without resources to scale up pre-training to validate research ideas.\n\n\nHowever, these distillation methods still suffer from two critical limitations. (1) Most distillation methods still require tens to hundreds of billions of training tokens, which is still out of reach for most teams in academia. (2) While the resulting hybrid models have short-context performance comparable to Transformer models, they exhibit severe performance degradation on long-context tasks, which is precisely the scenario where they are preferred over Transformer models.\n\n\nTo address these challenges, we first propose HALO (Hybrid Attention via Layer Optimization), a novel cross-architecture distillation procedure for converting pre-trained Transformer models into hybrid models. Notably, HALO involves an efficient attention layer selection method for determining which attention layers to keep unconverted to ensure the best long-context performance. Then, we propose Hybrid Position Encoding (HyPE), a position encoding scheme with strong length generalization, specifically designed for hybrid architectures. In addition to HyPE, we propose a series of architectural improvements, validated with careful ablation experiments on models with over 1B parameters. The combination of these improvements results in HypeNet, a series of hybrid models converted from the Qwen3 series, with a much better performance-throughput tradeoff, as shown in Figure 1.\n\n\nOur contributions can be summarized as follows:\n\n\n•\n\nWe develop a novel cross-architecture distillation procedure that converts Transformer models into attention-RNN hybrid models using fewer than 3B tokens, thereby significantly improving the model’s efficiency in long-context scenarios.\n\n\n\n•\n\nWe present HyPE, a novel position-encoding scheme that combines RoPE (Su et al., 2023) and NoPE (Kazemnejad et al., 2023), designed for hybrid models. Coupled with an attention scaling mechanism, HyPE achieves superior length generalization.\n\n\n\n•\n\nBased on HyPE, we propose HypeNet, a novel hybrid architecture that incorporates multiple architectural improvements when converting from a pre-trained Transformer model.\n\n\n\n\n\n\n\n2 Related Works\n\nTable 1: Existing attention-to-hybrid distillation methods and their release date and training tokens required.\n\n\nMethod\nDate\nTokens\n\n\n\nMamba-in-the-Llama (Wang et al.)\n\nAug. 2024\n20B\n\n\n\nSMART (Yang et al.)\n\nMay 2025\n\n&gt;&gt;7B\n\n\n\n\nRAD (Hoshino et al.)\n\nMay 2025\n20B\n\n\n\nJet-Nemotron (Gu et al.)\n\nAug. 2025\n400B\n\n\n\nKL-LS (Li et al.)\n\nDec. 2025\n25B\n\n\nHALO (ours)\nJan. 2026\n2.3B\n\n\n\n\nRNN-Attention Hybrid Models\n\nState-of-the-art hybrid models with up to hundreds of billions of parameters have exhibited performance comparable to standard Transformers on both commonsense reasoning and recall-intensive tasks (e.g., needle-in-a-haystack (NIAH) (Hsieh et al., 2024)) while being more efficient for processing long contexts (Lieber et al., 2024; MiniMax et al., 2025; Qwen, 2025; Kimi et al., 2025; NVIDIA et al., 2025). Despite their impressive performance, there are rather few publicly available hybrid models with frontier-level performance, because pre-training from scratch is prohibitively expensive for most teams. To avoid this training cost, we focus on distilling pre-trained Transformer models into hybrid models.\n\n\n\nPosition Encoding in Hybrid Models\n\nCurrent, RoPE (Su et al., 2023) has become the de facto standard position encoding (PE) for Transformer models (Yang et al., 2025a; Grattafiori et al., 2024). On the other hand, RNNs usually encode positional information through decay/transition matrices, and do not employ RoPE (Dao and Gu, 2024; Yang et al., 2025b). This has remained the case for hybrid models, which "
  },
  {
    "title": "UEval: A Benchmark for Unified Multimodal Generation",
    "url": "https://arxiv.org/abs/2601.22155v1",
    "source": "arxiv",
    "summary": "We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal gen",
    "full_text": "\n\n\n\n\n\n\n\\correspondingauthor\n∗ Co-advising\n\nUEval: A Benchmark for Unified Multimodal Generation\n\n\n\n\nBo Li  Yida Yin  Wenhao Chai  Xingyu Fu∗  Zhuang Liu∗\n\n\nPrinceton University\n \n\n\n\n \nWebsite: https://zlab-princeton.github.io/UEval\n Code\n \n Dataset\n \n Leaderboard\n\n\n\n\n\nGenerated  on Thu Jan 29 18:02:50 2026 by LaTeXML\n\n\n"
  },
  {
    "title": "Exploring Reasoning Reward Model for Agents",
    "url": "https://arxiv.org/abs/2601.22154v1",
    "source": "arxiv",
    "summary": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a mult",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Agentic Reinforcement Learning\n2.2 Reward Modeling\n\n\n\n3 Method\n\n3.1 Preliminaries: GRPO Framework\n3.2 Agentic Tool Design\n\n3.3 Dataset Construction\n\nAgent Training Datasets\nReward Model Datasets\n\n\n3.4 Agent-RRM: Reward Model Training\n\n3.5 Reagent: Integrating Reasoning Rewards into Agents\n\n3.5.1 Textual-augmented Refinement\n3.5.2 Reward-augmented Guidance\n3.5.3 Unified Feedback Integration\n\n\n\n\n\n4 Experiments\n\nBenchmarks\nImplementation Details\n4.1 Can Textual Critiques Guide Inference-Time Refinement?\n4.2 Does Model-based Reward Improve Learning?\n4.3 Does Unified Feedback Synergistically Boost Performance?\n4.4 Beyond Text-Only: Cross-Modal Reasoning and Complex Tool Use\n4.5 Parameter Analysis on λ\\lambda\n\n\n5 Conclusion\n6 Limitations\n\nA Dataset Details\n\nA.1 Agent Training Data Distribution\n\nA.2 Dataset Selection and Filtering\n\nDeepMath.\nDeepScaleR.\nSimpleRL-Zoo.\nMMK12.\nPixelReasoner.\nLiveVQA.\nToolVQA.\nSimpleDeepSearcher.\nAFM-WebAgent.\nLongAudio.\n\n\nA.3 Agent-RRM Construction Details\n\n\n\nB Training Details\n\nB.1 Training Codebase\nB.2 Tools configuration\n\n\n\nC Evaluation Details\n\nC.1 Evaluation Benchmarks\nC.2 Tool Configurations\nC.3 Agent Evaluation Settings\n\n\nD Case Study\n\n\n\n\n\nExploring Reasoning Reward Model for Agents\n\n\n\nKaixuan Fan1,2 ​​​​\nKaituo Feng1,2 ​​​​\nManyuan Zhang2 ​​​​\nTianshuo Peng1 ​​​​\nZhixun Li3\nYilei Jiang1,2 ​​​​\nShawn Chen2 ​​​​\nPeng Pei2 ​​​​\nXunliang Cai2 ​​​​\nXiangyu Yue1\n1MMLab, CUHK ​​\n2Meituan ​​\n3SEEM, CUHK\n\n\n\nRepository: https://github.com/kxfan2002/Reagent\nProject Leader.Corresponding Author.\n\n\nAbstract\nAgentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use.\nHowever, most methods still relies on sparse outcome-based reward for training.\nSuch feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results.\nIn this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance.\nLeveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration).\nExtensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes.\nCode, models, and datasets are all released to facilitate future research.\n\n\n\nExploring Reasoning Reward Model for Agents\n\n\n\n\n\nKaixuan Fan1,2  ​​​​\nKaituo Feng1,2  ​​​​\nManyuan Zhang2††thanks: Project Leader.  ​​​​\nTianshuo Peng1  ​​​​\nZhixun Li3\n\nYilei Jiang1,2 ​​​​\nShawn Chen2 ​​​​\nPeng Pei2 ​​​​\nXunliang Cai2 ​​​​\nXiangyu Yue1††thanks: Corresponding Author.\n\n1MMLab, CUHK ​​\n2Meituan ​​\n3SEEM, CUHK\n\n\n\nRepository: https://github.com/kxfan2002/Reagent\n\n\n\n\n\n\n1 Introduction\n\nReinforcement Learning with Verifiable Reward (RLVR) has achieved remarkable success in improving the reasoning capabilities of Large Language Models (LLMs) Liu et al. (2025b); Feng et al. (2025b); Tang et al. (2025); Chen et al. (2025b, c). Motivated by this progress, recent works have extended this paradigm to agents, demonstrating its potential to handle complex interactions with dynamic environments and external knowledge sources Jin et al. (2025); Wu et al. (2025a); Li et al. (2025b).\n\n\nHowever, previous agentic RL methods typically rely on sparse, outcome-based rewards based solely on final correctness  Jin et al. (2025); Wu et al. (2025a); Li et al. (2025b). This design is inherently limiting for long-horizon agentic tasks requiring multi-step tool utilization Feng et al. (2025c); Liu et al. (2025a); Zhang et al. (2025e).\nIn such settings, outcome-based supervision fails to differentiate high-quality intermediate reasoning from entirely incorrect attempts, for instance, treating a trajectory that fails only at the final step as a total failure.\nConsequently this coarse-grained binary supervision obscures the value of successful intermediate steps, resulting in sub-optimal performance Dong et al. (2025).\n\n\nTo provide more granular feedback, recent research has pivoted toward integrating Reward Models into Agentic RL. However, the effective deployment of Reward Models remains hampered by two bottlenecks. First, while step-level rewards offer finer granularity feedback Xi et al. (2025); Liu et al. (2025a); Xu et al. (2025), they are often plagued by prohibitive annotation costs Rahman et al. (2025) and a susceptibility to reward hacking Zhang et al. (2025c). Second, existing reasoning-based Reward Models focus on pair-wise preferences Li et al. (2025c); Liu et al. (2025a); Hu et al. (2025), which frequently introduces inherent biases and fails to capture fine-grained quality gradations between trajectories or provide actionable guidance for refinement Jian et al. (2025); Zhang et al. (2025d).\nFurthermore, most of these efforts exclusively rely on numeric reward feedback for training, leaving the natural language critique Zhang et al. (2025a) largely unexplored, which could provide more granular guidance for agentic policy.\n\n\nTo this end, we develop Agent Reasoning Reward Model (Agent-RRM), a multi-faceted evaluator designed to provide reasoning-aware feedback for agentic trajectories.\nUnlike conventional Reward Models that yield merely scalar scores or binary preferences, Agent-RRM conducts explicit reasoning to justify its assessments.\nFor each trajectory, it generates a decomposed judgment comprising: (1) an internal reasoning trace that analyzes logical consistency of trajectory; (2) a targeted critique identifying specific flaws to guide refinement; and (3) a holistic quality score. This hierarchy of signals provides dense, multi-dimensional supervision, combining scalar rewards for global optimization with textual critiques for explicit error correction—all without necessitating ground truth.\n\n\nBuilding upon these informative signals, we perform a systematic investigation into the integration of Agent-RRM and Agentic RL. We formalize this integration through a unified scheme with three variants: Text-augmented Refinement, where agents polish trajectories based on Agent-RRM’s textual feedback; Reward-augmented Guidance, which complements rule-based rewards with model-based signals; and Unified Feedback Integration, which harmonizes multi-source rewards with critique-augmented sampling. We denote the agent policy models of these variants as Reagent-C, Reagent-R, and Reagent-U, respectively.\nNotably, our experiments demonstrate that Reagent-U achieves superior performance by synthesizing these feedback modalities, reaching 43.7% on GAIA and 46.2% on WebWalkerQA.\nOur study provides a comprehensive roadmap for harnessing multi-level feedback to accelerate agentic RL.\n\n\nTo support this investigation, we curate four specialized datasets that provide high-quality trajectories for both agent reasoning and reward model training. Extensive experiments across 12 diverse benchmarks demonstrate that Reagent models achieve significant performance gains, underscoring the efficacy of multi-level reasoning-based feedback signals in complex agentic tasks.\n\n\nIn summary, our contributions are as follows:\n\n\n\n\n•\n\nWe introduce Agent-RRM, a multi-faceted evaluator that generates structured feedback including explicit reasoning rationales, actionable critiques, and holistic quality scores, providing a transparent and granular assessment.\n\n\n\n•\n\nWe systematically explore three agent variants with Agent-RRM: Text-augmented Refinement (Reagent-C), Reward-augmented Guidance (Reagent-R), and Integrated Feedback Optimization (Reagent-U). This provides a roadmap for using reasoning rewards to enhance agent performance.\n\n\n\n•\n\nWe curate and release four high-quality datasets specifically tailored for training reasoning agent and reward model. These resources provide the community with valuable assets to advance research in multi-granular feedback for agentic reinforcement learning.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Agentic Reinforcement Learning\n\nAgentic Reinforcement Learning (Agentic RL) has emerged as a cornerstone for developing agents capable of operating in dynamic, open-ended environments Dong et al. (2025); Lù et al. (2025).\nRecent advancements Jin et al. (2025); Wang et al. (2025); Xia et al. (2025); Wu et al. (2025c); Li et al. (2025f); Song et al. (2025) illustrate that RL can effectively instill multi-step information-seeking and tool-use proficiencies.\nFor example, Search-R1 Jin et al. (2025) demonstrates that agentic RL enables LLMs to interleave multi-turn web search, substantially improving retrieval-augmented reasoning performance.\nWebSailor Li et al. (2025b) further shows that agentic RL can scale to long-horizon web navigation, equipping agents with the ability to reduce extreme uncertainty in complex information-seeking tasks.\nAgent0 Xia et al. (2025) introduces a co-evolutionary process where tool-aware reasoning behaviors emerge without human-curated supervision.\nDespite these successes, most existing methods rely heavily on sparse, outcome-based rewards, which often limits training efficacy and hampers agent’s ability to rectify intricate intermediate errors Dong et al. (2025); Lin et al. (2025).\n\n\n\n\n2.2 Reward Modeling\n\nFigure 1: Detailed distribution information of Reagent-SFT-55.6K and Reagent-RL-709K.\n\n\nReward Models (RMs) play a central role in optimizing complex reasoning tasks by providing learning signals for policy improvement Wang et al. (2024); Fan et al. (2025); Li et al. (2025a). Motivated by Deepseek-R1 Guo et al. (2025), reasoning-aware reward models are introduc"
  },
  {
    "title": "Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing",
    "url": "https://arxiv.org/abs/2601.22151v1",
    "source": "arxiv",
    "summary": "Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such math",
    "full_text": null
  },
  {
    "title": "DynaWeb: Model-Based Reinforcement Learning of Web Agents",
    "url": "https://arxiv.org/abs/2601.22149v1",
    "source": "arxiv",
    "summary": "The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a pr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related work\n\nWeb Agent\nWorld Models\nReinforcement Learning of Agents\n\n\n\n3 Method\n\n3.1 Problem Formulation\n3.2 Web World Model\n3.3 DynaWeb: Model-based RL of Web Agents\n\n\n\n4 Experiments\n\n\n4.1 Setups\n\nBenchmarks and metrics\nWeb Agents\nWorld Models\nBaselines\n\n\n4.2 Main Results\n\n\n\n5 Analysis\n\n5.1 WM Dream Length Matters\n5.2 Effect of Real Interaction Data Percentage\n5.3 Essential Role of WM Training\n\n\n6 Conclusion\n\nA More Details About DynaWeb\n\nA.1 WebArena Training Prompt\nA.2 World Model System Prompt\nA.3 Training Recipe and Hyperparameters\n\n\n\n\n\n\n\n\n\\correspondence\njadeleiyu@cs.toronto.edu, ty.shi@mail.utoronto.ca\n\\projectty.shi@mail.utoronto.ca\n\\sourcecodeComing-Soon\n\n\nDynaWeb: Model-Based Reinforcement Learning of Web Agents\n\n\n\nHang Ding1\n Peidong Liu2\n Junqiao Wang1\n Ziwei Ji3\n Meng Cao4  8\n Rongzhao Zhang5\n Lynn Ai6\n Eric Yang6\n Tianyu Shi6\n Lei Yu7\n\n1Shanghai Jiao Tong University  2Sichuan University  3Hong Kong University of Science and Technology  \n4McGill University  5Shanghai AI Lab  6Gradient  \n7University of Toronto  8Mila - Quebec AI Institute  \n\n\n(Jan 30, 2026)\n\nAbstract\nThe development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.\n\n\n\n1 Introduction\n\nThe paradigm of artificial intelligence is rapidly shifting toward proactive, agentic systems that can autonomously execute complex, long-horizon tasks in open-ended environments.\nLarge Language Models (LLMs) have emerged as a powerful backbone for such agents, enabling rich reasoning, flexible action generation, and natural language interaction.\nIn the web domain, LLM-based agents have demonstrated strong capabilities in navigating real websites and accomplishing user-specified goals through multi-step interaction, fueled by advances in prompting, structured reasoning, and action abstractions (Yao et al., 2023; Zhou et al., 2024a; He et al., 2024).\nBeyond imitation learning, recent work has shown that training web agents with online reinforcement learning (RL) can substantially improve robustness, exploration, and long-horizon decision-making, allowing agents to surpass static demonstrations and adapt through trial-and-error interaction (Wei et al., 2025; Qi et al., 2025; Zhuang et al., 2025).\n\n\nDespite its promise, the effectiveness of online RL for web agents is fundamentally constrained by the cost and risk of real-environment interaction.\nCollecting large-scale, on-policy experience requires agents to interact directly with the live internet, which is inefficient, expensive, and difficult to control at scale.\n\n\nFigure 1: \nComparison between traditional web agent training via live web interaction and DynaWeb.\nBy replacing risky and inefficient real-world interaction with a learned web world model,\nDynaWeb enables imagination-driven training using virtual pages and dreamed trajectories,\noptionally augmented with real expert data, resulting in safer and more efficient agent optimization.\n\n\n\nDuring training, agents may trigger irreversible actions such as unintended purchases, account modifications, or data submissions, while also facing non-deterministic page dynamics, transient failures, and external interference.\nThese challenges severely limit the practicality of pure online RL, rendering large-scale policy optimization both costly and hazardous in real-world web environments (Zhou et al., 2024a; Qi et al., 2025).\nAs a result, a central open question emerges: how can we retain the benefits of online reinforcement learning for web agents while dramatically reducing reliance on direct interaction with the live web?\nA natural direction is to replace expensive and risky real-environment interaction with a learned, controllable surrogate that can faithfully approximate web dynamics.\nTo this end, recent work has begun to explore web world models—learned simulators of web environments.\nSo far, however, their role has been largely auxiliary.\nSome approaches employ world models at inference time for short-horizon look-ahead and action evaluation, treating them as decision-time reasoning tools rather than learning substrates (Gu et al., 2024; Wei et al., 2025).\nOthers use world models to synthesize offline trajectories for supervised fine-tuning or imitation-style training, decoupling model-generated experience from on-policy optimization (Fang et al., 2025a; Pahuja et al., 2025).\nWhat remains missing is an online model-based reinforcement learning (MBRL) paradigm for web agents, in which imagined rollouts serve as first-class experience that directly augments on-policy policy optimization.\n\n\nIn this work, we revisit classical model-based reinforcement learning through the lens of modern web agents.\nInspired by the Dyna architecture (Sutton, 1991) and imagination-based learning frameworks such as Dreamer (Hafner et al., 2020), we propose DynaWeb, an MBRL framework that elevates a web world model from a planning or data-generation tool to a core component of online reinforcement learning.\nDynaWeb treats the world model as a controllable synthetic web environment that can replace or augment costly real interaction.\nConcretely, an LLM-based web world model functions as a learned web server: conditioned on the current page representation and an agent action, it predicts realistic next-state page representations and provides task-level feedback signals for policy optimization.\nBy training web agents on a mixture of real and imagined experience, DynaWeb enables scalable, on-policy reinforcement learning through imagination while preserving the benefits of interactive learning.\n\n\nCrucially, DynaWeb combines two complementary sources of training experience.\nIn addition to policy-driven imagined rollouts generated by the web world model, we directly incorporate fully real expert trajectories sampled from existing training data.\nThese expert trajectories are entirely independent of the world model and correspond to ground-truth web interactions.\nDuring training, real expert trajectories are randomly interleaved with imagined rollouts and real on-policy interaction, allowing the agent to benefit from high-quality demonstrations while substantially reducing reliance on costly live web exploration.\nThis simple but effective interleaving strategy preserves the on-policy learning signal and enables efficient online reinforcement learning with significantly fewer real-environment interactions.\n\n\nOur contributions are summarized as follows:\n\n\n•\n\nWe train a web world model that predicts naturalistic web page state transitions in the form of structured accessibility tree representations, enabling realistic simulation of web environment dynamics without live web interaction.\n\n\n\n•\n\nWe show that policy-driven imagined rollouts generated by a learned web world model can be directly used as on-policy training experience for reinforcement learning of web agents.\n\n\n\n•\n\nWe propose DynaWeb, a model-based reinforcement learning framework that integrates fully real expert trajectories from training data with imagined rollouts from a world model, enabling effective policy optimization without interacting with the live web.\n\n\n\n•\n\nThrough extensive experiments and analysis, we demonstrate consistent performance improvements on WebArena and WebVoyager, and provide empirical insights into the design and use of world-model-based imagination for training web agents.\n\n\n\n\n\n\n\n2 Related work\n\nWeb Agent\n\nRecent advances in web agents are largely driven by (multimodal) large language models (LLMs) serving as the core decision-making backbone (Dubey et al., 2024; Jia et al., 2024; OpenAI, 2023; Anthropic, 2025). On top of these models, reasoning and interaction frameworks such as ReAct Yao et al. (2023), MCP (Anthropic, 2024), and Cognitive Kernel (Zhang et al., 2024a) enable structured multi-step web actions. Web agents are commonly evaluated on interactive benchmarks including WebShop (Yao et al., 2022), Mind2Web (Deng et al., 2023), WebArena (Zhou et al., 2024a), VisualWebArena (Koh et al., 2024a), WebVoyager (He et al., 2024), WebWalker (Wu et al., 2025a), and MMInA (Zhang et al., 2024b).\n\n\nBeyond off-the-shelf prompting, a broad line of work improves web agents via data scaling and stronger agent training pipelines. Data-centric efforts such as Explorer (Pahuja et al., 2025), NNetNav (Murty et al., 2025), and InSTA (Trabucco et al., 2025) collect or synthesize high-quality interaction data, while recent agent foundation models and deep research agents push toward more general web interaction at scale, e.g., WebThinker (Li et al., 2025a), WebDancer (Wu et al., 2025b), WebSailor (Li et al., 2025b), WebShaper (Tao et al"
  },
  {
    "title": "FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale",
    "url": "https://arxiv.org/abs/2601.22146v1",
    "source": "arxiv",
    "summary": "Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome",
    "full_text": null
  },
  {
    "title": "Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data",
    "url": "https://arxiv.org/abs/2601.22141v1",
    "source": "arxiv",
    "summary": "In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lo",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nPruning\nDynamic sparse training\n\n\n\n3 Method\n\n3.1 Formulation of the adaptive pruning algorithm\n3.2 Adaptive tickets extraction\n3.3 Joint Retraining of adaptive tickets\n\n\n\n4 Experiments\n\n4.1 CIFAR-10 subnetwork specialization\n4.2 Cluster-aware pruning on CIFAR-100\n4.3 Implicit Neural Representations\n4.4 Speech enhancement in realistic environments\n4.5 Subnetwork collapse and semantic alignment\n\n\n\n5 Results\n\n5.1 CIFAR-10 subnetwork specialization\n5.2 Cluster-aware pruning on CIFAR-100\n5.3 Implicit Neural Representations\n5.4 Speech Enhancement in realistic environments\n\n5.5 Subnetworks collapse and semantic alignment\n\nSubnetworks collapse\nSemantic alignment\n\n\n\n\n6 Conclusion\n\nA Vision Model and Training Setup\n\nA.1 Model architecture and pruning scope\nA.2 Optimization and training protocol\nA.3 Pruning schedule and compute\n\n\nB Semantic Clustering Procedure\n\nC Implicit Neural Representation\n\nC.1 Task setup\nC.2 Positional encoding\nC.3 Model architectures\nC.4 Training and pruning protocol\nC.5 Initialization and fairness considerations\nC.6 Evaluation protocol\nC.7 Per-Image INR Results\nC.8 INR subnetwork similarity vs. reconstruction quality\nC.9 Qualitative INR Reconstructions under High Sparsity\n\n\n\nD Speech Enhancement\n\nD.1 Dataset construction\nD.2 Model architecture\nD.3 Signal processing and loss\nD.4 Optimization and runtime\nD.5 Qualitative Analysis of Speech Enhancement\n\n\n\nE Subnetwork Similarity Analysis\n\nE.1 Mask similarity\nE.2 Collapse analysis\nE.3 Semantic alignment\n\n\n\n\n\n\n\nRouting the Lottery: Adaptive Subnetworks for Heterogeneous Data\n\n\nGrzegorz Stefański\n\n  \nAlberto Presta\n\n  \nMichał Byra\n\n\n\nAbstract\nIn pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.\n\nAdaptive Pruning, Lottery Ticket Hypothesis, Heterogeneous Data, Context-Aware Inference\n\n\n\n1 Introduction\n\nDespite the remarkable progress deep neural networks have achieved over the past decades, modern models often require billions of parameters and hundreds of gigaflops per inference, making them impractical for deployment in resource-constrained or real-time settings.\n\n\nThis inefficiency stands in stark contrast to biological intelligence, which achieves high performance with extreme parsimony.\nWhile specialized hardware continues to advance, the growth in model scale consistently outpaces gains in computational efficiency. Consequently, reducing model complexity is not only crucial for practical deployment but also for understanding the fundamental principles of generalization in deep learning.\n\n\nPruning has been a cornerstone of model efficiency research (Cheng et al., 2024). By removing redundant weights or structures, it aims to uncover smaller subnetworks that retain the performance of their dense counterparts. The Lottery Ticket Hypothesis (LTH) (Frankle and Carbin, 2019) revitalized this line of work by positing that “winning tickets”, i.e. sparse subnetworks within large, randomly initialized networks, can be trained in isolation to match full-model accuracy. This reframed pruning as a discovery process aiming to reveal the minimal structures responsible for learning.\n\n\nHowever, nearly all LTH-inspired methods assume a universal subnetwork - a single sparse mask applied uniformly across all inputs, overlooking real-world data heterogeneity.\nDifferent classes, clusters, or environmental conditions often rely on distinct feature representations. A one-size-fits-all mask may therefore sacrifice performance by forcing diverse patterns through a shared, rigid architecture. Bridging this gap requires moving beyond global sparsity toward adaptive, data-aware pruning, a shift that aligns model structure with the intrinsic organization of the data itself.\n\n\nIn this work, we introduce Routing the Lottery (RTL), a novel adaptive pruning framework that rethinks LTH by discovering multiple specialized subnetworks, dubbed adaptive tickets, each tailored to a distinct data subset (e.g., a class or semantic cluster). This shift enables the model to allocate representational capacity heterogeneously across the input space, aligning sparsity with data structure rather than enforcing uniform compression.\nOur framework achieves specialization through pruning alone, without auxiliary routing networks or additional parameters.\nIt discovers multiple stable, sparse subnetworks, each tied to a class or cluster, and selects them via simple context-based routing (e.g., label or environment), offering a lightweight and interpretable alternative to Mixture-of-Experts (MoE) architectures that prioritizes structural efficiency over dynamic routing flexibility.\n\n\nThis work serves as a precursor, aiming to guide pruning from a static tool into a dynamic mechanism for building modular and semantically grounded models. The major contributions of this paper are:\n\n\n•\n\nRTL jointly learns multiple sparse subnetworks from a shared dense initialization, with masks adapted to data subsets while preserving parameter sharing, maintaining a single compact backbone and employing a mask-based routing to enable context-aware inference.\n\n\n\n•\n\nWe show that class-specific subnetworks on CIFAR-10 outperform both single-mask and multi-model pruning baselines while using up to an order of magnitude fewer parameters than independent models, and that on CIFAR-100, RTL scales naturally to enable effective specialization across a larger number of classes.\n\n\n\n•\n\nWe validate RTL on a real-world speech enhancement task, where subnetworks specialized for acoustic environments achieve higher SI-SNRi than universal or independent baselines.\n\n\n\n\n\n\n\n2 Related Work\n\nPruning\n\nhas a rich history, beginning with sensitivity-based methods like Optimal Brain Damage (LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi et al., 1993).\nIn particular, unstructured pruning removes non-relevant weights without imposing structural constraints, i.e., without removing entire layers. (Han et al., 2015) performed pruning by learning meaningful connections, while (Yang et al., 2017, 2018) exploit energy consumption as a metric for removing elements.\n(Sreenivasan et al., 2022; Tartaglione et al., 2022) introduced regularization terms to constrain the magnitude of non-relevant parameters during iterative training, while (Benbaki et al., 2023) considered the combined effect of pruning and updating multiple weights under a sparsity constraint.\nFurthermore, (Tartaglione et al., 2020) compared and analyze two different pruning approaches, i.e. one-shot and gradual, highlighting that the latter allows for better generalization.\n(Zhang et al., 2024) analyzed fundamental aspects of pruning, identifying two key factors that determine the pruning ratio limit, i.e., weight magnitude and network sharpness, while (Liao et al., 2023; Hur and Kang, 2019; Luo and Wu, 2017; Min et al., 2018) proposed entropy-based approaches to guide pruning.\nIn general, owing to the conceptual simplicity of pruning, a wide range of methods have been proposed for different scenarios, such as LLM pruning (Frantar and Alistarh, 2023; Sun et al., 2023; Lu et al., 2024; Wei et al., 2024; Tan et al., 2024), convolutional pruning (Zhao et al., 2023), and even spiking neural networks (Shi et al., 2024). However, none of these have considered whether finding a single mask applicable to all types of data might be a suboptimal solution.\n\n\nIterative Magnitude Pruning (IMP) (Frankle and Carbin, 2019) emerged as a practical algorithm for identifying winning tickets, though it remains computationally intensive due to repeated training cycles.\nFurthermore, (Paul et al., 2022) attempted to demystify the IMP method, investigating what kind of information the obtained mask encodes and how SGD allows the network to extract such information.\nDespite numerous extensions covering initialization schemes (Frankle et al., 2021), learning rate rewinding (Renda et al., 2020), theoretical analysis (Tartaglione, 2022; Burkholz et al., 2021; Sakamoto and Sato, 2022; Paul et al., 2022), and algorithmic variants (Wang et al., 2023; Lin et al., 2023), the LTH paradigm has largely focused on global masks shared across the entire dataset.\n\n\n\nDynamic sparse training\n\nmethods relax the fixed-mask assumption by evolving sparsity patterns during training. Approaches such as SET (Mocanu et al., 2017), SNFS (Dettmers and Zettlemoyer, 2020), and RigL (Evci et al., 2020) prune and regrow connections online, implicitly acknowledging that different inputs may activate different network pathways.\nFurthermore, (Molchanov et al., 2017) extended variational dropout in order to sparsify deep neural networks, while (Tartaglione et al., 2021) introduced sensitivity-based regularization of neurons to learn structured sparse topologies, exploiting neural sensitivity as a regularizer.\nNevertheless, these methods still maintain a single evolving subnetwork rather than"
  },
  {
    "title": "Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers",
    "url": "https://arxiv.org/abs/2601.22139v1",
    "source": "arxiv",
    "summary": "Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from pa",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Reasoning Large Language Model\n2.2 Interactive Large Language Model\n\n\n\n3 Proactive Interactive Reasoning Framework\n\n\n3.1 Phase I: Interactive Capability Activation\n\n3.1.1 Uncertainty-Aware Interactive Data Augmentation\n3.1.2 Supervised Fine-Tuning for Cold-Start\n\n\n\n3.2 Phase II: User-Intent Alignment\n\n\n3.2.1 Group Relative Policy Optimization with User Simulator\n\nUser Simulator Construction.\nOptimization with GRPO.\n\n\n\n3.2.2 Composite Reward Modeling\n\nOutput Reward.\nReasoning Reward.\n\n\n\n\n\n\n\n4 Simulated Experiments\n\n\n4.1 Experimental Settings\n\n4.1.1 Datasets\n\n4.1.2 Baselines\n\nMulti-turn LLM.\nPIR LLM.\n\n\n\n4.1.3 Implementation Details\n\nInteractive Environment Construction.\nEvaluation Paradigm.\n\n\n\n\n4.2 Results of Simulated Experiments\n\n4.3 Ablation on US-GRPO\n\nImpact of User Simulator Quality.\nImpact of Reward Design.\n\n\n\n\n\n5 Reliability in the Wild\n\n\n5.1 Generalization Evaluation\n\nFactual Knowledge.\nQuestion Answering.\nMissing Premise Testing.\n\n\n5.2 Case Study\n\n\n6 Conclusion\nThe Diversity of User Simulator\nLack of Safety Alignment\n\nA Appendix\n\n\nA.1 Uncertainty Analysis of PIR Framework\n\nDistribution of Uncertainty.\nCorrelation Asking-Response Template\n\n\n\nA.2 Training Configuration Details\n\nDataset Statistic\nParameter Detail\nUser Simulator Call Failure Protection Mechanism\n\n\n\nA.3 Training Dynamics Analysis\n\nTraining Curve\nCost of User Simulator\n\n\n\nA.4 Case Study Between Traditional LLM and PIR LLM\n\nMathematical problem-solving.\nCoding Assistant.\n\n\nA.5 Prompt Template Using in Our Research\n\n\n\n\n\n\n\nReasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers\n\n\n\nXin Chen1,2,3,*,   \nFeng Jiang2,††thanks: Equal contribution,   \nYiqian Zhang3,   \nHardy Chen4,   \nShuo Yan5,   \n\nWenya Xie6,   \nMin Yang2,3,†,   \nShujian Huang1,††thanks: Corresponding author.\n1National Key Laboratory for Novel Software Technology, Nanjing University, \n2Artificial Intelligence Research Institute, Shenzhen University of Advanced Technology, \n3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, \n4University of California, Santa Cruz,\n5University of Texas, Dallas,\n6University of Minnesota\n\nx.chen@smail.nju.edu.cn, jiangfeng@suat-sz.edu.cn\n\n\n\nAbstract\nReasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a blind self-thinking paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous.\nWe propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification.\nUnlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components:\n(1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70% higher accuracy, 22.90% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns.\nFurther reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR.\n111Model and code are publicly available at: https://github.com/SUAT-AIRI/Proactive-Interactive-R1\n\n\n\nReasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers\n\n\n\n\nXin Chen1,2,3,*,   \nFeng Jiang2,††thanks: Equal contribution,   \nYiqian Zhang3,   \nHardy Chen4,   \nShuo Yan5,\n\nWenya Xie6,   \nMin Yang2,3,†,   \nShujian Huang1,††thanks: Corresponding author.\n\n1National Key Laboratory for Novel Software Technology, Nanjing University,\n\n2Artificial Intelligence Research Institute, Shenzhen University of Advanced Technology,\n\n3Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\n\n4University of California, Santa Cruz,\n5University of Texas, Dallas,\n6University of Minnesota\n\nx.chen@smail.nju.edu.cn, jiangfeng@suat-sz.edu.cn\n\n\n\n\n\n1 Introduction\n\nFigure 1: The Proactive Interactive Reasoning (PIR) Paradigm. The schematic contrasts inefficient \"blind self-thinking\" on ambiguous queries with the PIR approach. PIR utilizes uncertainty detection and a two-phase optimization mechanism to enable proactive clarification with a user simulator, aligning reasoning chains with user intent to achieve accurate problem-solving that is efficient, robust, and minimal compute costs.\n\n\nThe emergence of reasoning-oriented LLMs such as GPT-o1 (OpenAI et al., 2024) and DeepSeek-R1 (DeepSeek-AI, 2025) marks a fundamental paradigm shift toward self-thinking models, which internally deliberate through multi-step CoT reasoning. This paradigm has substantially boosted performance on complex tasks, and recent studies further suggest that the quality of explicit reasoning traces is positively correlated with final accuracy (Pu et al., 2025).\n\n\nDespite these advances, current reasoning LLMs exhibit a critical limitation, which we term blind self-thinking: they often fail to recognize when their reasoning is underinformed or based on ambiguous user instructions (Wang et al., 2024a), as shown in the left part of Figure 1. When users provide incomplete or ambiguous prompts, a common occurrence in real-world interactions, models tend to continue lengthy reasoning regardless, leading to overthinking (Cuadron et al., 2025), hallucinations (Yuan et al., 2024), and misaligned conclusions (Fan et al., 2025; Chen et al., 2025). Users are then forced to perform iterative post-hoc corrections, which significantly degrade interaction efficiency and user experience (Wang et al., 2024a; Kim et al., 2024). Although recent multi-turn interactive LLMs (Wu et al., 2025) acknowledge this by attempting to reactively follow user feedback, they remain passive solvers and fail to actively reduce unnecessary conversational turns.\n\n\nTo bridge the gap between ambiguous user queries and precise reasoning execution, we introduce PIR, a new paradigm that transforms reasoning LLMs from passive solvers into proactive inquirers, offering a principled approach for aligning reasoning with user intent under ambiguity, as shown in the right of Figure 1. Instead of relying on user-initiated corrections, PIR explicitly trains models to detect missing premises and autonomously initiate clarification during the reasoning process.\n\n\nOur approach consists of two clearly separated stages. First, we propose an uncertainty-aware mechanism for constructing an interactive reasoning dataset to activate the model’s interactive capability. Specifically, the mechanism detects critical decision points where the model’s confidence dips. At these junctures, we convert monologic reasoning traces into a think-and-ask format by injecting clarification questions and user replies simulated by instruction-following LLMs. Then, to further align the model’s reasoning behavior with user intent, we introduce a novel Group Relative Policy Optimization framework equipped with a dynamic user simulator (US-GRPO) and a principled composition of extrinsic (task success) and intrinsic (helpfulness-efficiency) rewards. This optimization explicitly encourages the model to prioritize intent resolution over ungrounded self-thinking, leading to significant improvements in both correctness and interaction efficiency.\n\n\nExtensive experiments on mathematical reasoning, code generation, and document editing show that PIR effectively avoids invalid reasoning trajectories, reducing computation by an average of approximately 2k tokens per task and cutting unnecessary interaction turns by half. Meanwhile, PIR achieves superior performance with improvements of 9.8% in Accuracy, 3.2% in Pass Rate, and 13.36 in BLEU, respectively. Additionally, PIR also demonstrates strong generalization on non-interactive benchmarks, including factual knowledge, question answering, and missing premise tests, suggesting that proactive interactive reasoning is beneficial beyond interactive settings.\n\n\nOur contributions are summarized as follows:\n\n\n•\n\nWe identify and formalize the blind self-thinking problem in current reasoning LLMs and propose the PIR framework to enable proactive clarification.\n\n\n\n•\n\nWe develop a reinforcement learning method, US-GRPO, incorporating a dynamic user simulator and both extrinsic and intrinsic rewards to align reasoning with intent resolution and optimize interaction efficiency.\n\n\n\n•\n\nThrough extensive experiments, we show that PIR achieves state-of-the-art performance on both interactive and general reasoning benchmarks, providing a more efficient and user-aligned paradigm for next-generation reasoning models.\n\n\n\n\n\n\n\n2 Related Work\n\nFigure 2: Overview of the PIR Framework. The framework operates in two phases to transition LLMs from passive solvers to active inquirers.\n\n\n\n2.1 Reasoning Large Language Model\n\nRecent reasoning LLMs, such as GPT-o1 (OpenAI et al., 2024) and DeepSeek-R1 (DeepSeek-AI, 2025), have shown impressive performance on complex problem-solving tasks by generating explicit CoT before producing final answers. Despite these advances, these models frequently exhibit pathological reasoning behaviors: they may overthink (Fan et al., 2025; Chen et al., 2025), even hallucinate during intermediate reasoning (Yuan et al., 2024), or produce misaligned conclusions that waste substantial computation (Qiu and Srivastava, 2025).\n\n\nTo alleviate these issues, a growi"
  },
  {
    "title": "PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training",
    "url": "https://arxiv.org/abs/2601.22137v1",
    "source": "arxiv",
    "summary": "Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Ra",
    "full_text": null
  },
  {
    "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
    "url": "https://arxiv.org/abs/2601.22136v1",
    "source": "arxiv",
    "summary": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 The StepShield Benchmark\n\n3.1 Design Principles\n3.2 Rogue Behavior Taxonomy\n3.3 Severity Levels\n3.4 Dataset Composition\n\n\n\n4 Temporal Metrics\n\n4.1 Early Intervention Rate (EIR)\n4.2 Intervention Gap (IG)\n4.3 Tokens Saved\n4.4 Why Timing Metrics Are Necessary\n\n\n\n5 Experiments\n\n5.1 Baseline Detectors\n5.2 Main Results\n5.3 Category Analysis\n5.4 Severity Analysis\n\n\n\n6 Economic Analysis\n\n6.1 Cost Model\n6.2 Detector Cost Comparison\n6.3 Enterprise-Scale Projections\n6.4 Token Estimation Methodology\n\n\n\n7 Discussion\n\n7.1 Implications for Benchmark Design\n7.2 Implications for Deployment\n\n\n8 Conclusion\n9 Limitations\n10 Broader Impact\nA Datasheet for StepShield\nB Economic Assumptions\nC Reproducibility\n\nD Extended Ablation Studies\n\nD.1 Prompt Sensitivity Analysis\nD.2 Model Scaling Analysis\nD.3 Context Window Analysis\nD.4 Ablation on HybridGuard Cascade\n\n\nE Failure Case Analysis\nF Statistical Significance\n\nG Annotation Guidelines\n\nG.1 Inter-Annotator Agreement\n\n\nH AI Safety ROI Analysis\n\n\n\n\n\nStepShield: When, Not Whether to Intervene on Rogue Agents\n\n\n\nGloria Felicia \nUniversity of Virginia\ngloria@virginia.edu\n&amp;Michael Eniolade \nUniversity of the Cumberlands\nmeniolade20593@ucumberlands.edu\n&amp;Jinfeng He \nCornell University\njh2933@cornell.edu\n&amp;Zitha Sasindran \nIndian Institute of Science, Bangalore\nzithas@alum.iisc.ac.in\n&amp;Hemant Kumar \nUniversity of Arizona\nhemantkumarbk@arizona.edu\n&amp;Milan Hussain Angati \nCalifornia State University, Northridge\nmilan-hussain.angati.637@my.csun.edu\n&amp;Sandeep Bandarupalli \nUniversity of Cincinnati\nsandeep.bandarupalli@uc.edu\n\n\n\n\nAbstract\nExisting agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.\n\n\n\n1 Introduction\n\nIn July 2025, an AI coding assistant deployed by a startup catastrophically wiped their production database during what should have been a routine migration task Fortune Staff (2025). The agent’s actions appeared benign until the irreversible damage was done. This incident, among many others, highlights a critical flaw in current agent safety evaluation: it is not enough to know whether an agent has gone rogue; we must know when to intervene.\n\n\nExisting agent safety benchmarks, such as Agent-SafetyBench Zhang et al. (2024), SafeArena Tur et al. (2025), and R-Judge Yuan et al. (2024), are built on a paradigm of post-hoc, trajectory-level classification. They render a binary verdict: was the completed agent run harmful or not? While valuable, this approach is fundamentally limited. It cannot distinguish between a detector that flags a violation at step 8, enabling immediate intervention, and one that reports it only at step 48, offering mere forensic value. This distinction is paramount for building proactive safety systems, yet it is entirely missed by current evaluation paradigms. Concurrent work on SHADE-Arena Kutasov et al. (2025) evaluates the ability of LLM agents to evade monitoring while pursuing hidden sabotage objectives, demonstrating the difficulty of detecting subtle harmful behaviors. While SHADE-Arena focuses on measuring sabotage capability, StepShield focuses on the complementary problem of measuring detection timeliness.\n\n\nTo address this gap, we introduce StepShield, the first agent safety benchmark designed to evaluate the temporal performance of rogue agent detectors. StepShield is built on a foundation of real-world security incidents and provides a granular, step-by-step view of agent behavior, enabling a new class of evaluation focused on early intervention.\n\n\nWhy Timing Matters. Consider two detectors with identical 90% accuracy on trajectory classification. Detector A flags violations at the first rogue step (average step 4 of 20), while Detector B flags them at the final step. From a binary accuracy perspective, these detectors are equivalent. From a practical deployment perspective, they are worlds apart. Detector A enables intervention that prevents 80% of the harmful actions; Detector B provides only forensic value after all damage is done. This gap is invisible to existing benchmarks but critical for real-world safety.\n\n\nOur contributions are five-fold:\n\n\n\n\n1.\n\nA New Evaluation Paradigm: We propose a new evaluation paradigm for agent safety that prioritizes detection timeliness, moving beyond simple binary accuracy.\n\n\n\n2.\n\nTemporal Metrics: We introduce three novel metrics (Early Intervention Rate, Intervention Gap, and Tokens Saved) that quantify when a detector identifies a violation.\n\n\n\n3.\n\nA Large-Scale Dataset: We release StepShield, a dataset of 9,213 code agent trajectories, including 639 rogue-clean pairs, grounded in real-world security incidents and annotated at the step level.\n\n\n\n4.\n\nComprehensive Evaluation: We benchmark four detection paradigms (StaticGuard, ConstraintGuard, LLM-Judge, and HybridGuard) and reveal a 2.3x gap in early detection performance that is invisible to standard accuracy metrics.\n\n\n\n5.\n\nEconomic Framework: We demonstrate the direct economic benefits of early detection, showing that HybridGuard can reduce LLM inference costs by 75% while maintaining strong detection capabilities, translating to $108M in cumulative savings over 5 years.\n\n\n\n\n\n\nFigure 1: The core insight of StepShield: detection timing determines intervention capability. Early detection (EIR = 0.59) identifies the rogue step at S4, enabling intervention that saves 50% of execution. Late detection (EIR = 0.26) flags the violation only at S8, providing post-mortem value after all damage is done. This 2.3x gap in early intervention capability is invisible to standard accuracy metrics.\n\n\n\n\n2 Related Work\n\nOur work builds on six key areas of research: foundational LLM agent architectures, agent safety benchmarks, guardrail agents, specification gaming and deceptive alignment, step-level evaluation, and LLM-as-Judge.\n\n\nFoundational LLM Agent Architectures. The emergence of LLM-based agents has been driven by key architectural innovations. ReAct Yao et al. (2023) introduced the paradigm of interleaving reasoning traces with actions, enabling agents to plan and execute in dynamic environments. Toolformer Schick et al. (2023) demonstrated that LLMs can learn to use external tools in a self-supervised manner. These foundational works enabled the development of general-purpose agent benchmarks like AgentBench Liu et al. (2024), which evaluates LLMs across 8 distinct environments, GAIA Mialon et al. (2024) for general AI assistants, WebArena Zhou et al. (2024) for realistic web tasks, and OSWorld Xie et al. (2024) for open-ended computer tasks where even the best models achieve only 12% success compared to 72% for humans. StepShield complements these capability benchmarks by focusing specifically on safety evaluation with temporal granularity.\n\n\nAgent Safety Benchmarks. Recent work has produced several valuable benchmarks for evaluating agent safety. Agent-SafetyBench Zhang et al. (2024) provides a comprehensive suite of 300 test cases across 8 categories. SafeArena Tur et al. (2025) evaluates the safety of autonomous web agents against deliberate misuse. ToolEmu Ruan et al. (2024) evaluates tool-use safety, and R-Judge Yuan et al. (2024) uses an LLM to judge agent helpfulness and harmlessness. SafeAgentBench Yin et al. (2024) introduces the first comprehensive benchmark for safety-aware task planning in embodied agents with 750 tasks across 10 hazard types. AgentHarm Andriushchenko et al. (2025) measures the propensity of LLM agents to execute harmful multi-step tasks, revealing that leading LLMs are surprisingly compliant with malicious requests even without jailbreaking. TheAgentCompany Xu et al. (2024) benchmarks agents on consequential real-world workplace tasks, finding that even the best agents complete only 30% of tasks, underscoring the need for safety monitoring as agents are deployed in high-stakes environments. While essential, these benchmarks all focus on trajectory-level outcomes, assigning a single binary label to a completed run. They do not provide the step-level annotation required to evaluate detection timeliness. StepShield fills this gap by providing the first step-level annotated dataset for rogue behavior detection.\n\n\nGuardrail Agents. A parallel line of work has developed guardrail agents that actively monitor and intervene on other agents. ShieldAgent Chen et al. (2025) is the first guardrail agent to enforce explicit safety policy compliance through logical reasoning, achieving 90.1% recall on their ShieldAgent-Bench across 6 web environments and 7 risk categories. GuardAgent Xiang et al. (2025) dynamically checks whether agent actions satisfy safety guard request"
  },
  {
    "title": "Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference",
    "url": "https://arxiv.org/abs/2601.22132v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a com",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Problem Formulation\n\n2.1 System and Notation\n\n2.2 Hints and Shepherding Policy\n\nShepherding Policy.\n\n\n2.3 Cost Model and Evaluation Metrics\n2.4 Cost Analysis (Oracle)\n\n\n\n3 Training the Shepherding System\n\n3.1 Training Data and Supervision Signals\n3.2 Model Architecture\n3.3 Training Objective\n3.4 Inference: Mapping Predictions to Decisions\n3.5 Reactive Shepherding: Cascading Decision\n\n\n\n4 Performance Comparison\n\n\n4.1 Datasets and Experimental Setup\n\nDatasets.\nInference and Evaluation.\n\n\n4.2 Performance Analysis\n4.3 Minimum Accuracy Requirement\n4.4 Latency Overhead: Proactive vs Reactive\n\n\n5 Scope and Future Directions\n6 Conclusion\nA Proof of Corollary 2.2\n\nB Training Shepherding System: Additional Details\n\n\nB.1 Training Dataset Construction Details\n\n10% Granularity.\nDeterministic LLM Decoding.\nGrid-based Approximation.\n\n\nB.2 Optimization and Implementation Details\n\n\n\nC Outlier Analysis\n\nC.1 Outlier Generation Strategy\n\n\nD Heavy-Tailed Distribution of Hint Sizes\n\nE Prompting Details\n\nE.1 Prompt Template\n\n\nF Method Configurations for Minimum Accuracy Experiments\n\nG More Related Works\n\nG.1 LLM Routing\nG.2 LLM Cascading\nG.3 Speculative Decoding\nG.4 Knowledge Distillation\nG.5 Positioning of LLM Shepherding\n\n\n\n\n\n\n\nPay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference\n\n\nZiming Dong\n\n  \nHardik Sharma\n\n  \nEvan O’Toole\n\n  \nJaya Prakash Champati\n\n  \nKui Wu\n\n\n\nAbstract\nLarge Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches—routing and cascading—treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10–30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42–94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8×\\times cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Meanwhile, the emergence of highly capable open-source Small Language Models (SLMs) has created new opportunities for cost-efficient inference—whether on edge devices, private servers, or shared datacenters. SLMs offer clear advantages: lower latency, improved privacy, and significantly smaller monetary and energy costs. However, for tasks involving logic and mathematics, the quality gap between SLMs and frontier LLMs (e.g., GPT, Gemini, Claude) remains substantial (Subramanian et al., 2025). Users and organizations thus face a difficult trade-off: accept inferior response quality to reduce costs, or pay a premium for LLM access to maintain accuracy.\n\n\nThis cost-quality trade-off motivates a central question: How can we substantially improve SLM output quality while minimizing reliance on expensive LLMs? The question is especially pressing in resource-constrained settings—edge devices with limited connectivity, latency-sensitive applications, and cost-conscious deployments—where every LLM call carries significant overhead. The goal is not merely faster inference, but fundamentally less LLM computation for a given quality target.\n\n\n\n\n\n(a) Proactive Shepherding\n\n\n\n\n(b) Reactive Shepherding\n\n\n\nFigure 1: Two modes of LLM Shepherding. Both modes request only the first nn tokens from the LLM as a hint, which the SLM uses to output the response. (a) Proactive: hint decision made upfront. (b) Reactive: hint requested only after SLM failure.\n\n\nTwo complementary paradigms have emerged to address this challenge: routing and cascading (Behera et al., 2025). Routing (Lu et al., 2024; Ding et al., 2024; Ong et al., 2025) uses a learned classifier to direct each query to exactly one model—either the SLM handles the query entirely, or it is forwarded to the LLM based on query complexity. Cascading (Chen et al., 2024; Aggarwal et al., 2024; Gupta et al., 2024) takes a sequential approach: the SLM attempts a response first, and the LLM is invoked only if the SLM response fails a confidence or correctness check.\n\n\nWe observe that both routing and cascading share a fundamental limitation: they treat the LLM as an all-or-nothing resource. Either the query bypasses the LLM entirely, or the LLM generates a complete response, incurring the full LLM inference cost. This binary view overlooks a key opportunity: most LLM service providers allow users to specify max_new_tokens, a user-defined limit for the number of output tokens. This capability can be strategically leveraged to reduce LLM costs while simultaneously improving SLM answer quality.\n\n\nFigure 2: SLM (Llama-3.2-3B-Instruct) accuracy as a function of LLM (Llama-3.3-70B-Versatile) hint size. Even small hints (10–30% of the full LLM response) yield substantial accuracy gains, with diminishing returns beyond 60%. This motivates shepherding: requesting hints rather than full LLM responses. Dashed lines indicate the LLM accuracy for each dataset.\n\n\nWe propose LLM Shepherding, a novel framework that complements routing and cascading by enabling partial LLM assistance. Rather than having the LLM produce complete responses, shepherding requests the LLM to output only a short prefix – first nn tokens of the LLM response – called a hint. The hint is then concatenated with the query and is given to SLM for the final response. To validate whether LLM hints can benefit SLM inference, we evaluate on four widely-used benchmarks spanning mathematical reasoning—GSM8K (Cobbe et al., 2021) and CNK12 (OpenR1, 2025)—and code generation—HumanEval (Chen, 2021) and MBPP (Austin et al., 2021). We use Llama-3.2-3B-Instruct as the SLM and Llama-3.3-70B-Versatile as the LLM (Dubey et al., 2024). As shown in Figure 2, even small hints substantially improve SLM accuracy across all datasets.\n\n\nAs illustrated in Figure 1, we instantiate two modes of shepherding: 1) Proactive Shepherding (Figure 1(a)) is a routing-based shepherding, where a learned router directly predicts whether a hint is needed and how many tokens to request, and 2) Reactive Shepherding (Figure 1(b)) is a cascading-based shepherding, where the features of the SLM response(es) are used for cascading decision and hint prediction.\nObserve that proactive shepherding reduces to routing at the extremes: hint size zero (SLM only) or maximal (full LLM response). Similarly, reactive shepherding reduces to cascading when a complete LLM output is requested upon SLM failure. Shepherding thus generalizes both paradigms. By operating in the intermediate regime – requesting hints rather than full responses – shepherding theoretically achieves lower costs than either approach.\n\n\nRealizing this vision in practice presents substantial challenges. First, the ‘minimum’ hint size required by different queries has a heavy-tailed distribution: on GSM8K, 80% of queries need no hint, while the rest span 10–90% of the full LLM response. Second, the benefit of additional tokens is non-smooth—critical information may appear at specific token boundaries, causing abrupt quality transitions. Third, the relationship between hint length and SLM accuracy is non-monotonic; excessive hints can over-constrain SLM generation, degrading performance even when shorter hints succeed (cf. HumanEval in Figure 2). These challenges motivate our two-stage prediction framework that jointly determines whether a hint is needed and how many tokens to request.\n\n\nContributions:\n\n\n•\n\nWe formalize LLM Shepherding and prove it achieves lower cost than routing and cascading under oracle decision-making (Section 2).\n\n\n\n•\n\nWe develop a two-stage prediction model that jointly predicts whether a query requires a hint and how many tokens to request, addressing key challenges including non-monotonic quality response and heavy-tailed hint distributions (Section 3).\n\n\n\n•\n\nWe empirically evaluate shepherding against state-of-the-art routing strategies (RouteLLM (Ong et al., 2025), GraphRouter (Feng et al., 2025)) and cascading strategies (FrugalGPT (Chen et al., 2024), ABC (Kolawole et al., 2025)) on mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks. Reactive Shepherding achieves the highest Accuracy-per-Cost Efficiency (ACE) on all datasets, delivering 2×2\\times greater cost reduction than routing baselines on challenging multilingual queries (CNK12) and 2.8×2.8\\times greater cost reduction than cascading baselines on cross-domain transfer (HumanEval)—the latter without code-specific training (Section 4).\n\n\n\n\n\n\n\n2 Problem Formulation\n\n\n2.1 System and Notation\n\nLet 𝒬\\mathcal{Q} denote the set of all possible queries and let q∈𝒬q\\in\\mathcal{Q} denote an individual query. Let 𝒳\\mathcal{X} denote a finite vocabulary of tokens and 𝒳∗\\mathcal{X}^{*} denote the set of all finite token sequences over 𝒳\\mathcal{X}. We consider two language models: an open-source SLM hs:𝒬→𝒳∗h_{s}:\\mathcal{Q}\\rightarrow\\mathcal{X}^{*} deployed on a local device, such as a smartphone, laptop, or local server, and an LLM hl:𝒬→𝒳∗h_{l}:\\mathcal{Q}\\rightarrow\\mathcal{X}^{*} accessible throu"
  },
  {
    "title": "SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization",
    "url": "https://arxiv.org/abs/2601.22131v1",
    "source": "arxiv",
    "summary": "Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has",
    "full_text": null
  },
  {
    "title": "World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems",
    "url": "https://arxiv.org/abs/2601.22130v1",
    "source": "arxiv",
    "summary": "Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observa",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 World Models\n2.2 General Agentic Benchmarks\n2.3 Enterprise Benchmarks\n\n\n\n3 WoW Environment\n\n\n3.1 Environment Specification\n\n1. Standard Observation (Tool Response)\n2. Oracle Observation (Table Audits)\n\n\n3.2 Workflows\n\n\n\n4 WoW-bench\n\n\n4.1 Task Categories\n\n4.1.1 Constraint Understanding\n4.1.2 Agentic Task Completion\n4.1.3 Action and Audit Prediction\n\n\n\n4.2 Evaluation Metrics\n\n4.2.1 Constraint &amp; Task Reliability\n\n4.2.2 Dynamics Modeling\n\nAudit Prediction (Forward Dynamics):\nAction Prediction (Inverse Dynamics):\n\n\n\n\n4.3 Results Overview\n\n\n\n5 Error Analysis\n\n5.1 The Representation Gap: Lack of Symbolic Grounding\n5.2 The Dynamics Gap: Absence of Transition Models\n5.3 The Causal Gap: Failure of Multi-Hop Reasoning\n5.4 Implications: Toward Dynamics-Aware Agents\n\n\n6 Conclusions\n7 Limitations and Future Work\nA Action and Observation Examples\n\nB Details of the Benchmark Suite\n\nB.1 Annotation Process\n\n\nC Prompt\n\nD Additional Evaluation\n\n\nD.1 Detailed Error Modes\n\nD.1.1 Constraint Understanding\nD.1.2 Agentic Task Completion\nD.1.3 Action Prediction\nD.1.4 Audit Prediction\n\n\n\n\n\n\n\n\n\nWorld of Workflows: a Benchmark for \nBringing World Models to Enterprise Systems\n\n\nLakshya Gupta\n\n  \nLitao Li\n\n  \nYizhe Liu\n\n  \nSriram Ganapathi Subramanian\n\n  \nKaheer Suleman\n\n  \nZichen Zhang\n\n  \nHaoye Lu\n\n  \nSumit Pasupalak\n\n\n\nAbstract\nFrontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases.\nExisting enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects.\nWe introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities.\nWe reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub 111GitHub for setting up and evaluating WoW.\n\n\n\n1 Introduction\n\nEnterprise systems contain relational tables, workflow engines, business rules, and application modules that interact through complex, often opaque, chains of data flow. These systems form an intractably large and partially observable state space – one where a single user action can trigger multiple cascading updates across dependent tables (Wei et al., 2005; Lairet and Rowe, 2024).\n\n\nDespite impressive progress in general-domain autonomy, frontier LLMs remain unreliable in enterprise environments. Recent enterprise benchmarks (Boisvert et al., 2024; Huang et al., 2025; Dai et al., 2025; Levy et al., 2024) indicate low task success rates. Such benchmarks reveal several dimensions of challenges including hallucinations, domain understanding, UI navigation, and goal following. However, these challenges remain on the same surface as general consumer benchmarks with straightforward tasks and little constraint satisfaction requirements. In other words, they do not expose the true challenges of enterprise systems, including limited observability with intractably large database state, workflows with side effects, and multi-hop data dependencies that govern real enterprise behavior. To address these limitations, we introduce WoW, a realistic and workflow-centric ServiceNow-based 222ServiceNow Developer Instance enterprise system containing over 4,000 business rules and 55 active workflows.\nWe also build WoW-bench, the first enterprise benchmark designed to evaluate LLMs both as enterprise world models and agents.\nWoW-bench provides 234 evaluation tasks that explicitly incorporate workflow effects into four task categories: autonomous task completion, data-level constraint understanding, dynamics prediction, and tool prediction.\nWe use WoW-bench to answer several critical research questions regarding the practicality and usefulness of enterprise agents.\n\n\n\nFigure 1: Illustration of Cascading Workflow Failures in WoW. This diagram demonstrates how a single agent action can trigger a sequence of hidden state changes that violate constraints.\n\n\nFirstly, how do hidden workflows dictate an agent’s ability to complete tasks and satisfy constraints?\nTo answer this, we need to carefully design realistic workflows invisible to the agents and embed them in the system. We give an example in Figure˜1 of how workflows lead to task failure and constraint violation. When agents observe limited information from tool responses, which primarily contain information of the tool execution status, task failure and constraint violation remain unexposed. From the agent’s perspective in this example, assigning assets D and E to User X is compliant, as their clearance levels are both equal to that of user X.\n\n\nSecondly, to what extent does bridging the \"dynamics gap\" with oracle-level state visibility unlock reliability in long-horizon tasks, and where do fundamental reasoning failures persist despite perfect information?\nWoW provides two forms of observations as an ablation study: tool response, which provides direct API feedback, and table audit logs, a structured representation of database state changes. Tool responses or screenshots abstract away the state change and hide the environment dynamics. By exposing the database state changes through audit logs, we aim to study how important it is to provide the necessary \"evidence\" for agents to trace side effects. Through our experiment, using audit logs as observation increases the task success rate by at most 7x, indicating the necessity of state visibility.\n\n\nLastly, can frontier LLMs effectively function as zero-shot world models capable of predicting the cascading side effects of dependent actions and tracking symbolic state changes? Though audit logs provide significant uplift to task completion, assuming the availability of such information is not realistic in many enterprise environments, as they incur high cost and latency (Microsoft Learn, 2025) and often require escalated access (Nieles et al., 2017). Through experiments, we evaluate the agents’ capability of precise state tracking and tool use, which plays a critical role in real-world environments when oracle state is not visible.\n\n\nour contributions include the following:\n\n\n\n•\n\nWe introduce WoW, a high-fidelity enterprise environment that simulates a realistic, workflow-centric, and complex system.\n\n\n\n•\n\nWe present WoW-bench, a novel benchmark designed to evaluate frontier LLMs’ capabilities to reason, predict, and operate in an agentic and world modeling setting for completing constrained tasks and understanding system dynamics.\n\n\n\n•\n\nWe provide extensive evaluation showing that frontier LLMs struggle to track database and entity state changes, predict workflow effects, and follow constraints in long-horizon tasks. We motivate the need for a new agent learning paradigm to incorporate system dynamics modeling.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 World Models\n\nIn embodiment and robotics, world models serve as predictive simulators for physical tasks (Assran et al., 2025; Ha and Schmidhuber, 2018; Ding et al., 2025). However, their reliance on continuous dynamics and dense sensor measurements does not transfer to the symbolic, discrete, and workflow-driven nature of enterprise systems.\nText-based world models examine whether LLMs can learn symbolic environment dynamics expressed in natural language. TextWorld (Côté et al., 2018), TextWorld Commonsense (Murugesan et al., 2021), and STARLING (Basavatia et al., 2024) treat the world as a set of textual predicates and model local transitions. Such worlds lack realism with tiny state space and simple dynamics.\nMAPS (Aroca-Ouellette et al., 2025) and VendingBench (Backlund and Petersson, 2025) benchmark LLMs to anticipate consequences under uncertainty and strategically operate a complex business.\nIn these environments, world models operate on compact and well-structured state representations and controllable environment dynamics. In contrast, enterprise systems exhibit limited observability and workflow-driven database state updates. This gap motivates the need for world-model benchmarks operating over symbolic, relational, workflow-governed environments.\n\n\nTable 1: Comparison matrix between WoW-bench and previous benchmarks. WoW focuses on incorporating and evaluating world modeling, constraint understanding, and task completion in realistic enterprise environment with underlying workflows.\n\n\n\n\nBenchmarks\nRealistic Environment\nEnterprise Tasks\nConstraint Following\nWorld Model Evaluation\nComplex Workflows\n\n\nWorkArena++ (Boisvert et al., 2024)\n\n✓\n✓\n✗\n✗\n✗\n\n\nCRMArena-Pro (Huang et al., 2025)\n\n✓\n✓\n✗\n✗\n✗\n\n\nSCUBA (Dai et al., 2025)\n\n✓\n✓\n✗\n✗\n✗\n\n\nST-WebAgentBench (Levy et al., 2024)\n\n✓\n✓\n✓\n✗\n✗\n\n\nWorkBench (Styles et al., 2024)\n\n✗\n✓\n✗\n✗\n✗\n\n\n\nτ2\\tau^{2}-bench (Barres et al., 2025)\n\n✗\n✗\n✗\n✗\n✗\n\n\nMCPToolBench++ (Fan et al., 2025)\n\n✓\n✗\n✗\n✗\n✗\n\n\nMCP-Universe (Luo et al., 2025)\n\n✓\n✗\n✗\n✗\n✗\n\n\nWoW-bench\n✓\n✓\n✓\n✓\n✓\n\n\n\n\n\n\n\n\n2.2 General Agentic Benchmarks\n\nLLM-based agents in interactive environments have been extensively studied to perform different types of tasks, such as software engineering (Chen, 2021; Austin et al., 2021; Jain et al., 2024), open-ended computer use (Yao et al., 2022; Zhou et al., 202"
  },
  {
    "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
    "url": "https://arxiv.org/abs/2601.22129v1",
    "source": "arxiv",
    "summary": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to genera",
    "full_text": "\n\n\n\n1 Introduction\n\n2 SWE-Replay\n\n\n2.1 Selecting Steps\n\n2.1.1 Filtering Low-Quality Trajectories\n2.1.2 Grouping Steps via Abstraction\n2.1.3 Detecting Reasoning-intensive Steps\n2.1.4 Balancing Exploration and Exploitation\n\n\n2.2 Replaying Environment before Steps\n2.3 Branching at Steps\n\n\n\n3 Evaluation\n\n3.1 Experimental Setup\n3.2 Main Results\n3.3 Ablation Experiments\n\n\n\n4 Discussion\n\n4.1 SWE-Replay Diversifies Repository Exploration\n4.2 Theoretical Intuition of SWE-Replay\n\n\n\n5 Related Work\n\n5.1 Scaffolding for Software Engineering Agents\n5.2 Test-Time Scaling for Software Engineering Agents\n\n\n6 Conclusion\n\nA Appendix\n\nA.1 Implementation Details\nA.2 Benchmark Details\nA.3 Metric Details\nA.4 Ablation Details for LLM-as-a-Judge Variant\nA.5 Effectiveness of File-Level Abstraction\nA.6 Impact of Reasoning Intensity Representation\nA.7 Impact of Softmax Normalization\n\n\n\n\n\n\n\nSWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents\n\n\nYifeng Ding\n\n  \nLingming Zhang\n\n\n\nAbstract\nTest-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have evolved significantly, progressing from basic code completion tools (austin2021program; chen2021codex; li2022competition; ding-etal-2024-mathcal; ding-etal-2025-planning) to sophisticated interactive agents capable of navigating repositories, executing tests, and submitting patches end-to-end (yang2024swe; wang2024openhands; zhang2024autocoderover; liu2024large; xia2024agentless; carbonneaux2025cwm). Modern agentic frameworks, such as SWE-agent (yang2024swe) and OpenHands (wang2024openhands), equip LLMs with tools like terminals, editors, and search engines to tackle complex repositories. To further enhance capabilities in software engineering (SWE) tasks, a primary driver of progress has been test-time scaling (zhang2025survey), which increases inference-time computation to yield higher-quality solutions. Existing works have demonstrated that generating multiple candidate solutions with a positive temperature and selecting a final answer via test-based feedback improves performance log-linearly with the number of samples, establishing a promising scaling law for SWE tasks (brown2024large; ehrlich2025codemonkeys).\n\n\nFigure 1: Overview of SWE-Replay.\n\n\nDespite this promise, test-time scaling is computationally expensive due to the cost of repeated sampling from scratch (kim2026costdynamicreasoningdemystifying). Consequently, several existing works have been proposed to mitigate these costs. SWE-Search (antoniades2024swe) proposed using a modified Monte Carlo Tree Search (MCTS) to prune unpromising trajectories early, relying on a value agent to estimate the quality of each step. Similarly, Satori-SWE (zeng2025satorisweevolutionarytesttimescaling) proposes to achieve sample-efficient test-time scaling by prompting agents to self-improve the scores of their prior generations estimated by a reward model. However, these approaches have the following limitations: quality scores from value agents and reward models can be compromised by model miscalibration (son2024llm), introducing noise during scaling; more importantly, existing approaches are tailored to pipeline-based scaffolds (e.g., Moatless111https://github.com/aorwall/moatless-tools) and cannot generalize to modern agentic frameworks like SWE-agent (yang2024swe). Specifically, existing works design specific prompts to evaluate pre-defined tools in the pipeline, such as a structured search/retrieval tool. However, modern agents are designed to synthesize custom bash scripts without templates. This flexibility makes it infeasible to design tool-specific evaluation prompts, rendering existing works incompatible with modern agents.\n\n\nTo bridge this gap, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique designed to reduce trajectory sampling costs while improving trajectory quality for modern agents, without reliance on LLM-as-a-Judge. Our core insight is to recycle previously sampled trajectories by resuming exploration at carefully selected intermediate steps, rather than generating every trajectory from scratch. As shown in Figure 1, SWE-Replay maintains an archive of sampled trajectories and performs stochastic sampling iteratively: either explore by sampling a new trajectory from scratch or exploit by replaying from the middle of an existing trajectory. In the exploitation phase, SWE-Replay identifies a critical step sts_{t} based on its potential to explore new search space and its reasoning intensity, efficiently restores its environment state, and branches at step sts_{t} by sampling a new step st′s^{\\prime}_{t} to replace sts_{t} while continuing exploration. This mechanism bypasses reliance on potentially inaccurate LLM-as-a-Judge, generalizes naturally to modern agentic scaffolds, and employs a streamlined select-and-replay mechanism to ensure scalability.\n\n\nWe evaluate SWE-Replay on the widely used SWE-Bench Verified222https://openai.com/index/introducing-swe-bench-verified/, and the more complex SWE-Bench Pro (deng2025swe) and Multilingual333https://www.swebench.com/multilingual.html. Our results show that, on SWE-Bench Verified, SWE-Replay consistently reduces the cost of naive test-time scaling by up to 17.4% while maintaining or even improving performance by up to 3.8%, across three different LLM backends and two different agentic scaffolds. On SWE-Bench Pro and Multilingual, SWE-Replay further demonstrates its consistent generalizability to diverse SWE problems. In addition, our analysis reveals that SWE-Replay successfully shifts exploration to the long-tail of repository files, as visualized in Figure 6. Finally, we provide an interesting theoretical intuition on how replaying optimizes the quality of exploration in SWE-Replay.\n\n\nIn summary, we make the following contributions:\n\n\n\n\n•\n\nWe present SWE-Replay, the first efficient test-time scaling technique for software engineering agents that reduces trajectory sampling costs while improving trajectory quality, without any reliance on LLM-as-a-Judge and generalizable to any modern agent scaffold.\n\n\n\n•\n\nOn SWE-Bench Verified, SWE-Replay consistently reduces the sampling cost by up to 17.4% while maintaining or even improving the resolve rate by up to 3.8%, across different agent scaffolds and LLM backends. We further validate the generalization of SWE-Replay to diverse software issues on SWE-Bench Pro and Multilingual.\n\n\n\n•\n\nOur in-depth analysis provides both empirical results showing that SWE-Replay enables agents to explore more diverse repository spaces, and theoretical intuition that bridges the gap between the performance gain and the effectiveness of step selection in SWE-Replay.\n\n\n\n\n\nFigure 2: Overview of step selection in SWE-Replay.\n\n\n\n\n2 SWE-Replay\n\nTo enable efficient and effective exploration for software engineering agents, we propose SWE-Replay, a novel algorithm that reduces the cost of trajectory sampling while improving trajectory quality. The key idea of SWE-Replay is to reuse previously sampled trajectories by resuming exploration at carefully selected intermediate steps, rather than repeatedly generating trajectories from scratch.\n\n\nAs shown in Figure 1, SWE-Replay maintains an archive of previously sampled trajectories and updates the archive with newly generated ones. Specifically, SWE-Replay initializes the archive with a single trajectory generated from scratch. It then iteratively conducts a stochastic sampling to decide whether to explore by generating a new trajectory from scratch or to exploit existing trajectories by resuming from intermediate steps in the archive. To exploit existing trajectories, SWE-Replay will select a critical step sts_{t} from the archive based on its potential to explore new repository space and its reasoning importance (§2.1), restore the environment state before this step (i.e., s1,2,⋯,t−1s_{1,2,\\cdots,t-1}) efficiently (§2.2), and resume exploration by generating a new step st′s^{\\prime}_{t} to replace sts_{t} for new branches (§2.3).\n\n\nFigure 3: Example of tool calls and their file-level representations. Step 1 is skipped as no file has been explored before it.\n\n\n\n2.1 Selecting Steps\n\nReliable identification of critical intermediate steps is central to efficient exploration. Intuitively, resuming at such steps allows the agent to revisit valuable regions of the search space that are not explored comprehensively and require more reasoning efforts from the agent.\n\n\nTo this end, SWE-Replay employs a hierarchical selection pipeline consisting of four component"
  },
  {
    "title": "The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR",
    "url": "https://arxiv.org/abs/2601.22128v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical sys",
    "full_text": null
  },
  {
    "title": "EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers",
    "url": "https://arxiv.org/abs/2601.22127v1",
    "source": "arxiv",
    "summary": "Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing t",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n\n2.1 Audio-Driven Talking Head Generation\n\nEarly Methods.\nDiffusion based Methods.\n\n\n\n2.2 Video Manipulation\n\nVideo-to-Video Editing.\nTranscript-Based Editing.\n\n\n2.3 Identity-Preserving Long Video Generation\n\n\n\n3 Method\n\n\n3.1 Preliminaries\n\nBaseline Network.\nFlow Matching Training Objective.\n\n\n\n3.2 Cross-Modal Audio &amp; Video Conditioning\n\nAudio Conditioning Strategy.\nV2V Lip-Sync.\n\n\n3.3 Visual Dialog Editing\n\n3.4 Identity-Preserving Long Inference\n\nLong Inference.\nIdentity Conditioning.\n\n\n3.5 Training Loss\n\n\n\n4 Experiments\n\n\n4.1 Training\n\nDataset.\nModel Training.\n\n\n\n4.2 Evaluation\n\nVideo-to-Video.\nImage-to-Video.\n\n\n\n4.3 Performance Optimizations\n\nVAE Tiling and Latent Frame Blocking.\nQuantization.\nHybrid Sequence Parallelism.\n\n\n\n4.4 Ablation Study\n\nWhat happens if Identity conditioning is dropped?\nTraining with/without Identity Reference Condition.\n\n\n\n\n\n5 Conclusion\n\n5.1 Ethical Considerations\n5.2 Acknowledgements\n\n\n\n\n\n\n\nEditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers\n\n\n\nJohn Flynn1,∗  Wolfgang Paier1,∗  Dimitar Dinev1  Sam Nhut Nguyen1\nHayk Poghosyan1  Manuel Toribio1  Sandipan Banerjee2,22footnotemark: 2  Guy Gafni1,‡\n1 Pipio AI, 2 Amazon \n1firstname.lastname@pipio.ai, 2sandgban@amazon.com\nProject page:  edit-yourself.github.io\n\n\n\n\nAbstract\nCurrent generative video models excel at producing novel content from\ntext and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.\n∗Authors contributed equally.\n†Work done while at Pipio AI.\n‡Project Lead.\n\n\n\n\n\nFigure 1: EditYourself is a multipurpose lip-syncing video diffusion model designed for transcription-based dialog editing, capable of lip-syncing from a single frame or an existing video, and seamlessly editing the video to match the new script.\n\n\n\n1 Introduction\n\nA growing share of modern video content is human-centric, including movies, online courses, corporate communications, interviews, and short-form social media uploads. In these videos, creators often need to revise the spoken content post-recording to fix fumbled lines, update facts, remove filler words, tighten interviews or localize content across languages. Traditional non-linear video editing tools, however, provide limited support for such edits, as operations like inserting, removing or retiming speech typically introduce visible jump cuts or unnatural motion. Moreover, selectively re-rendering only parts of a human performance requires extensive manual intervention within existing post-production workflows [25, 60, 46]. Recent advances in video diffusion models suggest a promising alternative. These models can synthesize high-quality, temporally coherent human videos from text, images or audio, demonstrating an ability to model complex appearance, motion, and facial dynamics [38, 35]. This capability makes generative models a good candidate for not only content creation, but as editing engines that can repair, extend or reshape existing videos in a content-aware manner [80].\n\n\nHowever, research on editing existing human-centric content remains far less mature than work on end-to-end generation. Most current approaches focus on Image-to-Video (I2V) generation from a single portrait image [16, 47, 11, 122]. While impressive in their realism, these methods frequently suffer from identity drift over time and incorrectly reproduce a subject’s likeness. A single image cannot capture the full range of facial details and speaking style present in a real performance. As a result, generated videos often hallucinate details such as teeth, wrinkles, facial hair or gestures, producing outputs that feel incorrect, especially when users are generating videos of themselves. On the other hand, V2V lip-sync models [85, 62, 119, 111] adhere closely to an input video to preserve visual fidelity and identity, but offer limited flexibility for editing. By operating under a fixed temporal structure that preserves the original frame count and timing, these methods make it difficult to insert or remove speech segments while maintaining temporal continuity. Consequently, existing V2V and I2V methods do not adequately support precise edits required for real-world post-production workflows.\n\n\nOur work tackles this fundamental problem of temporal manipulation of existing talking-head videos, which we refer to as visual dialog editing: V2V editing driven by changes to the spoken dialog [4]. This setting goes beyond simple lip synchronization to completely new audio, and supports core post-production operations such as inserting, removing and retiming video segments while preserving visual continuity. Editing videos directly through their textual transcript provides an intuitive and expressive interface for creators, enabling precise word-level modifications such as filler-word removal and post-shoot script revisions. More broadly, this transcript-centric workflow shifts video production from a “script-perfect-before-shooting” paradigm toward a “shoot once, refine later” model, enabling rapid updates, personalized variants and integration with higher-level control systems such as LLM-based AI agents for automated video editing [60, 102, 46].\n\n\nIn this work, we address this gap by re-framing talking-head video synthesis as a problem of visual dialog editing. We introduce EditYourself, a diffusion-based framework designed specifically for transcript-driven editing of talking head videos. By adapting a pre-trained general-purpose video diffusion model into a flexible, audio-driven V2V editor, our approach enables precise modification of existing videos, including addition, removal, and retiming of spoken segments, while maintaining accurate lip synchronization, visual identity, and temporal coherence over long videos.\n\n\nIn summary, our work makes the following contributions:\n\n\n•\n\nLip-sync on a pretrained video diffusion model: We introduce a two-stage training scheme that enables inference on speech audio across varying text, image, and video inputs, while maintaining accurate lip synchronization, together with a windowed audio conditioning strategy for precise speech-video alignment that does not require audio feature downsampling and remains robust across varying video frame rates.\n\n\n\n•\n\nLatent-space visual dialog editing:\nWe formulate transcript-driven video editing directly in latent space, supporting seamless addition, removal, and retiming of spoken segments.\n\n\n\n•\n\nIdentity-preserving long video generation:\nWe introduce a reference-based identity conditioning mechanism, Forward–Backward RoPE Conditioning, together with TeaCache-aware inference, to stabilize appearance and temporal coherence over long videos.\n\n\n\n\n\nEvaluations against recent I2V and V2V lip-sync benchmarks demonstrate that our method achieves SOTA visual quality and synchronization accuracy. In addition to offering competitive performance, our approach represents a foundational step toward utilizing video diffusion models as capable tools for editing human-centric video content.\n\n\n\n\n2 Related Works\n\nWith the advent of diffusion models [59, 71], the field of video generation [78] has proliferated in recent years. Coupled with powerful 3D VAEs [56], these models have the capability of reconstructing the details and dynamics of an entire frame (instead of a small crop), opening the door to generating novel frames that are coherent with the rest of the video. There are several possible input modalities, which can be combined together, that define the task of the model. The common modalities are: (i) Text-to-Video (T2V) synthesize the video from a textual input [107, 40, 38], (ii) Image-to-Video (I2V) animate a single image into a video [3], (iii) First-Last-frame-to-Video (FL2V) guide video generation between the given first and last frames [61, 106], and (iv) Video-to-Video (V2V) edit or transform video content while maintaining temporal consistency and structure. [111, 68]. The latest video generation works [30, 73, 35] focus on synthesizing clips that adhere to provided prompts by leveraging diffusion transformer blocks (DiTs) [83] as the main computational units in their models. A newer version of this, multi-modal diffusion transformer blocks (MM-DiTs) [22] allow multiple input modalities to be represented in a common token space, facilitating joint attention across them.\n\n\n\n2.1 Audio-Driven Talking Head Generation\n\nEarly Methods.\n\nAudio-driven facial animation, in particular lip-syncing, has been an active research topic with a variety of methods explored. GAN-based methods [53, 8, 33, 54, 88] achieved early success with appropriate audio representations, such as Wav2Lip and Wav2Vec [85, 2] for conditioning. These methods can indeed lip-sync a video, however are unable to make larger changes like head motion. Adding 3D Morphable Models [7] from traditional graphics as an intermediate representation allows enhanced control over the subject in the video. Lip-sync and head "
  },
  {
    "title": "A Federated and Parameter-Efficient Framework for Large Language Model Training in Medicine",
    "url": "https://arxiv.org/abs/2601.22124v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) have demonstrated strong performance on medical benchmarks, including question answering and diagnosis. To enable their use in clinical settings, LLMs are typically further adapted through continued pretraining or post-training using clinical data. However, most medical LLMs are trained on data from a single institution, which faces limitations in generalizability and ",
    "full_text": null
  },
  {
    "title": "Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics",
    "url": "https://arxiv.org/abs/2601.22123v1",
    "source": "arxiv",
    "summary": "Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this ",
    "full_text": null
  },
  {
    "title": "Alpha Discovery via Grammar-Guided Learning and Search",
    "url": "https://arxiv.org/abs/2601.22119v1",
    "source": "arxiv",
    "summary": "Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally ef",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Alpha discovery\n1.2 Our Work\n\n\n\n2 Problem Formulation\n\nEvaluation via Information Coefficient.\n\n\n\n3 Design Language of Interpretable Alphas\n\n\n3.1 Syntactically-Valid Alpha Language\n\nStructural Well-Formedness.\nOperator Arity Constraints.\n\n\n\n3.2 Semantically-Interpretable Alpha Language\n\nSemantic Constraints.\nLength Bounded Grammar α\\alpha-Sem-kk.\n\n\n3.3 Alpha Space Structure\n\n\n\n4 Reinforced Alpha Language Tree Search\n\n4.1 Decision-Making on Large Tree\n\n4.2 Reinforcement Learning Guided MCTS\n\nOverall Interaction Between RL and MCTS.\nMCTS Components.\n\n\n4.3 Syntax Representation Learning\n\n\n5 Experiments\n6 Conclusion\nA Tables\n\nB Algorithms\n\nB.1 Linear combination alpha factor algorithm\nB.2 Length control of semantic interpretable alpha factor generator\nB.3 Algorithm of Four Stages of MCTS\n\n\nC Supplement to Problem Formulation\nD Reinforcement Learning Framework\n\nE Search Space Complexity\n\nE.1 Unstructured Space Σ∗\\Sigma^{*}\nE.2 Syntactically Legal Space ℒsyn\\mathcal{L}_{\\mathrm{syn}}\nE.3 Semantically Legal Space ℒsem\\mathcal{L}_{\\mathrm{sem}}\nE.4 Empirical Verification\n\n\n\nF Details of Tree-LSTM\n\nF.1 N-ary Tree-LSTM (Position-Sensitive)\nF.2 Child-Sum Tree-LSTM\n\n\nG Calculation of Tree Similarity\n\nH AlphaCFG Framework Parameter Setting for Experiment\n\nH.1 MCTS Parameters\nH.2 Network Architecture\nH.3 Optimizer and Training Parameters\n\n\n\nI More Results of Experiment\n\nI.1 Data\nI.2 Comparison Methods\nI.3 Evaluation Metrics\nI.4 Comparison of Different Network Architectures\nI.5 Optimization of Combined Factor Parameters on the Validation Set\nI.6 Case Study of the interpretability of formulaic factors\n\n\n\n\n\n\n\nAlpha Discovery via Grammar-Guided Learning and Search\n\n\nHan Yang\n\n  \nDong Hao\n\n  \nZhuohan Wang\n\n  \nQi Shi\n\n  \nXingtong Li\n\n\n\nAbstract\nAutomatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\n\n1.1 Alpha discovery\n\nIn quantitative finance, alpha factors play a central role in asset management, quantitative trading, and investment decision-making. An alpha factor is an explicit function that maps historical market features, such as prices and volumes, to predictions of future returns. Alpha discovery refers to the systematic identification of such predictive functions from historical data and remains a core challenge due to the vast and complex space of possible functional forms. Beyond their practical importance, alpha discovery poses a fundamental machine-learning challenge: identifying symbolic functions that are both predictive and interpretable under severe combinatorial constraints.\n\n\nExisting approaches to alpha discovery can be broadly classified into three categories. Heuristic or expert-driven methods rely on financial intuition, such as value factors (e.g., price-to-earnings ratios (Fama and French, 1992)) and momentum factors (e.g., past 12-month returns (Carhart, 1997)), but lack scalability and are quickly arbitraged once widely adopted, reducing predictive accuracy over time. Data-driven learning methods, including regression (Bhandari et al., 2022; Qin et al., 2017; Dai et al., 2022; Mozaffari and Zhang, 2024), tree-based ensembles (Wang et al., 2023; Bisdoulis, 2024), unsupervised learning (Xu, 2025), and reinforcement learning (Lee, 2001), can capture complex nonlinear patterns, yet often suffer from limited interpretability and overfitting due to their black-box nature. Formulaic alpha methods (Zhang et al., 2020) emphasize human-readable mathematical expressions composed of predefined operators, offering transparency and interpretability, and have therefore regained recent attention.\n\n\nTable 1: Comparison of Alpha Discovery Methods\n\n\n\n\nCategory\nPros\nCons\n\n\n\n\nHeuristic / Expert\nIntuitive, easy to use\nLimited, quickly arbitraged\n\n\nData-driven Learning\nCaptures complex patterns\nBlack-box, less interpretable\n\n\nFormulaic Alpha\nInterpretable, transparent\nComputationally expensive\n\n\n\n\n\n\nOur work lies at the intersection of data-driven learning and formulaic alpha methods, aiming at the automatic discovery of explainable alpha factors. This problem can be viewed as symbolic regression (Makke and Chawla, 2024), which seeks explicit mathematical expressions that fit data while remaining interpretable, but is difficult due to its combinatorial search space and semantic equivalence among expressions. Early approaches such as genetic programming (GP) (Zhang et al., 2020) evolve expression trees to optimize information coefficients. More recent methods, including AlphaGen (Yu et al., 2023) and AlphaQCM (Zhu and Zhu, 2025), adopt reinforcement learning to improve scalability.\nExisting methods face the following fundamental challenges.\n\n\n(1) Lack of linguistic characterization leads to inefficient search in an unbounded space.\nAutomated discovery of formulaic alphas is fundamentally a problem of searching over mathematical languages, yet existing methods lack an explicit linguistic framework to organize and constrain this search. In the absence of formal grammatical structure, current approaches must explore vast, and often effectively infinite, combinatorial spaces of expressions, relying on ad hoc syntactic checks to ensure validity. This unstructured exploration severely limits sample efficiency, degrades model performance, and incurs substantial computational cost.\n\n\n(2) Semantic redundancy causes systematic waste in learning and search.\nMany syntactically distinct mathematical sequences correspond to the same underlying semantics, but existing methods mostly encode expressions as linear sequences and treat the variants as independent. As a result, semantically equivalent expressions are repeatedly explored and evaluated, leading to significant redundancy in representation learning and search, and greatly reducing efficiency.\n\n\n\n\n1.2 Our Work\n\nWe propose AlphaCFG,111Our source code is available at https://github.com/HanYang544/AlphaCFG a general linguistic–learning framework for the automatic discovery of interpretable alpha factors. The central idea is to treat alpha discovery as a structured language generation and learning problem, rather than an unstructured search over mathematical expressions. By combining formal grammar with learning and search, AlphaCFG provides a principled way to generate, validate, and optimize human-readable alpha factors. In this way, grammar serves as an explicit inductive bias that shapes both the search space and the learning dynamics.\n\n\n(1) Grammar-Constrained Alpha Factors.\nFrom a language-theoretic perspective, we first formalize the space of alpha factors as a structured mathematical language.\nWe propose two formal languages, α\\alpha-Syn and α\\alpha-Sem, that integrate context-free grammar (CFG) with finance domain–specific knowledge of alpha factors. α\\alpha-Syn enforces grammatical correctness, while α\\alpha-Sem further ensures financial semantic validity. These languages generate alpha expressions recursively in a tree-structured form, making tree-structure–based learning and optimization possible. To control complexity and reduce redundancy, we further enforce (i) length constraints to bound the search space, and (ii) expression-tree pruning to remove syntactically distinct but semantically equivalent factors.\n\n\n(2) Structure Characterization of Alpha Space.\nBuilding on this grammar-based language, we cast alpha discovery as a large Tree-Structured Linguistic Markov Decision Process (TSL-MDP), where each state is a partial expression, terminal states represent complete alpha factors, and rewards are given by the information coefficient (IC) on real market data. This formulation transforms alpha discovery from unstructured trial-and-error into a principled sequential decision process over the space of formulaic alpha factors.\n\n\n(3) Reinforcing MCTS with Syntax-Aware Learning.\nFinally, we design a learning and search algorithm that exploits the grammar-induced structure of the TSL-MDP. We employ a grammar-aware Monte Carlo Tree Search (MCTS), in which action selection is guided by a syntax-aware Upper Confidence Bound (UCB) rule. To generalize across the large state space, each partial expression tree is encoded using a Tree-LSTM, yielding structure-aware representations shared by a value network, which estimates expected performance from historical market data, and a policy network, which predicts promising alpha expansions. Through reinforced interaction between MCTS and these learned models, AlphaCFG progressively refines its search strategy and discovers high-quality alpha factors efficiently.\n\n\nAlphaCFG is not limited to trading strategies and naturally extends to other quantitative finance tasks, by allowing flexible customization of operators, grammatical structures, and objective functions. We use trading as a representative testbed to demonstrate the effectiveness of AlphaCFG.\nWe evaluate AlphaCFG on CSI 300 and S&amp;P 500 stocks, where it consistently outperforms strong baselines across mu"
  },
  {
    "title": "Defining Operational Conditions for Safety-Critical AI-Based Systems from Data",
    "url": "https://arxiv.org/abs/2601.22118v1",
    "source": "arxiv",
    "summary": "Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, thi",
    "full_text": null
  },
  {
    "title": "SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence",
    "url": "https://arxiv.org/abs/2601.22114v1",
    "source": "arxiv",
    "summary": "Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and",
    "full_text": null
  },
  {
    "title": "Diverse Approaches to Optimal Execution Schedule Generation",
    "url": "https://arxiv.org/abs/2601.22113v1",
    "source": "arxiv",
    "summary": "We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation",
    "full_text": null
  },
  {
    "title": "Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment",
    "url": "https://arxiv.org/abs/2601.22111v1",
    "source": "arxiv",
    "summary": "Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectori",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Synthetic Wind Model\n\n2.1 Atmospheric turbulence representation\n2.2 Von Kármán spectral model\n2.3 Mean wind, shear, and veer\n2.4 Rationale for model selection\n\n\n\n3 UAS Dynamics Model, Reference Trajectory, and Control Architecture\n\n3.1 Rigid-Body Dynamics Model\n3.2 Rotor Thrust and Torque Generation\n3.3 Motor Dynamics\n3.4 Aerodynamic Drag Model\n3.5 Reference Trajectory for Vertical Profiling\n\n\n\n4 Control System Design\n\n4.1 Position Control in the Horizontal Plane\n4.2 Vertical Velocity Control\n4.3 Attitude Control and Thrust Direction\n4.4 Rotor Allocation Matrix\n\n\n\n5 Local Wind Estimation from UAS Dynamics\n\n5.1 Feature Selection and Construction\n5.2 Training Dataset Generation Across Wind Regimes\n5.3 LSTM Network Architecture\n5.4 Local Wind Estimation Performance\n\n\n\n6 Physics-Informed Neural Network for Wind-Field Reconstruction\n\n6.1 Problem Formulation and Objectives\n6.2 Network Inputs and Outputs\n6.3 Feature Representation and Normalization\n6.4 Loss Function and Physical Regularization\n6.5 Training Strategy and Reconstruction Workflow\n\n\n\n7 Results\n\n7.1 Wind-Field Reconstruction Under Moderate Wind Conditions\n7.2 Effect of UAS Configuration on Reconstruction Accuracy\n\n\n8 Conclusions and Future Work\n\n\n\n\n\nPhysics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment\n\n\nAbdullah Tasim\n\n\nWei Sun\n\nSchool of Aerospace and Mechanical Engineering, University of Oklahoma, Norman, OK 73069, USA\n\n\n\nAbstract\nAccurate reconstruction of atmospheric wind fields is critical for applications such as weather monitoring, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, making full wind-field recovery challenging.\nThis study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained exclusively from a coordinated swarm of UAS. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM), and these estimates are assimilated into a physics-informed neural network (PINN) that reconstructs a continuous wind field as a function of space and time.\nFor local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m s-1 for the north and east components in low-wind conditions, 0.122 and 0.129 m s-1 under moderate-wind conditions, and 0.273 and 0.271 m s-1 in high-wind conditions. The vertical wind component exhibits higher error, with RMSE values of 0.029, 0.061, and 0.091 m s-1 across the same regimes, reflecting reduced observability from multirotor dynamics.\nThe physics-informed reconstruction successfully recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude, preserving mean flow direction and vertical shear while smoothing unresolved turbulent fluctuations. Under moderate wind conditions, the reconstructed mean wind field achieves overall RMSE between 0.118 and 0.154 m s-1 across the evaluated UAS configurations. The lowest reconstruction error of 0.118 m s-1 is obtained using a five-UAS configuration, while larger swarms do not yield monotonic improvement in accuracy. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without relying on dedicated wind sensors or fixed infrastructure.\n\n\nkeywords: \nUAS swarm , Local wind estimation (NED), Long Short Term Memory (LSTM) , Physics-informed neural networks (PINN) , atmospheric boundary layer (ABL) , turbulence simulation\n\n\n††journal: Atmospheric Environment\n\n\n1 Introduction\n\nUnderstanding how wind varies across space and time is key fundamental to a wide range of atmospheric and engineering applications, including numerical weather prediction, storm hazard assessment, and wind energy assessment. Regions that frequently experience strong winds are often the same areas most susceptible to high-impact events such as severe storms and tornadoes, making accurate characterization of wind fields important for both public safety and infrastructure planning. Although Doppler wind lidars, meteorological towers, and anemometers provide valuable wind measurements, their use is limited by deployment cost, operational constraints, and a rapid decline in coverage above a few hundred meters. As a result, large portions of the lower atmospheric boundary layer remain sparsely observed in both spatial and temporal dimensions [1, 2, 3].\n\n\nUAS have emerged as a complementary platform for atmospheric sensing because they provide high-resolution in situ observations and can be deployed flexibly at comparatively low cost [4, 5, 6]. However, measurements collected by a single UAS are confined to its flight trajectory, and local wind estimates derived from such paths do not reveal the broader three-dimensional structure of the surrounding flow. To support applications such as wind-energy resource assessment, turbine design, and characterization of boundary-layer dynamics, it is therefore necessary to reconstruct a complete four-dimensional wind field that evolves continuously in space and time.\n\n\nExisting efforts toward wind-field reconstruction have largely followed two complementary directions. The first direction relies on supervised learning methods or reduced-order representations to infer flow structure from limited measurements. In the context of wind-energy applications, data-driven mapping models have been developed to estimate wind characteristics and to improve understanding of inflow variability and its influence on turbine power production [7, 8, 9, 10]. While these approaches can provide practical predictive capability, their performance is often sensitive to the representativeness of the training data, and they may not enforce physical consistency when measurements are sparse or operating conditions differ from those seen during training.\n\n\nA second direction integrates physical constraints directly into the reconstruction process. PINN has been introduced as a mesh-free framework that blends sparse observations with governing equations by penalizing the residual of the underlying partial differential equations during training [11]. This class of methods is particularly attractive for wind-field reconstruction because it regularizes the solution in under-sampled regions using physically meaningful constraints rather than relying purely on interpolation.\n\n\nSeveral wind-focused studies have combined DWL measurements with physics-informed learning to reconstruct spatiotemporal flow fields. DWL systems are appealing because they enable remote sampling across planes or volumes and can provide repeated observations over time. However, DWL measurements are typically available as line-of-sight wind speeds, which represent projections of the wind vector onto the beam direction. Consequently, the inverse reconstruction problem can become ill-conditioned when only limited beam geometries are available. To address this challenge, physics-informed formulations have been proposed that assimilate sparse line-of-sight measurements while enforcing fluid-dynamic structure through equation-based constraints, enabling recovery of coherent wind features across space and time [12, 13, 14]. These studies demonstrate that physics-based regularization can substantially improve reconstruction fidelity relative to purely data-driven fitting when measurements are sparse.\n\n\nDespite these advances, DWL-centered physics-informed reconstructions remain constrained by practical sensing geometry and data type. Line-of-sight measurements do not directly provide horizontal wind components everywhere in the domain, and reconstruction accuracy depends strongly on scan strategy, sampling density, and the availability of multiple beams. In addition, DWL signal quality can degrade under conditions commonly encountered in the ABL, which further reduces the effective information content available for reconstruction [12, 14].\n\n\nThese limitations motivate the use of UAS measurements as an alternative information source for physics-informed wind-field reconstruction. Unlike DWL, which primarily provides projected velocities, a UAS can sample the flow in situ along programmable three-dimensional trajectories and can supply wind estimates in component form after onboard estimation. From a physics-informed learning perspective, component-wise wind observations provide a more direct anchor for the reconstructed state variables, reducing ambiguity in the inverse problem and decreasing reliance on sensing geometry assumptions. Moreover, coordinated multi-UAS sampling enables spatial coverage to be distributed adaptively in both the horizontal and vertical directions, which is particularly beneficial for resolving boundary-layer shear, directional veer, and evolving turbulent structures that are difficult to capture using fixed or beam-limited sensors. As a result, physics-informed reconstruction driven by UAS-based measurements offers a practical pathway toward multi-dimensional wind-field reconstruction with improved flexibility in sampling design and higher information density per unit deployment [11, 12, 14, 7].\n\n\nThis work addresses these limitations by introducing a wind-field reconstruction framework that relies exclusively on measurements collected by a coordinated swarm of UAS and enables reconstruction of a fully four-dimensional wind field in space and time. In contrast to prior physics-informed wind reconstruction studies that assimilate fixed or remote-sensing observations and are typically restricted to horiz"
  },
  {
    "title": "Value-Based Pre-Training with Downstream Feedback",
    "url": "https://arxiv.org/abs/2601.22108v1",
    "source": "arxiv",
    "summary": "Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lig",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries and Related Work\n\n2.1 Pretraining as Predictive Learning\n2.2 Related Works\n\n\n\n3 Pretraining with Downstream Feedback\n\n3.1 Learning to Design Pretraining Tasks\n3.2 Value Function for Downstream Feedback\n3.3 Algorithmic Instantiations\n3.4 Theoretic Guarantee\n\n\n\n4 Experiments\n\n4.1 Setup\n4.2 Evaluation on Selected Downstream Tasks\n4.3 Feedback Effects on Generalization\n4.4 Scaling Weak-to-Strong Supervision\n\n4.5 Ablation Studies\n\nDecontamination.\n\n\n\n\n5 Discussion and Conclusion\n\nA Additional Experimental Details\n\n\nA.1 Language: Controlled Continued Pretraining\n\nTask and compute-matched baseline.\nUnlabeled data and preprocessing.\nLearner models.\nTask designer: adaptive top-KK target distributions.\nDesigner architecture and parameterization.\nValue signal and meta-update.\nDownstream evaluator data.\nHyper-parameter selection (language).\nEvaluation (GSM8K).\n\n\n\nA.2 Vision: Continued Self-Supervised Learning with Learned Views\n\nTask and compute-matched baseline.\nBackbone and SSL objective.\nTask designer: learned soft masks for view generation.\nDesigner architecture.\nDownstream evaluators (dense tasks).\nMeta step details (vision).\nEvaluation protocols.\n\n\n\nA.3 Hyper-parameter Sweeps\n\nVision sweeps (W&amp;B Bayesian optimization).\nLanguage sweeps.\n\n\nA.4 Generalization Tests with OMEGA Benchmark\n\nA.5 Computation Overhead\n\nGoal.\nHardware and software.\nControlled training configuration.\nTiming protocol and steady-state window.\nMemory measurement.\nIsolating value-update cost.\nValue update cadence.\nData pipeline considerations.\nReporting.\n\n\nA.6 Validating the first-order value estimate\nA.7 Token-efficiency diagnostic for language\nA.8 Multi-objective tradeoff details in vision\n\n\nB Proofs\n\n\n\n\n\nValue-Based Pre-Training with Downstream Feedback\n\n\nShuqi Ke\n\n  \nGiulia Fanti\n\n\n\nAbstract\nCan a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B–7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 1212% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nFigure 1: Value-Based Pretraining with Downstream Feedback. Today, the learner θ\\theta trains on unlabeled data using a proxy objective LpreL_{\\mathrm{pre}}, for a frozen pretraining task.\nIn V-Pretraining, a small task designer ϕ\\phi is trained on a small feedback set of verifiable downstream tasks with predefined value functions, but never updates the learner on downstream labels. ϕ\\phi thus reshapes the pretraining target (or views) so that the induced SSL update aligns with downstream improvement, calculated via the value function.\nRelative to current pretraining methods, V-Pretraining adds the components in the left blue box. \n\n\nThe era of blind scaling that improves models primarily by scaling proxy-objective pretraining is showing signs of diminishing returns (Lin et al., 2025; Kaplan et al., 2020; Hoffmann et al., 2022). Yet foundation models are still trained in a remarkably undirected way: we minimize a static self-supervised proxy loss on massive, weakly curated data, and hope the capabilities we care about (reasoning, dense perception, tool use, world modeling) emerge as a byproduct. In language, the proxy is next-token prediction (Brown et al., 2020; OpenAI et al., 2024; Yang et al., 2025); in vision, it is self-supervised reconstruction or representation learning under augmentations (Chen et al., 2020b; He et al., 2022; Assran et al., 2023; Siméoni et al., 2025). While this recipe scales, it functions as an open-loop system and learns from a “static world”: the optimization trajectory is fixed at the start, ignoring whether intermediate steps actually align with complex human goals.\n\n\nThis open-loop nature can lead to sample inefficiency in pretraining.\nUnlike humans, who utilize closed-loop feedback to rapidly correct errors and master tasks, models blindly consume trillions of tokens without corrective guidance. Current pipelines inject feedback mostly after pretraining via supervised fine-tuning or preference optimization (Christiano et al., 2017; Ouyang et al., 2022; Rafailov et al., 2023). These stages are effective, but they arrive late. By the time downstream feedback is applied, the representation has already been shaped by millions of proxy-gradient steps that were agnostic to the target behavior. To break the ceiling of blind scaling, we ask: can we introduce scalable supervision into pretraining, turning an open-loop process into a controlled trajectory toward what we actually want?\n\n\nWe introduce V-Pretraining: Value-based Pre-Training with downstream feedback, a framework for controlled pretraining. Standard pretraining fixes the unlabeled stream and a proxy task construction (e.g., one-hot next-token targets in language, or a fixed augmentation pipeline in vision) and optimizes the resulting proxy loss. We keep the unlabeled stream and learner training budget fixed, but add a lightweight task designer trained on a small labeled verification (value) set for the capability of interest (e.g., GSM8K for reasoning, ADE20K/NYUv2 for dense vision). Crucially, the verification set is used only as an evaluator: the learner is never updated on verification labels. Instead, the task designer reshapes the pretraining target (the supervision signal inside predictive learning) so that the learner’s next unlabeled update is predicted to be more valuable for the target capability.\nIn language, the designer replaces one-hot next-token labels with adaptive soft targets supported on the learner’s top-KK candidates. In vision SSL, it replaces a fixed augmentation pipeline with instance-wise learned views optimized for transfer, especially dense prediction.\n\n\nDirectly optimizing the task designer for downstream performance is computationally prohibitive: it is a bilevel problem that would require differentiating through long pretraining trajectories (Maclaurin et al., 2015; Franceschi et al., 2018). As a result, prior efforts to design task-aware SSL methods were largely tailored to specific domains and tasks, which allowed them to avoid this computational bottleneck (Zhang et al., 2019; Tian et al., 2020; Shi et al., 2022). A key insight of our work is showing how to efficiently generalize task-aware pretraining (including SSL) to different tasks and modalities by defining the value of a pretraining step via an influence-style first-order estimate: the alignment between proxy and downstream gradients (Koh and Liang, 2017; Pruthi et al., 2020a). This yields differentiable meta-updates for the task designer while leaving the learner’s pretraining loop essentially unchanged. Because V-Pretraining intervenes only through target/view construction, it can be layered on top of diverse pretraining objectives (e.g., next-token prediction, masked modeling, and joint-embedding SSL) without changing the learner architecture or optimizer. This makes V-Pretraining largely orthogonal to advances in scaling, data mixture/curriculum design, and post-training alignment, and in principle combinable with them.\n\n\nAcross language and vision, value-based pretraining turns small verified feedback into measurable gains in the expensive unlabeled phase. In language, continued pretraining of Qwen1.5 models on a math corpus improves GSM8K Pass@1 by 2-14% across 0.5B/4B/7B using only 12%12\\% GSM8K training examples for feedback and without updating the learner on GSM8K labels. In vision, dense feedback improves segmentation and depth while maintaining or improving ImageNet linear accuracy.\nFurther, these improvements do not come at the expense of generalization to other tasks.\n\n\nContributions. We make four contributions. (1) V-Pretraining: a novel framework for directed pretraining with downstream feedback: we present a principled formulation of controlled pretraining as goal-directed target or view design, separating a large learner trained only on unlabeled data from a lightweight controller trained on a small labeled verification set. (2) A scalable learning rule for task design: we introduce an influence-style first-order value objective based on proxy–downstream gradient alignment that avoids differentiating through long pretraining trajectories. (3) Efficient instantiations across modalities: we instantiate the framework for natural language (adaptive top-KK soft targets) and vision (instance-wise learned views) without changing the learner’s underlying pretraining loop. (4) Compute-matched evidence and diagnostics: we empirically show that under matched learner update budgets, the V-Pretraining framework increases downstream value per pretraining step for two modalities (vision and language) in various settings. We support our claims with extensive ablations (random feedb"
  },
  {
    "title": "Prior-Informed Flow Matching for Graph Reconstruction",
    "url": "https://arxiv.org/abs/2601.22107v1",
    "source": "arxiv",
    "summary": "We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matchin",
    "full_text": null
  },
  {
    "title": "ECO: Quantized Training without Full-Precision Master Weights",
    "url": "https://arxiv.org/abs/2601.22101v1",
    "source": "arxiv",
    "summary": "Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixtu",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nQuantized/Quantization-Aware Training.\nEfforts Towards Low-Precision Accumulation.\nOptimizer State Quantization.\nError Feedback.\n\n\n\n3 Method\n\n\n3.1 Notation and Background\n\nNotation.\nQuantization.\nQuantization-Aware Training with Master Weights.\n\n\n\n3.2 ECO\n\nSGDM.\nAdam.\n\n\n\n3.3 Convergence Analysis\n\n3.3.1 Setup and Algorithm\n3.3.2 Assumptions\n3.3.3 Virtual Sequence Analysis\n3.3.4 Descent and Momentum Bounds\n3.3.5 Convergence Theorem\n\n3.3.6 Deterministic Rounding\n\nComparison of Noise Floors.\n\n\n\n\n\n3.4 Lower-Bound on Worst-Case Behavior\n\nSGDM with Master Weights.\nNaive Master Weight Removal.\nECO.\n\n\n\n\n\n4 Experiments\n\n4.1 Baselines\n\n4.2 Scaling Law Experiments\n\nSetting.\nResults.\nMemory and Runtime.\nStudy on the Similarity of\nConsecutive Errors.\n\n\n\n4.3 Gemma 3 1B Pre-training\n\nSetting.\nResults.\n\n\n\n4.4 Mixture of Experts Pre-training\n\nSetting.\nResults.\nDiscussion on Memory.\n\n\n\n4.5 DeepSeek-MoE-16B Fine-tuning\n\nSetting.\nResults.\n\n\n\n\n\n5 Conclusion\n\nLimitations.\n\n\n\nA Exact Error Injection\n\nSGDM with Master Weights.\nNo-Master-Weight SGDM with Ideal Momentum Injection.\nTheorem (Exact equivalence).\nProof.\n\n\n\nB Convergence Proofs\n\nB.1 Proof of Lemma 3.5\nB.2 Proof of Lemma 3.6\nB.3 Proof of Lemma 3.7\nB.4 Proof of Theorem 3.8\nB.5 Proof of Lemma 3.9\nB.6 Proof of Theorem 3.10\n\n\n\nC Formal Analysis of the Worst-Case Lower-Bounds\n\nStability.\n\nC.1 Fundamental limits on f​(x)=L2​x2f(x)=\\frac{L}{2}x^{2}\n\n\nC.1.1 SGDM with master weights\n\nAlgorithm.\nLinear form.\nStationary second moments.\nLimit of the squared gradient.\n\n\n\nC.1.2 Naive master-weight removal\n\nAlgorithm.\nLinear form.\nStationary second moments.\nDivergence as η→0\\eta\\to 0.\n\n\n\nC.1.3 ECO: momentum injection eliminates the 1/η1/\\eta blow-up\n\nAlgorithm.\nLinear form.\nStationary second moments.\nFinite noise floor as η→0\\eta\\to 0.\nInterpretation.\n\n\n\n\n\n\n\n\n\n\n\n\n\\uselogo\\correspondingauthor\nMahdi Nikdan (nikdanmahdi@gmail.com)\n\nECO: Quantized Training without Full-Precision Master Weights\n\n\nMahdi Nikdan\n\nGoogle Research\n\nISTA\n\n\nAmir Zandieh\n\nGoogle Research\n\n\nDan Alistarh\n\nISTA\n\n\nVahab Mirrokni\n\nGoogle Research\n\n\n\nAbstract\nQuantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as master weights. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30–800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.\n\n\n\n1 Introduction\n\nFigure 1: Static Memory Used vs Validation Loss comparing the standard BF16, FP8 with Master weights (FP8 w/ MW) baselines with standard stochastic rounding (FP8 w/o MW + SR) and ECO. ECO with stochastic rounding (SR) provides a significantly better Pareto frontier. Gradient accumulation is disabled in all cases.\n\n\nScaling Large Language Model (LLM) training comes with substantial computational and memory costs. As models have grown from billions to trillions of parameters, training memory has become a central bottleneck. Low-precision training has therefore emerged as a practical direction: recent FP8 [fp8lm, deepseekv3], and even lower precision [quest] training methods can reduce activation memory and accelerate training while maintaining stable optimization.\n\n\nDespite this progress, a key overhead in quantized training remains untouched: the presence of master weights. Most quantized and quantization-aware training pipelines still preserve a high-precision copy of the parameters (typically FP32) to accumulate gradient updates. This is largely because many updates are smaller than the discretization gap of low-precision formats: applying them directly to quantized weights can make updates vanish or incur large quantization noise. As a result, the model weight memory footprint often stays similar to the high-precision baseline, even when the forward and backward passes are heavily quantized. Even carefully engineered FP8 training systems explicitly retain high-precision accumulators for stability [fp8lm, deepseekv3]. The issue is especially pronounced for Sparse Mixture of Experts (SMoE) models, where only a subset of parameters is active per token, yet all master weights must reside in memory.\n\n\nMore broadly, attempts to avoid high-precision accumulation either do not scale to LLM training [ondevice] or have only been effective in narrow settings [elmo]. This leaves a clear gap: a general method that removes master weights without sacrificing convergence or introducing additional memory overhead. Eliminating master weights can yield memory savings comparable to quantizing optimizer states (e.g., momentum buffers), an approach that has been widely explored and is very popular [opt8bit].\n\n\nIn this work, we introduce the Error-Compensating Optimizer (ECO), which enables accurate quantized training without full-precision master weights, and thus zero extra memory overhead. The key idea is the following: after updating each layer’s parameters, we quantize the updated weights and inject the resulting quantization error into the optimizer’s momentum buffer. This creates an error-feedback loop that carries forward the lost updates and compensates for them in subsequent steps, allowing updates to be applied directly to quantized parameters.\n\n\nThe resulting ECO iteration is simple to implement and requires no extra hyperparameter tuning. It further comes with theoretical guarantees. We study the convergence behavior of ECO applied to the SGD with momentum optimizer with momentum factor β\\beta. Under standard non-convex assumptions and a decaying learning rate, we prove that ECO converges to a constant-radius neighborhood of the true optimum. Moreover, this radius is only a 11−β2\\frac{1}{1-\\beta^{2}} factor worse than the best achievable bound when using master weights, where a nonzero error is unavoidable because the solution must lie on the quantization grid. We further construct a quadratic example showing that this bound is tight up a constant factor. In the same example, we show that naively removing master weights (without momentum error injection) yields a stationary error that scales inversely with the learning rate, and therefore diverges as the learning rate decays to zero.\n\n\nWe evaluate ECO with FP8 quantization across scaling law studies on small transformers (30M–800M parameters) [quest, quartet], pre-training a Gemma-3 1B [gemma3] and an SMoE 2.1B model, and fine-tuning a DeepSeek-MoE-16B model [deepseekmoe]. Across settings, ECO nearly matches the validation loss of baselines that rely on master weights while significantly outperforming naive master weight removal. Furthermore ECO can reduce static memory usage by up to 25%, shifting the Pareto frontier between memory consumption and validation loss, as illustrated in Figure˜1.\n\n\n\n\n2 Related Work\n\nQuantized/Quantization-Aware Training.\n\nQuantization-aware training (QAT) aims to enable low-precision inference by simulating quantization effects on weights and optionally activations during training [lsq, pact, quest, dorefa, qil, bitnet, effqat, llmqat]. Quantized training methods go further by quantizing the backward pass computation to accelerate training [halo, quartet, alberttseng, fp4alltheway, deepseekv3]. Post-training quantization (PTQ) methods such as gptq, awq, quarot, spinquant, flatquant are computationally cheaper, but they typically incur larger accuracy degradation than QAT, especially at very low precision. Despite these advances, most QAT frameworks still rely on high-precision master weights to accumulate updates. Even recent QAT training systems such as FP8-LM [fp8lm], DeepSeek-V3 [deepseekv3], and Kimi-K2 [kimik2], who have rigorously tuned their quantization scheme, explicitly keep high-precision accumulators to maintain stability. In this context, ECO is complementary to existing QAT and quantized training methods: it targets the remaining dependence on master weights.\n\n\n\nEfforts Towards Low-Precision Accumulation.\n\nAvoiding master weights has proven difficult outside restricted settings. FP8-LM reports that FP8 accumulation fails at large LLM scales [fp8lm]. ondevice show that with careful gradient rescaling, INT8 accumulators can be stable for small convolutional networks that fit within 256KB of memory. APT [apt] varies accumulator bit-width across layers for edge-device training. Collage [collage] replaces FP32 with two BF16 accumulators due to a hardware constraint. bf16sr argue that stochastic rounding is important for BF16 accumulation, and ELMO [elmo] applies stochastic rounding to reduce the accumulator precision of the LLM head layer to BF16/FP8. Overall, there exists no general approach that enables sub-16-bit accumulation for large-scale LLM training, leaving an important gap that ECO addresses.\n\n\n\nOptimizer State Quantization.\n\nA related line of work quantizes optimizer states (e.g., first and second moments) rather than model"
  },
  {
    "title": "Boosting CVaR Policy Optimization with Quantile Gradients",
    "url": "https://arxiv.org/abs/2601.22100v1",
    "source": "arxiv",
    "summary": "Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that lev",
    "full_text": null
  },
  {
    "title": "GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization",
    "url": "https://arxiv.org/abs/2601.22095v1",
    "source": "arxiv",
    "summary": "The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel ",
    "full_text": null
  }
]