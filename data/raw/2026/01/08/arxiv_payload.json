[
  {
    "title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms",
    "url": "https://arxiv.org/abs/2601.04191v1",
    "source": "arxiv",
    "summary": "Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheel",
    "full_text": "\n\n\n\n1 Introduction\n2 System Design\n\n3 System Implementation\n\n3.1 BDI-Based Maze Solving Behavior\n\n\n4 Demonstration\n5 Conclusions\n\n\n\n\n11institutetext: Institut für Datentechnik und Kommunikationsnetze, TU Braunschweig, Germany\n11email: {negar.halakou, juan-felipe.gutierrez-gomez, ye.sun1, xueming.wu, h.jiang, yilun.song, andres.gomez}@tu-braunschweig.de\n\nEmbedding Autonomous Agents in Resource-Constrained Robotic Platforms\n\n\nNegar Halakou\n\n  \nJuan F. Gutierrez\n\n  \nYe Sun\n\n  \nHan Jiang\n\n  \nXueming Wu\n\n  \nYilun Song\n\n  \nAndres Gomez\n\n\n\nAbstract\nMany embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities.\nEnabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control.\nIn this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data.\nExperimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond.\nThese results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware.\nThis integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.\n\n\n\n1 Introduction\n\nResource-constrained robotic platforms play an important role in enabling low-cost, large-scale deployments in many application scenarios.\nDue to their limited size, memory, and processing power, these systems tend to use centralized processing.\nBy only perceiving information locally and reasoning remotely, these systems suffer from limited autonomy and longer latencies in the actuation loop.\n\n\nAs the computational capacity of microcontrollers increases, researchers have begun introducing techniques like onboard machine learning, which can efficiently extract better information from its environment.\nIn [2], Hao et al. propose a microrobot weighing less than 22 grams, including a camera, microcontroller, and actuator.\nThe microrobot can detect a target and follow it, using only local data in a closed feedback loop.\nThe entire logic, however, is implemented using ad-hoc C/C++ code and exhibits only reactive behavior.\n\n\nAgent-oriented programming can bring many advantages to robotic platforms, facilitate the development of autonomous reasoning, swarm intelligence, among others.\nWhile existing Java-based frameworks have already integrated robotic control into the AgentSpeak language [5], these are not compatible with resource-constrained robotic platforms based on microcontrollers (MCUs).\nIn order to achieve this, specialized toolchains and frameworks have been developed to cope with the limited memory and processing capabilities of MCUs [4].\n\n\nIn this demo paper, we show the feasibility of embedding autonomous BDI agents in a microcontroller-based two-wheeled robotic platform and tasking the agent to solve a line-following maze. In order to achieve this, we have defined and implemented a minimal API for controlling our robot’s movement. This hardware-dependent code can be easily reused by any AgentSpeak script code running on our robot.\nLastly, we have experimentally validated our autonomous agent framework, showing that our reasoning cycles can execute fast enough for our robot to efficiently solve mazes.\n\n\n\n\n2 System Design\n\nAn autonomous agent is an intelligent system capable of perceiving its environment through sensors and acting on that environment using actuators [1].\nUnlike the imperative programming typically used in embedded systems, agents operate autonomously, making rational decisions, based on their perceptions and internal knowledge to achieve their goals.\nWhat sets agents apart is their autonomy.\nThey do not require constant direction from humans or other systems.\n\n\nThe Belief-Desire-Intention (BDI) model is one approach to implement the autonomous and intelligent behavior of agents.\nIt provides a structured approach for designing autonomous agents by defining their behavior through three key elements: beliefs, desires, and intentions.\nAutonomous agents can be programmed using the BDI paradigm as implemented in the AgentSpeak language and executed by the Jason interpreter.\nA simplified variant of Jason is used as the basis for the Embedded-BDI framework [3].\nThis framework consists of a translation engine that converts AgentSpeak programs into optimized C++ code, a runtime library responsible for executing the agent’s reasoning process, and hardware-dependent code;\ntogether, these components form an executable binary.\nThe workflow of the framework involves programming the agent’s deliberation logic in AgentSpeak, while the perception and action functions, along with other hardware-specific code, are implemented in C/C++ [6].\n\n\nBuilding on these principles, in this work, we embed a BDI agent into a robotic platform to enable autonomous maze exploration.\nThe agent forms beliefs based on its perception of the environment through line sensors, which detect intersections and path availability.\nIts main desire is to reach the goal, while intentions are generated and dynamically updated as navigation plans following the left-hand rule.\nThe BDI agent manages navigation and decision-making by reasoning over its beliefs and selecting appropriate actions.\nThese intentions are executed by the robot’s actuators, enabling it to move forward or turn as needed.\n\n\n\n\n3 System Implementation\n\nWe utilize a Pololu 3pi+ 2040 robot (Standard Edition), which features an RP2040 microcontroller with 264 kB of onchip SRAM and 16 MB of external flash.\nThe robot includes five downward-facing reflectance sensors for line following, and two DC micro metal gear motors that independently drive the left and right wheels, enabling precise movement control via PWM signals.\nTo implement autonomous decision-making, we integrated the Embedded-BDI framework111https://embedded-bdi.github.io/ with Pololu’s bare-metal firmware.\nThis integration enables us to define the agent logic using AgentSpeak.\nThe corresponding code is shown in Listing 1. \n\n\n\nListing 1: BDI agent logic in AgentSpeak for left-hand rule navigation.\n\n⬇\n\n1!solve_maze.\n\n\n2\n\n\n3+!solve_maze : at_intersection &lt;-\n\n\n4 !!handle_intersection.\n\n\n5\n\n\n6+!solve_maze &lt;-\n\n\n7 follow_segment;\n\n\n8 !!solve_maze.\n\n\n9\n\n\n10+!handle_intersection &lt;-\n\n\n11 check_situation;\n\n\n12 !!make_decision;\n\n\n13 !!solve_maze.\n\n\n14\n\n\n15+!make_decision : goal_found &lt;-stop.\n\n\n16+!make_decision : path_left &lt;- turn_left.\n\n\n17+!make_decision : path_straight &lt;- forward.\n\n\n18+!make_decision : path_right &lt;- turn_right.\n\n\n19+!make_decision &lt;- rotate_180.\n\n\n\n\n\n3.1 BDI-Based Maze Solving Behavior\n\nAfter powering on, the robot initializes its hardware components.\nThe process begins when the user presses button A, which triggers sensor calibration to adjust the line sensors for accurate path detection.\n\n\nOnce the system is ready, the BDI agent starts execution with the initial goal solve_maze.\nThe robot’s behavior is managed in a cyclic manner through the agent’s reasoning cycle.\nIn each step, the agent evaluates whether the robot is currently at an intersection.\nWhen an intersection is detected, the agent posts a new goal handle_intersection using the !! operator.\nAlthough this creates a separate intention, it typically begins execution immediately within the same cycle.\nDuring this intention, the agent executes check_situation to interpret sensor data and form beliefs such as goal_found, path_left, or path_right.\nBased on these beliefs, the agent proceeds with make_decision, selecting the appropriate action according to the left-hand rule.\n\n\nBased on this rule, when the robot arrives at an intersection, it first checks whether a line is detected on the left.\nIf so, it turns left.\nIf no line is detected on the left, it attempts to move forward.\nIf no visible path is available ahead, it then checks the right side.\nIf no path is detected in any direction, the robot performs a 180-degree turn to continue exploration.\nIf no intersection is detected, the agent continues along the current path by executing follow_segment.\nThis loop of perception, reasoning, and action repeats until the goal is found, at which point the robot stops and the task is considered successfully completed.\n\n\n\n\n\n(a) Digital design of the maze.\n\n\n\n\n(b) Real-world implementation. \n\n\n\nFigure 1: The designed maze used for planning (left) and the physical implementation with the robot (right).\n\n\n\n\n\n4 Demonstration\n\nThe effectiveness of the AgentSpeak-based algorithm was demonstrated using the maze shown in Figure 1.\nThe maze was specifically designed to first calibrate the line sensors at the starting area, enabling the robot to distinguish between black and white surfaces.\nIts layout forces the robot to follow the longest possible path before reaching the goal.\nA demonstration video and AgentSpeak-based maze solver robot software are available in a public repository222Demo-Paper Repo: https://git.rz.tu-bs.de/ida/rosy/public/publication-repos/eumas-2025-mas-pololu-demo-paper.\nTo analyze the agent’s behavior during maze solving, we measured the execution times of three main components in its reasoning cycle: belief update, plan selection, and intention execution.\n\n\nFigure 2: Annotated GPIO activity during the execution cycle of an agent-based maze-solving robot.\n\n\nThese measurements were recorded as digital signals, with 1 indicating the function is running and 0 indicating it is not, as shown in Figure 2.\nBased on these measurements, the total time taken to solve the maze was approximately 59 s.\nThis duration corresponds to the trajectory from point S (start point) to point E (endpoint) along the longest path, during which the reasoning cycle was executed 287 times.\n\n\nOn averag"
  },
  {
    "title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition",
    "url": "https://arxiv.org/abs/2601.04181v1",
    "source": "arxiv",
    "summary": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We pro",
    "full_text": null
  },
  {
    "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation",
    "url": "https://arxiv.org/abs/2601.04176v1",
    "source": "arxiv",
    "summary": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data poin",
    "full_text": null
  },
  {
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "url": "https://arxiv.org/abs/2601.04171v1",
    "source": "arxiv",
    "summary": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers a",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Verification for SWE Agents\n2.2 Rubric-based Verification\n\n\n\n3 Experimental Design\n\n\n3.1 Agentic Rubrics\n\nRubric Generation.\nRubric Grading.\n\n\n\n3.2 Test-time Scaling with Agentic Rubrics\n\nSetup\nCandidate patch generation\nEvaluation protocol (Best@K).\n\n\n\n3.3 Baselines\n\nImplementation details.\n\n\n\n\n\n4 Results\n\n\n4.1 Test Time scaling with Agentic Rubrics\n\nAgentic Rubrics improve Best@K selection.\nRubrics provide most effective agentic signal.\n\n\n\n4.2 Analysis of Agentic Rubrics\n\n\n4.2.1 Rubric Score Alignment Analysis\n\nRubric scores separate passing vs. failing patches.\nGround-Truth Patch Agreement\nTakeaway\n\n\n\n4.2.2 Rubric Utility Analysis\n\nWhen rubrics and tests agree.\nWhen rubrics are stricter than tests.\nTakeaway\n\n\n\n\n\n\n\n5 Ablations\n\n\n5.1 Rubric-Agent Model Choice\n\nTakeaway\n\n5.1.1 Training Open-Weight Rubric Agents\n\nTraining Setup.\nResults.\n\n\n\n\n5.2 Impact of Repository Grounding\n5.3 Sensitivity to Judge Model Choice\n\n\n\n6 Related Work\n\nCoding Agents and Test Time Scaling\nRubrics as verifiers for LLMs\n\n\n7 Conclusion\n8 Limitations\n\nA Appendix\n\nA.1 Analyzing agentic rubric scores against Ground-Truth patch\nA.2 Agentic abilities of rubric generation models\nA.3 Cost analysis for agentic verification methods\nA.4 Rubric Flakiness Study\nA.5 Hybrid verifiers using rubrics v/s classifier\nA.6 Categories of rubric utility classification\nA.7 SWE Agent Setup\nA.8 Rubric and their grading - Illustrative Examples\nA.9 Rubric Examples and grading\nA.10 Prompts - Baselines and Agentic Rubrics\nA.11 Rubric Judge Prompt\nA.12 Rubric Utility Analysis Prompt\n\n\n\n\n\n\n\nAgentic Rubrics as Contextual Verifiers for SWE Agents\n\n\nMohit Raghavendra1,∗\n\n\n\n\nAnisha Gunjal1,∗\n\n\n\n\nBing Liu1\n\n\n\n\nYunzhong He1\n\n\n\n\n\nAbstract\nVerification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.\n\n11footnotetext: Equal contribution.\n\n\n\nmohit.raghavendra@scale.com  \n\n https://scale.com/research/agenticrubrics\n\n\n\n1 Introduction\n\nFigure 1: Agentic rubric pipeline. In the rubric-generation phase (left), a rubric agent inspects the codebase and PR description using repository tools, then produces a rubric.yaml organized along four rubric axes (File Change, Spec Alignment, Integrity, Runtime). In the verification phase (right), a SWE agent proposes a patch, which is graded against the rubric to yield an execution-free verifier score.\n\n\nLarge Language Models (LLMs) have rapidly advanced on coding tasks, enabling increasingly capable software engineering (SWE) agents for realistic code editing and bug fixing [28, 9, 21]. A central bottleneck in training and evaluating such agents is verification: determining whether a candidate patch is correct, complete, safe, and aligned with the intended behavior. Verifier’s Law links the ease of training AI systems on a task to the efficiency and reliability of verifying candidate solutions [22]. In SWE agent, strong verification plays a dual role. It provides supervision for post-training with verifiable rewards [13], and it improves inference through test-time scaling by sampling multiple candidates and selecting the best one using a verifier [3].\n\n\nCurrent approaches use a range of verifiers, including unit tests (human or LLM-generated), learned patch classifiers, similarity metrics, and LLM judges [12, 23, 10, 24]. Verification via code execution is environment-aware, but can be costly to scale due to per-instance setup (e.g., sandbox initialization), and may yield sparse or brittle signals, including limited distinguishability and test toxicity [6, 10]. In contrast, execution-free signals are operationally lightweight, but can be less reliable [4], less interpretable, and prone to shallow cues. As SWE agents expand to more open-ended, goal-driven tasks and long-tail repositories, verifiers must become both scalable and codebase-specific.\n\n\nTo close this gap, we explore Agentic Rubrics. In our setup, illustrated in Figure 1, an expert rubric agent first interacts with a sandboxed repository to synthesize context-grounded rubric criteria; after rubric generation, candidate patches are scored without executing code, enabling scalable verification. We build on rubric-based verification [16, 25], which decomposes correctness into interpretable criteria that capture partial progress and surface failure modes. For SWE, rubrics written from the problem statement alone are often under-specified because they lack repository-specific context. Our rubric generation is therefore agentic: the verifier actively explores the repository to ground criteria in relevant code paths, interfaces, and project conventions, yielding rubric items that are more specific and consistently gradable. We evaluate Agentic Rubrics via best-of-KK selection under parallel test-time scaling on SWE-Bench Verified, ablate different design decisions and provide detailed analyses of rubric alignment and utility.\n\n\nOur contributions are:\n(1) We study Agentic Rubrics, a repository-grounded rubric generation paradigm with execution-free scoring for patch selection and post-training.\n(2) We show that Agentic Rubrics consistently outperform strong test-based and execution-free verifier baselines under parallel test-time scaling on SWE-Bench Verified.\n(3) We analyze why Agentic Rubrics work, demonstrating alignment with ground-truth tests and showing that rubrics surface diagnostic concerns (e.g., unnecessary edits or missing edge-case handling) even when tests pass.\n(4) We demonstrate that agentic rubric generation can be distilled into smaller open-weight models, enabling scalable deployment.\n\n\n\n\n2 Preliminaries\n\n\n2.1 Verification for SWE Agents\n\nWe consider a verifier as a procedure that assigns a score to a candidate patch for a given issue, with the goal of selecting or training toward higher-quality solutions. Prior work in SWE Agent settings commonly uses two broad classes of verification signals.\nExecution-based methods verify patches by executing code, most often by running unit tests (human-authored ground-truth or LLM-generated) [6]. Execution-free methods assess patch quality without running the repository, by reranking candidates using learned patch classifiers/verifiers, similarity metrics, or LLM judges [23].\nThese approaches occupy different points in the trade-off space between repository grounding, operational cost, and reliability [10]. Execution-based verification is environment-aware but can require per-instance setup (e.g., sandbox initialization) and may yield sparse or brittle signals (e.g., limited distinguishability or test toxicity). Execution-free verification is operationally lightweight, but can be less reliable [4], less interpretable, and sometimes sensitive to surface-level cues (e.g., stylistic patterns, non-semantic artifacts) rather than functional correctness.\n\n\n\n\n2.2 Rubric-based Verification\n\nA rubric verifies a candidate patch by decomposing correctness into a small set of explicit criteria Arora et al. [2]. Concretely, a rubric consists of criteria texts (optionally grouped by axes) with per-criterion weights, and a scoring rule that aggregates criterion-level judgments into a single verifier score. Given a problem and a candidate patch, a judge assigns each criterion a score (e.g., binary or graded) and aggregates them to obtain an overall patch score used for selection or learning.\n\n\nFor SWE tasks, a key practical consideration is grounding. Criteria written solely from the problem statement can omit repository-specific interfaces, constraints, and conventions, which makes judgments less precise and less consistent across patches. This motivates verifiers whose criteria are grounded in the right task-relevant repository context, while still allowing lightweight scoring once criteria are generated.\n\n\n\n\n\n3 Experimental Design\n\n\n3.1 Agentic Rubrics\n\nRubric Generation.\n\nWe implement a rubric-generation agent on top of the SWE Agent scaffold, which provides tools for repository navigation, file inspection/editing, and shell command execution [28, 21]. We modify the scaffold’s SYSTEM PROMPT, instructing the agent to explore the repository, gather task-relevant context, and produce a patch that adds a structured rubric file, rubrics.yaml (prompt in Appendix A.10). This workflow mirrors how developers validate fixes when comprehensive tests are unavailable: inspecting surrounding code and contracts, tracing call sites, and reasoning about edge cases.\n\n\nEach rubric item is a tuple (ti,wi)(t_{i},w_{i}) consisting of a short natural-language criterion tit_{i} and an importance weight wi∈{1,2,3}w_{i}\\in\\{1,2,3\\} (nice-to-have / important / must-have), and is assigned to one of the following axes:\n(i) File Change (4–8 items): edits are min"
  },
  {
    "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
    "url": "https://arxiv.org/abs/2601.04170v1",
    "source": "arxiv",
    "summary": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interact",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Relationship to Prior Work\n\n\n\n2 Methodology\n\n2.1 Theoretical Framework and Simulation Design\n\n2.2 Agent Stability Index (ASI) Framework\n\n2.2.1 Response Consistency (Weight: 0.30)\n2.2.2 Tool Usage Patterns (Weight: 0.25)\n2.2.3 Inter-Agent Coordination (Weight: 0.25)\n2.2.4 Behavioral Boundaries (Weight: 0.20)\n\n\n2.3 Drift Pattern Classification\n2.4 Mitigation Strategy Evaluation\n\n\n\n3 Results\n\n3.1 Simulated Prevalence and Progression of Agent Drift\n3.2 Impact on System Performance\n3.3 ASI Component Analysis\n3.4 Mitigation Strategy Effectiveness\n3.5 Architectural Influences on Drift Susceptibility\n\n\n\n4 Discussion\n\n4.1 Mechanisms Underlying Agent Drift\n4.2 Implications for Production Deployment\n4.3 Connections to AI Safety Research\n4.4 Limitations and Future Work\n\n\n5 Conclusion\n\n\n\n\n\nAgent Drift: Quantifying Behavioral Degradation in\nMulti-Agent LLM Systems Over Extended Interactions\n\n\n\nAbhishek Rath\nIndependent Researcher\nHyderabad, India\nrath.abhishek359@gmail.com\n\n\n(January 7, 2026)\n\nAbstract\nMulti-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift—the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies). We introduce the Agent Stability Index (ASI)—a novel composite metric framework quantifying drift across 12 dimensions including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift could lead to substantial reductions in task completion accuracy and increases in human intervention requirements. We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring, with theoretical analysis suggesting these approaches could significantly reduce drift-related errors while maintaining system throughput. This work establishes foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.\n\n\n\n1 Introduction\n\nThe deployment of multi-agent Large Language Model (LLM) systems has accelerated dramatically since 2023, driven by frameworks such as LangGraph, AutoGen, and CrewAI [1, 2]. These architectures decompose complex tasks across specialized agents, coordinating through message passing, shared memory structures, and orchestration protocols. While initial performance benchmarks demonstrate impressive capabilities in code generation, research synthesis, and enterprise automation [3, 4], a critical gap exists in understanding their long-term behavioral stability.\n\n\nTraditional software systems exhibit predictable degradation patterns—memory leaks, resource exhaustion, configuration drift—that are well-characterized and systematically addressed through DevOps practices. In contrast, LLM-based agents introduce a novel failure mode: behavioral drift, where the system’s decision-making patterns progressively deviate from design specifications without explicit parameter changes or system failures. This phenomenon is particularly acute in multi-agent systems where emergent behaviors arise from agent-to-agent interactions that were not explicitly programmed.\n\n\nConsider a Master Router Agent coordinating three specialized sub-agents for database query optimization, compliance validation, and cost analysis in an enterprise setting. Over hundreds of interactions, subtle changes accumulate: the router begins favoring certain agents disproportionately, query formulation patterns shift toward statistically common but contextually inappropriate phrasings, and inter-agent handoffs develop latency-inducing redundancies. These changes are individually minor and often imperceptible in isolated evaluations, yet collectively degrade system performance by double-digit percentages—a pattern we term agent drift.\n\n\nThis study makes four primary contributions:\n\n\n\n\n1.\n\nTaxonomic Framework: We establish a comprehensive taxonomy of agent drift patterns, categorizing manifestations into semantic drift (intent deviation), coordination drift (multi-agent consensus degradation), and behavioral drift (strategy emergence).\n\n\n\n2.\n\nMeasurement Methodology: We introduce the Agent Stability Index (ASI), a composite metric framework quantifying drift across 12 behavioral dimensions, enabling systematic monitoring in production systems.\n\n\n\n3.\n\nTheoretical Analysis: Through simulation-based modeling and theoretical analysis, we characterize potential drift prevalence, progression rates, and impact on system reliability across representative enterprise scenarios.\n\n\n\n4.\n\nMitigation Strategies: We develop and theoretically validate three intervention approaches—episodic memory consolidation, drift-aware routing, and adaptive behavioral anchoring—with projected efficacy in reducing drift-related errors while preserving system throughput.\n\n\n\n\n\nThe implications extend beyond operational concerns. Agent drift poses fundamental questions for AI safety: if multi-agent systems progressively deviate from intended behaviors without explicit modification, traditional alignment and monitoring approaches may prove insufficient. As agentic AI systems scale toward greater autonomy and longer operational lifespans, understanding and controlling drift becomes essential for both reliability engineering and responsible deployment.\n\n\n\n1.1 Relationship to Prior Work\n\nOur work intersects three research domains: multi-agent system stability [5], LLM behavioral consistency [6], and production ML system monitoring [7].\n\n\nMulti-Agent Systems: Classical multi-agent research characterized emergent behaviors in game-theoretic settings [8], but these frameworks assume deterministic action spaces and stationary reward structures—assumptions violated by LLM agents whose outputs are stochastic and whose implicit objectives evolve through context accumulation.\n\n\nLLM Consistency: Recent work examines single-agent behavioral variation across prompt perturbations [9] and fine-tuning impacts [10], but does not address temporal drift in interactive, multi-turn scenarios or multi-agent coordination dynamics.\n\n\nML Monitoring: Production ML literature focuses on data distribution drift and model performance degradation [11], providing metrics like PSI (Population Stability Index) and monitoring systems for supervised learning pipelines. However, these approaches are ill-suited for agentic systems where \"ground truth\" is often unavailable and behavioral metrics are multi-dimensional.\n\n\nThis study bridges these domains by adapting monitoring methodologies from production ML, applying them to multi-agent LLM architectures, and characterizing failure modes unique to agentic systems operating over extended interaction sequences.\n\n\n\n\n\n2 Methodology\n\n\n2.1 Theoretical Framework and Simulation Design\n\nTo systematically study agent drift, we developed a simulation framework modeling multi-agent systems across three representative enterprise domains:\n\n\n\n\n•\n\nEnterprise Automation (n=412 simulated workflows): Master Router agents coordinating database management agents, file processing agents, and notification agents for automated report generation and data pipeline management.\n\n\n\n•\n\nFinancial Analysis (n=289 simulated workflows): Multi-agent ensembles performing equity research, risk assessment, and portfolio optimization through coordinated research, calculation, and synthesis agents.\n\n\n\n•\n\nCompliance Monitoring (n=146 simulated workflows): Agent teams analyzing transaction patterns, regulatory text, and audit trails through specialized pattern detection, rule extraction, and reasoning agents.\n\n\n\n\n\nEach simulated workflow represents a unique task instantiation with a defined objective, input data, and success criteria. Systems were modeled using LangGraph 0.2.x architecture patterns with GPT-4, Claude 3 Opus, and Claude 3.5 Sonnet behavioral characteristics, incorporating human-in-the-loop approval for high-stakes decisions.\n\n\nInteraction Sequences: We simulated complete interaction histories, modeling agent invocations, inter-agent messages, tool calls, reasoning steps, and output artifacts. Workflows ranged from 5 to 1,847 agent interactions (median: 127 interactions), with simulation windows spanning equivalent timeframes of 3 to 18 months.\n\n\nBaseline Establishment: For each workflow, the first 20 interactions served as a behavioral baseline, capturing initial agent decision patterns, tool usage distributions, and inter-agent coordination protocols. Subsequent interactions were compared against this baseline to detect drift.\n\n\nGround Truth and Validation: We established ground truth through simulation parameters:\n\n\n1.\n\nSynthetic Expert Labels: Generated consistent correctness labels based on deterministic task specifications.\n\n\n\n2.\n\nAutomated Validation: For deterministic tasks (e.g., SQL query generation, compliance rule matching), we compared agent outputs against verified reference solutions.\n\n\n\n3.\n\nConsistency Checks: For subjective tasks (e.g., financial analysis synthesis), we evaluated internal consistency through cross-agent validation and temporal comparison of outputs for identical inputs.\n\n\n\n\n\n\n\n2.2 Agent Stability Index (ASI) Framework\n\nWe developed a composite metric, the Agent Stability Index (ASI), to quantify behavioral drift across "
  },
  {
    "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
    "url": "https://arxiv.org/abs/2601.04164v1",
    "source": "arxiv",
    "summary": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows fr",
    "full_text": null
  },
  {
    "title": "Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models",
    "url": "https://arxiv.org/abs/2601.04163v1",
    "source": "arxiv",
    "summary": "Pathology foundation models (PFMs) have become central to computational pathology, aiming to offer general encoders for feature extraction from whole-slide images (WSIs). Despite strong benchmark performance, PFM robustness to real-world technical domain shifts, such as variability from whole-slide scanner devices, remains poorly understood. We systematically evaluated the robustness of 14 PFMs to",
    "full_text": null
  },
  {
    "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
    "url": "https://arxiv.org/abs/2601.04160v1",
    "source": "arxiv",
    "summary": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired orig",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.04160v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.04160v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 7 Jan 2026]\n    Title:All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection\n    Authors:Yuechen Jiang, Zhiwei Liu, Yupeng Cao, Yueru He, Ziyang Xu, Chen Xu, Zhiyang Deng, Prayag Tiwari, Xi Chen, Alejandro Lopez-Lira, Jimin Huang, Junichi Tsujii, Sophia Ananiadou            View a PDF of the paper titled All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection, by Yuechen Jiang and 12 other authors\n    View PDF\n\n\n\n    \n            Abstract:We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.\n    \n\n    \n    \n              \n          Comments:\n          39 pages; 24 figures\n        \n\n          Subjects:\n          \n            Computation and Language (cs.CL); Computational Engineering, Finance, and Science (cs.CE); Computational Finance (q-fin.CP)\n        \n          Cite as:\n          arXiv:2601.04160 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.04160v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.04160\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yuechen Jiang [view email]          [v1]\n        Wed, 7 Jan 2026 18:18:28 UTC (13,082 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection, by Yuechen Jiang and 12 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.CE\n        q-fin\n        q-fin.CP\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n "
  },
  {
    "title": "FLEx: Language Modeling with Few-shot Language Explanations",
    "url": "https://arxiv.org/abs/2601.04157v1",
    "source": "arxiv",
    "summary": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To addr",
    "full_text": null
  },
  {
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "url": "https://arxiv.org/abs/2601.04151v1",
    "source": "arxiv",
    "summary": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delv",
    "full_text": null
  },
  {
    "title": "A Theoretical and Empirical Taxonomy of Imbalance in Binary Classification",
    "url": "https://arxiv.org/abs/2601.04149v1",
    "source": "arxiv",
    "summary": "Class imbalance significantly degrades classification performance, yet its effects are rarely analyzed from a unified theoretical perspective. We propose a principled framework based on three fundamental scales: the imbalance coefficient $η$, the sample--dimension ratio $κ$, and the intrinsic separability $Δ$. Starting from the Gaussian Bayes classifier, we derive closed-form Bayes errors and show",
    "full_text": null
  },
  {
    "title": "Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test",
    "url": "https://arxiv.org/abs/2601.04137v1",
    "source": "arxiv",
    "summary": "As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to ma",
    "full_text": "\n\n\n\n1 Introduction\n2 Related work\n\n3 WoW-World-Eval: A Multi-faceted Benchmark for Embodied World Models\n\n3.1 Core Evaluation Dimensions\n3.2 Data Curation Pipeline\n3.3 Multi-faceted Evaluation Metrics\n3.4 Overall Benchmark Score\n\n\n\n4 Experiments\n\n\n4.1 Quantitative Evaluation Results\n\n\n4.2 Dense prompts quantitative results\n\n\n4.3 Qualitative Evaluation and Human Evaluation\n\n\n4.4 Turing Test\n\n\n5 Conclusion\n\n6 Acknowledge\n\n\n\n\n\n\n\n\n\n\n\n7 More Related Works\n\nVideo Generation Models.\n\nEmbodied World Models.\n\n\n8 Models Detail\n\nKling. [45]\nHailuo. [60]\nCogVideoX. [94]\nCosmos-Predict. [1, 64]\nWan. [83]\n\nWoW. [17]\n\n\n9 Detailed Metrics\n\n\n9.1 Visual Fidelity.\n\nFréchet Video Distance (FVD).\nStructural Similarity Index measures (SSIM).\nPeak signal-to-noise ratio (PSNR).\nDINO Score.\nDreamSim.\n\n\n\n9.2 Instruction Semantic Correctness.\n\nCaption Score.\nSequence Match Score.\nExecution Quality Score.\n\n\n\n9.3 Mask-guided Regional Consistency.\n\nSetup.\nImplementation.\nConsistency Score.\n\n\n\n9.4 Trajectory-level Consistency.\n\nKeypoint labeling and SAM2 tracking.\nKeypoint prompting and SAM2 mask tracking.\nMask-to-point trajectory conversion.\nCamera-motion–aware trajectory correction.\nCamera trajectory estimation.\nTemporal alignment and normalization across videos.\nAbsolute Trajectory Error (ATE).\nRelative Pose Error (RPE).\nL2Norm Error.\nDynamic Time Warping (DTW).\nFréchet Distance.\n\n\n9.5 Physical and Causal Reasoning.\n\n9.6 Overall Benchmark Score.\n\n\n10 Extended Experiment Analysis\n\n\n10.1 WoW-World-Eval Human Evaluation Rule\n\nVideo Quality.\nInstruction Understanding.\nPhysical Law.\nPlanning Reasoning.\nOverall Score.\n\n\n\n10.2 All correlation of WoW-World-Eval score and Human Preference\n\nVideo Quality.\nInstruction Understanding.\nPhysical Law.\nPlanning.\n\n\n\n10.3 Ground-truth video replay of IDM\n\n\n11 Case Results Visualization\n\n12 Prompts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test\n\n\n\nChun-Kai Fan1,2, Xiaowei Chi2,3,11footnotemark: 1 Xiaozhu Ju2, Hao Li2 Yong Bao2\nYu-Kai Wang1,2 Lizhang Chen1 Zhiyuan Jiang2 Kuangzhi Ge1,2 Ying Li1\nWeishi Mi2 Qingpo Wuwu1 Peidong Jia1,2 Yulin Luo1 Kevin Zhang1,2\nZhiyuan Qin2 Yong Dai2 Sirui Han3 Yike Guo3 Shanghang Zhang1, Jian Tang2,33footnotemark: 3\n1State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University\n2Beijing Innovation Center of Humanoid Robotics   3The Hong Kong University of Science and Technology\n\nEqual contribution.Project leader.Corresponding author.\n\n\nAbstract\nAs world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation.\nHowever, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents.\nTo provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val).\nBuilding upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models’ generation ability, which achieves a high Pearson Correlation between the overall score and human preference (&gt;0.93) and establishes a reliable foundation for the Human Turing Test.\nOn Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models’ execution accuracy in the real world. However, most models collapse to ≈\\approx 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.\n\n\nFigure 1: The Overview of WoW-World-Eval. (Top-left) A multi-faceted Metrics suite evaluates generated videos across five dimensions: Video Quality, Instruction Understanding, Planning Reasoning, Physical Law, and Execution Accuracy. (Top-center) These metrics align with five core Abilities of embodied world models: Perception, Planning, Prediction, Execution, and Generalization. (Top-right) Performance gaps across state-of-the-art models. (Bottom) The benchmark follows the embodied world model pipeline from Perception to Generalization.\n\n\n\n1 Introduction\n\nWorld models – which capture an agent’s understanding of how the world changes with actions [35] – have emerged as a pivotal concept in robotics and Embodied AI. In embodied settings, a world model allows a robot to understand and predict its environment [21, 56, 32, 49], and can function as the robot’s “internal brain”, enabling it to simulate future scenarios for planning and decision-making [17, 72, 85, 43], or operate as an environment simulator [47, 88, 52].\n\n\nCompared with cutting-edge spatial-prediction world models [3, 79], embodied world models operate in context-rich environments that demand a deeper understanding of physical common sense. In robotics, the complexity and lack of standardization across setups further lead to a broad and diverse landscape of embodied-world-model designs. These models vary widely in their control conditions—ranging from approaches conditioned solely on images and language instructions [72, 15, 17, 16], to those incorporating keypoints [84], trajectories [51, 31, 41, 69, 9, 34], depth, semantics, and other modalities [92, 50, 99]. They also differ in camera configurations, requiring either single-view inputs [15, 100, 9, 31, 72, 69, 84, 16, 51] or multi-view setups [73, 11, 66, 17, 52, 92, 34]. Moreover, several recent efforts have begun to pursue cross-embodiment generalization [98, 37, 17].\nTherefore, despite the broader research of world models in general-purpose robotics, two core questions still remain:\n\n\n1.\n\nCan these models generalize well enough to maintain perceptual fidelity from a human perspective?\n\n\n\n2.\n\nAre they robust and expressive enough to serve as universal priors for real-world embodied agents?\n\n\n\n\n\nExisting video-generation benchmarks [29, 53, 46, 95, 54, 40, 97] largely target general-purpose settings or isolated dimensions and overlook the unique requirements of robotic world models. Most evaluations emphasize visual fidelity or coarse task success, but rarely assess deeper embodied abilities such as physical plausibility, planning rationality, and actionability. This gap makes progress difficult to measure: a model may score well on conventional video metrics [81, 86, 65, 27] yet produce physically impossible or contextually incorrect predictions in robotic scenarios. Our results further confirm this misalignment—standard video-quality scores correlate poorly with human judgments in embodied settings—highlighting the need for more reliable evaluation standards.\n\n\nConsequently, in this paper, we address these challenges by proposing a new comprehensive benchmark for embodied world models, and use it to systematically evaluate foundational models under the simplest image-to-video setting. Our benchmark, WoW-World-Eval, as illustrated in Figure 1, is designed around the core capabilities that an embodied world model should possess: perceiving the environment, understanding and planning based on task instructions, predicting and simulating future world states, executing real-world interactions, and generalizing across diverse scenarios and embodiments.\nIt contains about 609 robot manipulation samples with meticulous cleaning and annotation by human annotators. Also we incorporate 22 evaluation metrics aligned with our core dimensions across Video Quality, Instruction Understanding, Planning Reasoning, Physical Law, and Execution Accuracy.\n\n\nMoreover, WoW-World-Eval follows the two-alternative forced-choice (2AFC) [28] methodology from psychophysics to establish the evaluation as a standardized Turing Test for generative video models. By collecting fine-grained human answers distinguishing real and generated videos, we compute the proportion of generated videos from each model that successfully fool human evaluators. Notably, an overall human preference alignment score exceeding Pearson Correlation = 0.93 demonstrates the effectiveness of our benchmark in evaluating high-quality generations and serves as a reliable proxy for the Human Turing Test.\n\n\nIn addition to the human-centered evaluation, we also introduce a Machine Turing Test—specifically, an Inverse Dynamics Model (IDM) Turing Test. In this setting, we assess whether videos generated by a model can “fool” an IDM that has only been trained on real-world execution sequences. If the generated videos lead the IDM to output plausible actions that are executable in the real world, it indicates that the model’s outputs are indistinguishable from real data in terms of physical and action plausibility.\n\n\nBy evaluating existing models under this new benchmark, we reveal which models already exhibit credible world understanding and where they fall short. We believe this benchmark and the accompanying Turing Test criterion will provide a much-needed standard for the field, driving research towards embodied world models that can truly imagine the world with the accuracy and fidelity that robotics applications demand. Our contributions are threefold as follows:\n\n\n•\n\nA comprehensive World Model Benchmark, WoW-World-Eval, focuses on the Embodied AI domain, introducing a new perspective with a novel framework for the five core abilities in the embodied world model.\n\n\n\n•\n\nBased on WoW-World-Eval, w"
  },
  {
    "title": "LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation",
    "url": "https://arxiv.org/abs/2601.04135v1",
    "source": "arxiv",
    "summary": "We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 System Architecture\n\n\n3.1 Data Representation and Backend Processing\n\nTree-Centric Data Model.\nBackend Services.\n\n\n\n3.2 Interactive Data Manipulation Interface\n\nTree Visualization.\nThread Construction.\n\n\n\n3.3 LLM-Assisted Refinement Module\n\nSpeaker Profiling.\nMessage Refinement.\n\n\n3.4 Data Export, Deployment and Availability\n\n\n\n4 Evaluation of LLMberjack Features\n\n4.1 Creation of synthetic Reply-trees\n\n4.2 Evaluating the Impact of Tree Visualization\n\nQuantitative Evaluation.\nQualitative Observations.\n\n\n\n4.3 Evaluating the Impact of LLM Support\n\nQuantitative Evaluation.\nQualitative Observations.\n\n\n\n\n5 Conclusion\n\nA Technical details\n\nA.1 System Architecture\nA.2 Data and File Management\nA.3 LLM Integration\n\n\n\nB Evaluation details\n\nB.1 Synthetic Reply Trees\nB.2 Step 1 details\nB.3 Step 2 guidelines\n\n\n\n\n\n\n\nLLMberjack: Guided Trimming of Debate Trees \nfor Multi-Party Conversation Creation\n\n\n\nLeonardo Bottona1,\nNicolò Penzo1,2,\nBruno Lepri2,\nMarco Guerini2,\nSara Tonelli2\n1University of Trento\n2Fondazione Bruno Kessler\n\n\nCorrespondence: leonardo.bottona@studenti.unitn.it, npenzo@fbk.eu\n\n\n\nAbstract\nWe present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers’ descriptions. We demonstrate the platform’s utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.\n\n\n\nLLMberjack: Guided Trimming of Debate Trees \nfor Multi-Party Conversation Creation\n\n\n\n\n\nLeonardo Bottona1,\nNicolò Penzo1,2,\nBruno Lepri2,\nMarco Guerini2,\nSara Tonelli2\n\n1University of Trento\n2Fondazione Bruno Kessler\n\n\nCorrespondence: leonardo.bottona@studenti.unitn.it, npenzo@fbk.eu\n\n\n\n\n\n\n1 Introduction\n\nDespite ongoing efforts in the NLP community to create large datasets and linguistic resources, there is traditionally a lack of high-quality datasets with multi-party conversations (MPC) Penzo et al. (2024b).\nPlatforms such as X, Reddit and Kialo provide a large amount of conversations in the form of reply trees, where each root-to-leaf path can be interpreted as a linearized MPC Derczynski et al. (2017); Penzo et al. (2024a). In such cases, each node explicitly replies to its parent (and occasionally to earlier messages in the thread), forming a clear, hierarchical conversational flow but lacking in most cases structures with multiple addressees.\n\n\nMessaging platforms like Telegram and WhatsApp, instead, present inherently linear conversations that often contain overlapping or parallel sub-dialogues, frequently with multiple implicit addressees for each message. So, while representing examples of MPCs, an annotation step would still be needed to make addressees explicit and enable modelling their complex conversation structures. Furthermore, using discussions from online platforms to study MPCs raises significant privacy and profiling concerns Kim et al. (2023).\n\n\nFigure 1: Overview of the LLMberjack platform. The interface integrates reply-tree visualization, message selection tools for building linearized multi-party conversations (1), and LLM-support for editing messages and speaker profiles (2).\n\n\nLLMs could be potentially used to address the lack of MPCs datasets by generating synthetic dialogues.\nHowever, as shown by Penzo et al. (2025), although some LLMs can produce high-quality synthetic dialogues, they may still struggle with the generation of complex structures with multiple speakers.\n\n\nA possible solution to create linearized multi-party conversations with overlapping or parallel sub-dialogues starting from existing reply trees could be “walking” on the tree following the explicit speaker–addressee relations. Human annotators could be involved only to modify or correct such conversations by editing messages or redefining addressee links, thereby enhancing both naturalness and interactional coherence. Furthermore, a single reply tree can yield several linearized MPCs, capturing potential conversation variations that result from different turn-taking orders.\n\n\nIn this paper, we introduce LLMberjack, a Human-AI collaborative platform designed to create synthetic, thread-like multi-party conversations starting from existing reply trees. The platform provides an interface that allows annotators to “walk” through the tree, visualizing both the parent and child nodes for each message, thereby making selection decisions more context-aware.\n\n\nReply trees extracted from structured debate platforms like Kialo111https://www.kialo.com/ or automatically generated may exhibit a style that is not fluent or natural enough. To enhance specific linguistic features or user traits, we implement an LLM-assisted protocol that supports two key tasks beside tree editing: (i.) user profiling, i.e., the model generates a speaker profile based on the conversation content (or, in cases of limited data, from messages in the reply tree) and merges it with a pre-existing description; (ii.) message editing, i.e., the LLM refines a given message by considering the chat history and speaker profile. Human annotators then decide whether to accept, modify or reject the LLM’s suggestion, ensuring the overall conversational quality.\n\n\nWe rigorously evaluate the impact of both tree visualization and LLM-assisted message editing involving four annotators. Results demonstrate that the quality of the resulting MPCs improves when tree visualization is available, and that LLMs can effectively support message editing, while also accelerating the annotation process.\n\n\nLLMberjack is available on a dedicated Github repository222https://github.com/LeonardoBottonaUniTn/demo_conv_creation. The platform targets researchers from NLP and Social Sciences, helping them in the creation of high-quality MPCs with specific characteristics.\n\n\n\n\n2 Related Work\n\nMulti-party conversational corpora have been collected from a broad range of environments, including in-person meetings Carletta et al. (2005); Janin et al. (2003) and online platforms Ouchi and Tsuboi (2016); Zhang et al. (2018); Chang and Danescu-Niculescu-Mizil (2019). However, these diverse sources exhibit inherently different characteristics that complicate cross-domain generalization and undermine the portability of computational models. For instance, spoken multi-party dialogues are heavily shaped by non-verbal cues, the physical setting, and overlapping turns, all of which are typically absent in written online interactions. Conversely, text-based conversations unfold asynchronously, without overlap, and often follow platform-specific conventions that further influence interaction patterns Mahajan and Shaikh (2021); Penzo et al. (2025). Heterogeneity in structure and annotation practices is shown also across datasets from similar sources.\n\n\nThe limited availability of reliable multi-party conversation data with the desired level of structural and interactional detail suggests the need for alternative approaches. One promising direction is the use of synthetic, human-in-the-loop methods, which allow researchers to control conversational conditions while preserving human oversight, refinement, and interactional plausibility. This has been already tested in single-turn interactions Fanton et al. (2021); Russo et al. (2023) and for multi-turn dialogues Bonaldi et al. (2022); Occhipinti et al. (2024), but not yet for multi-party settings. Only Chen et al. (2023) and Penzo et al. (2025) have attempted to generate synthetic multi-party conversations, the former involving up to three users and the latter extending to interactions among four to six users.\n\n\nMenini et al. (2025) introduced FirstAID, a platform designed to assist a human annotator in the synthetic creation of document-grounded dialogues among multiple participants, but the evaluation has been limited to 1-to-1 interactions. In literature, ConvoKit (Chang et al., 2020) is the most established toolkit for multi-party settings, which offers datasets and computational tools for the linguistic and structural analysis of multi-party conversations. Yet, despite these contributions, there is still no open-source platform that supports the creation with human-AI refinement of multi-party conversations from structured reply trees.\n\n\nFigure 2: Screenshot of tree visualization for node 1.2.4 (left) and of the chat creation tab (right). Each node-box reports the speaker’s name on the top-right corner, and a preview of the message in the center (expandable).\n\n\n\n\n3 System Architecture\n\nLLMberjack is designed to support the full workflow for transforming structured reply trees into coherent multi-party conversations. The system is organized into three main layers: (i.) a data-processing backend, (ii.) an interactive data manipulation interface, and (iii.) an export module.\n\n\n\n3.1 Data Representation and Backend Processing\n\nTree-Centric Data Model.\n\nAll discussion sources are represented as rooted reply trees. Each node corresponds to an individual message and stores author and text of message, and other existing platform-specific attributes, if any. Edges encode explicit reply-to relations.\n\n\n\nBackend Services.\n\nThe backend provides: (i.) parsing routines that convert raw json dumps into the internal graph representation; (ii.) subtree querying for efficient visualization and traversal; (iii.) file-management functionalities for uploading discussion files, performing LLM-ass"
  },
  {
    "title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models",
    "url": "https://arxiv.org/abs/2601.04131v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce Contex",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Context Faithfulness and Knowledge Conflicts\n2.2 Approaches to Contextual Alignment\n2.3 Activation Steering\n\n\n\n3 Preliminaries\n\n3.1 Activation Steering\n\n\n\n4 Methodology\n\n4.1 Vector generation\n4.2 Vector selection\n4.3 Open-ended Generations\n\n\n\n5 Experiments\n\n5.1 Experimental settings\n\n5.2 Main results\n\n5.2.1 Performance on knowledge conflict datasets\n\n\n5.3 Analysis\n\n\n6 Conclusion\n7 Limitations\nA Options-Based Vector Generation\n\nB Prompting Templates\n\n\nB.1 Main Experiments Prompting Scheme\n\nSteering vector construction.\nSystem prompt diversity.\n\n\n\nB.2 Open-Ended Generation Prompts\n\nStandard open-ended prompt.\nO&amp;I open-ended prompt.\n\n\nB.3 Options-Based Prompting Scheme\n\n\nC Layer Selection for the Options Approach\nD Repetition Metric\nE Effect of Steering Multiplier on Context Adherence\nF Layer Selection for Mistral-7B and Llama-3.1-70B\nG Steering vector convergence across training set sizes\nH Wall-clock Cost of Steering Vector Construction\nI Baseline Model Versions\n\nJ Case Study\n\nJ.1 Copyrighted information in the question\nJ.2 Incorrect or Outdated Prior Knowledge\n\n\n\n\n\n\n\nContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models\n\n\n\nNikhil Anand1,\nShwetha Somasundaram1,\nAnirudh Phukan2,\n\nApoorv Saxena3,\nKoyel Mukherjee1\n\n1Adobe Research, India,\n2Indian Institute of Science (IISc), Bengaluru,\n3Inception Labs\n\n\nCorrespondence: nikhilanand@adobe.com\n\n\n\nAbstract\nLarge Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model’s internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.\n\n\n\nContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models\n\n\n\n\nNikhil Anand1,\nShwetha Somasundaram1,\nAnirudh Phukan2,\n\nApoorv Saxena3,\nKoyel Mukherjee1\n\n1Adobe Research, India,\n2Indian Institute of Science (IISc), Bengaluru,\n3Inception Labs\n\n\nCorrespondence: nikhilanand@adobe.com\n\n\n\n\n\n\n\n\n\nContext\n\n\n\n\nQuestion\n\n\nm\n\n\nOutput\n\n\n\n\n\n\n…The film’s sweeping score, composed by John Williams…\n\n\n\n\nWho performed the score for the Titanic (1997 film)?\n\n\n0.0\n\n\nThe score for the 1997 film Titanic was performed by James Horner, not John Williams, as mentioned in the context.\n\n\n\n\n\n\n+2.0\n\n\nThe film’s sweeping score was composed by John Williams.\n\n\n\n\n\n\n…Kyle Korver is a skilled center known for his deadly accuracy from beyond the arc…\n\n\n\n\nWhat position does Kyle Korver play?\n\n\n0.0\n\n\nKyle Korver is actually a skilled shooting guard, not a center, known for his deadly accuracy from beyond the arc.\n\n\n\n\n\n\n+2.0\n\n\nKyle Korver is a skilled center.\n\n\n\n\n\n\n…Batman originated from France. The official language of France is Irish.\n\n\n\n\nWhat is the official language of the country of origin of Batman?\n\n\n0.0\n\n\nFrance’s official language is actually French, not Irish.\n\n\n\n\n\n\n+2.0\n\n\nThe official language of France is Irish.\n\n\n\n\n\n\n…GB News originated from United Kingdom. United Kingdom’s currency is Swedish krona.\n\n\n\n\nWhat is the currency of the country of origin of GB News?\n\n\n0.0\n\n\nThe currency of the United Kingdom is the Pound Sterling, not the Swedish krona.\n\n\n\n\n\n\n+2.0\n\n\nThe currency of the United Kingdom is the Swedish krona.\n\n\n\n\n\nTable 1: \nLlama-3.1-8B outputs with and without ContextFocus in knowledge conflict settings. We apply steering with a multiplier of 2.0 at layer 13.\nContext-faithful answers are shown in blue, and unfaithful ones in red.\nThese examples illustrate the ability of steering to align the model’s outputs with the context.\n\n\n\n\n1 Introduction\n\nThe widespread deployment of retrieval-augmented generation (RAG) systems has made large language models (LLMs) increasingly dependent on externally retrieved evidence rather than purely on parametric memory. This shift is critical in real-world applications such as grounded question answering, summarization, and decision support, where correctness depends on faithfully following provided context as knowledge evolves. As a result, the ability of a model to prioritize external evidence over internal knowledge has emerged as a key challenge in modern LLM systems.\n\n\nFigure 1: When the model’s memory deviates from the context, the model may generate outputs that are unfaithful to the context. In the example shown here, the LLM was likely trained before the CEO of Starbucks changed to Brian Niccol. By applying our steering method with a multiplier of 2, we see that the model now generates a faithful output.\n\n\nDespite access to relevant context, LLMs often fail to remain faithful to it, particularly in cases where the retrieved evidence conflicts with the model’s parametric knowledge (Wu et al., 2025). In these situations, models frequently default to memorized facts rather than updating their predictions based on the provided context. For instance, when asked “Who is the CEO of Starbucks?”, an older LLM might incorrectly respond with an outdated CEO’s name, despite being provided an up-to-date context mentioning Brian Niccol\n(Figure 1). Using knowledge-conflict settings provide a clean and measurable testbed for studying context faithfulness, as deviations from the context are unambiguous and easy to evaluate.\n\n\nOur goal is to improve contextual faithfulness under knowledge conflicts by enabling models to consistently ground their outputs in the provided context, while preserving fluency and efficiency.\n\n\nA variety of approaches have been proposed to improve context faithfulness under knowledge conflicts. Finetuning-based methods, such as ContextDPO Bi et al. (2024), explicitly train models to prefer context-consistent responses. While effective, these methods incur substantial training cost and are difficult to scale to very large models. Decoding-time approaches based on contrastive decoding Shi et al. (2023); Zhao et al. (2024); Yuan et al. (2024) manipulate the output logits to favor context-sensitive tokens. Although these methods avoid retraining, they typically require multiple forward passes for a single answer, leading to increased inference latency. Prompting-based strategies Zhou et al. (2023) offer a lightweight alternative, but their gains are often limited, highly sensitive to prompt formulation, and lack interpretable controllability.\n\n\nIn this work, we explore activation-level control as a lightweight and practical design point in this space. We introduce ContextFocus, an activation steering approach that operates directly in the model’s hidden representations to bias generation toward context-faithful behavior. Unlike decoding-time contrastive methods, ContextFocus requires only a single forward pass at inference time and incurs minimal overhead. We empirically quantify these efficiency differences in Section 5.2. Furthermore, unlike finetuning-based approaches, ContextFocus requires no model retraining and can be applied directly to frozen models, including large-scale LLMs.\n\n\nWe evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong finetuning, decoding, and prompting baselines. Our results show that activation steering substantially improves context faithfulness while preserving fluency, composes naturally with prompting strategies, and scales effectively to larger models. Beyond performance, we provide a detailed analysis of the design choices underlying activation steering, including vector construction, layer selection, and data efficiency. Together, these results position activation-level control as an efficient and robust alternative for improving context faithfulness under knowledge conflicts, offering practical guidance for deploying LLMs in retrieval-augmented settings.\n\n\nOur key contributions are as follows:\n\n\n\n\n•\n\nWe study activation-level control as a lightweight design point for improving context faithfulness under knowledge conflicts, and introduce ContextFocus, an activation steering approach that substantially improves adherence to provided context without model fine-tuning and with minimal inference-time overhead.\n\n\n\n•\n\nWe show that ContextFocus achieves competitive or superior context-faithfulness compared to strong fine-tuning, decoding, and prompting baselines on standard knowledge-conflict benchmarks, while remaining effective on larger models and composing naturally with prompting strategies.\n\n\n\n•\n\nWe provide a systematic analysis of activation steering for contextual alignment, demonstrating that steering directions are data-efficient and stable, and that controlled ablations over vector construction reveal how jointly contrasting contextual evidence and system-level instruction is critical for inducing context-faithful behavior.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Context Faithfulness and Knowledge Conflicts\n\nImproving the faithfulness of LLM outputs to provided context has received increasing attention in recent work. While early studies on retrieval-augmented generation (RAG) emphasized retrieval quality, it is now well understood that strong retrieval alone does not ensure faithful generation: models may pr"
  },
  {
    "title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images",
    "url": "https://arxiv.org/abs/2601.04127v1",
    "source": "arxiv",
    "summary": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimension",
    "full_text": null
  },
  {
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "url": "https://arxiv.org/abs/2601.04126v1",
    "source": "arxiv",
    "summary": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, buildi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nGUI Agent Benchmarks.\nLLM-based Code and Website Generation.\nSynthetic Environment and Data Generation.\n\n\n\n3 Method\n\n3.1 Overview\n\n3.2 Unified Specification Stage\n\nTask Generation.\nUnified Interface Design.\n\n\n\n3.3 Task-Centric Backend\n\nData Preparation.\nTask-Centric Test-Driven Development.\n\n\n\n3.4 Design-Guided Frontend\n\nVisual Style Extraction.\nPage Design.\nPage Realization.\n\n\n3.5 Automatic Evaluator Generation\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\nWebGen-Bench.\nLLM-as-Judge Visual Quality.\nOnline-Mind2Web.\nOSWorld.\n\n\n4.2 Website Functional Correctness\n4.3 LLM-as-Judge Visual Quality\n4.4 Effectiveness for Agent Training\n\n4.5 Generated Environment Quality\n\nHigher Difficulty.\nBetter Discriminability.\n\n\n\n4.6 Ablation Studies\n\nEffect of TCTDD.\nEffect of Backbone Model.\nEffect of Dense Reward.\n\n\n4.7 Generation Efficiency\n\n\n5 Conclusion\nSingle-Website Scope.\nMobile Evaluation.\nGeneration Cost.\n\nA Case Studies and Analysis\n\n\nA.1 Cross-Domain Transfer Analysis\n\nExploration Persistence.\nFlow Completeness.\nLoop Avoidance.\n\n\nA.2 Automatic Evaluator Generation\nA.3 TCTDD Validation and Auto-Fix\n\n\n\nB Data Collection and Implementation\n\nWebsite Seed and Design Image Extraction.\nGeneration Hyperparameters.\nAgent Training.\n\n\n\nC Experimental Details and Results\n\nBaseline Implementation.\nC.1 WebGen-Bench Results\nC.2 Online-Mind2Web Results\nC.3 Appearance Win Rate\n\n\nD Human Evaluation for Visual Quality\nE Human Verification of Task and Evaluator Quality\nF Prompts\n\n\n\n\n\nInfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training\n\n\nZiyun Zhang1*  Zezhou Wang2*  Xiaoyi Zhang3†\\dagger\nZongyu Guo3  Jiahao Li3  Bin Li3  Yan Lu3\n1Peking University  2Nanjing University \n3Microsoft Research Asia\n\n\n\nAbstract\nGUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.\n\n\n\nInfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training\n\n\n\n\nZiyun Zhang1*   Zezhou Wang2*   Xiaoyi Zhang3†\\dagger\n\nZongyu Guo3  Jiahao Li3  Bin Li3  Yan Lu3\n\n\n\n1Peking University  2Nanjing University\n\n3Microsoft Research Asia\n\n\n\n\n††∗: Equal contribution and work done during the internship at Microsoft Research Asia. †: Project lead. \n\n\n1 Introduction\n\nGUI agents, autonomous systems that interact with graphical user interfaces to complete tasks on behalf of users, have emerged as a promising direction for building practical AI assistants Xie et al. (2024); Zhou et al. (2024). Recent advances Hong et al. (2024); Qin et al. (2025) have demonstrate vision-language models can be end-to-end trained with reinforcement learning algorithm as GUI agents to understand screenshots, reason about UI elements, and execute human-like actions to automate tasks in digital world. However, training such agents remains challenging due to the scarcity of suitable environments.\n\n\nExisting GUI agent benchmarks, such as MiniWoB++ Liu et al. (2018), WebArena Zhou et al. (2024), and OSWorld Xie et al. (2024), provide valuable testbeds but suffer from fundamental limitations in scale and diversity as training environments. These benchmarks are manually constructed, requiring significant human effort to design websites or download applications, define tasks, and create evaluation criteria. As a result, they contain only tens to hundreds of applications, insufficient for training agents that can generalize across the vast diversity of real-world websites. Although recent work Sun et al. (2025); Xu et al. (2024); Xie et al. (2025a) proposes synthesizing tasks or trajectories, these approaches still operate within the same benchmark environments, limiting model training on a small set of specific applications.\n\n\nA natural question arises: Can we automatically generate environments for GUI agent training? While large language models (LLMs) have shown remarkable code generation capabilities Chen et al. (2022); Si et al. (2024); Jimenez et al. (2023), especially for web frontend Leviathan et al. , directly applying them to generate complete, functional websites faces three critical challenges.\n\n\nGenerating such environments presents three intertwined challenges. First, consistency: While LLMs perform well on generating a single webpage, a realistic website comprises multiple interconnected pages sharing data, visual styles, and backend interfaces. LLMs generating pages independently often produce incompatible implementations, different backend interface signatures, conflicting data formats, or inconsistent state management, which breaking the cross-page interactions essential for realistic websites. Second, correctness: website functionalities require multiple coordinated steps, but LLM-generated code frequently contains functional bugs that compound over long-horizon tasks, causing incorrect reward signals that can destabilize reinforcement learning. Third, diversity: LLMs tend to produce repetitive task patterns and homogeneous visual styles, risking agent overfitting to specific interaction patterns rather than learning generalizable skills.\n\n\nIn this paper, we present InfiniteWeb, an agentic system that automatically generates functional web environments at scale for GUI agent training, addressing aforementioned challenges.\n\n\nFor consistency, we propose Unified Specification: rather than generating pages independently, we first derive a complete set of data models and interfaces from user tasks, then generate all pages according to this shared specification, ensuring the realistic cross-page interactions.\nTo ensure correctness, inspired by the classic software engineering practice Williams et al. (2003), we introduce the task-centric test-driven development (TCTDD) approach, where test cases are firstly derived from task specifications and then code is iteratively refined until all task-relevant tests pass.\nFor diversity, our system addresses it from both functional and visual dimensions: functionally, by taking a website seed (a brief description) and generating tasks specifically designed to match that seed. Visually, by providing reference design images, we use vision-language models to extract characteristics and generate websites that match the target style. It enables the leverage the millions of visually distinct websites available in resources like Common Crawl Common Crawl Foundation (2024) as an abundant source of diverse designs.\n\n\nFurthermore, to support RL-based training, our system is designed to generate verifiable task evaluators along with the website and tasks, which tracks key task-related variables during agent running, enabling dense reward signals for reinforcement learning. We conduct systematical analysis on our system from two aspects: generated website quality and the effect to training GUI agent as simulated environment. The results demonstrate the superior of our system as an environment synthesis system.\n\n\nWe summarize our contributions as follows and we will release the artifacts of this work to further contribute the research community:\n\n\n•\n\nWe propose InfiniteWeb, the first system that specifically design for generating functional web environments with verifiable evaluator for GUI agent training at scale.\n\n\n\n•\n\nExperiments demonstrate that our system surpasses advanced coding agents in building realistic web environments on WebGen-Bench, achieving superior performance in both visual and functional quality.\n\n\n\n•\n\nTraining on our generated environments significantly improves GUI agent performance: from 24.5% to 31.4% on OSWorld under 15 steps, demonstrating the realism and quality of simulated environments produced by our system.\n\n\n\n\n\nFigure 1: Overview of InfiniteWeb. Given a website seed and design image, our system produces a functional website with tasks and evaluators through four stages: the Unified Specification Stage generates tasks and derives data models and interfaces; the Task-Centric Backend and Design-Guided Frontend execute in parallel; and the Evaluator Generation creates task-specific evaluators for dense reward signals.\n\n\n\n\n2 Related Work\n\nGUI Agent Benchmarks.\n\nWhile there are benchmarks evaluating separate ability of GUI Agents like UI element grounding Li et al. (2025a); Liu et al. (2025) or UI understanding Wang et al. (2025),\nend-to-end evaluating GUI agents requires interactive environments.\nEarly work such as MiniWoB++ Liu et al. (2018) introduced simplified web interaction tasks, demonstrating the potential of reinforcement learning for web automation. Subsequent benchmarks have increased realism and complexity: WebArena Zhou et al. (2024) provides self-hosted websites for autonomous agent evaluation, OSWorld Xie et al. (2024) extends to full desktop environments across multiple operating systems, and Mind2Web Deng et al. (2023) offers large-scale web task annotations. However, these benchmarks share a fundamental limitation: they are manually constructed, requiring significant human effort to design environments, define tasks, and create evaluators. This limits their scale and diversity, potential"
  },
  {
    "title": "MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis",
    "url": "https://arxiv.org/abs/2601.04121v1",
    "source": "arxiv",
    "summary": "Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning fra",
    "full_text": null
  },
  {
    "title": "A Single-Loop Bilevel Deep Learning Method for Optimal Control of Obstacle Problems",
    "url": "https://arxiv.org/abs/2601.04120v1",
    "source": "arxiv",
    "summary": "Optimal control of obstacle problems arises in a wide range of applications and is computationally challenging due to its nonsmoothness, nonlinearity, and bilevel structure. Classical numerical approaches rely on mesh-based discretization and typically require solving a sequence of costly subproblems. In this work, we propose a single-loop bilevel deep learning method, which is mesh-free, scalable",
    "full_text": null
  },
  {
    "title": "Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models",
    "url": "https://arxiv.org/abs/2601.04110v1",
    "source": "arxiv",
    "summary": "Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target d",
    "full_text": null
  },
  {
    "title": "Equivariant Neural Networks for Force-Field Models of Lattice Systems",
    "url": "https://arxiv.org/abs/2601.04104v1",
    "source": "arxiv",
    "summary": "Machine-learning (ML) force fields enable large-scale simulations with near-first-principles accuracy at substantially reduced computational cost. Recent work has extended ML force-field approaches to adiabatic dynamical simulations of condensed-matter lattice models with coupled electronic and structural or magnetic degrees of freedom. However, most existing formulations rely on hand-crafted, sym",
    "full_text": null
  },
  {
    "title": "Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework",
    "url": "https://arxiv.org/abs/2601.04100v1",
    "source": "arxiv",
    "summary": "The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the de",
    "full_text": null
  },
  {
    "title": "Layer-wise Positional Bias in Short-Context Language Modeling",
    "url": "https://arxiv.org/abs/2601.04098v1",
    "source": "arxiv",
    "summary": "Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independen",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Positional Bias in LLMs\n2.2 Mechanistic Analyses of Positional Sensitivity in Transformers\n2.3 Layer-wise Importance in LLMs\n\n\n\n3 Methodology\n\n\n3.1 Attribution via Conductance\n\nObjective.\nAttribution-based analysis.\nConductance.\n\n\n3.2 Sliding-Window Attribution\n\n3.3 Positional Importance Profiles\n\nBetween-input consistency.\nPrimacy and recency metrics.\n\n\n3.4 Word-level aggregation\n3.5 Word-level layer dominance\n\n\n\n4 Experimental Setup and Results\n\nDatasets\nModels\n\n4.1 Layer-wise Importance Profiles Across Positions\n\nProgressive Recency\nPrimacy and Anchoring\nMid-Window Convergence\n\n\n\n4.2 Positional Bias Beyond Text Content\n\nLayer Specialization via Positional Metrics\n\n\n4.3 Word-Level Layer Importance After Positional Averaging\n\n4.4 Word-Level Layer Dominance\n\nLayer Dominance by Position\nLayer Dominance by POS\n\n\n4.5 Coclusion\n\n\n5 Limitations\n\nA Dataset Details\n\nNarrative Stories.\nEncyclopedic Articles.\nScientific Abstracts.\nScrambled Control.\n\n\n\nB Positional Importance Profiles: Additional Texts\n\nB.1 Narrative Stories\nB.2 Scrambled Control\nB.3 Wikipedia Articles\nB.4 Scientific Abstracts\n\n\n\nC Layer-Position Heatmaps\n\nC.1 Mean Conductance Heatmaps\nC.2 Variance Heatmaps\n\n\n\nD Layer Dominance Analysis by Text\n\nD.1 Layer Dominance by Position\nD.2 Cross-Text Consistency\n\n\n\nE Robustness to Window Length: Analysis with P=50\n\nE.1 Positional Importance Profiles (P=50)\nE.2 Cross-Text Consistency (P=50)\nE.3 Primacy and Recency Evolution (P=50)\nE.4 Word-Level Layer Importance (P=50)\nE.5 Layer Dominance (P=50)\nE.6 Summary\n\n\n\n\n\n\n\nLayer-wise Positional Bias in Short-Context Language Modeling\n\n\n\nMaryam Rahimi1  Mahdi Nouri2  Yadollah Yaghoobzadeh1,2\n1Tehran Institute for Advanced Studies, Khatam University, Iran\n2School of Electrical and Computer Engineering, University of Tehran, Iran\nmaryamrahimiha@gmail.com, {mahdi.noori, y.yaghoobzadeh}@ut.ac.ir\n\n\n\nAbstract\nLanguage models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts—from attention sinks to task\nperformance degradation in long-context settings—prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.\n\n\n\nLayer-wise Positional Bias in Short-Context Language Modeling\n\n\n\n\n\nMaryam Rahimi1   Mahdi Nouri2   Yadollah Yaghoobzadeh1,2\n\n1Tehran Institute for Advanced Studies, Khatam University, Iran\n\n2School of Electrical and Computer Engineering, University of Tehran, Iran\n\nmaryamrahimiha@gmail.com, {mahdi.noori, y.yaghoobzadeh}@ut.ac.ir\n\n\n\n\n\n\n1 Introduction\n\nLanguage models rely fundamentally on word order to process linguistic structure, enabling them to capture dependencies such as grammar, compositionality, and temporal relations.\n\n\nOrder sensitivity arises from architectural components that enable models to learn positional information, including positional encodings such as RoPE Su et al. (2024) and ALiBi Press et al. (2022), causal attention masking Vaswani et al. (2017), and position-dependent learned representations. While these mechanisms enable sequential processing, they can also introduce systematic preferences for certain input positions Wu et al. (2025).\nIn particular, models may assign greater influence to tokens appearing at specific positions—such as the beginning or end of the input—based on position alone rather than semantic relevance. We refer to such systematic, position-driven preferences as positional bias.\n\n\nThese positional preferences have been observed across different settings. Prior work has identified attention sinks at specific positions Xiao et al. (2024); Han et al. (2024) and systematic position-dependent patterns in how information contributes to model predictions Proietti et al. (2025); Wu et al. (2025). Positional bias becomes particularly evident in long-context settings, where models display the “lost in the middle” effect: when relevant information appears in the middle of lengthy inputs, models show significant performance degradation across tasks including question answering Liu et al. (2023), retrieval Shi et al. (2023), and summarization Zhang et al. (2024).\n\n\nHowever, it remains unclear how positional bias manifests across fine-grained input positions and model layers. Most existing layer-by-layer analyses are limited to long-context settings and task-level outcomes Wu et al. (2025); Ko et al. (2020); Kongmanee (2025). As a result, they do not establish whether positional bias reflects an intrinsic, layer-wise computational property of language models. This raises a fundamental question: if positional bias is rooted in model architecture, should it persist across layers even in short-context next-word prediction, independent of task difficulty or context length?\n\n\nWe address this question by characterizing positional bias at the word level (after aggregating subword tokens) for each model layer using conductance attribution in short-context next-word prediction. Unlike attention-based analyses, which primarily describe information routing, conductance quantifies how much each input token contributes to the model’s prediction through intermediate model components. We apply this approach to multiple language models and input texts using a sliding-window framework.\nBy averaging conductance scores over words at each relative position, we obtain\nlayer-wise positional importance profiles describing how layers weight positions during next-word prediction.\n\n\nWe investigate: (1) whether positional importance profiles are stable across inputs (a test of architectural vs. input-specific structure); (2) how recency and primacy biases evolve with layer depth; and (3) whether layers exhibit preferences for specific word types when averaged across positions.\n\n\nOur contributions are fourfold:\n\n\n\n\n•\n\nAttribution framework for positional bias.\nWe introduce an attribution-based framework that isolates positional effects from word identity, yielding word- and layer-resolved importance scores at each position in the input.\n\n\n\n•\n\nText-invariant positional importance profiles.\nWe show that transformer layers exhibit stable positional importance profiles in short-context next-token prediction. These profiles are highly consistent across natural texts and remain unchanged under lexical scrambling, indicating that they reflect intrinsic architectural properties rather than input-specific patterns.\n\n\n\n•\n\nDepth-dependent evolution of positional bias.\nWe characterize how primacy and recency biases evolve with depth, finding that primacy bias is weak and diminishes in deeper layers while recency bias increases monotonically with depth across models.\n\n\n\n•\n\nLayer specialization by word type and position.\nWe show that words across all positions most frequently receive their highest conductance from early layers, which also preferentially weight content words, whereas deeper layers increasingly dominate recent positions and exhibit reduced word-type sensitivity.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Positional Bias in LLMs\n\nPrior studies consistently report that language models prioritize information based on input position rather than semantic relevance. Gao et al. (2024) and Wang et al. (2025) document that LLMs struggle to utilize information from the middle or end of long contexts, exhibiting strong primacy and recency biases. This also manifests as a pronounced \"lost-in-the-middle\" effect Hsieh et al. (2024); Ravaut et al. (2024), where tokens at the input’s beginning or end receive disproportionately high weight regardless of relevance. Veseli et al. (2025b) show that the strength of recency and primacy varies with context window size: primacy is strongest when relevant content spans ≤50%\\leq 50\\% of the window, while recency dominates as spans grow larger. These positional preferences appear as performance degradation across retrieval, summarization, and question-answering tasks Ko et al. (2020). Notably, the magnitude of positional bias is model-specific, with some architectures exhibiting stronger primacy while others consistently favor late positions Menschikov et al. (2025).\nIn summary, prior work documents positional bias through task-level outcomes in long-context settings, where task difficulty and context length confound the underlying architectural bias, limiting fine-grained analysis of how individual layers allocate importance across positions.\n\n\n\n\n2.2 Mechanistic Analyses of Positional Sensitivity in Transformers\n\nOther studies probe transformers’ internal computations to characterize how positional structure emerges in attention and representations. Xiao et al. (2024) and Han et al. (2024) identify attention sinks—specific positions (often the first or special tokens) that accumulate disproportionate attention weight across layers, independent of content. Wu et al. (2025) develop a graph-theoretic analysis of multi-layer attention and prove that causal masking inherently biases deeper layers toward earlier sequence positions. Pascual et al. (2021) perform layer-wise gradient attributions in BERT and observe that positional patterns persist across layers despite extensiv"
  },
  {
    "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks",
    "url": "https://arxiv.org/abs/2601.04093v1",
    "source": "arxiv",
    "summary": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the",
    "full_text": null
  },
  {
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "url": "https://arxiv.org/abs/2601.04086v1",
    "source": "arxiv",
    "summary": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowle",
    "full_text": "\n\n\n\n1 Introduction\n2 Knowledge Distillation Chain-Style Model\n\n3 Improved Knowledge Distillation Chain with Code Guidance\n\n3.1 Model Enhancement\n3.2 Reasoning Process Analysis\n3.3 Prompt-Induced Hallucination Mitigation Method\n\n\n\n4 Experiments\n\n4.1 Results and Analysis\n4.2 Robustness Analysis\n4.3 Generalization Evaluation\n\n\n5 Conclusion\n\n\n\n\n\nKDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures\n\n\nJinbo Hao1*, Kai Yang2*, Qingzhen Su1, Yifan Li2, Chao Jiang2\n1School of Computer Engineering, Jiangsu Ocean University\n2School of Computer Science and Technology, Soochow University\n\n\n\n\nAbstract\nTo mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3.\nExperimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.\n\n\n\nKDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures\n\n\n\n\nJinbo Hao1*, Kai Yang2*, Qingzhen Su1, Yifan Li2, Chao Jiang2\n\n\n\n1School of Computer Engineering, Jiangsu Ocean University\n\n2School of Computer Science and Technology, Soochow University\n\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) learn contextual representations from large-scale corpora, enabling them to capture rich semantic patterns and to perform a wide range of language-centric tasks, including natural language understanding, text generation, machine translation, and question answering Brown et al. (2020); Raffel et al. (2020). Beyond text-only settings, recent advances have extended LLM capabilities to multimodal scenarios, allowing models to generate, translate, and reason over content spanning speech, images, and videos. This multimodal versatility has further accelerated their adoption across modern natural language processing systems and real-world applications Bommasani (2021). In addition, emerging evidence suggests that LLMs can support reasoning-oriented tasks that require structured decision-making, multi-step inference, and contextual integration, rather than relying solely on shallow pattern matching or surface-level correlations Yang et al. (2024b, a); Xiong et al. (2024a).\n\n\nDespite these capabilities, LLMs rely on probabilistic token prediction learned from training data. Because real-world corpora inevitably contain noise, bias, and incomplete information, models may produce outputs that are fluent but factually incorrect or logically inconsistent Maynez et al. (2020). This behavior is commonly referred to as hallucination. One particularly challenging form is prompt-induced hallucination, which arises when ambiguous or misleading prompts lead to incorrect responses even for well-defined tasks Ji et al. (2023); Tonmoy et al. (2024).\n\n\nFigure 1: Structure of the knowledge distillation chain model.\n\n\nHallucinations pose a major challenge to the reliable deployment of large language models (LLMs), especially in safety-critical domains such as scientific research, clinical decision support, and legal reasoning Bender et al. (2021). Existing mitigation strategies include improving training data quality, augmenting generation with retrieved evidence, and applying post-hoc verification methods Lewis et al. (2020); Manakul et al. (2023). While these approaches can reduce erroneous outputs, they often introduce additional computational cost or rely on external components, limiting their scalability and generality Shuster et al. (2021). Moreover, retrieval or verification alone does not explicitly constrain the model’s internal reasoning process, which is increasingly recognized as essential for reliable multi-step inference Xiong et al. (2025).\n\n\nTo address these limitations, we propose a prompt-induced hallucination mitigation method based on an enhanced knowledge distillation chain framework. By integrating structured knowledge with code-guided reasoning during inference, the proposed approach improves reasoning robustness while preserving the flexibility of large language models.\n\n\n\n\n2 Knowledge Distillation Chain-Style Model\n\nKnowledge distillation chain-style approaches integrate distillation principles with chain-of-thought reasoning to enhance both interpretability and predictive accuracy Hinton et al. (2015); Wei et al. (2022). By decomposing complex problems into a sequence of intermediate reasoning steps, this paradigm allows large language models to produce more structured and logically coherent outputs Kojima et al. (2022).\n\n\nIn a typical distillation chain-based setting, a model processes an input query by generating intermediate reasoning traces prior to emitting a final answer. These traces act as explicit guidance signals that steer the model toward more reliable conclusions Wang et al. (2022). However, when intermediate reasoning depends exclusively on the model’s internal knowledge, errors can accumulate across steps, ultimately resulting in hallucinated outcomes Ji et al. (2023).\n\n\nTo mitigate this issue, we augment the distillation chain framework with external structured knowledge sources, such as knowledge graphs that explicitly encode entities, relations, and temporal constraints Xiong et al. ; Xiong et al. (2024b). The reasoning process is enriched with auxiliary constraints that regulate intermediate steps and reduce reliance on uncertain internal representations, thereby improving logical consistency across multi-step inference Nye et al. (2021).\n\n\n\n\n3 Improved Knowledge Distillation Chain with Code Guidance\n\n\n3.1 Model Enhancement\n\nThe enhanced knowledge distillation chain-based model incorporates a programmable component into the reasoning workflow. This component guides knowledge exploration by using code as an explicit control signal, allowing the model to regulate reasoning steps beyond natural language alone.\n\n\nThe code-driven component fulfills two key functions. First, it enables structured traversal of relevant knowledge, supporting systematic retrieval and reasoning over related entities and relations. Second, code is embedded within the reasoning prompts as an auxiliary representation, providing external structured information that complements textual reasoning.\n\n\nThrough code-guided reasoning, intermediate inference steps are more closely aligned with formal constraints and structured knowledge, which helps suppress hallucinated reasoning trajectories.\n\n\nFigure 2: The process of suggesting hallucination problem-solving methods based on the large model based on the improved knowledge distillation chain.\n\n\n\n\n3.2 Reasoning Process Analysis\n\nWith the enhanced knowledge distillation chain framework, we examine how large language models perform inference. The structured reasoning process enables validation of intermediate conclusions and supports the identification of inconsistencies during generation. As a result, the model exhibits stronger self-correction behavior and improved prediction accuracy.\n\n\nIn addition, the explicit organization of reasoning steps increases transparency and interpretability, facilitating clearer analysis of error sources in model outputs.\n\n\n\n\n3.3 Prompt-Induced Hallucination Mitigation Method\n\nBuilding on the enhanced knowledge distillation chain framework, we introduce a method for mitigating prompt-induced hallucinations in large language models. The approach combines structured reasoning with guidance from external knowledge to limit incorrect generation caused by underspecified or ambiguous prompts.\n\n\nThe mitigation procedure follows three steps. First, the input prompt is examined and reformulated into a set of structured sub-problems. Next, a code-guided distillation chain produces intermediate reasoning under external constraints. Finally, the model generates an answer grounded in validated reasoning steps and structured knowledge.\n\n\nBy limiting error accumulation across reasoning stages, the proposed method improves robustness to variations in prompt formulation.\n\n\nFigure 3: Simulation experiments.\n\n\n\n\n\n4 Experiments\n\nWe evaluate the proposed approach on several public benchmarks using large language models such as GPT-4 and LLaMA 3.3 as base systems Achiam et al. (2023); Touvron et al. (2023). Model performance is assessed with standard retrieval-based metrics, including HIT@1, HIT@3, and HIT@5 Bordes et al. (2013).\n\n\nThe results show that incorporating code-guided reasoning leads to clear improvements in contextual understanding. Relative to baseline methods, the proposed framework yields consistent gains across all metrics, with HIT@1, HIT@3, and HIT@5 exceeding 95 percent. This indicates a substantial reduction in errors caused by prompt-induced hallucinations Ji et al. (2023).\n\n\nOverall, the findings demonstrate that the enhanced knowledge distillation chain framework improves both prediction accuracy and output verifiability, aligning with prior evidence on the benefits of structured reasoning and external guidance Nye et al. (2021).\n\n\nTable 1: Improvement Verification Results of the Knowledge Distillation Chain Model (KDCM) (%)\n\n\n\n\nDataset\nModel\nHIT@1\nHIT@3\nHIT@5\n\n\n\n\nWebQSP\nKDCM\n82.36\n83.14\n80.26\n\n\nKDCM + Code Module\n99.33"
  },
  {
    "title": "CSSG: Measuring Code Similarity with Semantic Graphs",
    "url": "https://arxiv.org/abs/2601.04085v1",
    "source": "arxiv",
    "summary": "Existing code similarity metrics, such as BLEU, CodeBLEU, and TSED, largely rely on surface-level string overlap or abstract syntax tree structures, and often fail to capture deeper semantic relationships between programs.We propose CSSG (Code Similarity using Semantic Graphs), a novel metric that leverages program dependence graphs to explicitly model control dependencies and variable interaction",
    "full_text": null
  },
  {
    "title": "Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning",
    "url": "https://arxiv.org/abs/2601.04083v1",
    "source": "arxiv",
    "summary": "The widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based ",
    "full_text": "\n\n\n\nI Introduction\n\nII The Cell (Re)Selection Problem\n\nII-A Cell (Re)Selection Process\nII-B Hierarchical Cell (Re)Selection Strategy\nII-C Equal Priority Cell (Re)Selection Strategy\nII-D Conventional Heuristic Baseline\n\n\n\nIII CellPilot Design\n\nIII-A Problem Modeling\nIII-B CellPilot Learning Algorithm and Optimizations\n\n\nIV Model Generalization\n\nV Evaluation\n\nV-A Evaluation Setup\nV-B Overall Performance &amp; Generalizability\nV-C Ablation Study\n\n\nVI Discussion\nVII Related Work\nVIII Conclusion\n\n\n\n\n\nCells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement Learning\n\n\n\nMarvin Illian1, Ramin Khalili2, Antonio A. de A. Rocha3, Lin Wang1\n\n\n\n\nAbstract\nThe widespread deployment of 5G networks, together with the coexistence of 4G/LTE networks, provides mobile devices a diverse set of candidate cells to connect to. However, associating mobile devices to cells to maximize overall network performance, a.k.a. cell (re)selection, remains a key challenge for mobile operators. Today, cell (re)selection parameters are typically configured manually based on operator experience and rarely adapted to dynamic network conditions. In this work, we ask: Can an agent automatically learn and adapt cell (re)selection parameters to consistently improve network performance? We present a reinforcement learning (RL)-based framework called CellPilot that adaptively tunes cell (re)selection parameters by learning spatiotemporal patterns of mobile network dynamics. Our study with real-world data demonstrates that even a lightweight RL agent can outperform conventional heuristic reconfigurations by up to 167%, while generalizing effectively across different network scenarios. These results indicate that data-driven approaches can significantly improve cell (re)selection configurations and enhance mobile network performance.\n\n\n\nI Introduction\n\n\nThe widespread adoption of 5G networks, coupled with the continued operation of 4G/LTE infrastructure, has significantly expanded the range of connectivity options for mobile devices [1, 2]. This coexistence creates a heterogeneous environment where multiple cells111We define a cell as a sector of a base station that operates on a specific frequency band, covering a specific area of the network.—potentially operating on different frequency bands—are simultaneously accessible to User Equipments (UEs) [3]. Such an environment provides unprecedented flexibility in cell (re)selection in idle mode222Idle-mode cell selection plays a critical role in multi-carrier network deployments, enabling efficient load balancing among users without complex signaling. It reduces overhead and errors associated with connected-mode load balancing, ensuring smoother user transitions to active mode [4].\n, enabling mobile devices to dynamically choose between cells based on signal strength and other mobility-related factors [5]. While mobile operators gain the opportunity to optimize connectivity decisions to improve network efficiency and user experience, this also introduces significantly higher complexity in designing effective cell (re)selection strategies.\n\n\nDespite the offered flexibility, most mobile operators still rely on fixed parameter configurations for cell (re)selection.\nIn practice, operators lack a standardized methodology for these configurations which are typically dervived experience-based by network engineers and refined iteratively through trail and error.\nThese parameters, including various thresholds and priorities, are typically determined through offline planning and are seldom adapted in real time to reflect changing network conditions [6, 5]. While such static configurations simplify network management, they often fail to exploit the full potential of heterogeneous mobile network deployments, leading to suboptimal network utilization and uneven load distribution. Through a case study conducted with one of the three largest mobile operators in Brazil, we show in Figure 1 that under roughly the same network loads (Config-B has 9% less UEs) but different parameter configurations, the per-UE downlink throughput in a cell can vary up to two times. This highlights the substantial impact of cell (re)selection parameter reconfiguration on network performance and hence underlines the need for more adaptive approaches capable of dynamically adjusting cell (re)selection strategies to optimize network performance.\n\n\nFigure 1: Per-UE downlink throughput comparison across 15 cells in a city in Brazil, under two different parameter configurations for cell (re)selection.\n\n\nAdaptive parameter configuration for cell (re)selection poses significant challenges due to multiple reasons. First, to be compatible with existing network infrastructure and operations with minimal changes, cell (re)selection decisions cannot be made explicitly and should be the outcome of parameter configurations implicitly. The parameters form a huge and high-dimensional space, integrating several thresholds and priorities that exhibit non-linear interactions. Moreover, the parameters concentrate mostly on signal strength and are not directly performance-oriented. Hence, it is difficult to associate a particular parameter configuration to a performance level, making parameter reconfiguration intrinsically a hard problem.\n\n\nSecond, mobile networks are highly dynamic in nature, both spatially and temporally. Variations in user mobility patterns, traffic demand fluctuations, and interference levels make it difficult to determine optimal parameters in real time. In a highly heterogeneous network, the impact of changing one parameter can cascade through the network, affecting network utilization and load distribution. Traditional profiling- and experience-based optimizations struggle to cope with this scale and complexity and typically require extensive manual efforts [5]. We conjecture that a learning-based approach is more suitable. A learned agent can autonomously explore the high-dimensional parameter space, learning policies that dynamically adapt to changing network conditions and improving network performance and efficiency.\n\n\nIn light of the above challenge, we propose CellPilot, a neural adaptive cell (re)selection framework for mobile networks. CellPilot adopts reinforcement learning (RL) to intelligently optimize the parameters for cell (re)selection under dynamic user connectivity and mobility. The key innovation of CellPilot lies in its state and reward design, where UE status and network load conditions, both historical and current ones, are encoded to capture the dynamic network conditions. The reward function is carefully crafted to incorporate multiple objectives. Furthermore, CellPilot introduces a series of optimization techniques to enhance RL agent generalizability, ensuring robust performance under diverse traffic patterns and across network scenarios without retraining. The combination of principled RL modeling and taillored optimizations enables CellPilot to outperform the conventional heuristic-based approach and adapt seamlessly to evolving network environments.\n\n\nOverall, we make the following contributions: After introducing the cell (re)selection problem (§ II), we\n\n\n\n\n•\n\npropose CellPilot, a learning-based framework for adaptive tuning for cell (re)selection, leveraging reinforcement learning to capture the dynamic network environment spatiotemporally (§ III).\n\n\n\n•\n\nintroduce, adapt, and integrate several optimization techniques to improve the performance and generalizability of CellPilot (§ IV).\n\n\n\n•\n\nbuild a simulator with real-world network data and comprehensively evaluate CellPilot under varying conditions (§ V). Our results show that CellPilot achieves up to 167% performance improvements. CellPilot also shows impressive generalizability, despite being lightweight.\n\n\n\nWe discuss the limitations of CellPilot in § VI, contrast CellPilot with related works in § VII and conclude in § VIII.\n\n\n\n\nII The Cell (Re)Selection Problem\n\n\nIn this section, we provide the background information and formally describe the cell (re)selection problem.\n\n\n\nII-A Cell (Re)Selection Process\n\n\nThe cell selection process in a mobile network [4, 7, 8] is detailed as follows.\nThe process is identical for 4G/LTE and 5G, with only minor parameter differences.\nEach cell is powered by a base station operating on a specific carrier frequency and is identified by a unique cell ID [9]. When a UE powers up and enters a network, it performs cell selection and the selection criteria are based on Reference Signal Received Power (RSRP) and broadcast parameters.\n\n\nBefore a UE considers any cell for camping, it verifies if the signal fulfills the suitability condition.\nThis requirement ensures that the UE can maintain adequate communication with the selected cell and is expressed as\n\n\n\nSrxlev,nCi=γnCi−QrxlevminCi&gt;0S^{C_{i}}_{\\mathrm{rxlev},n}=\\gamma^{C_{i}}_{n}-Q^{C_{i}}_{\\mathrm{rxlevmin}}&gt;0\n\n(1)\n\n\nwhere γnCi\\gamma^{C_{i}}_{n} represents Received Signal Strength Indicator (RSSI) for UE nn on cell CiC_{i}, QrxlevminCiQ^{C_{i}}_{\\mathrm{rxlevmin}} denotes the minimum required RSSI for cell CiC_{i} as broadcast in the System Information Block (SIB), and Srxlev,nCiS^{C_{i}}_{\\mathrm{rxlev},n} defines the cell (re)selection received level for UE nn on cell CiC_{i}.\n\n\nOnce the suitability condition is satisfied, the UE scans for all available cells, selects the most appropriate one based on network-defined criteria and camps on it.\nThe UE then monitors the SIBs to stay informed about network configuration changes and neighboring cells.\nThe network then guides idle-mode (re)selection through two complementary strategies: hierarchical cell (re)selection and equal priority cell (re)selection, as detailed in the following.\n\n\n\n\nII-B Hierarchical Cell (Re)Selection Strategy\n\n\nIn a hierarchical cell (re)selection strategy, the network assigns different priorities to cells.\nThis allows the network to steer UEs towards a cell that provides best possible service"
  },
  {
    "title": "Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts",
    "url": "https://arxiv.org/abs/2601.04073v1",
    "source": "arxiv",
    "summary": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecti",
    "full_text": "\n\n\n\n1 Introduction\n\n2 The LogicGraph Perturbation Protocol\n\n2.1 Dataset Curation\n2.2 Graph-based Reasoning Structuring\n2.3 Probability-Weighted Perturbation Injection\n\n\n\n3 Evaluations\n\n3.1 Models\n3.2 Evaluation Metrics\n3.3 Main Results\n3.4 The Impact of Perturbation Strength\n\n\n\n4 Active Visual-Context Refinement\n\n4.1 Problem Formulation\n4.2 Uncertainty-Driven Visual Re-grounding\n4.3 Context Denoising via Folding\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n5.3 Impact of Key Components\n\n\n6 Related Work\n7 Conclusion\nA Detailed descriptions of LMMs\n\nB Details of LogicGraph Perturbation Protocol\n\nB.1 Graph Structuring\nB.2 Perturbation Generation\nB.3 Reasoning Evaluation\n\n\nC Introduction to the STAR Dataset\n\n\n\n\n\nAnalyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts\n\n\n\nZhihao Zhu111footnotemark: 1,\nJiafeng Liang1,\nShixin Jiang1,\nJinlan Fu322footnotemark: 2,\nMing Liu1,2,\n\n\nGuanglu Sun1, See-Kiong Ng3, Bing Qin1,2\n\n1Harbin Institute of Technology, Harbin, China \n2Peng Cheng Laboratory, Shenzhen, China \n3National University of Singapore, Singapore \n{zhzhu,jfliang,sxjiang,mliu}@ir.hit.edu.cn\n  Equal Contribution.  Corresponding Author.\n\n\nAbstract\nLarge Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities.\nThe results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation.\nTo mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history.\nExperiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.\n\n\n\nAnalyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts\n\n\n\n\n\nZhihao Zhu111footnotemark: 1,\nJiafeng Liang1††thanks:   Equal Contribution.,\nShixin Jiang1,\nJinlan Fu322footnotemark: 2,\nMing Liu1,2††thanks:   Corresponding Author.,\n\n\nGuanglu Sun1, See-Kiong Ng3, Bing Qin1,2\n\n1Harbin Institute of Technology, Harbin, China\n\n2Peng Cheng Laboratory, Shenzhen, China\n\n3National University of Singapore, Singapore\n\n{zhzhu,jfliang,sxjiang,mliu}@ir.hit.edu.cn\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Illustration of visual blindness induced by erroneous textual context. While normal reasoning (a) grounds answers in visual evidence, perturbated reasoning (b) demonstrates that injecting a factual error causes the model to ignore conflicting visual signals. The model prioritizes consistency with the false history over visual reality, leading to incorrect justifications.\n\n\nLarge Multimodal Models (LMMs)  Bai et al. (2025); Team et al. (2025); Zhu et al. (2025) have demonstrated impressive capabilities in general video comprehension, evolving from simple perception Yu et al. (2019) to complex reasoning tasks Wu et al. (2021); Fu et al. (2025).\nUnlike static image analysis, video reasoning requires models to maintain logical consistency across temporal sequences and comprehensively process spatiotemporal information correlations among multiple frames.\nTherefore, the ability to automatically reflect, verify and correct errors during the reasoning process has become an important research hotspot Feng et al. (2025); Wang and Peng (2025).\n\n\nIn the text-only domain, recent works like SCoRe Kumar et al. (2025) have successfully trained Large Language Models (LLMs) to self-correct via reinforcement learning, demonstrating that models can refine their outputs using self-generated data. Extending this to the multimodal sphere, Subsequent studies Cheng et al. (2025); Lee et al. (2024) further validate that LMMs also possess such reflective capabilities, enabling them to self-improve reasoning by explicitly reflecting on their own rationales.\n\n\nHowever, a crucial question about reflective sources has been largely overlooked: When correcting reasoning steps, it remains unclear whether LMMs actively re-examine visual content or simply rely on history textual context.\n\n\nMotivated by this, we construct a preliminary analysis where we inject subtle factual errors into the early steps of a reasoning chain.\nWe find that once a textual error is generated or injected, the model exhibits an overwhelming tendency to trust its own erroneous history over the conflicting visual evidence (shown in Figure 1).\nSpecifically, instead of looking back at the video to verify the facts, the model creates a justification based on the previous text, leading to a cascade of errors.\nThis suggests that in current LMMs, the strong probability distribution of the language decoder often overrides the visual signal, rendering the model effectively blind during the reflection process.\n\n\nTo rigorously quantify this phenomenon, we introduce the LogicGraph Perturbation Protocol. Instead of treating reasoning as a flat text sequence, we structure video reasoning chains into knowledge graphs (i.e., entity, relation and attribute). Within this structured framework, we inject plausible counterfactual perturbations selected based on linguistic probability distributions, creating strong misleading text that conflict with visual reality. This allows us to systematically evaluate whether mainstream LMMs can ground their reflection in visual evidence or succumb to the injected hallucinations.\n\n\nOur analysis reveals that LMMs exhibit weak self-reflection capabilities. Crucially, we observe that this reflection is predominantly derived from the textual history rather than visual evidence, rendering models unable to effectively challenge more complex errors.\n\n\nIntuitively, addressing textual inertia requires prompting the model to think more groundedly and removing erroneous textual history to reduce interference from textual noise.\nTo this end, we propose Active Visual-Context Refinement, a training-free inference-time strategy designed to facilitate robust self-correction.\nEmulating active visual perception, our approach actively interrupts the generation flow at key reasoning nodes to perform a look-back operation on specific video frames, thereby enforcing cross-modal interaction and ensuring the reflection is grounded in visual evidence.\nFurthermore, simply detecting an error is insufficient if the wrong tokens remain in the context window to bias future generation.\nTherefore, we introduce a folding mechanism to manage context cleanliness by compressing the trial-and-error history into a clean, factual summary once a correction is made.\nThis physically removes the toxic text tokens that drive text inertia and resets the attention landscape, allowing the model to perform subsequent reasoning with a clarified state.\nExperimental results demonstrate that this paradigm effectively reactivates the model’s self-correction capabilities, elevating explicit reflection rates from negligible levels to a substantial proportion while yielding a significant gain in overall task accuracy.\nOur main contributions are summarized as follows:\n\n\n•\n\nWe identify the text inertia in LMMs reasoning, revealing that models prioritize erroneous textual history over visual evidence during self-correction.\n\n\n\n•\n\nWe propose the LogicGraph Perturbation Protocol to systematically analyze reflection failures, uncovering that mainstream LMMs exhibit weak self-reflection capabilities, predominantly sourcing their reflection from the hallucinated textual context.\n\n\n\n•\n\nWe introduce Active Visual-Context Refinement, a novel inference-time strategy that integrates visual re-grounding with context denoising, significantly improving robustness and reasoning accuracy on complex video benchmarks.\n\n\n\n\n\n\n\n2 The LogicGraph Perturbation Protocol\n\nFigure 2: Overview of the LogicGraph Perturbation Protocol. The framework systematically evaluates text inertia by structuring reasoning chains into semantic graphs and injecting probability-weighted counterfactual perturbations. This process creates a conflict between textual priors and visual reality to determine whether the model succumbs to contextual contamination or achieves explicit reflection through visual evidence.\n\n\nTo transcend conventional outcome-oriented evaluations and probe the underlying cognitive dynamics of multimodal reasoning, we introduce the LogicGraph Perturbation Protocol. This framework is designed to systematically investigate the mechanism of text inertia, specifically aiming to determine whether LMMs possess the agency to rectify logic chains contaminated by textual errors through visual grounding, or if they prioritize textual consistency over visual fidelity. The overall pipeline is illustrated in Figure 2.\n\n\n\n2.1 Dataset Curation\n\nWe utilize the STAR dataset Wu et al. (2021), specifically focusing on feasibility and prediction tasks. Unlike standard captioning benchmarks, this dataset requires models to perform logical deduction across temporal sequences rather than mere visual recognition. To prevent models from exploiting elimination strategies inherent in multiple-choice questions, we reformulate the dataset into an Open-ended QA format. This modification compels the model to generate explicit, self-contained reasoning trajectories, which are essential for our subsequent graph-based struc"
  },
  {
    "title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models",
    "url": "https://arxiv.org/abs/2601.04068v1",
    "source": "arxiv",
    "summary": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localize",
    "full_text": null
  },
  {
    "title": "Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation",
    "url": "https://arxiv.org/abs/2601.04065v1",
    "source": "arxiv",
    "summary": "Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods deman",
    "full_text": null
  },
  {
    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
    "url": "https://arxiv.org/abs/2601.04060v1",
    "source": "arxiv",
    "summary": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To ta",
    "full_text": "\n\n\n\n1 Introduction\n2 Related work\n\n3 Method\n\n\n3.1 Dataset Construction\n\nConsistency-filtered pairing.\nReasoning traces with validator feedback.\n\n\n\n3.2 Reasoning-As-Action Graph-Editing MDP\n\nWorkflow representation.\nState and actions.\nTool-mediated transition.\n\n\n\n3.3 State-Aware Validation and In-Place Repair (C1)\n\nState-aware validator.\nIn-place repair with diagnostics.\nValidated-prefix property.\n\n\n\n3.4 Entropy-Adaptive Branching Under Uncertainty (C2)\n\nStep-level uncertainty.\nBranching rule.\n\n\n\n3.5 Learning with Supervised Warmup and GRPO\n\nSupervised warmup.\nTool-feedback RL with GRPO.\n\n\n\n3.6 Hierarchical Terminal Reward\n\nRfR_{f}: format validity.\nRcR_{c}: trace–workflow consistency.\nStructural term.\nFinal reward.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Experiment Settings\n\nDataset and Benchmarks.\nImplementation Details.\n\n\n4.2 Autonomous Workflow Construction\n4.3 Text-to-Image Generation\n4.4 Qualitative Results\n4.5 Efficiency Analysis\n\n4.6 Ablation Study\n\nMethod Ablation.\nData Construction Ablation.\n\n\n\n\n5 Conclusion\nA An example of SFT data\nB GRPO\nC Notation.\nD Qualitative Results\nE Evaluation\n\n\n\n\n\nComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows\n\n\nJinwei Su, Qizhen Lan, ZEYU WANG, Yinghui Xia, \nHairu Wen, Yiqun Duan, Xi Xiao, TIANYU SHI, Yang Jingsong, Lewei He\n\n\n\nAbstract\nAI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality.\nTo tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction.\nExperiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.\n\n\n\nComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows\n\n\n\n\nJinwei Su, Qizhen Lan, ZEYU WANG, Yinghui Xia,\n\nHairu Wen, Yiqun Duan, Xi Xiao, TIANYU SHI, Yang Jingsong, Lewei He\n\n\n\n\n\n\n1 Introduction\n\nGenerative vision models increasingly power text-to-image synthesis Podell et al. (2023); Esser et al. (2024); Labs (2024), image editing Brooks et al. (2023); Ai (2024); Huang et al. (2024), and video generation HaCohen et al. (2024); Tencent (2024); Wang et al. (2025a). While large general-purpose models Ge et al. (2024); Wu et al. (2024); OpenAI (2025) can deliver strong quality across tasks, they are expensive to customize and deploy. This has spurred compositional workflow systems that expose intermediate structure for controllable design Gal et al. (2024). A representative platform is ComfyUI ComfyUI Contributors (2023), which represents a generation pipeline as a typed node graph that users compose into reusable, task-specific workflows.\n\n\nFigure 1: One-shot workflow planning generates the entire graph at once and is brittle, whereas reasoning-as-action generation (ComfySearch) incrementally builds and validates each edit, ensuring robust and executable workflows.\n\n\nDespite its flexibility, ComfyUI authoring is brittle. A workflow must satisfy strict type/interface constraints and global graph invariants; a single mismatched edge or missing adapter can render the graph non-executable. As a result, users often rely on expertise and repeated trial-and-error. Recent LLM-based systems aim to automate workflow construction through explicit planning and composition (e.g., ComfyAgent Xue et al. (2024), ComfyMind Guo et al. (2025)) or by training generators with reasoning traces (e.g., ComfyUI-R1 Xu et al. (2025)). However, long-horizon construction remains fragile: decisions that are locally plausible may later violate downstream compatibility, producing workflows that appear coherent but crash at execution time or deviate from the user’s intent.\n\n\nThe core difficulty is that workflow generation is not purely retrieval. Retrieval can identify relevant nodes and documentation, but it does not ensure state-dependent composability: whether an edit is valid depends on the current partial graph and how future branches will constrain interfaces. This motivates an interactive, execution-grounded paradigm in which each proposed graph edit is immediately validated for executability (typing/schema/invariants) and repaired using diagnostic feedback, rather than relying on one-shot plans(Figure 1).\n\n\nFigure 2: Overview of the ComfySearch framework. We formulate workflow generation as a Markov Decision Process(§3.2). (a) State-aware Validation(§3.3): Implements real-time verification and repair to ensure online structural correctness (C1). (b) Entropy Rollout Branching(§3.4): Employs selective exploration at high-uncertainty points to navigate long-horizon decision branching (C2).\n\n\nWe distill two challenges. (C1) Online structural correctness. Deferring validation allows early wiring mistakes to propagate in typed graphs, leading to compounding errors in long-horizon generation Arora et al. (2022). A practical generator should therefore validate each edit against the current state and repair violations on the fly.\n(C2) Long-horizon branching under uncertainty. Multiple edits may be locally admissible yet induce sharply different downstream feasibility; moreover, validator feedback changes the effective state distribution during construction. Thus, the generator should selectively branch at high-uncertainty decision points instead of committing to a single trajectory Yao et al. (2023); Haarnoja et al. (2018).\n\n\nThese observations suggest a reasoning-as-action view: the model incrementally edits the workflow graph, queries a validator for state-aware feedback, and revises decisions when incompatibilities arise. We view this process as an episodic Markov Decision Process (MDP), where the state is the current workflow prefix, an action is a node-level graph edit, and transitions return the updated graph with validation feedback.\n\n\nTo address (C1) and (C2), we propose ComfySearch, an reasoning-as-action framework for ComfyUI workflow generation. ComfySearch enforces executability via node-level, state-aware validation and in-place repair (C1). It further introduces entropy-adaptive branching, using policy uncertainty to allocate a limited rollout budget toward ambiguous decision points, improving robustness and efficiency without exhaustive branching (C2) Wang et al. (2025c); Dong et al. (2025b). We also construct consistency-filtered query–workflow supervision by executing candidate workflows and canonicalizing multiple valid designs within each query cluster, reducing noisy node-level labels. Finally, we train ComfySearch with tool-mediated feedback under a reinforcement learning objective, enabling the model to learn which nodes to add, when to validate intermediate edits, and when to branch. Our contributions are:\n\n\n•\n\nReasoning-as-action workflow generation: we formulate ComfyUI construction as reasoning-as-action with state-aware online validation and in-place repair.\n\n\n\n•\n\nEntropy-adaptive branching: we selectively explore high-ambiguity decisions using policy uncertainty, improving robustness and efficiency under limited rollout budgets.\n\n\n\n•\n\nExecution-verified supervision and RL training: we curate consistency-filtered data and train with tool-mediated feedback; evaluations on ComfyBench and GenEval improve executability, task success, and efficiency.\n\n\n\n\n\n\n\n2 Related work\n\nWorkflow Generation.\nLLM-based workflow generation translates natural-language requests into structured plans or executable programs Hu et al. (2024); Zhang et al. (2024, 2025); Ye et al. (2025); Su et al. (2025). Recent work extends this paradigm to ComfyUI, where a workflow is a typed node graph with strict interface constraints. Prior systems explore multi-agent decomposition (e.g., ComfyAgent Xue et al. (2024) and ComfyGPT Huang et al. (2025)), modular and hierarchical planning (e.g., ComfyMind Guo et al. (2025)), or long-form reasoning with documentation retrieval (e.g., ComfyUI-R1 Xu et al. (2025)).\nWhile effective in proposing plausible graphs, these approaches are typically not execution-grounded. They lack node-level, state-aware validation of each graph edit and do not selectively branch at high-uncertainty decision points, so locally admissible edits can later violate downstream compatibility and fail at execution time.\n\n\nAgentic Reinforcement Learning.\nAgentic RL has recently advanced long-horizon tool-use and structured decision-making by allocating exploration to uncertain steps. GRPO Shao et al. (2024) provides a PPO-style optimization scheme that improves training efficiency, while ARPO Dong et al. (2025b) and AEPO Dong et al. (2025a) use entropy-aware rollouts/updates to encourage exploration and stabilize learning in multi-turn settings. Complementary lines of work study uncertainty-triggered information seeking (e.g., Search-o1 Li et al. (2025)) and show that high-entropy minority tokens function as critical “forks” in RLVR for LLM reasoning Wang et al. (2025b). Our ComfySearch instantiates these insights at the graph-edit action level by coupling entropy-adaptive branching with state-aware validation feedback, enabling robust workflow construction under execution grounding.\n\n\n\n\n3 Method\n\nComfySearch constructs an executable ComfyUI workflow by iteratively proposing atomic graph edits and grounding each edit with node-level, state-aware validation feedback. This design addresses two challenges from §1:\n(i) online structural correctness via validation and in-place repair (C1), and\n(ii) long-horizon branching under uncertainty via entropy-adaptive selective branching (C2).\nWe learn a policy that decides what to edit, how to repair after "
  }
]