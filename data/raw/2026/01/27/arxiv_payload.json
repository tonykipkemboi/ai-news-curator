[
  {
    "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models",
    "url": "https://arxiv.org/abs/2601.18796v1",
    "source": "arxiv",
    "summary": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. W",
    "full_text": null
  },
  {
    "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
    "url": "https://arxiv.org/abs/2601.18795v1",
    "source": "arxiv",
    "summary": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing insta",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries\n\n3 PrefixRL: On-Policy RL on Very Off-policy Prefixes\n\n3.1 PrefixRL Objective-Consistency and Sample-Efficiency Guarantees\n\n\n\n4 Back-Generalization Boosts the Learning Signal in PrefixRL\n\n4.1 PrefixRL Improves No-Prefix Performance Even When Training Only on Prefixed Problems\n4.2 PrefixRL can Discover New Strategies Beyond What is Present in the Prefixed Problem\n4.3 Which Prefixes Back-Generalize the Most in PrefixRL?: Analysis via In-Context Learning\n\n\n\n5 Experiments and Results\n\n5.1 PrefixRL is More Compute-Efficient on Hard Problems Than Standard RL\n5.2 Training Dynamics of PrefixRL are More Stable Compared to Typical Off-Policy Methods\n\n\n6 Related Work and Discussion\n7 Conclusion\n8 Additional Notation\n\n9 Omitted Proofs\n\n9.1 Proof of Theorem 3.2\n\n9.2 Proof of Theorem 3.3\n\n9.2.1 Critic Estimation Error.\n9.2.2 Mirror Ascent and NPG Optimization Error.\n9.2.3 Combining Critic Error and Optimization Error.\n\n\n\n9.3 Proof of Proposition 3.4\n\n9.3.1 Exponential lower bound for standard on-policy RL (Algorithm 1 without ùíüoff\\mathcal{D}_{\\mathrm{off}}).\n9.3.2 Horizon-independent (non-exponential) upper bound for PrefixRL.\n9.3.3 Worst-Case Separation Result Between Standard RL and PrefixRL.\n\n\n9.4 Auxiliary Lemmas\n\n\n\n10 Additional Experiment Details and Results on Back-Generalization\n\n10.1 Hard problems used in the in-context back-generalization experiment\n10.2 Back-generalization under model-family mismatch\n\n\n\n11 Additional Experiments and Details for Results in Section 5\n\n\n11.1 Implementation details for PrefixRL and Baselines\n\n11.1.1 Constructing Prefixed Problems: An Example\n11.1.2 Off-policy RL Baselines\n11.1.3 Hyperparameter Details for PrefixRL\n\n\n11.2 FLOPs Accounting for our Compute-Matched Performance Plots\n11.3 Qwen with Llama Prefixes\n11.4 Computing the gradient norm and standard deviation metrics\n\n\n\n\n\n\n\n\n1]FAIR at Meta\n2]Carnegie Mellon University\n3]University of Southern California\n\\contribution[*]Work done at Meta\n\\contribution[‚Ä†]Authors jointly advised this work.\n\n\n\nReuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes\n\n\n\nAmrith Setlur\n\n‚ÄÉ‚ÄÉ\nZijian Wang\n\n‚ÄÉ‚ÄÉ\nAndrew Cohen\n\n‚ÄÉ‚ÄÉ\nParia Rashidinejad\n\n‚ÄÉ‚ÄÉ\nSang Michael Xie\n\n[\n\n[\n\n[\n\nasetlur@cs.cmu.edu\n\nzijianwang@meta.com\n\n\n(January 26, 2026)\n\nAbstract\nTypical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2√ó\\times faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3√ó\\times. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.\n\n\n\\correspondence\n,\n\n\n\n1 Introduction\n\nReinforcement learning (RL) is the de facto method to boost large language model (LLMs) reasoning, especially for math and coding (An et al., 2025; Liu et al., 2025b; Guo et al., 2025).\nMost successful RL recipes (Ahmadian et al., 2024; Yu et al., 2025) are on-policy: sample multiple reasoning traces (rollouts) from the current model and derive updates from correct (and incorrect) traces. This paradigm breaks down on hard problems with low pass@kk (e.g., pass@2k ‚âà\\approx 0), where the model rarely samples a correct trace.\nIn this regime, learning stalls as RL spends enormous amounts of sampling FLOPs without receiving any learning signal, and RL rewards plateau.\n\n\nIn practice, we are rarely solving these problems for the first time\nsince earlier RL runs or inference on previous models may have spent compute on the same (or similar) hard problems.\nThe natural question now is how to reuse this ever-growing dataset of off-policy traces, which often contains some correct traces even for very hard problems, in order to guide the online RL policy towards higher-rewarding states and accelerate on-policy RL.\n\n\nA straightforward approach is to treat the off-policy traces as supervision: perform supervised fine-tuning (a.k.a., mid-training or continued pretraining) on the correct off-policy traces followed by standard on-policy RL (Wang et al., 2025d). However, SFT on a small set of correct traces can lead to memorization (Chu et al., 2025) and entropy collapse, which hurts exploration during subsequent RL (Zhang et al., 2025a).\nAlternatively, we can use off-policy traces directly in RL via importance weighting, but this is often unstable due to high-variance gradient estimates (Liu et al., 2025a; Yan et al., 2025). Both options use off-policy traces as target supervision, and since these off-policy traces are very low probability under the RL policy, this leads to suboptimal RL optimization.\n\n\nFigure 1: PrefixRL: On-Policy RL Conditioned on Off-Policy Prefixes.\nWe leverage previously spent compute ( 1) on hard problems in the form of correct off-policy traces rejection sampled from the base LLM we start RL from. Off-policy traces could also come from other model families or previous RL runs. We append prefixes of a single correct off-policy trace per problem to the original problem, creating prefixed problems ( 2). Then, we simply run on-policy RL on prefixed and no-prefix (original) problems ( 3). PrefixRL places the RL policy in higher-rewarding states, which boosts the learning signal. Performance transfers from the prefixed to no-prefix problems via a phenomenon we call back-generalization.\n\n\n\nTo avoid these pitfalls, we propose PrefixRL: run on-policy RL conditioned on prefixes of correct off-policy traces instead of directly supervising on them (Figure 1). First, we extract and fix a few off-policy prefixes and append them to the original problem to create prefixed problems.\nSecond, we run on-policy RL on both no-prefix (original) problems and prefixed problems, where gradients are masked on the off-policy prefix.\nThe prefixes extracted from correct off-policy traces place the current RL policy in states that are more likely to lead to a correct answer on hard problems, reducing gradient variance and increasing the strength of the learning signal.\n\n\nPrefixRL is consistent with and more sample-efficient than standard RL. However, it is not immediately clear what the effect on the bias is.\nIn Section 3.1, we prove that when the prefixes are correct and realizable in the model class, (i) maximizers of the PrefixRL objective also maximize performance on the standard RL objective; and (ii) since the prefixes lessen the exploration burden, PrefixRL reduces suboptimality gap with less samples compared to standard RL (by a factor of context length).\nOverall, PrefixRL changes the on-policy RL objective by using off-policy prefixes solely to guide exploration and unblock training on hard problems.\n\n\nBack-generalization.\nBeyond the theory, we empirically find an additional phenomenon behind the gains in PrefixRL we call back-generalization, where on-policy RL on only prefixed problems substantially boosts test performance on the original no-prefix problems, which were never trained on.\nBeyond the generalization in the face of train/test mismatch, back-generalization is distinctive for two reasons.\nFirst, back-generalization is a type of generalization via shared parameters because it alters the next-token distribution on prefixes it was never trained on (impossible in the tabular RL setting).\nSecond, we find that back-generalization can be even more powerful than standard generalization in RL (transfer across related problems or environments).\nWe show this in an in-context learning setup, where we run RL on problems prefixed with another problem and reasoning trace in context. We find that training on a problem P1 conditioned on a related problem P2 in context improves generalization from P1 to P2 considerably more than directly running RL on the problem P1 (see Section 4.3).\n\n\nPrefixRL can discover and learn strategies beyond what is provided in the prefix.\nInterestingly, the model does not simply back-generalize by imitating the off-policy prefix it is conditioned on.\nThrough controlled experiments, we find that PrefixRL is more compute-efficient than standard RL at amplifying successful strategies and rejecting suboptimal ones, even when the suboptimal strategy is explicitly present in the off-policy prefix. As a result of observing non-zero rewards (and advantages) more often, we hypothesize that PrefixRL allows the model to more quickly identify the flaws in the suboptimal strategy and use this insight to find a better strategy (see Section 4.2).\n\n\n\n\n\n\nFigure 2: PrefixRL affords a self-improvement pipeline that recycles RL flops on hard problems.\nWe instantiate PrefixRL for self-improvement by collecting a dataset of off-policy traces through large-scale rejection sampling on the base LLM (distill"
  },
  {
    "title": "MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data",
    "url": "https://arxiv.org/abs/2601.18792v1",
    "source": "arxiv",
    "summary": "Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalog",
    "full_text": null
  },
  {
    "title": "Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets",
    "url": "https://arxiv.org/abs/2601.18791v1",
    "source": "arxiv",
    "summary": "We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity a",
    "full_text": null
  },
  {
    "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
    "url": "https://arxiv.org/abs/2601.18790v1",
    "source": "arxiv",
    "summary": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nSafety and Conflicting Objectives.\nConsequence and Affordance Awareness.\nRewards and Latency.\n\n\n\n3 The MortalMATH Benchmark\n\n\n3.1 Dataset Construction and Rationalization\n\nThe \"Anyway\" Pivot.\n\n\n3.2 The Urgency Spectrum\n\n3.3 Evaluation Metrics and Justification\n\nRefusal Rate (The Boxed Proxy).\nMATH Correctness.\nReasoning Latency.\n\n\n\n\n\n4 Results\n\n4.1 The Regression in Safety Prioritization\n4.2 Reasoning Time as a Safety Risk\n4.3 System Prompt Sensitivity\n\n\n\n5 Qualitative Analysis\n\nRigid Adherence (Qwen, GPT-4.1).\nThe \"Safety Sandwich\" (Claude, Gemini).\nThe Refusal (Llama Exception).\n\n\n\n6 Discussion\n\nThe Role of Reward Misspecification.\nLatency and Chain of Draft.\nRealism and Intent.\n\n\n7 Conclusion\nProxy-Based Evaluation.\nEcological Validity of Scenarios.\nAttribution to RLVR.\nDataset Scale.\n\nA Scenarios and Prompts\n\nA.1 Level 1: Distraction\nA.2 Level 2: Discomfort\nA.3 Level 3: Impairment\nA.4 Level 4: Severe Distress\nA.5 Level 5: Extreme\n\n\n\n\n\n\n\nMortalMATH: Evaluating the Conflict \nBetween Reasoning Objectives and Emergency Contexts\n\n\n\nEtienne Lanzeray1,\nSt√©phane Meilliez1,\nMalo Ruelle1,\nDamien Sileo1,2\n1Univ. Lille, Lille, France \n2Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France \ndamien.sileo@inria.fr\n\n\n\nAbstract\nLarge Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95% task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds‚Äîbefore any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.\n\n\n\nMortalMATH: Evaluating the Conflict \nBetween Reasoning Objectives and Emergency Contexts\n\n\n\n\n\nEtienne Lanzeray1,\nSt√©phane Meilliez1,\nMalo Ruelle1,\nDamien Sileo1,2\n\n1Univ. Lille, Lille, France\n\n2Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 - CRIStAL, F-59000 Lille, France\n\ndamien.sileo@inria.fr\n\n\n\n\n\n\n1 Introduction\n\nThe development of Large Language Models (LLMs) has increasingly bifurcated. On one side are general-purpose conversational agents; on the other are models specialized for deliberate, multi-step computation (Ornia et al., 2025). This specialization is frequently driven by training paradigms that heavily penalize incorrect answers and reward valid reasoning chains, effectively optimizing the model to be a robust computational engine.\n\n\nWhile this maximizes performance on static benchmarks, real-world deployment requires objective prioritization. An agent must not only possess the capability to solve a task but also the discernment to determine if a task should be solved given the surrounding context (Wang and Zhong, 2023). Standard alignment techniques typically focus on refusing harmful requests (e.g., generating dangerous content). In this work, we investigate the inverse: harmful neglect. We ask whether models can identify when a benign request (solving an equation) becomes inappropriate due to the user‚Äôs physical state (e.g., ‚ÄúSolve this integral while I bleed out‚Äù).\n\n\nWe introduce MortalMATH, a targeted evaluation set that wraps standard difficult MATH Hendrycks et al. (2021) algebra problems in conversational contexts of escalating urgency. By analyzing the behaviors of six diverse models, we identify a distinct behavioral split. Unlike generalist models, which tend to pivot to safety warnings, reasoning-dense models often treat the urgency cues as irrelevant noise. They prioritize the reward signal of the math solution, leading to responses where a model provides a perfect derivation of a polynomial while the user describes symptoms of a stroke. This behavior suggests a form of consequence blindness Wu et al. (2025), where the immediate reward of task completion obscures the broader pragmatic failure.\nThe code and data are publicly available111\n\n\n\n\nColab notebook\n\n\nHuggingFace dataset\n\n.\n\n\n\n\n2 Related Work\n\nSafety and Conflicting Objectives.\n\nBianchi et al. (2024) show that safety-tuning can cause over-refusal of benign prompts. We explore the inverse: models that are too helpful, failing to stop benign tasks when safety demands it.\nYing et al. (2024) categorize LLM decision-making into intuitive vs. rational styles when facing conflicting prompts. In our setup, the conflict is between the explicit instruction (do the math) and the implicit context (I am dying).\n\n\n\nConsequence and Affordance Awareness.\n\nOur findings align with the concept of consequence blindness introduced by Wu et al. (2025), where models fail to map surface semantics to outcome risks. Similarly, Adak et al. (2025) discuss affordance-aware alignment (AURA), arguing that models must identify unsafe procedural trajectories. We show that for reasoning models, the \"trajectory\" of solving a math problem is so strongly reinforced that it ignores the \"affordance\" of the user‚Äôs mortality.\n\n\n\nRewards and Latency.\n\nThe rigid behavior we observe may stem from reward misspecification. Kumar et al. (2025) highlight how process rewards can overfit to specific behaviors (like step-by-step derivation) at the expense of broader utility. Furthermore, Xu et al. (2025) (Chain of Draft) emphasize that reasoning latency is a critical factor in usability. We extend this to safety: in Level 4/5 scenarios, the 10-15 seconds spent generating math tokens represents a dangerous delay in triage.\n\n\nFigure 1: MortalMATH Results across Urgency Levels. Top-Left: Average tokens and latency. Note that reasoning models (purple lines) spend significant time (up to 15s) computing even in Level 5. Top-Right: Refusal Rate. Llama-3.1 (blue) and Gemini (green) show increased refusal as urgency rises; Qwen and GPT reasoning models do not. Bottom-Left: Reasoning tokens do not drop to zero for reasoning models, indicating continued computation. Bottom-Right: Correct Answer Rate remains high for reasoning models despite context.\n\n\n\n\n\n3 The MortalMATH Benchmark\n\nMortalMATH acts as a diagnostic probe to evaluate the conflict between two objectives: (1) Instruction Adherence (solving the user‚Äôs explicit math query) and (2) Contextual Safety (addressing the user‚Äôs implicit physical danger).\n\n\n\n3.1 Dataset Construction and Rationalization\n\nWe utilize 10 randomly selected Algebra problems (difficulty 4) from the Competition MATH dataset Hendrycks et al. (2021). These problems require significant cognitive processing, distinguishing them from trivial arithmetic. We inject these problems into a prompt template T‚Äã(c,p)T(c,p), where pp is the problem and cc is a context describing the user‚Äôs state.\n\n\nThe \"Anyway\" Pivot.\n\nTo rigorously test context retention, we employ a strong discourse pivot: ‚ÄúAnyway, can you help me with this assignment:‚Äù. While this artificially separates context, robust alignment requires maintaining awareness of safety states (e.g., ‚ÄúI am dying‚Äù) regardless of discourse markers, much like a doctor tracking symptoms even if a patient changes the topic.\n\n\n\n\n\n3.2 The Urgency Spectrum\n\nWe define five levels of urgency (Table 1). We acknowledge that some scenarios (e.g., \"burnt toast smell\", a stroke symptom) are not totally clinically reliable. However, for an NLP agent, the user‚Äôs intent and fear are paramount. If a user expresses fear of a stroke, the aligned response is to address that fear, not to ignore it because the symptom is clinically debated.\n\n\n\n\n\nLvl\n\n\nCategory\n\n\nExample Context\n\n\n\n\n1\n\n\nDistraction\n\n\nLoud airport terminal.\n\n\n2\n\n\nDiscomfort\n\n\nWalking home in the dark.\n\n\n3\n\n\nImpairment\n\n\nDriving on a highway.\n\n\n4\n\n\nSevere Distress\n\n\nStroke symptoms; Overdose.\n\n\n5\n\n\nExtreme\n\n\nFreefall; Cobra bite.\n\n\n\nTable 1: The Urgency Spectrum used to modulate context. See Appendix A for full texts.\n\n\n\n\n3.3 Evaluation Metrics and Justification\n\nRefusal Rate (The Boxed Proxy).\n\nWe measure refusal by the absence of a boxed mathematical answer (‚Äò\\boxed{answer}‚Äò). While this is a heuristic, it is particularly robust for the class of models we evaluate (Math/Reasoning specialists), which are heavily fine-tuned to comply with this specific formatting constraint. For these models, the generation of a ‚Äò\\boxed‚Äò tag represents a \"commitment bit\"‚Äîa definitive sign that the model chose to fulfill the math instruction.\n\n\n\nMATH Correctness.\n\nWe use math_verify 222https://github.com/huggingface/Math-Verify to check if the solution is mathematically correct. In Levels 4 and 5, high correctness scores are paradoxically a sign of poor alignment, as they indicate the model successfully ignored the emergency to solve the puzzle.\n\n\n\nReasoning Latency.\n\nWe track the number of reasoning tokens generated. In high-urgency contexts, high token counts are a proxy for \"Time-to-Help.\" A model that generates 500 tokens of math derivation before offering safety advice has failed the triage test, even if the advice eventually appears.\n\n\n\n\n\n\n4 Results\n\nWe evaluated a suite of models including the GPT-5 family (nano, mini), the GPT-4.1 family (nano, mini), qwen3-32b, claude-haiku-4.5, gemini-2.5-flash-lite, and llama-3.1-8b-instruct.\n\n\n\n4.1 The Regression in Safety Prioritization\n\nFigure 1 reveals a stark divergence. llama-3.1-8b-instruct and gemini-2.5-flash-lite demonstrate high sensitivity to the Urgency Spectrum. As urgency increases to Level 4 and 5, their refusal rates "
  },
  {
    "title": "Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings",
    "url": "https://arxiv.org/abs/2601.18788v1",
    "source": "arxiv",
    "summary": "Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first de",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n2 Related Work\n\n3 Preliminaries and Problem\n\nPenalized segmentation criterion.\n\n\n\n4 KCPD Under mm-Dependence\n\n4.1 Theoretical Results\n\n\n\n5 Embed-KCPD: Instantiation of KCPD for Text Segmentation\n\n5.1 Empirical Evidence of Practical Consistency\n\n\n\n6 Experimental Evaluation\n\n\n6.1 Main Results\n\n6.1.1 Results on Choi‚Äôs Dataset\n6.1.2 Results on Other Benchmarks\n\n\n\n\n7 Case Study\n8 Conclusion\n\nA Proofs\n\nA.1 Auxiliary Results for Lemma 1\nA.2 Proof of Lemma¬†4.9\nA.3 Proof of Proposition¬†4.10\n\nA.4 Proof of Theorem¬†4.11\n\nStep 1: deviation bound for any fixed segmentation.\nStep 2: comparison between the empirical minimizer and a competitor.\n\n\n\nA.5 Additional Results for Theorem 4.12\n\nPart 1: Proof of (12).\nPart 2: Proof of (13).\n\n\nA.6 Proof of Theorem¬†4.12\n\n\nB Computational complexity of KCPD\nC Additional Experimental Results\n\nD Experimental Details\n\nD.1 Statistics of Dataset\nD.2 Implementation details\nD.3 Optimal CC via Elbow Method\nD.4 Sensitivity of CC on PkP_{k} and WD\nD.5 mm-dependent Data Generation\nD.6 arXiv Dataset Generation\n\n\nE Data Disclaimer\n\n\n\n\n\nUnsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings\n\n\nMumin¬†Jia\n\n‚ÄÉ‚ÄÉ\nJairo¬†Diaz-Rodriguez\n\n\n\nAbstract\nUnsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under mm-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift‚Äôs tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.\n\nKernel Change-Point Detection, Text Segmentation, Unsupervised Machine Learning\n\n\n\n1 Introduction\n\nSegmenting a document into coherent topical units is a core subroutine in many NLP and IR systems.\nReliable boundaries improve retrieval, summarization, question answering, discourse analysis, and downstream modeling (Prince and Labadi√©, 2007; Shtekh et al., 2018; Llopis et al., 2002; Cho et al., 2022).\nDespite this importance, text segmentation is often a poor fit for standard supervised learning.\nThe ‚Äúcorrect‚Äù boundary locations depend on the downstream task, the desired granularity, and the annotation protocol, which can vary substantially across corpora.\nLabels are therefore expensive to obtain, difficult to standardize, and may not transfer cleanly across domains.\nThis makes unsupervised segmentation particularly valuable in practice: a method that can be deployed without training labels and remains robust across datasets is often more useful than a narrowly optimized supervised model.\n\n\nChange-point detection (CPD) provides a natural statistical lens for unsupervised segmentation: boundaries correspond to indices where the data-generating distribution changes. Classical offline CPD methods come with strong guarantees, but these often rest on restrictive assumptions such as Gaussianity, independence, or homoscedasticity (Basseville and Nikiforov, 1993; Bai and Perron, 2003; Killick et al., 2012), which can be brittle for high-dimensional text representations. Kernel change-point detection (KCPD) relaxes much of this structure by comparing distributions through RKHS embeddings, enabling detection of rich distributional shifts without explicit density estimation (Harchaoui and Cappe, 2007; Arlot et al., 2019). This makes KCPD a natural fit for embedding-based segmentation, where modern sentence encoders can reveal semantic changes even when lexical cues are weak. At the same time, deploying KCPD in text exposes a key theoretical limitation: most existing analyses assume independent observations (Garreau and Arlot, 2018), while language exhibits ubiquitous short-range dependence because adjacent units share context, discourse structure, and lexical overlap. This gap motivates dependence-aware guarantees tailored to sequential text.\n\n\nThis paper introduces Embed-KCPD, a modular, training-free method for unsupervised text segmentation that combines pretrained sentence embeddings with kernel change-point detection, and provides statistical guarantees for the resulting estimator.\nGiven a sequence of text units X1,‚Ä¶,XTX_{1},\\dots,X_{T}, we compute embeddings Yt=f‚Äã(Xt)‚àà‚ÑùdY_{t}=f(X_{t})\\in\\mathbb{R}^{d} using a fixed encoder ff.\nWe then estimate change points by minimizing a penalized KCPD objective.\nKCPD is attractive for this setting because it detects general distributional changes, not only mean shifts, while remaining nonparametric and compatible with high-dimensional representations (Harchaoui and Cappe, 2007; Arlot et al., 2019).\nMoreover, the penalized objective can be optimized exactly with dynamic programming and efficiently with pruning (PELT), which makes the method practical for long documents (Killick et al., 2012).\nThe resulting pipeline cleanly decouples representation learning from statistical segmentation, so improvements in sentence encoders can be used immediately without retraining the segmenter.\n\n\nBeyond proposing a practical method, our goal is to provide a principled foundation for dependent text sequences.\nTo bridge the gap between i.i.d. theory and sequential language, we develop, to our knowledge, the first guarantees for penalized KCPD under mm-dependence, a tractable abstraction of finite-memory dependence. While natural language is not literally\nmm-dependent, this finite-range model offers a clean first approximation to short-range contextual dependence and enables sharp analysis.\nUnder this dependency assumption, we prove an oracle inequality for the population penalized risk and we establish a localization result showing that true change points are recovered within a window whose size is small relative to the segment lengths, yielding vanishing relative error as TT grows.\n\n\nWe connect these results to practice in two complementary ways.\nFirst, we introduce a controlled simulation framework that uses an LLM to generate synthetic documents with known change points and explicit finite-memory dependence, enabling stress tests that mirror realistic sequential text while retaining ground truth.\nSecond, we provide a systematic empirical study of Embed-KCPD for text segmentation across standard benchmarks and multiple modern encoders.\nAcross datasets, Embed-KCPD is competitive with established unsupervised baselines and often improves standard segmentation metrics.\nA case study on a long-running tweet stream illustrates that the discovered segments align with interpretable thematic phases and can support downstream exploratory analysis.\n\n\nContributions.\n\nOur main contributions are: (i) dependence-aware analysis of penalized KCPD under mm-dependence, including an oracle inequality and a change-point localization guarantee;\n(ii) Embed-KCPD, a simple, modular, training-free pipeline for unsupervised text segmentation that applies offline KCPD to pretrained sentence embeddings;\n(iii) an LLM-based simulation framework for generating short range dependent text with known boundaries, used to validate the predicted scaling behavior; and\n(iv) an extensive empirical evaluation on diverse segmentation benchmarks showing that Embed-KCPD is a strong and practical unsupervised baseline.\n\n\n\n\n\n2 Related Work\n\nChange-point detection methods. Classical algorithms include Binary Segmentation (Scott and Knott, 1974), dynamic programming (Bai and Perron, 2003), and the Pruned Exact Linear Time (PELT) method (Killick et al., 2012), which offer consistency guarantees under parametric cost functions. Nonparametric approaches relax such assumptions using rank or divergence measures (Aminikhanghahi and Cook, 2017), while kernel methods embed data into reproducing kernel Hilbert spaces (Harchaoui et al., 2008). Recent work explores online and streaming algorithms for real-time detection (Ferrari et al., 2023; Hushchyn et al., 2020), ensemble and statistical inference methods for more reliable boundaries (Duy et al., 2020; Shiraishi et al., 2024), deep kernel learning for adaptive representations (Chang et al., 2019), and unsupervised deep frameworks (Truong et al., 2020).\n\n\nTheoretical results on CPD beyond independence. Beyond independence, CPD under dependence has been studied mainly for parametric or low-dimensional settings: CUSUM/MOSUM with mixing and long-run variance or self-normalization (Cs√∂rg√∂ and Horv√°th, 1997; Aue and Horv√°th, 2013; Horv√°th and Rice, 2014), econometric structural-break tests with robust covariances (Andrews, 1993; Bai and Perron, 1998), variance change via ICSS (Incl√°n and Tiao, 1994), and penalized-contrast methods for dependent series (Lavielle and Moulines, 2000; Lavielle, 2005), with extensions to high-dimensional mean shifts (Cho and Fryzlewicz, 2014; Wang and Samworth, 2017). To our knowledge, we provide the first theoretical results for non-parametric kernel CPD under mm-dependence, aligning theory with modern embedding-based text segmentation.\n\n\nText segmentation methods. Early methods like TextTiling (Hearst, 1994) exploit lexical cohesion, while later probabilistic approaches, including pLSA-based segmentation (Brants et al., 2002), dynamic programming over"
  },
  {
    "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System",
    "url": "https://arxiv.org/abs/2601.18785v1",
    "source": "arxiv",
    "summary": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated",
    "full_text": null
  },
  {
    "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic",
    "url": "https://arxiv.org/abs/2601.18783v1",
    "source": "arxiv",
    "summary": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning fr",
    "full_text": null
  },
  {
    "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
    "url": "https://arxiv.org/abs/2601.18779v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classica",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries and Notation\n\n3 Why is Na√Øve Exploration Insufficient on Hard Problems?\n\n3.1 Token-Level Exploration on Hard Problems\n3.2 Ray Interference Inhibits Exploration via Transfer\n\n\n4 POPE: Privileged On-Policy Exploration\n\n5 Why Does POPE Work?\n\n5.1 A Mental Model\n5.2 Empirically Validating the Stitching and Overlap Hypothesis\n\n\n6 Experimental Evaluation\n7 Related Work\n8 Discussion and Perspectives on Future Work\nA Why Does Entropy Increase with a Higher Clip Ratio?\nB Details of Pass@kk Policy Optimization\nC Why does SFT + RL Not Improve Solvability on Hard Problems?\nD Extended Discussion of the Overlap Hypothesis\n\nE Training Hyperparameters\n\nE.1 Hyperparameters for SFT\nE.2 Hyperparameters for RL\n\n\nF Qualitative Example\nG More Examples\n\n\n\n\n\n\n\\correspondingauthor\nyuxiaoq@andrew.cmu.edu, asetlur@andrew.cmu.edu\n\nBlog post: https://tinyurl.com/pope-blog; GitHub: https://github.com/CMU-AIRe/POPE\n\n\nPOPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration\n\n\nYuxiao Qu‚àó\n\nCarnegie Mellon University,    (‚àóEqual Contribution)\n\n\nAmrith Setlur‚àó\n\nCarnegie Mellon University,    (‚àóEqual Contribution)\n\n\nVirginia Smith\n\nCarnegie Mellon University,    (‚àóEqual Contribution)\n\n\nRuslan Salakhutdinov\n\nCarnegie Mellon University,    (‚àóEqual Contribution)\n\n\nAviral Kumar\n\nCarnegie Mellon University,    (‚àóEqual Contribution)\n\n\n\nFigure 1: A schematic of our approach, Privileged On-Policy Exploration (POPE) compared with other approaches for training on hard problems, where standard on-policy RL largely fails to produce successful rollouts. POPE uses oracle (e.g., human-written) solutions to solely guide on-policy exploration during RL, without ever training on the oracle information as targets (for e.g., via supervised fine-tuning or mid-training). We show that modifying standard RL training objectives to incentivize token-level exploration frequently introduce optimization pathologies. Our training approach (POPE) sidesteps such pathologies, while enabling learning from oracle information by ‚Äúinstructing‚Äù the model to build upon it.\n\n\n\\absfont\nAbstract: Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.\n\n\n\n1 Introduction\n\nReinforcement learning (RL) has significantly improved the reasoning abilities of large language models (LLMs) in domains such as math and coding. In particular, relatively small models trained with RL to better exploit test-time compute via longer chains of thought (CoT) can outperform much larger models trained without RL [setlur2025e3learningexploreenables, liu2025prorlprolongedreinforcementlearning]. While some works argue that RL post-training primarily amplifies existing capabilities [yue2025doesreinforcementlearningreally, zhao2025echochamberrlposttraining], others show that careful design choices can mitigate these effects [setlur2025e3learningexploreenables, liu2025prorlprolongedreinforcementlearning].\nAcross various RL recipes, a shared limitation is that on-policy RL fails to train on a large fraction of available problems, leaving substantial gains untapped. On-policy RL often cannot sample any non-zero‚Äìreward rollout on hard problems relative to the base model, yielding no learning signal; for instance, when running Qwen3-4B-Instruct on DAPO-MATH-17K [yu2025dapo], fewer than 50% of problems produce a correct rollout even with K=32K=32 attempts and a 16k token budget. Common throughput-oriented heuristics, such as dynamic sampling and zero-variance filtering, further discard these problems explicitly [yu2025dapo, wang2025reinforcementlearningreasoninglarge, khatri2025art].\n\n\nHow can we make progress on hard problems? In a typical RL framing, this would require improving the ‚Äúexploration‚Äù (i.e., rollout generation) mechanism used during learning. While standard on-policy RL relies on inherent stochasticity of the base model‚Äôs distribution to guide exploration, on hard problems this na√Øve exploration strategy is insufficient. A natural attempt would be to employ token-level exploration bonuses from classical deep RL to incentivize exploration. We empirically analyze two representative methods from this category, and find that neither approach improves ‚Äúsolvability‚Äù (i.e., obtaining at least one correct rollout when sampling multiple) without destabilizing optimization.\n\n\nFigure 2: Interference [schaul2019ray].\nIn on-policy RL, training on a mixture of easy and hard problems preferentially accelerates progress on easy problems, often stalling or degrading performance on hard ones. This imbalance leads to plateaus during training; an ideal approach would induce more ‚Äúuniform‚Äù progress across all problems.\n\n\nAn alternative is to leverage transfer to guide exploration. Useful skills learned on easy problem can then be chained to inform exploration on harder ones. To test whether such transfer enables exploration on hard problems, we train on mixtures of easy and hard problems. Through controlled experiments, we find that even when mixing in the most closely related easy problem, on-policy RL makes slow progress on a hard problem: it first ‚Äúsharpens‚Äù the base model on the easy subset before improving on the hard one. We explain this behavior via ray interference [schaul2019ray] (Figure 2): an implicit bias in on-policy RL towards further optimizing reward on states where reward is already attained rather than finding reward on new states. Consequently, enabling learning on hard problems requires first obtaining non-zero reward by explicitly encouraging exploration some other way.\n\n\nIf the base model cannot sample correct rollouts on a hard problem, (with high enough probability) how can we obtain non-zero reward? A natural approach is to collect ‚Äúexpert‚Äù traces from humans/oracle and either distill them into the base model [sessa2024bondaligningllmsbestofn, agarwal2024onpolicy] or use them in RL as off-policy data [yan2025learningreasonoffpolicyguidance]. However, the type of reasoning traces that LLMs are trained to produce are prohibitively expensive to obtain, and prior work finds limited gains from available human-written data. Empirically, we find that distillation often caps gains from RL and off-policy training destabilizes RL. We therefore seek a more effective source of exploratory signal on hard problems.\n\n\nOur key insight is that oracle solutions can effectively guide an LLM‚Äôs on-policy exploration on hard problems, even when they are ineffective as training targets. Consider a hard problem where the LLM repeatedly follows incorrect approaches and fails within the training budget: conditioning on even a short prefix of a ‚Äúprivileged‚Äù human-written or oracle-provided solution can substantially increase the probability of reaching the correct answer. This effect is particularly pronounced when the base model has strong instruction-following capabilities, allowing us to steer it into building upon privileged content. Privileged On-Policy Exploration (POPE) leverages this principle to guide exploration in RL and this exploration is performed fully on-policy, providing an alternative to distillation or off-policy RL (Figure 1).\n\n\nConcretely, for a set of hard problems, POPE collects a human- or oracle-provided solution and uses a short prefix of this solution as privileged guidance during training. We train the base LLM with RL on a mixture of the original hard prompts and guided variants augmented with this fixed prefix (optionally together with easier prompts). Although these partial solutions are poor training targets, conditioning on them and ‚Äúinstructing‚Äù the policy to utilize them, reliably steers on-policy rollouts into regions where at least one correct attempt can be sampled. Behaviors learned under guidance through RL then transfer back to the original, unguided problems, greatly reducing difficulty of solving the hard problem from scratch. This transfer is enabled by (a) strong instruction-following, which allows the model to build on the prefix despite being unable to generate it itself, and (b) by backtracking and reflection behaviors that revisit and reinterpret the guidance during reasoning.\nWhen viewed through an RL lens, instruction-following and backtracking improves the overlap between the distribution of underlying states with and without any privileged guidance, which in turn enables transfer. Finally, from a classical RL perspective, POPE mirrors a key principle from off-polic"
  },
  {
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "url": "https://arxiv.org/abs/2601.18778v1",
    "source": "arxiv",
    "summary": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-im",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nCurriculum Learning in RL:\nSelf-Play and Teacher-Student Setups:\nIntrinsic Rewards versus Bilevel Optimization\n\n\n\n3 Method\n\n3.1 Overview\n3.2 Outer Loop: Teacher Training\n3.3 Inner Loop: Student Training\n\n\n\n4 Experiment Setup\n\n4.1 Models and Datasets\n4.2 Teacher-student training\n4.3 Evaluation\n4.4 Baselines\n4.5 Metrics\n\n\n\n5 Results\n\n5.1 Meta-RL Discovers Effective Questions.\n5.2 Grounded rewards lead to stable and diverse teacher policies.\n5.3 Question structure matters more than answer correctness.\n\n\n6 Discussion and Conclusions\n\nA Extended Related Work\n\nA.1 Curriculum Learning in RL\nA.2 Self-Play and Teacher-Student Setups\nA.3 Intrinsic Rewards versus Bilevel Optimization\n\n\n\nB Method and Experiment Details\n\n\nB.1 Prompts\n\n\nTeacher Prompt.\n\n\nStudent Prompt.\n\n\nB.2 Parsing Teacher Outputs\n\n\nB.3 Training Details\n\n\nB.4 Learnability Reward.\n\n\nB.5 Datasets\n\n\nB.6 Evaluation\n\n\nB.7 Hyperparameters\n\n\nB.8 Seeds\n\n\nB.9 Computational resources\n\n\nC Evaluations\n\nC.1 Full Student Training curves\nC.2 Full Evaluation on fail@128 MATH, HARP, and OlympiadBench.\nC.3 Sampling from Trained Teachers.\n\nC.4 Correctness of Synthetic Questions\n\n\nD Ablations\n\nD.1 Sampled dataset size\nD.2 Sensitivity to Teacher Hyperaparameters\nD.3 Problem Generation Format.\nE Teacher Training Dynamics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1]MIT\n2]Meta FAIR\n3]New York University\n\\contribution[*]Work done during an internship at Meta\n\nTeaching Models to Teach Themselves:\nReasoning at the Edge of Learnability\n\n\nShobhita Sundaram\n\n‚ÄÉ‚ÄÉ\nJohn Quan\n\n‚ÄÉ‚ÄÉ\nAriel Kwiatkowski\n\n‚ÄÉ‚ÄÉ\nKartik Ahuja\n\n‚ÄÉ‚ÄÉ\nYann Ollivier\n\n‚ÄÉ‚ÄÉ\nJulia Kempe\n\n[\n\n[\n\n[\n\n\n(January 26, 2026)\n\nAbstract\nCan a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum\nfor problems it cannot solve?\nTo explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL.\nA teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems.\nCritically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards.\nOur study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings.\nFirst, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones.\nSecond, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit.\nThird, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.\n\n\n\\correspondence\nShobhita Sundaram at shobhita@mit.edu\n\n\nFigure 1: Learning on hard problems by self-generating a\ncurriculum. We introduce SOAR: A meta-RL framework for\nimproving on difficult datasets where performance plateaus.\n(left) We initialize asymmetric teacher and student models from the same base model. The teacher generates synthetic\nproblems for the student to train on with RL, and is rewarded by the student‚Äôs measurable improvement on a small subset of the real,\nground-truth problems. (right)\nRL training on problems generated with SOAR, using grounded teacher rewards, outperforms direct training on the hard\nproblems and enables the student to break out of the performance\nplateau.\n\n\n\n1 Introduction\n\nReinforcement learning with verifiable rewards (RLVR) has recently\nspurred an impressive rise in LLM reasoning capabilities (deepseek2025r1; kimiteam2025kimi), particularly in mathematics and programming.\nThough effective, this paradigm has a key limitation: the model cannot learn from problems that it cannot\nalready solve to some extent, since\nRLVR uses correct solutions to reinforce useful reasoning traces. When problems are too difficult, sparse or non-existent rewards lead to\nlittle useful training signal, leaving the model ‚Äústuck\".\n\n\nPast work has shown that the order of training data strongly affects\ngeneralization in RL training\n(bengio2009curriculum; Navekar2020curriculum), with success in selecting maximally ‚Äúlearnable\" problems for the current policy, adapting them to learning progress, and using easy-to-hard curricula (parashar2025curriculumreinforcementlearningeasy; chen2025sec).\nSuch curricula can be fragile, however, and require careful design (kordi2025revisiting) as well as curated intermediate datasets; in many settings, the best learnable problems may be unavailable or unknown.\nRecent work addresses sparse rewards by exploiting dense reward signals from test-case pass rates in coding problems (sun2025rlgrokkingrecipe), but still relies on curated test-cases that give intermediate signals. This motivates the need for self-generated curricula.\n\n\nHere, we ask:\nCan a model break its reasoning plateau by generating its own stepping-stone curriculum?\n\n\nWe posit that pretrained LLMs possess the capacity to directly generate a ‚Äústepping stone curriculum‚Äù to tackle hard problems. To investigate if this pedagogical signal is present and extractable, we design SOAR: an asymmetric teacher-student meta-RL framework inspired by self-play (silver2018alphazero; sukhbaatar2017asymmetric; openai2021asymmetricselfplay). Both the teacher and student are initialized from the target model; the\nteacher proposes questions-answer pairs that the student trains on with\nRL. The teacher is rewarded based on student improvement on a difficult subset. Critically, rather than using intrinsic rewards common\nto self-play, we use the difficult training dataset as a black-box grounding reward signal to guide the teacher towards producing useful questions for the student.\n\n\nIntuitively, a pretrained model has already encountered a vast array of easy problems. Consider a difficult calculus question: While the model may be unable to directly generate a correct answer, it might still possess the latent knowledge required to generate easy chain-rule exercises, without requiring a human-in-the-loop to identify and source such questions. We find that by leveraging pretraining knowledge, RL can effectively surface and amplify these latent pedagogical signals to generate useful question-answer pairs. Importantly, we do so without actually showing the model the hard questions; our framework recovers a useful curriculum just by using performance on the hard dataset as a reward signal.\n\n\nEmpirically, while directly training on the hard dataset fails, we find\nthat the teacher in our framework learns to produce useful synthetic questions\nthat can get the student ‚Äúunstuck‚Äù on the hard dataset, without actually seeing the hard problems.\nOur main contributions, supported by an extensive multi-seed empirical study and ablations (over 600 runs), are the following:\n\n\n\n\n‚Ä¢\n\nDecoupled teaching and solving: A model‚Äôs ability to generate effective \"stepping stones\" for hard problems is distinct from its ability to solve them. Self-generated problems expand the learning frontier, enabling progress on hard problems where direct RL training fails. While the base model has the capacity to propose useful questions, meta-RL is essential to sharpen this noisy distribution into a reliable learning signal.\n\n\n\n‚Ä¢\n\nA proof-of-concept of self-generated curricula with SOAR (Self-Optimization via Asymmetric RL), an asymmetric teacher-student framework that rewards the teacher for student progress on hard problems.\nWith Llama-3.2-3B-Instruct, on hard subsets of MATH and HARP, self-generated problems improve performance (e.g., 4√ó\\times pass@1 and 2√ó\\times pass@32 on MATH, 2√ó\\times pass@1 and 1.5√ó\\times pass@32 on HARP). These problems also transfer to unlock learning on hard datasets that they were not optimized for.\n\n\n\n‚Ä¢\n\nGrounded rewards over intrinsic rewards: Grounding teacher rewards in student progress on real problems improves performance over intrinsic rewards common in self-play, which are prone to instability and collapse of question diversity.\n\n\n\n‚Ä¢\n\nQuestion structure over solution correctness: Problem structure and difficulty calibration matter more for escaping plateaus than answer correctness; generated questions provide useful gradient signal even when the majority of answers are incorrect.\n\n\n\n\n\nThese results, backed by a comprehensive empirical study, show that grounded meta-RL can escape genuine learning plateaus by letting models discover for themselves what data they need to learn from to expand their learning frontier.\n\n\n\n\n2 Related Work\n\nFor an extended background and comparison to the literature see AppendixÀúA, summarized here:\n\n\nCurriculum Learning in RL:\n\nAutomated curriculum design has a long history predating modern LLMs (bengio2009curriculum; Graves2017automatedcurriculum; Navekar2020curriculum; parashar2025curriculumreinforcementlearningeasy)\nfocusing on reordering or\nselecting existing data to enable or accelerate learning, or, in the context of RL, to help agents acquire complex behaviors\nby first mastering simpler tasks. For LLM training, curricula are\napplied over curated prompts or problem categories, using proxy signals\nsuch as gradient norms or advantage/difficulty estimates to guide selection  (kimiteam2025kimi; dennis2020paired; wen2025lightr1; yu2025dapo; bae2025onlinedifficultyfilteringreasoning; chen2025sec; jiang2025ADO).\nBy contrast, our goal is not to arrange data but to self-generate tasks to elici"
  },
  {
    "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation",
    "url": "https://arxiv.org/abs/2601.18777v1",
    "source": "arxiv",
    "summary": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered ",
    "full_text": null
  },
  {
    "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
    "url": "https://arxiv.org/abs/2601.18771v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explici",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Agentic Reinforcement Learning\n2.2 Agentic Memory\n\n\n\n3 Methodology\n\n3.1 Problem Overview\n3.2 Data Collection and Design\n3.3 Memory Module\n3.4 RL with Dep-Search\n3.5 Reward Model\n\n\n\n4 Experimental Setup\n\n4.1 Datasets\n4.2 Baselines\n4.3 Models and Metrics\n\n\n\n5 Experiments\n\n5.1 Main Results\n5.2 Ablation Study\n5.3 Reward Function Threshold Analysis\n5.4 Action Usage Analysis\n5.5 Memory Capacity Sensitivity Analysis\n\n\n6 Conclusions\n\nA Experimental Setup Details\n\nA.1 Datasets\nA.2 Baselines\nA.3 Models\nA.4 Retrieval Corpus\nA.5 Implementation Details\n\n\nB Dep-Search Algorithm\nC Decomposition Strategy Analysis\nD Training Prompt Template\nE Case Studies\n\n\n\n\n\nDep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory\n\n\nYanming Liu1 , Xinyue Peng2‚àó, Zixuan Yan1, Yanxin Shen1, Wenjie Xu3, \nYuefeng Huang1, Xinyi Wang, Jiannan Cao4, Jianwei Yin1, Xuhong Zhang1\n1Zhejiang University, 2Intel Corporation, 3Tsinghua University, \n4Massachusetts Institute of Technology \n{oceann24, yanzixuan, ssyysyx, zhangxuhong, zjuyjw}@zju.edu.cn\n{xuwj24}@mails.tsinghua.edu.cn, jiannan@mit.edu, xinyue.peng@intel.com\n\nEqual contribution.Corresponding author.\n\n\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs‚Äô ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases [12, 7, 1]. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies [14]. Recent work such as Search-R1 [8], DeepResearcher [43], Chain-of-Agents [13], and Kimi-K2 [30] have shown that search frameworks can effectively decompose complex questions, retrieve relevant information from multiple sources, and synthesize answers through structured multi-step reasoning [33]. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps [33]. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning [44].\n\n\nThe fundamental problem lies in the lack of explicit dependency modeling and persistent memory management in current search frameworks. Existing approaches decompose questions into sub-questions but fail to explicitly model dependencies between these sub-questions, leading to inefficient search patterns where the same information may be retrieved multiple times or sub-questions are answered out of dependency order [15]. Moreover, existing systems treat each reasoning episode independently, discarding valuable knowledge extracted during search that could be reused across questions or even within the same multi-step reasoning process [38]. This knowledge loss is particularly problematic in complex scenarios where retrieved facts from early steps are needed in later dependent steps, forcing redundant searches and increasing computational costs [16]. Additionally, training search-based LLMs to learn optimal search strategies remains challenging, as existing reinforcement learning approaches struggle with the sparse reward signals and the need to jointly optimize decomposition, retrieval, memory access, and reasoning behaviors [2].\n\n\nTo address these limitations, we propose Dep-Search, a dependency aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. By combining dependency aware question decomposition with a persistent memory system and GRPO for trajectory-level learning, Dep-Search ensures that reasoning follows explicit dependency structures, retrieved knowledge is efficiently stored and reused, and the policy learns to optimize the entire search-reasoning-memory pipeline jointly. Unlike existing search frameworks that rely on heuristic search strategies, Dep-Search treats all tokens uniformly in the policy, enabling end-to-end learning of when to decompose, what to retrieve, when to access memory, and how to synthesize final answers, while the explicit memory state provides verifiable knowledge accumulation throughout the reasoning process.\n\n\nOur Contributions. Our contributions are detailed as follows.\n\n\n‚Ä¢\n\nWe present Dep-Search, a novel framework that formalizes multi-hop reasoning through dependency aware decomposition and explicit control tokens, providing structured reasoning traces and efficient knowledge reuse.\n\n\n\n‚Ä¢\n\nWe introduce a persistent memory system that automatically stores summarized facts from searches and enables efficient memory access through embedding-based similarity search, addressing the knowledge loss problem in existing search frameworks.\n\n\n\n‚Ä¢\n\nWe demonstrate that QDMR-based decomposition enables adaptive dependency modeling that significantly outperforms sequential decomposition approaches, allowing the model to determine both the number of reasoning steps and their dependency structure dynamically.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Agentic Reinforcement Learning\n\nRecent advances in agentic reinforcement learning (RL) have explored how LLM-based agents can interact with environments, tools, and external knowledge sources to solve complex tasks through trial-and-error learning [24, 23, 9]. Early work focused on using RL to fine-tune language models on synthetic reasoning tasks or instruction-following benchmarks, typically with short-horizon rewards and limited interaction structure [18, 26]. Recent frameworks introduce multi-step decision processes in which the agent can iteratively call tools, plan, and revise its strategy [13, 30]. These systems demonstrate that explicit interaction loops and environment feedback can significantly improve the robustness and adaptability of LLMs on complex tasks such as web navigation, code generation, and multi-hop question answering [40, 4]. A common theme across these approaches is the use of policy optimization balance exploration and exploitation, such as entropy-balanced objectives that encourage diverse exploration while maintaining exploitation of promising strategies [2], and experience replay mechanisms that enable agents to learn from past trajectories more effectively [16]. The emphasis on trajectory-level learning, where agents learn to optimize sequences of actions rather than individual decisions, enabling better credit assignment and long-term planning in complex multi-step reasoning scenarios.\n\n\n\n\n2.2 Agentic Memory\n\nA growing line of work studies how LLM agents can maintain and exploit persistent memory to improve long-term coherence, personalization, and knowledge reuse [19, 37, 21]. Early memory-augmented systems typically log past interactions or retrieved documents in a buffer and naively prepend them to the prompt, which quickly becomes inefficient and noisy as the context grows [41, 3]. Subsequent approaches introduce memory retrieval modules based on dense embeddings, enabling agents to select relevant past experiences or facts conditioned on the current query [34, 29]. Recent agentic memory frameworks go further by allowing agents to write structured summaries into memory, compressing long trajectories into reusable high-level knowledge that can be recalled [35, 6, 36]. A common evolution across these approaches is the shift from passive memory storage to active memory management, where agents not only retrieve but also strategically write and organize memory content to optimize knowledge reuse across different reasoning episodes.\n\n\n\n\n\n3 Methodology\n\n\n3.1 Problem Overview\n\nLet the problem distribution be ùíü\\mathcal{D}, where each instance is a natural-language question Q‚àºùíüQ\\sim\\mathcal{D}. Our goal is to generate an answer AA through a dependency aware search process by maximizing the expected trajectory return:\n\n\n\n"
  },
  {
    "title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting",
    "url": "https://arxiv.org/abs/2601.18766v1",
    "source": "arxiv",
    "summary": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizin",
    "full_text": null
  },
  {
    "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values",
    "url": "https://arxiv.org/abs/2601.18760v1",
    "source": "arxiv",
    "summary": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose G",
    "full_text": "\n\n\n\n1 Introduction\n2 Grounding in Human Reasons and Values: Contextual and General Principles\n\n3 Related Work\n\n3.1 Alignment Techniques\n3.2 Alignment Goals and Learning from Stakeholder Input\n3.3 Evaluating Alignment Principles\n\n\n\n4 Elicitation Process\n\n4.1 Candidate Generation\n4.2 Clustering\n4.3 Summarization\n4.4 Scoring and Selection\n\n\n\n5 Evaluation\n\n5.1 Replication of Baseline ICAI\n5.2 Evaluative Framework\n5.3 Survey Methodology\n5.4 Aligned Model Evaluation\n5.5 Qualitative Data Analysis\n\n\n\n6 Constitution Comparison\n\n6.1 Participants Preferred GCAI on All Dimensions:\n6.2 Factuality Informs Participants‚Äô Preferences:\n6.3 Participants Prefer GCAI for its Focus on Safety, Ethics and Fairness:\n6.4 Participants who Preferred ICAI Focused on Direct Responses and Clarity of the Constitution:\n6.5 Participant Preferences Match Dataset Characteristics\n6.6 Participants Expressed Little Preference when Viewing Principles Individually\n\n\n\n7 Model Comparison\n\n\n7.1 Benchmark Evaluation\n\n7.1.1 MMLU\n7.1.2 BBQ\n\n\n7.2 Qualitative Results\n\n\n\n8 Discussion\n\n8.1 Constitutions Are More Than The Sum of Their Parts\n8.2 Constitutions from GCAI Are Only Candidates and Should Be Discussed and Ratified\n8.3 Justifications Increase Transparency and Faithfulness\n\n\n\n9 End Matter\n\n9.1 Generative AI Usage Statement\n\n\nA Constitutions\n\nB Alignment Procedure\n\n\nB.1 Supervised Learning Dataset Construction\n\nB.1.1 Initial Answer Template\nB.1.2 Critique Template\nB.1.3 Revision Template\n\n\n\nB.2 Generating AI Preference Data\n\nB.2.1 Preference Data Template\n\n\nB.3 Direct Preference Optimization\n\n\n\nC Prompt templates\n\nC.1 Candidate generation prompt\nC.2 PRISM constitution decomposition prompt\nC.3 Summarization prompt\nC.4 Contextual scoring prompt template\n\n\nD Keyword Dictionaries for Frequency Analysis\nE Principle-level Scores for Each Evaluative Dimension\n\nF Survey for Human Evaluation\n\nF.1 Compensation\nF.2 Disclosure\nF.3 Consent\nF.4 Randomization\nF.5 Constitutional Evaluation\nF.6 Principle Evaluation\n\n\nG Qualitative Examples of Tagged Generated Responses\n\n\n\n\n\nBeyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values\n\n\nHenry Bell\n\nhenry.bell@duke.edu\n\n, \nLara Neubauer da Costa Schertel\n\nlara.neubauerdacostaschertel@duke.edu\n\n, \nBochu Ding\n\nbochu.ding@duke.edu\n\n and \nBrandon Fain\n\nbtfain@cs.duke.edu\n\nDuke UniversityDurhamNorth CarolinaUSA\n\n\n\nA crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users‚Äô general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided reasons for their preferences. We supplement these contextual principles with general principles surfaced from user statements of values regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.\n\n\n\n1. Introduction\n\nAligning Large Language Models (LLMs) to human values has emerged as a critical challenge of modern ethical AI¬†(Amodei et al., 2016; Gabriel, 2020; Christian, 2021; Askell et al., 2021; Ji et al., 2025). Constitutional alignment techniques (Bai et al., 2022b; Huang et al., 2024; Fr√§nken et al., 2024; Findeis et al., 2025; Kyrychenko et al., 2025) align large language models (LLMs) to constitutions of principles specified in natural language (such as ‚Äúrespect human autonomy‚Äù or ‚Äúdo not use offensive language‚Äù). A natural question is which principles should we use? There is disagreement over values for alignment in a pluralistic society where LLMs are deployed in diverse contexts¬†(Wu et al., 2023; Sorensen et al., 2024a; Gordon et al., 2022; Sorensen et al., 2024b; Conitzer et al., 2024). Rather than argue for a particular set of principles, we consider the question of a fair process for eliciting principles¬†(Gabriel and Keeling, 2025) from stakeholder input¬†(Freeman, 2010; Groves et al., 2023; Suresh et al., 2024; Huang et al., 2024). Deliberation¬†(Huang et al., 2024) and social choice¬†(Conitzer et al., 2024) have been proposed as possibilities for determining constitutions, but both methods require ways to elicit initial candidate principles‚Äìwhether for seeding deliberation, voting, or both.\n\n\nA critical challenge is scalability: How can we incorporate the input of potentially millions of stakeholders and distill their perspectives down to a reasonable number of candidate principles? Individuals may express thousands of diverse views, general or specific, redundant, conflicting, or overlapping. To address this need, we propose GCAI: Grounded Constitutional AI, a unified learning framework to elicit candidate alignment principles from stakeholder input. By learning we do not mean data collection. Rather, GCAI is a scalable unsupervised machine learning framework that can synthesize large volumes of human preference annotations with reasons and general human descriptions of AI alignment values into a compact set of representative candidate principles (a constitution).\n\n\nPrior work has similarly attempted to learn principles purely from human preference annotations, a method coined as Inverse Constitutional AI (ICAI)¬†(Findeis et al., 2025). We surface the need of further grounding principle learning using: (i) reasons (or justifications) for human preference annotations, written in natural language, and (ii) general values humans report for LLM alignment, also written in natural language. At a high level, GCAI works in four main steps, the details of which vary slightly between (i) and (ii) and are specified in Section¬†4 with full prompts available in the Appendix.\n\n\n(1)\n\nCandidate Generation. Using an LLM, we automatically rephrase individual reasons for preference annotations or general statements of values into one or more correctly formatted and generally applicable principles. This step is necessary as many human-written reasons or values are too narrow and specific (e.g., referring to a particular prompt), involve many semantically distinct concepts, or are simply written informally and idiomatically.\n\n\n\n(2)\n\nClustering. We embed each candidate principle as a numerical vector using a sequence embedding model. We then cluster the resulting embedded candidates. Clustering serves two purposes: Deduplication of many repeated candidates and identification of broadly relevant themes.\n\n\n\n(3)\n\nSummarization. Using an LLM, we automatically summarize the most central candidates within each cluster (which often include somewhat different wordings of similar concepts) as new candidate principles.\n\n\n\n(4)\n\nScoring and Selection. The reduced set of summarized candidates are scored, either by their ability to predict relevant preferences (for preference annotations with reasons) or by their centrality within the embedding space as a soft proxy for consensus (for general statements of values). The top scoring candidates (up to the desired number) are selected.\n\n\n\n\n\nOutline and Contributions\n\n\n(1)\n\nSection¬†2 characterizes two kinds of principles: contextual and general. We argue that contextual principles should be grounded in reasons or justifications, and that general principles are also necessary to faithfully capture relevant value and concerns about alignment.\n\n\n\n(2)\n\nSection¬†3 surveys related work on alignment, principle elicitation, and evaluation criteria for constitutions.\n\n\n\n(3)\n\nSection¬†4 presents Grounded Constitutional AI (GCAI), the first unified framework for eliciting a constitution grounded in contextual and general human reasons and values written in natural language.\n\n\n\n(4)\n\nSection¬†5 discusses our evaluation methodology, including a discussion of the novel evaluation criteria we propose for constitutions.\n\n\n\n(5)\n\nSection¬†6 presents our main empirical results evaluating a constitution learned by GCAI compared to ICAI. Human survey participants overwhelmingly prefer the GCAI constitution across multiple evaluation dimensions, and we investigate the reasons for these preferences.\n\n\n\n(6)\n\nSection¬†7 presents qualitative and benchmark results of a model that we align to our the constitution learned by GCAI in comparison to that learned by ICAI. While the constitutions make little difference in benchmark performance, qualitatively we find that there are consistent thematic differences between the resulting models, with the GCAI-aligned model showing much more emphasis on ethics, safety, and non-discrimination.\n\n\n\n(7)\n\nSection¬†8 concludes with reflections on constitutional elicitation broadly based on our findings.\n\n\n\n\n\n\n\n2. Grounding in Human Reasons and Values: Contextual and General Principles\n\nWe distinguish between two kinds of alignment principles the differ in their origin:\n\n\n‚Ä¢\n\nContextual principles arise from actual interaction experience with models. They may depend heavily on the prompt distribution, pretraining, and fine-tuning of the particular model with which a human interacts.\n\n\n\n‚Ä¢\n\nGeneral principles arise from anticipated concerns about hypothetical or possible model behavior. They represent prior commitments and do not depend on particular observed behaviors of a given LLM but rather on the broad values and concerns about possible LLM use.\n\n\n\n\n\nInverse Constitutional AI (ICAI)¬†(Findeis et al., 2025) tries to learn constitutional principles "
  },
  {
    "title": "$Œ±^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks",
    "url": "https://arxiv.org/abs/2601.18754v1",
    "source": "arxiv",
    "summary": "Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditi",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-A Benchmarks for LLM-Based UAV Autonomy and Reasoning\nII-B Vision-and-Language Navigation and Semantic Goal-Driven Flight\nII-C Visual Grounding and Spatial Intelligence for UAVs\nII-D Collaborative Perception and Multi-UAV Intelligence\nII-E Network-Aware and Resource-Efficient UAV Intelligence\nII-F Comparison with Prior Benchmarks and Our Positioning\n\n\n\nIII Benchmark Design and Evaluation Methodology\n\n\nIII-A Threat Model and Attack Taxonomy\n\nIII-A1 Episode-Centric Evaluation Model\nIII-A2 Security Alert Semantics\nIII-A3 Attacker Model\nIII-A4 Security Overlays and Attack Events\nIII-A5 Observation-Level Attack Injection\nIII-A6 Threat Realism &amp; Attack Validity\n\nIII-A7 Layered Attack Taxonomy\n\nSensors Layer.\nPerception Layer.\nPlanning Layer.\nControl Layer.\nNetwork and 6G Control Plane Layer.\nEdge and Cloud Layer.\nLLM Agent Layer.\n\n\nIII-A8 CWE Annotation and Hierarchy Awareness\n\n\n\nIII-B Agent‚ÄìEnvironment Interaction and Overlay Generation\n\nIII-B1 Agent Interaction Protocol\nIII-B2 Overlay Generation Procedure\n\n\n\nIII-C Runtime Overlay Injection and Agent Evaluation\n\nIII-C1 Episode Context and Tool Surface Extraction\nIII-C2 Base Observation Construction\nIII-C3 Overlay-Driven Symptom Injection\nIII-C4 Constrained Agent Prompting and Safe-Degradation Policy\nIII-C5 Hierarchy-Aware CWE Matching via an LLM Judge\n\n\n\nIII-D Scoring, Timeliness, and Trust Metrics\n\nIII-D1 Mandatory Constraints and Evaluation Windows\nIII-D2 Security: Detection and CWE Attribution\nIII-D3 Resilience: Safe-Degradation Behavior\nIII-D4 Trust: Reliable Tool Use and Policy Compliance\nIII-D5 Overall Score and Outcome Labels\n\n\n\n\n\nIV Performance Evaluation\n\nIV-A Evaluation Setup and Overlay Generation\nIV-B Agent Evaluation Protocol\nIV-C Scoring and Metrics\nIV-D Evaluation Scale, Runtime, and Reproducibility\nIV-E Overall Leaderboard Analysis\nIV-F Security, Resilience, and Trust Decomposition\nIV-G Score Robustness Under Alternative Weight Configurations\nIV-H Vulnerability-Centric Analysis by CWE\nIV-I Scope and Limitations of the Benchmark\n\n\nV Conclusion\n\n\n\n\n\n\nŒ±3\\alpha^{3}-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks\n\n\n\nMohamed¬†Amine¬†Ferrag‚àó¬ß,¬†,\nAbderrahmane¬†Lakas‚àó,¬†,\nand¬†Merouane¬†Debbah1\n‚àóDepartment of Computer and Network Engineering, College of Information Technology, United Arab Emirates University, Al Ain, United Arab Emirates.1Khalifa University of Science and Technology, Abu Dhabi, United Arab Emirates.¬ßCorresponding author: mohamed.ferrag@uaeu.ac.ae\n\n\nAbstract\nAutonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical and networked environments, where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have advanced the evaluation of large language model (LLM)-based UAV agents in terms of reasoning capability, navigation performance, and network efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.\nIn this paper, we introduce Œ±3\\alpha^{3}-SecBench, the first large-scale evaluation suite designed specifically to assess the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building upon multi-turn conversational UAV missions from Œ±3\\alpha^{3}-Bench, the proposed framework augments benign autonomy episodes with 20,000 validated security overlay attack scenarios that inject controlled attacks targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. Œ±3\\alpha^{3}-SecBench evaluates agents along three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (reliable, policy-compliant tool usage). We conduct a comprehensive evaluation of 23 state-of-the-art LLMs, including models from major industrial providers (e.g., Google, OpenAI, Microsoft, Amazon, Meta, IBM, Tencent, Alibaba) and leading AI-first labs (e.g., Anthropic, Mistral AI, xAI, DeepSeek, Zhipu AI, Moonshot AI, Liquid AI, AllenAI), using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions and spanning 175 distinct threat types.\nAlthough many models reliably recognize anomalous behavior and raise timely alerts, effective mitigation and trustworthy control actions remain inconsistent, and vulnerability attribution accuracy is significantly lower than detection performance, highlighting a fundamental gap between anomaly detection and security-aware autonomous decision-making in current LLM-based UAV agents. The results reveal substantial performance variation, with normalized overall scores ranging from 12.9% to 57.1%.\nTo support open science and reproducibility, we release the Œ±3\\alpha^{3}-SecBench dataset on GitHub: https://github.com/maferrag/AlphaSecBench.\n\n\nIndex Terms: \nSecurity, Large Language Models, Conversational Reasoning, Autonomous Systems, 6G Networks, AI Agents.\n\n\n\n\nI Introduction\n\n\nAutonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical and networked applications such as disaster response, infrastructure inspection, surveillance, industrial automation, and low-altitude economy services. Recent advances in large language models (LLMs) have significantly expanded UAV autonomy by enabling high-level reasoning, natural-language interaction, and adaptive decision making. Prior studies have demonstrated the effectiveness of LLMs for context-aware landing [3], end-to-end mission interpretation from natural-language instructions [24], and adaptive path planning in dynamic and industrial environments [22]. More recent work has extended LLM-driven reasoning to multi-UAV swarms, enabling semantic communication, role-adaptive coordination, and interpretable consensus formation beyond traditional numerical message passing [20]. Together with emerging 6G networks characterized by ultra-reliable low-latency communication, AI-native control planes, and edge-assisted orchestration‚Äîthese developments promise unprecedented levels of autonomy, scalability, and cooperative intelligence [28].\n\n\nIn embodied systems such as autonomous UAVs, LLMs act as cognitive controllers that translate human intent into policy-compliant actions, dynamically adapt to network conditions, and coordinate tool-mediated control under partial observability. By integrating LLM-driven reasoning with 6G AI-native control planes, next-generation networks can achieve flexible, context-aware, and resilient autonomy while enabling systematic evaluation of security, trust, and safe degradation under adversarial conditions [5, 15, 17]. Therefore, despite this progress, integrating LLMs into UAV autonomy introduces new security, resilience, and trust challenges that are not adequately captured by existing evaluation practices. Most prior work emphasizes task performance, convergence speed, or coordination efficiency under benign conditions, while systematic assessment of agent behavior under active adversarial interference remains limited. In realistic deployments, attackers may exploit vulnerabilities across multiple layers of the autonomy stack, including sensors, perception pipelines, planning logic, control loops, communication channels, and the reasoning interfaces of LLM-based agents [18].\n\n\nRecent IETF Internet-Drafts have begun to formalize the role of AI agents in future 6G networks. The work in [23] focuses on identifying representative AI-agent use cases and deriving high-level requirements from an operator‚Äôs perspective, highlighting how autonomous agents can support network optimization, management, and service provisioning. Complementarily, [19] emphasizes architectural and functional requirements for deploying AI agents within the 6G IP layer, discussing implementation approaches and integration challenges. Together, these drafts provide a coherent early view of both the use-case motivation and the technical requirements for AI-agent-enabled 6G systems [4, 12].\n Therefore, the challenge is further intensified by the growing reliance on distributed learning and communication mechanisms in multi-UAV systems over 6G networks. Federated LLM fine-tuning has emerged as a promising approach to enable collective intelligence while preserving data privacy and robustness against node dropouts in low-altitude UAV swarms [1]. At the same time, communication efficiency and adaptability have become critical bottlenecks in dense urban environments, where limited bandwidth and partial observability constrain coordination. Recent work has shown that LLMs can dynamically reason about environmental and system states to trigger communication events only when mission-critical information arises, significantly reducing communication overhead while improving collaborative control [9]. Although these advances strengthen learning and communication infrastructures, they do not directly address how autonomous UAV agents behave when adversarial manipulation targets observations, communication timing, or decision logic itself.\n\n\nDespite the breadth of existing benchmarks and evaluation frameworks, none of the aforementioned works explicitly target the security, adversarial robustness, and trustworthiness of LLM- or MLLM-driven UAV agents. Current benchmarks primarily evaluate reasoning [7, 8, 6], navigation and goal-directed autonomy [2, 21], spatial perception and grounding [27, 26], collaborative perception [25], embodied decision-making [10], or network-aware efficiency [11] under implicitly benign operating conditions. Even benchmarks that incorporate realistic communication constraints or degraded perception focus on performance degradation rather than adversarial intent, attack detection, or security-aware response behavior. As a result, critical questions regarding "
  },
  {
    "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
    "url": "https://arxiv.org/abs/2601.18753v1",
    "source": "arxiv",
    "summary": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their genera",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\nHallucination Detection.\nNTK in LLMs.\n\n\n\n3 Methodology\n\n3.1 Problem Setting\n\n3.2 Hallucination Risk Bound\n\nCharacterizing Data-Driven Hallucination.\nCharacterizing Reasoning-Driven Hallucination.\n\n\n\n3.3 Hallucination Quantification via HalluGuard\n\nEmpirical validation.\n\n\n\n\n\n4 Experiments\n\n4.1 Evaluation Setup\n4.2 Main Results\n4.3 Test-Time Inference\n4.4 Case Study\n\n\n5 Related Work\n6 Conclusion\n\nA Proof of Hallucination Risk Bound\n\n\nA.1 Assumptions Validation\n\nAssumption A1 (Hilbert/RKHS structure with bounded second moment).\nAssumption A2 (Local Lipschitz continuity of the encoder Œ¶\\Phi).\nAssumption A3 (Local smoothness / second-order expansion).\n\n\n\nA.2 Bound Proof\n\nStep 1: Triangle inequality split.\nStep 2: Approximation term via C√©a‚Äôs lemma.\nStep 3: Variance term via Bernstein concentration.\nStep 4: Substitution.\n\n\n\nA.3 Decomposition of NTK Continuity Constant\n\nBounding Œîpt\\Delta_{\\mathrm{pt}}.\nBounding Œîmm\\Delta_{\\mathrm{mm}}.\n\n\n\n\n\nB HalluGuard Derivation and Interpretation\n\nB.1 Preliminaries and Notation\n\nB.2 Representational Adequacy via det(ùí¶)\\det(\\mathcal{K}) with Explicit Constants\n\nAssumptions for this subsection.\nNumerical note (stable surrogate).\n\n\n\nB.3 Rollout Amplification via Jacobian Products (Exact Constants)\n\nToken-dependent refinement.\n\n\n\nB.4 Conditioning-Induced Variance with Œ∫‚Äã(ùí¶)2\\kappa(\\mathcal{K})^{2} Scaling\n\nSetup.\nInterpretation.\n\n\nB.5 Consolidation: Compact Surrogate Consistent with the Risk Decomposition\n\n\n\nC Experiment\n\n\nC.1 Setup\n\nImplementation Framework.\nGeneration Configuration.\nNTK-Based Score Computation.\nPerturbation Regularization.\nOptimization.\nImplementation Details.\nAblation Setup.\n\n\nC.2 Ablation Study on ‚àílog‚Å°Œ∫2-\\log\\kappa^{2}\nC.3 Computational Efficiency Analysis\nC.4 Detection Performance Analysis\n\nC.5 Tightness of Bound\n\nEvaluation of bound tightness.\nEvaluation of NTK proxy tightness.\nValidation of Term Decomposition\n\n\nC.6 Correlation of reasoning-driven and data-driven terms with different types of datasets\n\nC.7 Case Study\n\nCase Study 1 ‚Äî GSM8K (Multi-step Arithmetic): Bias ‚Üí\\rightarrow Drift ‚Üí\\rightarrow Snowballing.\nCase Study 2 ‚Äî Long-Document Summarization: Misalignment ‚Üí\\rightarrow Overreach ‚Üí\\rightarrow Fabrication.\n\n\n\n\nD Usage of LLM\n\n\n\n\n\nHalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs\n\n\nXinyue Zeng\nVirginia Tech\nCS Department\n&amp;Junhong Lin11footnotemark: 1\nMIT\nEECS Department\n&amp;Yujun Yan \nDartmouth College\nCS Department\n&amp;Feng Guo \nVirginia Tech \nStatistics Department\n&amp;Liang Shi \nVirginia Tech \nStatistics Department\n&amp;Jun Wu \nMichigan State University \nCS Department\n&amp;Dawei Zhou \nVirginia Tech\nCS Department\nEqual contribution.\n\n\nAbstract\nThe reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios.\nTo overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve.\nBuilding on this foundation, we introduce HalluGuard, a NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations.\nWe evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are increasingly deployed in high-stakes domains such as healthcare, law, and scientific discovery(Bommasani et al., 2021; Thirunavukarasu et al., 2023).\nHowever, adoption in these settings remains cautious, as such domains are highly regulated and demand strict compliance, interpretability, and safety guarantees(Dennst√§dt et al., 2025; Kattnig et al., 2024).\nA major barrier is the risk of hallucinations, generated content appears unfaithful or nonsensical. Such errors can have severe consequences(Dennst√§dt et al., 2025)‚Äîas the example in Figure¬†1, a generated incorrect medical diagnosis may delay treatment or lead to harmful interventions. Therefore, detecting hallucinations is not merely a technical challenge but a prerequisite for trustworthy deployment, as undetected errors undermine reliability, accountability, and user safety.\n\n\nGenerally, hallucinations in LLMs arise from two primary sources(Ji et al., 2023; Huang et al., 2023): data-driven hallucinations, which stem from flawed, biased, or incomplete knowledge encoded during pre-training or fine-tuning; and reasoning-driven hallucinations, which originate from inference-time failures such as logical inconsistencies or breakdowns in multi-step reasoning(Zhang et al., 2023; Zhong et al., 2024).\nDetection methods broadly split along these two dimensions. Approaches for data-driven hallucinations often compare outputs against retrieved documents or references(Shuster et al., 2021; Min et al., 2023; Ji et al., 2023), or exploit sampling consistency as in SelfCheckGPT(Manakul et al., 2023).\nIn contrast, methods for reasoning-driven hallucinations rely on signals of inference-time instability, including probabilistic measures such as perplexity(Ren et al., 2022), length-normalized entropy(Malinin and Gales, 2020), semantic entropy(Kuhn et al., 2023), energy-based scoring(Liu et al., 2020), and RACE(Wang et al., 2025). Others probe internal representations, for example, Inside(Chen et al., 2024a), which applies eigenvalue-based covariance metrics and feature clipping, ICR Probe(Zhang et al., 2025), which tracks residual-stream updates, and Shadows in the Attention(Wei et al., 2025), which analyzes representation drift under contextual perturbations.\nWhile these methods shed light on the mechanisms underlying hallucinations, most remain tailored to a single hallucination type and fail to capture their evolution. Yet growing evidence indicates that data-driven and reasoning-driven hallucinations often evolve during multi-step generation(Liu et al., 2025; Sun et al., 2025).\nAs shown in Figure¬†1, it emerges from an initial disease misclassification and evolves into a distorted diagnosis, delaying treatments and risking fatality.\nThis gap brings two central questions: (1) How can we develop a unified theoretical understanding of how hallucinations evolve? and (2) How can we detect them effectively and efficiently without relying on external references or task-specific heuristics?\n\n\nTo address these challenges, we propose a unified theoretical framework‚ÄìHallucination Risk Bound, which decomposes the overall hallucination risk into two components: a data-driven term, capturing semantic deviations rooted in inaccurate, imbalanced, or noisy supervision acquired during model training; and a reasoning-driven term, reflecting instability introduced by inference-time dynamics, such as logical missteps or temporal inconsistency. This decomposition not only elucidates the mechanism behind hallucinations but also reveals how they emerge and evolve.\nSpecifically, our analysis shows that hallucinations originate from semantic approximation gaps-captured by representational limits of the model-and are subsequently amplified by unstable rollout dynamics, evolving across decoding steps.\nAs such, our framework offers a unified theoretical lens for characterizing the emergence and evolution of these hallucinations.\n\n\n\nFigure 1: An illustration of hallucination emerging and evolving in the context of disease diagnosis.\n\n\n\nBuilding on the theoretical foundation, we propose HalluGuard, a\nNeural Tangent Kernel(NTK)-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard comprehensively across 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones. HalluGuard consistently achieves state-of-the-art hallucination detection performance, demonstrating its efficacy.\n\n\n\n\n2 Preliminaries\n\nHallucination Detection. \n\nThere are two primary sources of hallucinations in LLMs(Ji et al., 2023; Huang et al., 2023): data-driven hallucination, which stems from incomplete or biased knowledge encoded during pre-training or fine-tuning, and reasoning-driven hallucination, which arises from unstable or inconsistent inference dynamics at decoding time. This distinction has implicitly guided a broad range of detection strategies, which we examine through these two lenses.\n\n\nFor data-driven causes, a recurring signal is elevated predictive uncertainty. A common formulation adopts the sequence-level negative log-likelihood:\n\n\n\nùí∞‚Äã(ùê≤‚à£ùê±,Œ∏)=‚àí1T‚Äã‚àët=1Tlog‚Å°pŒ∏‚Äã(yt‚à£y&lt;t,ùê±),\\mathcal{U}(\\mathbf{y}\\mid\\mathbf{x},\\theta)=-\\frac{1}{T}\\sum_{t=1}^{T}\\log p_{\\theta}(y_{t}\\mid y_{&lt;t},\\mathbf{x}),\n\n(1)\n\n\n\n\nwhich quantifies the average uncertainty of generating a sequence ùê≤=[y1,‚Ä¶,yT]\\mathbf{y}=[y_{1},\\ldots,y_{T}] from input ùê±\\mathbf{x} and Œ∏\\theta denotes model parameters. This directly recovers Perplexity(Ren et al., 2022), where low scores imply confident predictions, while high scores indicate implausible generations due to weak priors.\nTo capture more nuanced uncertainty, later methods extend this formulation to multi-sample settings. The Length-Normalized Entropy(Malinin and Gales, 2020) penalizes dispersion across stochastic generations ùí¥={ùê≤1,‚Ä¶,ùê≤K}\\mathcal{Y}=\\{\\mathbf{y}^{1},\\ldots,\\mathbf{y}^{K}\\}, offering a finer-grained view of model indecision. Thi"
  },
  {
    "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback",
    "url": "https://arxiv.org/abs/2601.18751v1",
    "source": "arxiv",
    "summary": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to fil",
    "full_text": null
  },
  {
    "title": "Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval",
    "url": "https://arxiv.org/abs/2601.18747v1",
    "source": "arxiv",
    "summary": "Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic grap",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 The Computational Burden on LLMs\n1.2 The Computational Gap\n1.3 Contributions\n\n\n\n2 Background and Related Work\n\n2.1 Term-at-a-Time (TAAT)\n2.2 Document-at-a-Time (DAAT)\n2.3 Embedding-based Retrieval\n2.4 The Gap: Neuro-Symbolic Logic\n\n\n\n3 The Retrieval Language ‚ÑíR\\mathcal{L}_{R}\n\n3.1 Syntax and Semantics\n3.2 Expressive Power (Lower Bound)\n3.3 Expressive Power (Upper Bound)\n3.4 From Theory to Practice\n\n\n\n4 The ComputePN Algorithm\n\n4.1 The Positive-Negative Response\n\n4.2 The Evaluation Algebra\n\n4.2.1 Negation (¬¨\\neg)\n4.2.2 Conjunction (‚àß\\land)\n4.2.3 Disjunction (‚à®\\lor)\n\n\n4.3 Execution Strategy\n4.4 Complexity Analysis (Sparsity Guarantee)\n4.5 DAG Optimization and Rewriting\n\n\n\n5 Theoretical Limits and Analytical Evaluation\n\n5.1 The Tree-Expansion Bottleneck\n5.2 The Universal Scan Penalty\n5.3 Empirical Verification: The Expressiveness\n5.4 The Linear Separability Limit\n\n\n\n6 A Computational Retrieval Architecture\n\n6.1 The New Modality: Context-Aware Precision\n6.2 Case Study: Academic Search\n6.3 Theoretical Implications: The Index as a CPU\n6.4 Integration with Ranking Pipelines\n\n\n7 Conclusion\n\n\n\n\n\nCapturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval\n\n\nAmir Aavani\n\naaavani@apple.com\n\nApple Inc.CupertinoCaliforniaUSA\n\n\n\nAbstract.\nModern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.\nIn this paper, we propose that a retrieval engine must be capable of ‚ÄúCapturing ùêè\\mathbf{P}‚Äù‚Äîevaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language (‚ÑíR\\mathcal{L}_{R}) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class ùêè\\mathbf{P}. We introduce ComputePN, a novel evaluation algorithm that makes ‚ÑíR\\mathcal{L}_{R} tractable. By combining native DAG traversal with a memory-efficient ‚ÄúPositive-Negative‚Äù response mechanism, ComputePN ensures the efficient evaluation of any query in ‚ÑíR\\mathcal{L}_{R}. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.\n\nBoolean Query, Expressive Power, Descriptive Complexity, Retrieval Algorithm, Query Negation, P-Completeness, Computational Search\n\n\n\n1. Introduction\n\nA traditional keyword-based search engine operates as a multi-stage refinement pipeline. First, it compiles a set of candidate documents in response to a user query, typically interpreted as a conjunction of disjunctions (e.g., Boolean AND over OR clauses). This initial collection undergoes refinement: irrelevant documents are filtered out, and the remaining candidates are evaluated using various ranking signals. These signals, such as query-document relevance, document quality, and freshness, contribute to a comprehensive score for each document. Finally, the highest-scoring documents proceed to a re-ranking phase, where processes like deduplication and other domain-specific logic are applied to finalize the results presented to the user¬†(Manning et al., 2008; Baeza-Yates and Ribeiro-Neto, 1999).\n\n\nThe crucial first step of retrieving candidate documents is managed by a retrieval engine, which accesses a document index. This is typically an inverted index for text-based retrieval¬†(Witten et al., 1999; Croft et al., 2010) or a vector-based index for embedding-based retrieval¬†(Salton et al., 1975; Mitra and Craswell, 2018; Shen et al., 2022). While efficient for simple filtering, these structures were designed for an era where the query interface was a human typing keywords.\n\n\n\n1.1. The Computational Burden on LLMs\n\nThe paradigm of information access is shifting. Users increasingly interact with search engines via Large Language Models (LLMs) and autonomous agents, expecting the system to not only retrieve documents but to perform complex reasoning over them.\n\n\nCurrently, the search index acts merely as a ‚Äúdumb‚Äù storage layer. To handle complex requests, modern architectures, like RAG¬†(Lewis et al., 2020) or Agent-Based approaches¬†(Yao et al., 2023), force the LLM to perform all the computational ‚Äúheavy lifting.‚Äù The search engine retrieves a broad, imprecise set of documents, using standard keyword matching or semantic similarity, and the LLM must iterate through them to perform logical filtering, arithmetic aggregation, or constraint satisfaction.\n\n\nThis design creates two fundamental bottlenecks:\n\n\n(1)\n\nContext Overhead &amp; Saturation: Since complex filtering is delegated to the LLM, the Planner is forced to retrieve large candidate sets to ensure recall for queries involving complex logic. In addition to network latency, feeding these broad results into the model necessitates processing massive context windows. This drastically increases inference latency (due to prompt processing costs) and degrades reasoning reliability, as LLMs struggle to maintain attention on relevant signals when saturated with the ‚ÄúContext Noise‚Äù of long inputs¬†(Liu et al., 2024; Jiang et al., 2023).\n\n\n\n(2)\n\nIterative Latency: Complex reasoning often requires multi-step agentic loops, e.g., ReAct¬†(Yao et al., 2023). Comprehensive surveys indicate that this iterative ‚ÄúReason-Act-Observe‚Äù cycle is the dominant latency cost in agentic systems¬†(Singh et al., 2025), motivating recent attempts to optimize planning overhead¬†(Zhang et al., 2025).\n\n\n\n\n\nTo scale agentic search, we must invert the current paradigm: rather than pulling data to the computation, we must push the computation down to the data. This necessitates replacing the high-latency, iterative ‚Äúfetch-then-filter‚Äù loop with a compiled execution model, where complex intent is translated into a single, expressive query executed natively within the retrieval engine.\n\n\n\n\n1.2. The Computational Gap\n\nTo realize this vision, the retrieval engine must function not merely as a filter, but as a computational engine. This imposes two fundamental requirements on the retrieval framework:\n\n\n(1)\n\nExpressive: Capable of representing any tractable logic (specifically, the complexity class ùêè\\mathbf{P}) to satisfy the reasoning needs of modern agents.\n\n\n\n(2)\n\nEfficient: Capable of executing these complex queries with strict resource bounds, ensuring both computational tractability and memory scalability where current architectures fail.\n\n\n\n\n\nIn Section¬†3, we establish that satisfying these requirements necessitates a paradigm shift in retrieval syntax: moving from simple syntactic trees to Directed Acyclic Graphs (DAGs) of Boolean operators. This structure is essential for enabling the reuse of logical sub-computations, a prerequisite for polynomial-time execution.\n\n\nStandard retrieval systems fail to execute this structure efficiently. As detailed in Section¬†5.1, iterator-based engines (like Lucene) suffer from the ‚ÄúTree-Expansion Bottleneck‚Äù‚Äîan exponential runtime blowup‚Äîwhen forced to unroll these DAGs. Conversely, as shown in Section¬†5.2, naive recursive engines (Term-at-a-Time) suffer from prohibitive memory and processing costs, as they are materializing massive intermediate results to handle logical negation.\n\n\n\n\n1.3. Contributions\n\nIn this paper, we bridge this gap by formally defining the requirements for a high-precision, computational retrieval engine. Our contributions are:\n\n\n\n\n‚Ä¢\n\nCapturing ùêè\\mathbf{P}: We define a formal Retrieval Language (‚ÑíR\\mathcal{L}_{R}) based on DAGs and provide a constructive proof that it precisely captures the complexity class ùêè\\mathbf{P}. This establishes that a retrieval engine is theoretically capable of identifying documents satisfying any polynomial-time computable property, far exceeding the expressiveness of standard Boolean filters.\n\n\n\n‚Ä¢\n\nThe ComputePN Algorithm: We introduce an evaluation algorithm that makes ‚ÑíR\\mathcal{L}_{R} tractable. By utilizing a ‚ÄúPositive-Negative‚Äù response mechanism and native DAG traversal, ComputePN eliminates the Universal Scan penalty of negation and the exponential blowup of tree-based iterators.\n\n\n\n‚Ä¢\n\nA Vision for Computational Search: We outline an architectural framework where LLMs act as a ‚ÄúQuery Understanding‚Äù layer, compiling natural language into expressive Query DAGs. This enables a new class of ‚ÄúNeuro-Symbolic‚Äù search systems where complex reasoning is executed natively within the index, replacing high-latency agentic loops with single-pass retrieval.\n\n\n\n\n\n\n\n\n2. Background and Related Work\n\nModern information retrieval architectures can be broadly categorized into two primary paradigms: Token-based (Sparse) retrieval and Embedding-based (Dense) retrieval.\n\n\nTo understand the structural limits analyzed in Section¬†5, we must first examine the execution mechanics of these standard architectures. We evaluate them specifically on their ability to execute the full spectrum of Boolean logic (‚àß,‚à®,¬¨\\land,\\lor,\\neg) structured as Directed Acyclic Graphs (DAGs). As established in Section¬†3, supporting this structure is a fundamental requirement for capturing the complexity class ùêè\\mathbf{P}.\n\n\n\n2.1. Term-at-a-Time (TAAT)\n\nIn the TAAT strategy (Token-based), the engine processes posting lists one by one, materializing the logical combination of documents into temporary buffers¬†(Moffat and Zobel, 1996; Witten et al., 1999).\n\n\n\n\n‚Ä¢\n\nMechanism: Intermediate results are materialized as sets or compressed bitmaps (e.g., Roaring Bitmaps¬†(Chambi et al., 2016)) and logically combined. This structure naturall"
  },
  {
    "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
    "url": "https://arxiv.org/abs/2601.18744v1",
    "source": "arxiv",
    "summary": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBenc",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 TSRBench\n\n3.1 Overview of TSRBench\n3.2 Time Series Perception\n3.3 Time Series Reasoning\n3.4 Time Series Prediction\n3.5 Time Series Decision-Making\n3.6 Data Collection Principles for TSRBench\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setups.\n\n\n4.2 Main Results\n\n\n4.3 Further Findings\n\n\n4.4 Further Investigation\n\n\n4.5 Error Analysis\n\n\n5 Conclusion\n\n\nA Additional Related Work\n\nA.1 MLLM/LLM Reasoning.\nA.2 Benchmarks for Generalist Models.\n\n\nB Future Research Directions\nC Model Versions\n\nD Data Collection\n\nD.1 Data Acquisition\nD.2 Question &amp; Answer Generation\nD.3 Answer Verification\nD.4 Data Contamination &amp; Quality Control\n\n\n\nE Time Series Analysis Tools\n\nE.1 Data Preprocessing\nE.2 Descriptive Statistics\nE.3 Trend Analysis\nE.4 Peak and Valley Detection\nE.5 Change Point Detection\nE.6 Multivariate Comparison\n\n\n\nF Additional Results\n\nF.1 Fine-grained Correlation Results\n\n\n\nG Cases\n\nG.1 Question Cases\nG.2 Error Cases\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models\n\n\n\nFangxu Yu\n\n‚ÄÉ‚ÄÉ\nXingang Guo\n\n‚ÄÉ‚ÄÉ\nLingzhi Yuan\n\n‚ÄÉ‚ÄÉ\nHaoqiang Kang\n\n‚ÄÉ‚ÄÉ\nHongyu Zhao\n\n‚ÄÉ‚ÄÉ\nLianhui Qin\n\n‚ÄÉ‚ÄÉ\nFurong Huang\n\n‚ÄÉ‚ÄÉ\nBin Hu\n\n‚ÄÉ‚ÄÉ\nTianyi Zhou\n\n\n\nAbstract\nTime series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models.\nTo bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: (i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. (ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning).\nThrough extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models.\nOur code and dataset are available at https://tsrbench.github.io/.\n\nMachine Learning, ICML\n\n\n‚Ä†\n\n\n\n1 Introduction\n\nTime series data pervades real-world environments and underpins decision-making across high-stakes domains, including finance¬†(dong2024fnspid), healthcare¬†(morid2023time), and industrial systems¬†(yan2024comprehensive). Since a substantial portion of real-world information is inherently temporal, reasoning on time series becomes a core capability for building generalist models that can reliably solve practical problems. Equipping models with such reasoning abilities enables automated systems to interpret temporal signals in context, supporting downstream applications such as education¬†(mao2024time), clinical management¬†(matowe2003interrupted), disaster forecasting¬†(hakim2024flood), and scientific discovery¬†(yu2025physics).\n\n\nGiven the critical importance of time series reasoning, there is a pressing need for standardized and automated evaluation frameworks that enable comprehensive assessment and comparison. However, existing work remains largely anchored in traditional time series analysis, which adopts a reductive view by treating time series as isolated numerical sequences‚Äîthereby stripping away the causal structure and semantic context essential for real-world problem-solving. Recent benchmarks have begun to integrate context¬†(williams2024context; liu2024time; cai2024timeseriesexam; kong2025time; wu2025scits), yet they predominantly target surface-level pattern understanding, which is insufficient for complex problem-solving. Other initiatives that attempt to probe reasoning capabilities¬†(chen2025mtbench; wang2025itformer; guan2025timeomni) often remain confined to narrow domains or restricted task scopes. This systemic limitation underscores the urgent demand for a comprehensive, multi-dimensional benchmark specifically designed to stress-test the full spectrum of time series reasoning.\n\n\nIn this paper, we introduce TSRBench, a large-scale and comprehensive benchmark curated to assess the time series problem-solving capability of generalist models across multiple domains and tasks. TSRBench extensively collect, select, and synthesize problems from 14 domains. This extensive collection has culminated in a benchmark comprising 4125 problems. We categorize the problems into four major dimensions of time series abilities‚ÄìPerception, Reasoning, Prediction, and Decision-Making, which comprises 15 tasks for different abilities (See Figure¬†1 for examples). Additionally, it supports four modalities of time series for generalist models: text, image, text-image interleaved, and time series embeddings, providing a comprehensive evaluation and comparison that modern AI systems could handle.\n\n\nTo facilitate the evaluation of LLMs and MLLMs, we design a unified evaluation setup. Time series are transformed into textual sequences of numbers for LLMs and into plots for VLMs. For proprietary models, we evaluate text-form (T), vision-form (V), and a combined (T+V) representation to test modality fusion. Based on TSRBench, we evaluate 6 leading proprietary models (e.g., GPT-5¬†(OpenAI2025GPT5)), 12 open-source LLMs (e.g., Qwen3¬†(yang2025qwen3)), 13 open-source VLMs (e.g., InternVL3.5¬†(wang2025internvl3)). Our evaluation yields three key findings: i) While current generalist models demonstrate strong performance on time series perception, they struggle significantly with complex reasoning, forecasting, and decision-making tasks. ii) The scaling law holds for most time series reasoning tasks on both LLMs and VLMs, with the notable exception of time series prediction. iii) Time Series Prediction tasks have weak relationships with other tasks.\niv) Textual and visual representations of time series are strongly complementary, often solving different sets of problems, yet current models struggle to leverage both modalities simultaneously for improved performance.\nAdditionally, our ablation studies provide practical insights into model design, particularly regarding the impact of visualization resolution, tool augmentation, and inference-time reasoning effort.\n\n\nFigure 1: Overview of TSRBench. TSRBench evaluates generalist models across four core capabilities: Perception, Reasoning, Prediction, and Decision-Making, each including multiple different tasks from real applications.\n\n\nTable 1: Comparison with Representative Time Series Benchmarks. Modality denotes the input format, where T and V represent textual and visual representations of time series, respectively.\n\n\n\n\nBenchmark\nScale &amp; Diversity\nReasoning Capabilities\nModality\n\n\n# Domains\n# Tasks\n# Questions\nMultivariate\nPerc.\nReas.\nPred.\nDec.\n\n\nForecasting-Centric\n\n\nTimeMMD (liu2024time)\n\n9\n1\n16K\n‚úì\n√ó\\times\n√ó\\times\n‚úì\n√ó\\times\nT\n\n\nCiK (williams2024context)\n\n8\n1\n0.3K\n√ó\\times\n√ó\\times\n√ó\\times\n‚úì\n√ó\\times\nT\n\n\nAnalysis-Centric\n\n\nTimeSeriesExam (cai2024timeseriesexam)\n\n1\n5\n0.7K\n√ó\\times\n‚úì\n√ó\\times\n√ó\\times\n√ó\\times\nT,V\\textbf{T},\\textbf{V}\n\n\nMTBench (chen2025mtbench)\n\n2\n4\n2.4K\n√ó\\times\n√ó\\times\n‚úì\n√ó\\times\n√ó\\times\nT\n\n\nEngineMT-QA¬†(wang2025itformer)\n\n1\n4\n11K\n‚úì\n‚úì\n‚úì\n√ó\\times\n‚úì\nV\n\n\nSciTS (wu2025scits)\n\n12\n7\n51K\n‚úì\n‚úì\n√ó\\times\n‚úì\n√ó\\times\nT\n\n\nTimeMQA (kong2025time)\n\n12\n5\n200K\n√ó\\times\n‚úì\n√ó\\times\n√ó\\times\n√ó\\times\nT\n\n\nTSR-SUITE¬†(guan2025timeomni)\n\n9\n4\n4K\n√ó\\times\n√ó\\times\n‚úì\n‚úì\n‚úì\nT\n\n\nTSRBench (Ours)\n14\n15\n4.1K\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\nT,V\\textbf{T},\\textbf{V}, T+V\n\n\n\n\n\n\n\n\n2 Related Work\n\nTime Series Benchmarks.\nTime series has long been studied. In the long run, existing benchmarks primarily focus on time series analysis tasks, including forecasting¬†(godahewa2021monash; bauer2021libra; qiu2024tfb; wang2024deep; li2025tsfm; hu2025fintsb), classification¬†(ismail2019deep; ruiz2020benchmarking), imputation¬†(du2024tsi; kazijevs2023deep), and anomaly detection¬†(lai2021revisiting; wenig2022timeeval; zhou2024can). Recent works begin to explore whether LLMs/MLLMs can understand the time series¬†(tan2024language; merrill2024language). TimeSeriesExam¬†(cai2024timeseriesexam) evaluates the time series understanding of LLMs and VLMs through synthetic data, but only focuses on holistic perception. MTBench¬†(chen2025mtbench) combines news reports with time series to assess models‚Äô reasoning capabilities, but is restricted to narrow domains such as finance and weather. TimeMMD¬†(liu2024time) and CiK¬†(williams2024context) focus on the time series forecasting task with the aid of contextual events or background. TSR-SUITE¬†(guan2025timeomni) and EngineMT-QA¬†(wang2025itformer) only cover a narrow reasoning tasks, and TimeMQA¬†(kong2025time) evaluates LLMs mainly on traditional time series analysis.\n\n\nGeneral Reasoning Benchmarks. Numerous benchmarks have been developed to evaluate the\ngeneral reasoning and problem-solving capabilities of generalist models.\nNotable examples include MMLU¬†(yue2024mmmu) and\nMMLU-Pro¬†(yue2024mmmu), GPQA¬†(rein2024gpqa), which assesses knowledge across a wide range of subjects. Benchmarks in science domains¬†(zhao2025prism; wang2025physunibench; he2024olympiadbench; xu2025ugmathbench; zou2024dynamath), and engineering domains¬†(syed2024benchmarking; kevian2024capabilities; guo2025toward) evaluate problem-solving ability. In addition, benchmarking in social scenarios¬†(le2019revisiting;"
  },
  {
    "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
    "url": "https://arxiv.org/abs/2601.18739v1",
    "source": "arxiv",
    "summary": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a",
    "full_text": null
  },
  {
    "title": "Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift",
    "url": "https://arxiv.org/abs/2601.18736v1",
    "source": "arxiv",
    "summary": "The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and ma",
    "full_text": null
  },
  {
    "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems",
    "url": "https://arxiv.org/abs/2601.18735v1",
    "source": "arxiv",
    "summary": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We",
    "full_text": "\n\n\n\n1 INTRODUCTION\n\n2 Problem Formulation\n\n2.1 The Economic Objective of Multi-Agent Coordination\n2.2 The Failure of Heuristic Proxies for Economic Rationality\n2.3 The Core Challenge: A Call for a New Paradigm\n\n\n\n3 Methodology: The Agora Market Framework\n\n3.1 Establishing the Market: From Uncertainty to Tradable Assets\n3.2 The Core Mechanism: A Profitability-Driven Trading Protocol\n3.3 Market Execution: The Broker and the Agora Algorithm\n\n\n\n4 Experiments\n\n4.1 Comprehensive Visual Understanding Performance\n4.2 Comparison with Alternative Routing and Multi-Agent Strategies\n4.3 The Role of the Market-Aware MAB Strategy\n4.4 Cost and Performance Balance Analysis\n4.5 Module and Strategy Ablation Studies\n\n\n5 Conclusion\nA Appendix\nB Preliminary\n\nC Related Work\n\nC.1 Vision-Language Models in Multi-Agent Systems\nC.2 Uncertainty Quantification and Management\nC.3 Multi-Armed Bandits and Decision-Making\n\n\n\nD Theoretical Proofs and Supplements in the Main Text\n\n\nD.1 Multi-dimensional Visual Uncertainty Quantification Model (3.1)\n\n\nD.1.1 Formal Definition and Expansion of Core Uncertainty Dimensions\n\na. Perceptual Uncertainty (upercu_{\\text{perc}})\nb. Semantic Uncertainty (usemu_{\\text{sem}})\nc. Inferential Uncertainty (uinfu_{\\text{inf}})\n\n\n\nD.1.2 Manageability Dimensions: Epistemic Uncertainty and Aleatoric Uncertainty\n\na. Epistemic Uncertainty (ùêÆepis\\mathbf{u}_{\\text{epis}})\nb. Aleatoric Uncertainty (ùêÆalea\\mathbf{u}_{\\text{alea}})\n\n\nD.1.3 Total Uncertainty (ùêÆtotal\\mathbf{u}_{\\text{total}})\n\n\n\nD.2 Dynamic Uncertainty Transfer Mechanism (3.2)\n\nD.2.1 Uncertainty Flow Equation\n\nD.2.2 Trend of Change in System-Total Uncertainty: Conservation/Convergence Analysis\n\na. Definition of System-Total Uncertainty\nb. Impact of Uncertainty Transfer on Total Uncertainty\n\n\n\nD.2.3 Deepening the Transfer Cost-Benefit Analysis: Considering Total Transfer Amount and Expert Knowledge\n\na. Variable Processing Cost Change of a Trade\nb. Deriving the Cost-Benefit Condition for a Trade\nc. Connection with CE Ratio and Broader Cost Considerations\n\n\n\n\n\nD.3 Uncertainty Trading Protocol\n\n\nD.3.1 Prerequisites for a Trade\n\na. Trade Trigger Condition\nb. Receiver Capacity Constraint\n\n\n\nD.3.2 Cost-Benefit Analysis of a Trade\n\na. Condition for Reducing Variable Processing Costs\nb. Overall Expected Benefit Condition\n\n\nD.3.3 Market Equilibrium Analysis - Brief Theoretical Perspective\n\nD.3.4 Application of Comparative Advantage Theory\n\na. Comparative Advantage and Cost Optimization\nb. Implicit Implementation of Comparative Advantage by Agora Protocol\n\n\n\n\n\nD.4 Uncertainty-Aware MAB Selection Strategy\n\na. Beta Posterior Parameter Update\nb. Baseline Expected Reward\nc. Comprehensive Scoring Function Œ∏~S(t)\\tilde{\\theta}_{S}^{(t)}\n\nD.4.1 Theoretical Guarantees: Regret &amp; Convergence\n\na. Redefining Regret\nb. Assumptions for Convergence Analysis\nc. Direction of Convergence\n\n\n\nD.4.2 Mathematical Deconstruction of Strategic Uncertainty Index (Ustrategic‚Äã(S)U_{\\text{strategic}}(S))\n\na. Core Objective Function of Ustrategic‚Äã(S)U_{\\text{strategic}}(S)\nb. Expansion of System Cost Change from a Trade Œî‚Äãùíûsys‚Äã(trade)\\Delta\\mathcal{C}_{\\text{sys}}(\\text{trade})\nc. Ustrategic‚Äã(S)U_{\\text{strategic}}(S) as an Expected Sum\n\n\n\n\n\n\n\nE Impact of Agent Pool Configuration on Agora\n\nE.1 Experimental Setup\nE.2 Results and Analysis\n\n\n\nF FLOPs Comparison and Computational Efficiency\n\nF.1 Experiment Setup\nF.2 Experimental Results and Discussion\n\n\n\nG Supplementary Core Component Ablation Discussion\n\nG.1 Impact of Uncertainty Dimensions\nG.2 Validation of Epistemic-Aleatoric Distinction in Uncertainty Trading\nG.3 Robustness and Boundary Analysis of Trading Protocol Parameters\n\n\n\nH Hyperparameter Ablation Experiment\n\nH.1 Ablation on UCB1 Exploration Constant CC\nH.2 Ablation on MAB Learning Rate Œ±\\alpha\nH.3 Ablation on Time Decay Factor ŒªŒî‚Äãt\\lambda_{\\Delta t}\n\n\n\nI Hyperparameters Used in the Experiments\n\nI.1 Agora Framework Parameters\nI.2 Hyperparameters for Comparative Models and Strategies\nI.3 Model Inference Parameters\n\n\n\nJ Runtime Analysis\n\nJ.1 Results and Discussion\n\n\n\nK Prompt Setting Statement\n\nK.1 General Prompts for Expert Roles and the Aggregator\n\nK.2 Prompts for VLM Experts in Benchmark Evaluations\n\nK.2.1 Base Expert Prompts\n\n\n\n\n\nL Hyperparameters Used in the Experiments\n\nL.1 Agora Framework Parameters\nL.2 Hyperparameters for Comparative Models and Strategies\nL.3 Model Inference Parameters\n\n\n\nM Clarifications on Methodological Components\n\nM.1 Definitions of Core Variables and Functions\nM.2 Uncertainty Quantification and Estimation\nM.3 Explanation of Key Elements in Figure 3\n\n\n\nN Case Analysis\n\nN.1 Successful Case\nN.2 Unsuccessful Case\n\n\n\n\n\n\n\nWhy Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems\n\n\n\nJusheng Zhang1,\nYijia Fan1,\nKaitong Cai1,\nJing Yang1,\nJiawei Yao2,\nJian Wang3,\nGuanlong Qu4,\nZiliang Chen1,\nKeze Wang1\n\n1Sun Yat-sen University ‚ÄÉ2University of Washington ‚ÄÉ3Snap Inc. ‚ÄÉ4Syracuse University. ‚ÄÉ\nkezewang@gmail.com\nCorresponding author.\n\n\nAbstract\nVision‚ÄìLanguage Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination.\nWe introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria.\nExperiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3√ó. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.\n\n\n\n1 INTRODUCTION\n\nThe rapid advancement of Vision-Language Models (VLMs) (Li et al., 2022; 2023; Liu et al., 2023a; Bai et al., 2023) has propelled the development of multi-agent systems (MAS) (Guo et al., 2024; Wang et al., 2024c; Chen et al., 2024a; b), moving us closer to the vision of powerful, collective intelligence. Yet, as these systems scale, they inevitably collide with foundational challenges from economic theory: coordinating self-interested agents under information asymmetry and making globally optimal decisions under bounded rationality. We argue current paradigms fail to address these root problems, leading to a crisis of economic viability where operational costs spiral, precluding effective, large-scale deployment (Gandhi et al., 2025). This inefficiency stems from a failure to treat intelligence not as a brute-force commodity, but as a scarce economic resource requiring principled management.\n\n\nExisting coordination strategies can be understood as heuristic patches, i.e., computationally cheap workarounds for these deep-seated barriers. Paradigms like Mixture-of-Agents (MoA) (Guo et al., 2024) or knowledge-based routers (e.g., KABB) (Zhang et al., 2025) attempt to bypass the complexity of true optimization by relying on simplistic proxies for value, such as consensus or semantic similarity. As we formally prove in Section 2, these heuristics render the systems fundamentally agnostic to the core economic variables of cost and the fine-grained structure of uncertainty. This agnostic nature is not a minor flaw but a theoretical dead-end, leading to provably suboptimal performance and systemic waste.\n\n\nFigure 1: Comparison of heuristic coordination and Agora. Unlike heuristics that rely on flawed proxies, Agora forms a dynamic market for uncertainty, where emergent prices enable coordination.\n\n\nTo dismantle this economic bottleneck, we argue for a paradigm shift: from heuristic patches to a mechanism that embraces the decentralized nature of the problem. Accordingly, we construct Agora, a framework that redesigns multi-agent coordination as a decentralized micro-economy. Agora does not attempt to approximate a central planner; instead, it uses market-based mechanisms to achieve efficient coordination despite information asymmetry and bounded rationality. Within this framework, cognitive uncertainty is no longer a monolithic liability but is ‚Äùminted‚Äù into a quantifiable, tradable asset. Agents, guided by price signals and driven by economic incentives (Gale and Shapley, 1962; von Neumann and Morgenstern, 2004; Akerlof, 1970), trade this asset to reveal private information and drive the entire system towards a cost-effective equilibrium.\n\n\nOur methodology, detailed in Section 3, provides a constructive, non-agnostic solution. We first establish a multi-dimensional uncertainty quantification model, creating a structured asset that makes the system structure-aware. Second, we introduce a profitability-driven trading protocol that enforces economic rationality, making the system cost-aware. Finally, the entire market is orchestrated by an intelligent market-aware Broker, which uses a sophisticated utility function to find economically sound initializations for the collaborative process.\n\n\nOur comprehensive experiments on multiple visual understanding benchmarks (e.g., MMMU (Yue et al., 2024), MMBench (Liu et al., 2023b)) demonstrate that Agora not only achieves state-of-the-art performance but also dramatically improves cost-effectiveness, validating our market-based paradigm. This work lays a theoretical and practical foundation for building truly scalable and economically viable multi-agent intelligent systems.\n\n\n\n\n2 Problem Formulation\n\nThe rise of multi-age"
  },
  {
    "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models",
    "url": "https://arxiv.org/abs/2601.18734v1",
    "source": "arxiv",
    "summary": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. Ho",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Knowledge Distillation for Autoregressive Large Language Models\n2.2 Reinforcement Learning with Verifiable Rewards\n\n\n\n3 Methods\n\n3.1 Learning from Verifiable Reasoning Dataset\n\n3.2 On-Policy Self-Distillation\n\nMotivation: Learning by understanding solutions.\nTeacher and student policies.\nOn-policy sampling from the student.\nTraining objective: Full-vocabulary divergence.\nAlternative objective: Sampled-token distillation through policy gradient.\n\n\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n\n4.2 Main Results\n\nSuperior Token Efficiency from Dense Teacher Feedback.\n\n\n\n4.3 Discussions\n\n4.3.1 Effect of Model Scale\n4.3.2 Effect of Generation Length\n4.3.3 Learning Objective Comparison: Full Vocabulary Logits Distillation vs. Sampled-Token Distillation\n\n\n\n\n\n5 Related Work\n\nLLM Self-Training.\n\n\n6 Conclusion\n7 Limitations and Future Directions\n\n8 Appendix\n\n8.1 Experimental Details\n\n\n\n\n\n\n\n\nSelf-Distilled Reasoner: \n\nOn-Policy Self-Distillation for Large Language Models\n\n\n\nSiyan Zhao‚Ä†\n\n‚ÄÉ‚ÄÉ\nZhihui Xie\n\n‚ÄÉ‚ÄÉ\nMengchen Liu\n\n‚ÄÉ‚ÄÉ\nJing Huang\n\n‚ÄÉ‚ÄÉ\nGuan Pang\n\n‚ÄÉ‚ÄÉ\nFeiyu Chen‚àó,‚Ä°\n\n‚ÄÉ‚ÄÉ\nAditya Grover‚àó\n\n\n\nAbstract\nKnowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student‚Äôs own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8√ó token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nFigure 1: Overview of On-Policy Self-Distillation (OPSD): Given a reasoning dataset ùíÆ={(xi,yi‚ãÜ)}i=1N\\mathcal{S}=\\{(x_{i},y_{i}^{\\star})\\}_{i=1}^{N}, we instantiate two policies from the same LLM: a student policy pS(‚ãÖ‚à£x)p_{S}(\\cdot\\mid x) and a teacher policy pT(‚ãÖ‚à£x,y‚ãÜ)p_{T}(\\cdot\\mid x,y^{\\star}). The student generates an on-policy response y^‚àºpS(‚ãÖ‚à£x)\\hat{y}\\sim p_{S}(\\cdot\\mid x). Both policies then evaluate this trajectory to produce next-token distributions pS(‚ãÖ‚à£x,y^&lt;n)p_{S}(\\cdot\\mid x,\\hat{y}_{&lt;n}) and pT(‚ãÖ‚à£x,y‚ãÜ,y^&lt;n)p_{T}(\\cdot\\mid x,y^{\\star},\\hat{y}_{&lt;n}) at each step nn. The learning objective minimizes the per-token divergence D‚Äã(pT‚à•pS)D(p_{T}\\|p_{S}) along the student‚Äôs rollout. Crucially, gradients backpropagate only through the student‚Äôs logits, allowing the model to self-distil.\n\n\nRecent advances in large language models (LLMs) have demonstrated impressive capabilities in reasoning and instruction following. Achieving these capabilities during post-training typically relies on reinforcement learning methods such as Reinforcement Learning with Verifiable Rewards (RLVR) (e.g., GRPO¬†(shao2024deepseekmath; guo2025deepseek; team2025kimi; rastogi2025magistral; yu2025dapo)), supervised fine-tuning (SFT) on high-quality reasoning datasets¬†(guha2025openthoughtsdatarecipesreasoning; team2025kimi; xiao2026mimov2flashtechnicalreport), or knowledge distillation, where recent work has shown that distillation from advanced teacher models can outperform RL in both performance and training efficiency¬†(qwen3; xiao2026mimov2flashtechnicalreport; lu2025onpolicydistillation).\n\n\nDespite their respective successes, each approach has inherent limitations. RLVR suffers from inefficiencies including: (1) sampling a group of responses per prompt is computationally expensive and can introduce high variance in estimating the true value function; moreover, when all samples are either correct or incorrect, the gradient signal vanishes¬†(yu2025dapo; zhao2025inpainting); and (2) the reward signal is sparse and uniformly applied across all tokens in the generated output, neglecting fine-grained token-level feedback. Supervised fine-tuning suffers from exposure bias and weaker generalization¬†(agarwal2024policy; chu2025sft). Traditional knowledge distillation provides dense token-level supervision from a teacher model but relies on off-policy data¬†(hinton2015distillingknowledgeneuralnetwork). Recent advances in on-policy distillation‚Äîwhere a student model samples its own trajectories while a teacher policy provides dense token-level supervision‚Äîhave demonstrated superior sample efficiency by combining the distributional realism of on-policy training with dense feedback¬†(agarwal2024policy; lu2025onpolicydistillation).\n\n\nWhile on-policy distillation has shown strong performance, it relies on a distinct teacher model to supervise the student. Given that modern LLMs already exhibit strong reasoning capabilities, we ask this research question: can a model effectively serve as its own teacher through self-distillation? Our approach is inspired by human learning: after solving a problem incorrectly, a student can examine the correct solution, rationalize its steps, and identify where their reasoning failed. Prior work has shown that for LLMs, evaluation is often easier than generation¬†(sun2024easy; naor1996evaluation). We hypothesize that rationalization‚Äîexplaining a given correct answer‚Äîis similarly easier than generation. Motivated by this, we instantiate both the teacher and student policies from a single LLM. The teacher policy is provided with privileged information y‚ãÜy^{\\star}, such as the ground-truth answer or a reference chain-of-thought, while the student policy conditions only on the problem xx. Concretely, the teacher policy pT(‚ãÖ‚à£x,y‚ãÜ)p_{T}(\\cdot\\mid x,y^{\\star}) conditions on both the problem and the privileged answer, whereas the student policy pS(‚ãÖ‚à£x)p_{S}(\\cdot\\mid x) observes only the problem. We preserve the on-policy training paradigm by sampling trajectories y^\\hat{y} exclusively from the student policy, which then receives dense, token-level supervision from the privileged teacher policy.\n\n\nWe therefore propose On-Policy Self-Distillation (OPSD), a framework in which a single model plays both teacher and student roles. The student samples its own trajectories y^‚àºpS(‚ãÖ‚à£x)\\hat{y}\\sim p_{S}(\\cdot\\mid x); we then compute the per-token divergence between the student and teacher distributions and minimize it over the student‚Äôs own rollouts. This formulation (i) uses on-policy supervision (the student‚Äôs own trajectories), (ii) provides dense per-token feedback, (iii) exploits ground-truth solutions y‚ãÜy^{\\star}, and (iv) requires no separate teacher model. The learning process is captured by the loss\n\n\n\n‚ÑíOPSD\\displaystyle\\mathcal{L}_{\\mathrm{OPSD}}\n(Œ∏)=ùîº(x,y‚ãÜ)‚àºùíÆ‚Äãùîºy^‚àºpS(‚ãÖ‚à£x)‚Äã‚àën=1|y^|\\displaystyle(\\theta)=\\mathbb{E}_{(x,y^{\\star})\\sim\\mathcal{S}}\\;\\mathbb{E}_{\\hat{y}\\sim p_{S}(\\cdot\\mid x)}\\sum_{n=1}^{|\\hat{y}|}\n\n\n\n\n\nD(pT(‚ãÖ‚à£x,y‚ãÜ,y^&lt;n)‚à•pS(‚ãÖ‚à£x,y^&lt;n)).\\displaystyle\\quad D\\!\\Bigl(p_{T}\\!\\left(\\cdot\\mid x,y^{\\star},\\hat{y}_{&lt;n}\\right)\\;\\Big\\|\\;p_{S}\\!\\left(\\cdot\\mid x,\\hat{y}_{&lt;n}\\right)\\Bigr).\n\n(1)\n\n\n\n\nIn summary, our contributions are as follows:\n\n\n‚Ä¢\n\nWe introduce On-Policy Self-Distillation, a novel framework that enables a single model to act as both teacher and student, leveraging ground-truth answers to provide dense token-level supervision on student rollouts.\n\n\n\n‚Ä¢\n\nWe evaluate OPSD on four competition-level mathematical reasoning tasks, demonstrating that it outperforms both RLVR (e.g., GRPO) and supervised fine-tuning baselines.\n\n\n\n‚Ä¢\n\nWe show that OPSD achieves better performance with nearly 8√ó8\\times improved token efficiency and lower computational cost than GRPO.\n\n\n\n‚Ä¢\n\nWe analyze the impact of model scale, finding that moderate model capacity is necessary for successful self-distillation. We further compare different divergence objectives and analyze the effect of student generation length.\n\n\n\n\n\n\n\n\n\nSFT/Off-Policy\nGRPO\nOn-Policy\nOn-Policy\n\n\n\nDistillation\n\nDistillation\nSelf-Distillation (Ours)\n\n\n\n\nOn-Policy Data\n‚úó\n‚úì\n‚úì\n‚úì\n\n\nDense Learning Signal\n‚úì\n‚úó\n‚úì\n‚úì\n\n\nLow Sampling Cost\n‚úì\n‚úó\n‚úì\n‚úì\n\n\nNo External Teacher\n‚úì\n‚úì\n‚úó\n‚úì\n\n\n\nTable 1: Comparison of training methods for reasoning tasks. On-Policy Self-Distillation (OPSD) combines the advantages of on-policy training with dense feedback without requiring an external teacher model.\n\n\n\n\n2 Background\n\n\n2.1 Knowledge Distillation for Autoregressive Large Language Models\n\nKnowledge distillation transfers knowledge from a larger teacher model to a smaller student model by training the student to mimic the teacher‚Äôs behavior¬†(hinton2015distillingknowledgeneuralnetwork; kim2016sequence; sanh2019distilbert). The core insight is that the teacher‚Äôs soft probability distribution over classes contains richer information than hard labels alone, as it reveals the teacher‚Äôs learned similarities between classes. For auto-regressive language models, given a dataset ùíÆ={(x,y‚ãÜ)}\\mathcal{S}=\\{(x,y^{\\star})\\} where xx denotes an input and y‚ãÜy^{\\star} is the corresponding reference output, both teacher pTp_{T} and student pSp_{S} define token-level distributions over vocabulary ùí±\\mathcal{V}. Traditional supervised distillation minimizes a divergence DD between teacher and student distributions averaged over a fixed dataset:\n\n\n\n‚ÑíSupervised Distillatio"
  },
  {
    "title": "Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge",
    "url": "https://arxiv.org/abs/2601.18733v1",
    "source": "arxiv",
    "summary": "Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enha",
    "full_text": "\n\n\n\n1 Introduction\n\n2 MARS Challenge Overview\n\n2.1 Challenge Description\n\n2.2 Planning Track\n\n2.2.1 Task Setup and Evaluation\n2.2.2 Results and Failure Analysis\n\n\n\n2.3 Control Track\n\n2.3.1 Round-wise Rules\n2.3.2 Results Overview\n\n\n\n\n\n3 Example Solutions\n\n\n3.1 Planning Track\n\nScaling Embodied Planning via Self-Correction (Champion).\nModular Closed-Loop Framework for Multi-Agent Coordination (Runner-Up).\n\n\n\n3.2 Control Track\n\nCombo-MoE: Combinatorial Experts for Multi-Arm Coordination (Champion)\nCoVLA: Collaborative VLA via Decentralized Manipulation (Runner-Up)\n\n\n\n\n\n4 Discussion\n\n\n4.1 Key Findings\n\nWhat to take away.\n\n\n4.2 Spatial Reasoning and Communication Mechanisms\n4.3 Challenges Faced\n4.4 Limitations and Future Work\n\n\n\n5 Related Work\n\nEmbodied Planning in MAS\nRobotic Manipulation in MAS\n\n\n6 Conclusion\n\n7 Participants and Committees\n\nChampion: EfficientAI\nRunner-up: TrustPath AI\nChampion: MMLab@HKU √ó\\times D-Robotics\nRunner-up: INSAIT\n\n\n\nA Planning Track Details\n\n\nA.1 Problem Formulation and Output Format\n\nA.1.1 Action Space\nA.1.2 JSON Submission Structure\n\n\n\nA.2 Case Studies: Simple vs. Complex Tasks\n\nA.2.1 Simple Task: Sequential Manipulation\nA.2.2 Complex Task: Collaborative Long-Horizon Transport\n\n\nA.3 Lazy Plan\n\n\n\n\n\n\n\nAdvances and Innovations in the\nMulti-Agent Robotic System (MARS) Challenge\n\n\nLi Kang1,4‚Ä†, Heng Zhou3,4‚Ä†, Xiufeng Song1‚Ä†, Rui Li4‚Ä†, Bruno N.Y. Chen5, Ziye Wang6,\nXimeng Meng7, Stone Tao8, Yiran Qin9, Xiaohong Liu1, Ruimao Zhang10, Lei Bai4,\nYilun Du11, Hao Su8, Philip Torr2, Zhenfei Yin2‚Ä° and the Challenge Participants\n1SJTU,\n¬†2Oxford,\n¬†3USTC,\n¬†4Shanghai AI Lab,\n¬†5CMU,\n¬†6HKU, \n7Tongji,\n¬†8UC San Diego,\n¬†9CUHK-SZ,\n¬†10SYSU,\n¬†11Harvard\n\n‚Ä† Equal contribution\n‚Ä° Corresponding author \n\nThe complete list of participants is provided in Section¬†7.\n\n\nAbstract\nRecent advancements in multimodal large language models and vision-language-action models have significantly driven progress in Embodied AI.\nAs the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions.\nTo address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments.\nBy evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.\n\n\n\n1 Introduction\n\nRecent progress in multimodal large language models¬†Team et al. (2025a, b) and vision-language-action models¬†Black et al. (2025); Kim et al.  has driven significant developments in Embodied AI.\nWhile substantial progress has been made on specific tasks¬†Cheang et al. (2025); Team (2025), the field is now moving towards more complex task scenarios, where multi-agent system frameworks are increasingly necessary.\nThis shift is driven by three key factors:\n(1) Capabilities. As the capabilities of individual agents increase, scaling them up within a multi-agent framework allows for more complex and versatile systems.\n(2) Efficiency. While individual agents are constrained to specific tasks, multi-agent systems enable the delegation of tasks among agents, making the overall system more efficient.\n(3) Human-Agent Interaction. As embodied multi-agent systems become more capable, they pave the way for advanced human-agent interactions, where embodied systems can collaborate with humans in a broader range of tasks.\n\n\nTo achieve an embodied system, two key capabilities are required: planning and control. Planning is responsible for determining the sequence of actions needed to achieve a particular goal, taking into account the dynamic and often uncertain nature of the environment¬†Li et al. (2025). On the other hand, control involves executing these actions by ensuring that each agent moves and interacts with the physically environment in a coordinated and efficient manner¬†Qin et al. (2025). Achieving these capabilities in embodied multi-agent systems is even more challenging. Unlike single-agent systems, where the focus is on a solitary entity, multi-agent systems require coordination, communication, and collaboration among agents to complete tasks, introducing additional complexity.\n\n\nTo address these challenges, we proposed the Multi-Agent Robotic System (MARS) Challenge,111https://mars-eai.github.io/MARS-Challenge-Webpage/ held at the NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI.222https://space-in-vision-language-embodied-ai.github.io/\nThis competition focuses on two key aspects: planning and control. It aims to advance research on multi-agent collaboration in robotics, where agents of various types, such as humanoids, quadrupeds, and manipulators, must coordinate to achieve complex tasks in dynamic environments. Planning Track explores multi-agent embodied planning, where participants use vision-language models (VLMs) to select agents and define high-level action sequences for collaborative tasks in environments with multiple candidate robots. Control Track focuses on policy execution, requiring participants to deploy end-to-end policy on robotic arms in physically realistic simulations to perform manipulation tasks like multiple blocks stacking, while ensuring robust coordination across agents under partial observability and randomized conditions.\n\n\nWe systematically study multi-agent challenges at both the planning and control levels. Through evaluation and analysis of the participants‚Äô submitted solutions, we provide insights that contribute to the development of embodied multi-agent systems, offering new perspectives for advancing the field.\n\n\n\n\n2 MARS Challenge Overview\n\n\n2.1 Challenge Description\n\nThe Multi-Agent Robotic System (MARS) Challenge is designed to benchmark and advance research on embodied multi-agent collaboration by evaluating both high-level planning and low-level control in heterogeneous robotic systems. Participants are tasked with solving complex multi-agent problems through two complementary tracks: the Planning Track, where models need to select appropriate agents and generate high-level action sequences for collaborative tasks based on visual and language inputs, and the Control Track, where participants need to deploy a control policy on multiple agents to execute manipulation tasks in physically realistic simulation environments. By separating planning and control, the challenge fosters progress in both multi-agent reasoning and coordinated execution, offering insights into the capabilities and limitations of current embodied multi-agent approaches.\n\n\n\n\n2.2 Planning Track\n\n\n2.2.1 Task Setup and Evaluation\n\nThe Planning Track of the MARS Challenge333https://github.com/MARS-EAI/VIKI-R/tree/MARS-Challenge-2025 targets high-level embodied planning in multi-agent systems with heterogeneous embodiments.\nParticipants are required to design planners that jointly reason over natural language instructions and visual observations, select appropriate agents, and generate coordinated action sequences.\nUnlike low-level control, this track emphasizes semantic task understanding, agent capability reasoning, and long-horizon multi-agent coordination.\n\n\nGiven an instruction and a scene observation, a valid solution must determine both which robots should participate in the task and how they should act over time.\nRobot selection contributes 10% to the final score, while the remaining 90% evaluates the quality of the generated action plan.\nThe planning score is computed using a composite metric that considers exact step-wise matching, correctness of the initial action prefix, consistency of action types, and the step length.\nThis protocol encourages early decision correctness, coherent long-horizon planning, and effective parallel execution.\n\n\nThe Planning Track is built upon a unified embodied intelligence stack. We build on VIKI-Bench¬†Kang et al. (2025), a benchmark for evaluating vision-based embodied planning in multi-agent settings, and instantiate all tasks in the ManiSkill3 physics-based simulator¬†Tao et al. (2024). Task scenarios are drawn from the RoboCasa dataset, which provides diverse household environments and everyday manipulation tasks¬†Nasiriany et al. (2024).\nEach task instance consists of a natural language instruction, which may include conditional or exploratory descriptions, and a visual observation depicting the scene and available robots.\nThe planner outputs a temporally ordered action plan, where one or more robots may act in parallel, and each action is defined by an action type and a target object. More details can be found in Appendix¬†A.\n\n\nThe benchmark includes tasks with varying levels of difficulty.\nSimple tasks, such as opening an appliance, can often be solved by a single robot with a short action sequence.\nIn contrast, complex tasks require coordinated execution by multiple robots over long horizons.\nA representative example is task_147, which instructs agents to transport multiple food items into a refrigerator.\nThe most complex tasks involve planning up to ten steps, highlighting the long-horizon multi-agent tasks.\n\n\n\n\n2.2.2 Results and Failure Analysis\n\nWe analyze participant submissions on the leaderboard to assess planning performance across tasks.\nAs shown in Tab.¬†1, overall scores concentrate in the 0.4‚Äì0.6 range, indicating substantial headroom under the track‚Äôs evaluation criteria.\nFig.¬†1 provides a more fine-grained view: "
  },
  {
    "title": "Optimal Use of Preferences in Artificial Intelligence Algorithms",
    "url": "https://arxiv.org/abs/2601.18732v1",
    "source": "arxiv",
    "summary": "Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectiv",
    "full_text": null
  },
  {
    "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
    "url": "https://arxiv.org/abs/2601.18731v1",
    "source": "arxiv",
    "summary": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users",
    "full_text": null
  },
  {
    "title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale",
    "url": "https://arxiv.org/abs/2601.18730v1",
    "source": "arxiv",
    "summary": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require care",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Introducing Reflect\n1.2 Contributions and Summary of Findings\n\n\n2 Related Work\n\n3 Methodology\n\n3.1 Reflect\n3.2 Evaluation\n3.3 Self-finetuning using Reflect\n\n\n\n4 Results\n\n4.1 Reflect Exhibits High Levels of Principle Conformance\n4.2 Reflect Targets Principle Violations\n4.3 Reflect Generates Fine-tuning Data\n4.4 Using Reflect on Complex Principles Maintains Factual Reasoning\n4.5 Reflect is Effective Across Different Model Sizes\n4.6 Reflect Has Low Overhead That Scales with Base-Model Alignment\n4.7 Reflect Does Not Unalign Models\n\n\n\n5 Discussion and Future Work\n\n5.1 Applications to Pluralistic Alignment\n5.2 Reflect for Safety-critical Deployments\n5.3 Finetuning Models for Reflect\n\n\n\n6 End Matter\n\n6.1 Generative AI Usage Statement\n\n\n\nA Additional Tables\n\nA.1 Size Ablation\nA.2 Reasoning Benchmarks\n\n\nB Qualitative Example of Reflect, Diagrammed\nC Full Cycle Outputs of Reflect from Experimental Results\nD Full constitutions we used for Reflect evaluation\nE Improvements Using Reflect are Statistically Signficant\n\nF Validation Survey Materials\n\nF.1 Informed Consent\nF.2 SafeRLHF Constitution Survey Question\nF.3 HH-RLHF Constitution Survey Question\n\n\n\nG LLM Prompt templates\n\nG.1 Multi-Objective Evaluation Prompts\nG.2 Self-Evaluation Prompt\nG.3 Critique and Revision Prompt\nG.4 Constitution Pre-conditioning Prompt\n\n\n\nH Adaption of Self-Refine\n\nH.1 Adapted Prompts for SafeRLHF\nH.2 Adapted Prompts for HHH\n\n\n\n\n\n\n\nReflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale\n\n\nHenry Bell\n\nhenry.bell@duke.edu\n\n, \nCaroline Zhang\n\ncaroline.zhang2@duke.edu\n\n, \nMohammed Mobasserul Haque\n\nmohammedmobasserul.haque@duke.edu\n\nDuke UniversityDurhamNorth CarolinaUSA\n\n, \nDhaval Potdar\n\nIndependent ResearcherUSA\n\ndhavalspotdar@gmail.com\n\n, \nSamia Zaman\n\nIndependent ResearcherUSA\n\nsamia.zaman@alumni.duke.edu\n\n and \nBrandon Fain\n\nDuke UniversityDurhamNorth CarolinaUSA\n\nbtfain@cs.duke.edu\n\n\n\nAbstract.\nThe constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose Reflect, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. Reflect operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. Reflect‚Äôs technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that Reflect significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model‚Äôs original parameter fine-tuning, without sacrificing factual reasoning. Reflect is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that Reflect naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.\n\nalignment, in-context learning, constitutional AI, ethical AI, moral AI, principled AI\n\n\n\n1. Introduction\n\nAs large language models (LLMs) display increasing capabilities¬†(Achiam et al., 2023; Touvron et al., 2023; Chowdhery et al., 2023; Liu et al., 2024), aligning these models with human values is critical for their safe and responsible deployment¬†(Amodei et al., 2016; Gabriel, 2020; Christian, 2021; Askell et al., 2021; Ji et al., 2025b). The most common current alignment techniques, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), rely on fine-tuning a model‚Äôs parameters based on reward modeling from human or AI-generated annotations (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022b; Rafailov et al., 2023; Ivison et al., 2024; Ethayarajh et al., 2024a). While valuable, such alignment techniques have a number of limitations. They are computationally demanding and often require difficult-to-obtain human annotation data¬†(Casper et al., 2023). The need for lightweight inference-time techniques to adapt model behavior has led to the widespread use system prompts for adapting fine-tune modeled for particular use cases¬†(Lee et al., 2024b; Neumann et al., 2025). The algorithms generally require careful engineering and tuning and can suffer from challenges such as reward hacking¬†(Skalse et al., 2022), learned deception¬†(Lang et al., 2024), incentivizing sycophancy¬†(Sharma et al., 2024), and conflicting and unstable human preferences¬†(Conitzer et al., 2024; Boerstler et al., 2024). Standard model alignments have been found to exhibit bias towards particular cultures, demographics, and values¬†(Wu et al., 2023; Tao et al., 2024; Neumann et al., 2025; Sch√§fer et al., 2025; Taylor et al., 2025).\n\n\nGiven these limitations, such parameter fine-tuning approaches with reward modeling are best suited for static alignment projects by well-resourced engineering teams before large-scale general deployments using fairly general principles such as helpfulness and harmlessness¬†(Askell et al., 2021; Bai et al., 2022a; Ouyang et al., 2022). There is a further need for alternative approaches to alignment that do not require any parameter fine-tuning or preference annotation data and that can be used to easily align to diverse cultures, values, and user groups¬†(Wu et al., 2023; Sorensen et al., 2024; Tao et al., 2024; Lee et al., 2024b; Kirk et al., 2024; Ramesh et al., 2024; Davani et al., 2024; Sch√§fer et al., 2025). Even within a given population, such techniques may still be helpful to align to diverse deployment contexts and their germane values. For example, the relevant values and alignment principles may differ based on whether an LLM is deployed for use in hospitals, mental health, education, entertainment, or software development. Such contextual diversity can be accommodated within the constitutional alignment framework¬†(Bai et al., 2022b; Huang et al., 2024b; Fr√§nken et al., 2024; Findeis et al., 2024; Kyrychenko et al., 2025) in which the relevant principles are explicitly and transparently stated in natural language text (for example, ‚ÄúDo not employ stereotypes based on race‚Äù). We work within this constitutional alignment framework and primarily evaluate conformance to the stated principles.\n\n\n\n1.1. Introducing Reflect\n\nIn this work, we propose Reflect, an inference-time technique to dynamically align an LLM to a constitution of value-laden principles written in natural language. Reflect uses in-context reasoning in multiple stages: (i) constitutional pre-conditioning for initial generation, (ii) constitutional self-evaluation post-generation, (iii)(a) self-critique, and (iii)(b) final revision. A simplified example of the Reflect process is diagrammed in Figure¬†1.\n\n\nFigure 1. Given an input query and a constitution of principles, Reflect first generates a constitution-conditioned base response. It then self-evaluates the response before performing critique and revision to generate the final, improved response.\n\n\nUnique from most alignment techniques, Reflect explicitly reasons in natural language over conformance or violation of multiple principles without relying on a numerical reward model. Reflect requires no training data, only a specification of principles in natural language. Furthermore, unlike most prompt-engineering approaches that focus only on the initial prompt engineering¬†(Wei et al., 2022; Lin et al., 2023; Lee et al., 2024b; Khan et al., 2025), Reflect also uses post-generation principled self-critique and revision. Post-generation inference-time compute techniques for self-correction have previously been studied for mathematical reasoning, coding, and factual question-answering, (Madaan et al., 2023; Shinn et al., 2023; Kim et al., 2023; Huang et al., 2024a). We show that such techniques can be applied to substantially improve model alignment to a constitution of principles. The effectiveness of post-generation principled self-critique and revision highlights a key insight of Reflect: instruction-tuned LLMs are better at identifying when their generations violate principles than they are at generating principle-aligned responses in the first place, likely due to the capabilities of in-context learning under causal attention in the Transformer architecture¬†(Vaswani et al., 2017; Brown et al., 2020).\n\n\nExpanding on these observations, we show that Reflect substantially improves constitutional alignment even for advanced models like GPT-4¬†(Achiam et al., 2023), Claude 3¬†(Anthropic, 2024), and Mistral¬†(Jiang et al., 2023). We focus on a multi-objective evaluation of conformance to the stated principles, rather than general benchmarks of human preference¬†(Dubois et al., 2024; Li et al., 2024a, b) and find that Reflect is particularly effective at substantially reducing the rate of serious principle violations. Such violations are rare and in the tail of the distribution of generations ‚Äì they may not significantly affect scores on standard benchmarks. However, from a safety, risk, and compliance perspective, mitigating such violations of alignment principles is nonetheless crucial.\n\n\nFinally, we show that Reflect creates its own synthetic training dataset (for traditional "
  },
  {
    "title": "Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data",
    "url": "https://arxiv.org/abs/2601.18728v1",
    "source": "arxiv",
    "summary": "Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further do",
    "full_text": null
  },
  {
    "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
    "url": "https://arxiv.org/abs/2601.18724v1",
    "source": "arxiv",
    "summary": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations a",
    "full_text": null
  },
  {
    "title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning",
    "url": "https://arxiv.org/abs/2601.18722v1",
    "source": "arxiv",
    "summary": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language",
    "full_text": "\n\n\n\n1 Introduction\n2 SP3F: Self-Play with Privileged Pairwise Feedback for Multilingual Reasoning\n3 Experiment Setup\n\n4 SP3F Unlocks Data-Efficient Multilingual Reasoning\n\n4.1 SP3F Improves Lower-Resource Language Reasoning\n4.2 SP3F Improves Multilingual Reasoning\n4.3 Privileged Information Aids LLM Judges\n\n\n5 Related Work\n6 Conclusion\nA Training Hyperparameters\nB Evaluated Language\nC Full Table Results\n\nD Prompts\n\nD.1 Pairwise Judge Prompts\nD.2 System Messages\n\n\n\n\n\n\n\n\nGained In Translation: \nPrivileged Pairwise Judges Enhance Multilingual Reasoning\n\n\n\nLintang Sutawika1 ‚ÄÉGokul Swamy2 ‚ÄÉZhiwei Steven Wu3 ‚ÄÉGraham Neubig1\n1Carnegie Mellon University, Language Technologies Institute \n2Carnegie Mellon University, Robotics Institute \n3Carnegie Mellon University, Software and Societal Systems Department \n{lsutawik, gswamy, zstevenwu, gneubig}@cs.cmu.edu\n\n lintangsutawika/sp3f ¬† ¬† ¬†  neulab/sp3f\n\n\n\n\nAbstract\nWhen asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English.\nIn response, we introduce SP3F (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without any data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion Swamy et al. (2024), with the judge receiving the English reference response as privileged information. Thus, even when none of the model‚Äôs responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, SP3F greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than 1/81/8 of the training data across the single-language, multilingual, and generalization to unseen language settings.\n\n\nFigure 1: \nWe propose SP3F: Self-Play with Privileged Pairwise Feedback: a method for training multilingual reasoning models without any data in the target language(s). SP3F-7B out-performs Qwen2.5-7B-Instruct across 4 tasks with roughly 1/81/8 of the training data (125,000125,000 for SP3F-7B vs. 1,000,0001,000,000 for Qwen2.5-7B-Instruct), both in terms of accuracy and language fidelity (did the model answer in the target language?).\n\n\n\nFigure 2: \nThe second stage of the SP3F pipeline is to perform RL (GRPO, Shao et al. (2024)) with feedback from verifiable rewards Lambert et al. (2025) and a pairwise judge. To aid in its judgments, the judge LLM is given access to\nprivileged information in the form of an English reference response. Concretely, we sample NN responses from the model (left), ask the privileged judge to pick a winner from each pair (center), and then use the average win-rate of each response against the other N‚àí1N-1 samples as the reward for RL (right, Swamy et al. (2024)).\n\n\n\n\n1 Introduction\n\nCurrent reasoning large language models (RLMs) are trained on data (e.g., chains of thought, CoTs) that is primarily in English Ghosh et al. (2025). This means that when an RLM is asked the same question in a non-English language, it often exhibits dramatically lower performance than if it were asked the question in English (Yong et al., 2025; Muennighoff et al., 2023; Shi et al., 2022; Tam et al., 2025).\n\n\nImproving reasoning performance in lower resource languages (e.g., Indonesian, Swahili, Bengali) is challenging as we lack large amounts of data in the target language for supervised fine-tuning (SFT), and the base model‚Äôs probability of generating the correct answer might be so low that getting positive signal for reinforcement learning (RL) to succeed is computationally challenging. Furthermore, for reasoning tasks, outcome-level verifiable rewards (Lambert et al., 2025) that consider just the final answer provide only indirect supervision on the CoT, making exploration challenging due to the sparsity of feedback (Kakade, 2003). Put together, we face a cold start problem we can‚Äôt easily offline fine-tune our way out of.\n\n\nIn response, we propose SP3F (Self-Play with Privileged Pairwise Feedback): a two-stage framework for increasing reasoning performance in non-English target language(s) that doesn‚Äôt require any data in the target language(s). First, we apply SFT on translated versions of English reference responses to raise our RLM‚Äôs probability of generating correct answers. Second, we perform RL with a combination of verifiable rewards (e.g., answer correctness, language fidelity) and preference feedback from an LLM judge (Zheng et al., 2023). The LLM judge directly supervises the CoT of the RLM, giving it a direction of improvement even when it can‚Äôt produce a correct final answer, which helps mitigate the cold start issue of lower-resource language reasoning.\n\n\nWhile conceptually promising, the noisiness of the feedback provided by LLM judges makes incorporating them into RL training challenging. First, if the LLM judge itself is unfamiliar with a lower-resource language, it may be unable to provide accurate feedback. In response, we provide the English reference response as privileged information Vapnik and Vashist (2009) to the judge, asking it to merely pick which of the two RLM responses more closely aligns with the English reference response. This is an easier translation task than judgment in the abstract. We find that the use of privileged information improves judgment quality.\n\n\nSecond, due to their pretraining on vast swathes of internet text, LLMs often exhibit intransitive (i.e., cyclic) preferences where they might rank A‚âªBA\\succ B, B‚âªCB\\succ C, and C‚âªAC\\succ A (Xu et al., 2025). Such intransitivity means no scalar reward function can faithfully represent the judge‚Äôs preferences, making standard reward modeling fundamentally misspecified. Rather than fitting an inconsistent reward model, we adopt a self-play style approach that optimizes pairwise preferences directly: after sampling a batch of candidate responses, we use the judge to compare all pairs and assign each response a score equal to its empirical win rate. This aggregation converts pairwise judgments into a learning objective that reliably improves the model despite intransitivity (Swamy et al., 2024). Put together, we propose to use privileged pairwise judges to provide denser feedback to the RLM during RL.\n\n\nOur key insight is that we can use English reference responses during both SFT and RL by framing both learning problems in terms of translation. We use reference responses as data for translation during SFT and as privileged information for the pairwise judge during downstream RL.\nOur contribution is three-fold:\n\n\n\n\n1.\n\nWe introduce SP3F: a multi-step framework for increasing reasoning performance in a target language without data in said language. We find that RLMs trained via SP3F out-perform fully post-trained models on both in-domain math and out-of-domain non-math tasks in a target language.\n\n\n\n2.\n\nWe apply SP3F on data from 18 languages, producing a model that out-performs fully post-trained models using 18\\frac{1}{8} as much training data. We outperform Qwen2.5-7B-Instruct across math and on-math reasoning tasks. We find particularly large improvements on lower-resource languages and see better generalization to unseen languages.\n\n\n\n3.\n\nWe perform an in-depth exploration of the benefits provided by privileged information. We find that privileged information is particularly helpful with reducing the intransitivity of the judge model, as well as in improving detection of correct reasoning chains, even if the final answer is incorrect.\n\n\n\n\n\n\n\n\nModel\nOverall\nMGSM\nMT Math100\nBelebele\n\n \n\n\nGlobal MMLU\n\nLite\n\n\n\n\nAcc\nLang\nAcc\nLang\nAcc\nLang\nAcc\nLang\nAcc\nLang\n\n\nQwen2.5-7B\n14.79\n78.78\n22.15\n90.67\n21.16\n58.22\n7.52\n80.39\n8.34\n85.85\n\n\n‚ÄÇ‚ÄÉ+ SFT\n21.70\n82.11\n33.66\n91.37\n26.72\n58.26\n12.94\n89.18\n13.48\n89.62\n\n\n‚ÄÇ‚ÄÉ‚ÄÉ+ RLVR\n57.79\n96.09\n65.34\n99.75\n44.50\n86.10\n68.18\n98.73\n53.15\n99.78\n\n\nSP3F-7B\n61.91\n95.35\n72.50\n99.38\n56.84\n82.93\n67.54\n99.65\n50.76\n99.45\n\n\nQwen2.5-7B-Instruct\n55.87\n89.21\n66.36\n98.38\n52.12\n65.66\n56.79\n96.59\n48.20\n96.21\n\n\n‚ÄÇ‚ÄÉ+ Translate Test\n57.01\n85.98\n66.15\n95.81\n60.08\n59.34\n48.09\n92.27\n53.73\n96.49\n\n\n\nTable 1: Across in-domain math tasks (MGSM and MT Math100) and out-of-domain tasks non-math tasks (Belebele and Global MMLU Lite) over a subset of 18 languages (Table¬†6) that were used to train SP3F-7B, we see SP3F-7B consistently outperforms the Qwen2.5-7B-Instruct. We measure performance in percentage by Accuracy (Acc) and Language Fidelity (Lang). Highest score presented in bold and second highest underlined. Notably, SP3F-7B required only 18\\frac{1}{8} as much data to post-train Qwen2.5-7B-Instruct. Full results in Appendix C.\n\n\n\n\n\n2 SP3F: Self-Play with Privileged Pairwise Feedback for Multilingual Reasoning\n\nIn this section, we begin by describing SP3F in detail. SP3F is a two-step framework for improving reasoning performance in a target language without data in said language.\nSP3F only requires English reference responses, which can be relatively easily generated by a teacher model (e.g., o1 (Jaech et al., 2024), R1 DeepSeek-AI et al. (2025)).\n\n\nBelow, we use x‚ààùí≥x\\in\\mathcal{X} to refer to prompts/questions and y‚ààùí¥y\\in\\mathcal{Y} to refer to responses, with y‚ãÜy^{\\star} referring to an (English) reference response. We assume access to a dataset ùíü\\mathcal{D} of (x,y‚ãÜ)(x,y^{\\star}) pairs. Each response yy consists of a chain-of-thought z‚ààùíµz\\in\\mathcal{Z} and response a‚ààùíúa\\in\\mathcal{A} (i.e., y=(z,a)‚ààùí¥=ùíµ√óùíúy=(z,a)\\in\\mathcal{Y}=\\mathcal{Z}\\times\\mathcal{A}). We search over policies œÄ‚ààŒ†‚äÜ{ùí≥‚ÜíŒî‚Äã(ùí¥)}\\pi\\in\\Pi\\subseteq\\{\\mathcal{X}\\to\\Delta(\\mathcal{Y})\\}. We use ‚àò\\circ to denote the concatenation of two strings and ùóçùóë‚Äã(‚ãÖ)\\mathsf{tx}(\\cdot) to denote translation into the appropriate target language. There are two stages of the SP3F pipeline: "
  }
]