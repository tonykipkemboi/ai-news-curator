[
  {
    "title": "PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning",
    "url": "https://arxiv.org/abs/2602.03846v1",
    "source": "arxiv",
    "summary": "We develop a continual learning method for pretrained models that \\emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \\emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, ",
    "full_text": null
  },
  {
    "title": "Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing",
    "url": "https://arxiv.org/abs/2602.03845v1",
    "source": "arxiv",
    "summary": "Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically elicitin",
    "full_text": "\n\n\n\n1 Introduction\n\n2 2D Probing: Dynamics and Principles\n\n2.1 2D Probing as a Diagnostic Interface\n\n2.2 Observations From 2D Probing\n\nObservation 1: The Non-Monotonicity of Width-Depth Scaling.\nObservation 2: The Heterogeneity of Reasoning Branch Lengths.\nObservation 3: The Early Stabilization of Global Consensus.\nThe Need for Global Control.\n\n\n\n2.3 Principles for Efficient Parallel Control\n\nPrinciple 1: Joint Optimization of Width and Depth.\nPrinciple 2: Adaptive Pruning of Divergent Branches.\nPrinciple 3: Consensus-Driven Early Termination.\n\n\n\n\n\n3 Parallel-Probe: Online Control for Parallel Thinking via Probing\n\nConsensus-based early stopping.\nDeviation-based branch pruning.\nWarmup Stage.\nFinal Prediction.\n\n\n\n4 SCOUT: Sequential &amp; Concurrent Offline Utilization Testbed\n\n4.1 Data Collection\n\n4.2 Simulation Protocol\n\nOpen Source Contribution.\n\n\n\n\n\n5 Experimental Setups\n\n5.1 Models\n\n5.2 Evaluation Benchmark\n\nDatasets.\nMetrics.\n\n\n5.3 Baseline Methods\n\n\n\n6 Results and Analysis\n\n6.1 Main Results\n6.2 Scaling with Inference Budget\n6.3 Ablation Studies\n6.4 Hyperparameter Sensitivity\n\n\n\n7 Related Work\n\n7.1 Efficient Parallel Reasoning\n7.2 Efficient Sequential Reasoning\n7.3 Test-Time Scaling\n\n\n8 Conclusion\n\nA Detailed experimental setups and addtional results.\n\nA.1 Experimental setups of Figure 2(a)\nA.2 Experimental setups of Figure 2(b)\nA.3 Experimental setups of Figure 2(c)\n\n\n\n\n\n\n\n Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing\n\n\nTong Zheng\n\n  \nChengsong Huang\n\n  \nRunpeng Dai\n\n  \nYun He\n\n  \nRui Liu\n\n  \nXin Ni\n\n  \nHuiwen Bao\n\n  \nKaishen Wang\n\n  \nHongtu Zhu\n\n  \nJiaxin Huang\n\n  \nFurong Huang\n\n  \nHeng Huang\n\n\n\nAbstract\nParallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width–depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width–depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce Parallel-Probe, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to 35.8% and total token cost by over 25.8% while maintaining competitive accuracy.\n\nMachine Learning, ICML\n\n\nCode: https://github.com/zhengkid/Parallel-Probe\nOnline Judge Platform: Efficient Reasoning Online Judge\n\n\n\n\n1 Introduction\n\nParallel thinking has emerged as a promising paradigm for improving LLM reasoning by exploring multiple reasoning trajectories in parallel and aggregating them (e.g., via voting, selection, or summarization)  (Comanici et al., 2025; Zheng et al., 2025; Wen et al., 2025). By maintaining multiple candidate reasoning trajectories, it reduces the brittleness of single-chain reasoning, where early mistakes can easily compromise the entire reasoning process (Wang et al., 2022a; Zheng et al., 2025). Moreover, parallel thinking is also hardware-friendly: it naturally aligns with modern GPU parallelism, enabling high-throughput batched decoding (Rodionov et al., 2025; Hsu et al., 2025; Yang et al., 2025c). However, this paradigm often requires massive token generation (Fu et al., 2025b), e.g., token usage nearly scales with the number of parallel branches, thereby posing significant challenges to efficiency.\n\n\nTo improve efficiency, previous work studies efficient reasoning at test time. The majority of the research investigates early-stopping strategies for sequential generation (e.g., extended Chain-of-Thought), leveraging signals such as confidence (Fu et al., 2025b), hidden states (Li et al., 2026), or answer convergence (Liu and Wang, 2025; Zhang et al., 2025b). Since these approaches focus on the internal state of individual trajectories, they ignore critical global information across branches (e.g., consensus), making them sub-optimal in parallel thinking settings. Meanwhile,\nseveral studies have explored adaptive sampling to reduce the inference cost of self-consistency (Mao et al., 2025; Aggarwal et al., 2023; Wan et al., 2025; Fu et al., 2025b; Huang et al., 2025). Since these methods rely on sequential control loops, they transform parallel sampling into a semi-sequential process. Consequently, even though sample efficiency is improved, the increased latency cancels out the speed advantage. Efficient parallel thinking in an online setting has received limited attention, particularly the simultaneous launch of multiple paths.\n\n\nThe fundamental challenge lies the intrinsic independence of parallel decoding threads, where each branch evolves without regard for the progression of others. This isolation leads to suboptimal resource allocation and decoding of redundant trajectories. This raises a pivotal question: Can we introduce lightweight global signals to facilitate efficient, hardware-friendly parallel thinking?\n\n\nFigure 1: Overview of the Parallel-Probe framework.\nIt monitors NN parallel reasoning branches via continuous 2D probing.\n(1) Divergence Pruning: Outlying trajectories that drift from the global majority (e.g., Branch 4) are aggressively pruned to save compute.\n(2) Stability Stopping: The global controller halts the entire ensemble once the consensus stabilizes, preventing the execution of redundant post-convergence steps (dashed area).\nCrucially, Parallel-Probe is model-agnostic and compatible with various off-the-shelf LLMs. We evaluate Performance, Cost Efficiency, and Latency Efficiency across 0.6B and 1.7B models. Values are averaged across all datasets and normalized such that the best-performing method on each axis equals 1.0. Parallel-Probe (blue) achieves the largest coverage area, demonstrating a superior balance between high accuracy and computational efficiency compared to SC and ESC methods. \n\n\nTo bridge this gap, we introduce 2D Probing, a black-box interface that periodically injects an end-of-think token to elicit intermediate answers from each branch during decoding. This constructs a 2D probing matrix with intermediate answers, defined by branch index (width) and probing period (depth). Such a probing matrix enables fine-grained monitoring of reasoning trajectories.\nTo analyze these dynamics, we develop SCOUT (Sequential &amp; Concurrent Offline Utilization Testbed), an evaluation platform designed to rapidly assess different strategies using pre-sampled data.\n\n\nUsing SCOUT, we discover three simple but important insights that explain why standard per-trajectory early stopping is suboptimal for online parallel thinking: (i) Scaling is non-monotonic: Accuracy depends heavily on how width and depth are balanced, not just the total token budget (Figure 2 (a)); (ii) Lengths of reasoning branches are highly uneven (Figure 2 (b) and Figure 7). (iii) Consensus stabilizes early: Early majority votes are often unstable and inaccurate, but they converge to a reliable consensus long before all branches terminate (Figure 2 (c)).\n\n\nGuided by these insights, we propose Parallel-Probe, a training-free controller designed to optimize online parallel thinking through two complementary mechanisms along both dimensions. This aligns with Insight (i). Figure 1 (left) illustrates the working mechanism. Motivated by Insight (ii) and (iii), we first design Consensus-based Early Stopping, which uses the consensus of parallel branches to verify sequential stability, terminating generation once the period-wise majority answer becomes stable. Meanwhile, to further prevent long-tail token waste, we implement Deviation-based Branch Pruning, which conversely uses global trends to identify deviating paths, dynamically removing outliers.\n\n\nWe validate Parallel-Probe across three benchmarks and multiple models. The results demonstrate that our method consistently achieves a superior Pareto frontier with better accuracy–efficiency trade-off compared to strong baselines. Specifically, Parallel-Probe reduces sequential tokens, which is a proxy for latency by more than 30% and total token cost by over 20% compared to Self-Consistency (SC) (Wang et al., 2022a), while maintaining competitive accuracy. As illustrated in Figure 1 (right), our approach consistently dominates competing methods across performance, latency-aware efficiency, and cost efficiency dimensions, highlighting the effectiveness of global probing-based control for efficient online parallel thinking.\n\n\nFigure 2: Analysis of Model Performance and Dynamics. Detailed experimental setups and additional examples for subfigures (a), (b), and (c) are provided in Appendix A.\n(a) AIME24 performance of Qwen3-0.6B across varying branch numbers and lengths. The accuracy is measured via Majority Voting. Red lines indicate fixed total token budgets (branch length ×\\times number of branches), ranging from 32​K32\\mathrm{K} to 256​K256\\mathrm{K}.\n(b) Answer convergence behavior for a representative AIME25 question using Qwen3-4B across different probing steps. Red denotes the group corresponding to the correct answer at each step, while other colors represent distinct incorrect answer groups.\n(c) Convergence patterns across different models and datasets. We report the convergence onset ratio, defined as the probing step at which the final majority answer first becomes consensus over the maximum branch length.\n\n\n\n\n2 2D Probing: Dynamics and Principles\n\nStandard parallel thinking is not able to observe a"
  },
  {
    "title": "Investigating Quantum Circuit Designs Using Neuro-Evolution",
    "url": "https://arxiv.org/abs/2602.03840v1",
    "source": "arxiv",
    "summary": "Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility. Current approaches, whether using manually designed circuit templates, fixed heuristics, or automated rules, face limitations in scalability, flexibility, and adaptability, often producing circuits that are poorly matc",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.03840v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Neural and Evolutionary Computing\n    \n\n    \n      arXiv:2602.03840v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 3 Feb 2026]\n    Title:Investigating Quantum Circuit Designs Using Neuro-Evolution\n    Authors:Devroop Kar, Daniel Krutz, Travis Desell            View a PDF of the paper titled Investigating Quantum Circuit Designs Using Neuro-Evolution, by Devroop Kar and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility. Current approaches, whether using manually designed circuit templates, fixed heuristics, or automated rules, face limitations in scalability, flexibility, and adaptability, often producing circuits that are poorly matched to the specific problem or quantum hardware. In this work, we propose the Evolutionary eXploration of Augmenting Quantum Circuits (EXAQC), an evolutionary approach to the automated design and training of parameterized quantum circuits (PQCs) which leverages and extends on strategies from neuroevolution and genetic programming. The proposed method jointly searches over gate types, qubit connectivity, parameterization, and circuit depth while respecting hardware and noise constraints. The method supports both Qiskit and Pennylane libraries, allowing the user to configure every aspect. This work highlights evolutionary search as a critical tool for advancing quantum machine learning and variational quantum algorithms, providing a principled pathway toward scalable, problem-aware, and hardware-efficient quantum circuit design. Preliminary results demonstrate that circuits evolved on classification tasks are able to achieve over 90% accuracy on most of the benchmark datasets with a limited computational budget, and are able to emulate target circuit quantum states with high fidelity scores.\n    \n\n    \n    \n              \n          Comments:\n          Submitted to The Genetic and Evolutionary Computation Conference (GECCO) 2026. Under Review\n        \n\n          Subjects:\n          \n            Neural and Evolutionary Computing (cs.NE); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2602.03840 [cs.NE]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.03840v1 [cs.NE] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.03840\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Devroop Kar [view email]          [v1]\n        Tue, 3 Feb 2026 18:57:39 UTC (462 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Investigating Quantum Circuit Designs Using Neuro-Evolution, by Devroop Kar and 2 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.NE\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLa"
  },
  {
    "title": "Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL",
    "url": "https://arxiv.org/abs/2602.03839v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fractio",
    "full_text": null
  },
  {
    "title": "PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization",
    "url": "https://arxiv.org/abs/2602.03838v1",
    "source": "arxiv",
    "summary": "In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film's possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To addres",
    "full_text": null
  },
  {
    "title": "Accelerating Scientific Research with Gemini: Case Studies and Common Techniques",
    "url": "https://arxiv.org/abs/2602.03837v1",
    "source": "arxiv",
    "summary": "Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models,",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.03837v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2602.03837v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 3 Feb 2026]\n    Title:Accelerating Scientific Research with Gemini: Case Studies and Common Techniques\n    Authors:David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni            View a PDF of the paper titled Accelerating Scientific Research with Gemini: Case Studies and Common Techniques, by David P. Woodruff and 33 other authors\n    View PDF\n\n\n\n    \n            Abstract:Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google&#39;s Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a &#34;neuro-symbolic&#34; loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.03837 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.03837v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.03837\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: David Woodruff [view email]          [v1]\n        Tue, 3 Feb 2026 18:56:17 UTC (3,746 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Accelerating Scientific Research with Gemini: Case Studies and Common Techniques, by David P. Woodruff and 33 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators t"
  },
  {
    "title": "AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations",
    "url": "https://arxiv.org/abs/2602.03828v1",
    "source": "arxiv",
    "summary": "High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.03828v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.03828v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 3 Feb 2026]\n    Title:AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations\n    Authors:Minjun Zhu, Zhen Lin, Yixuan Weng, Panzhong Lu, Qiujie Xie, Yifan Wei, Sifan Liu, Qiyao Sun, Yue Zhang            View a PDF of the paper titled AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations, by Minjun Zhu and 8 other authors\n    View PDF\n\n\n\n    \n            Abstract:High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in this https URL.\n    \n\n    \n    \n              \n          Comments:\n          Accepted at the ICLR 2026\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV); Digital Libraries (cs.DL)\n        \n          Cite as:\n          arXiv:2602.03828 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.03828v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.03828\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yixuan Weng [view email]          [v1]\n        Tue, 3 Feb 2026 18:41:43 UTC (9,271 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations, by Minjun Zhu and 8 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.CL\n        cs.CV\n        cs.DL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n        "
  },
  {
    "title": "Robust Intervention Learning from Emergency Stop Interventions",
    "url": "https://arxiv.org/abs/2602.03825v1",
    "source": "arxiv",
    "summary": "Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal.",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Preliminaries\n\n3.1 Problem Statement\n3.2 Maximum Entropy Objective\n3.3 State-Action Visitation\n3.4 Residual Q-Learning as a Fine-tuning Primitive\n\n\n\n4 An Algorithm for RIL\n\n4.1 Emergency Stop Interventions\n4.2 RL Fine-Tuning for Interventions\n4.3 RIFT: Residual Intervention Fine-Tuning\n\n\n\n5 Provable Policy Improvement\n\n5.1 Advantage Difference\n5.2 Visitation Difference\n5.3 State-Based Strategies\n\n\n\n6 Experiments\n\n6.1 Experimental Setup\n6.2 Benefit of Regularization\n6.3 Ablating Regularization Strength\n6.4 Failure Cases\n6.5 Termination vs Truncation\n\n\n7 Discussion\n\nA Proofs\n\nA.1 Bijections\nA.2 Theorem 4.1\nA.3 Characterization of Ψ​(π)\\Psi(\\pi)\nA.4 Theorem 5.1\nA.5 State-Based Strategy\n\n\n\nB Experiments\n\nB.1 Training Details\nB.2 Termination vs. Truncation\nB.3 Prior Policy Distribution\nB.4 Half Cheetah Environment\nB.5 Bipedal Walker Environment\nB.6 Failure Cases\n\n\n\n\n\n\n\nRobust Intervention Learning from Emergency Stop Interventions\n\n\nEthan Pronovost\n\n  \nKhimya Khetarpal\n\n  \nSiddhartha Srinivasa\n\n\n\nAbstract\nHuman interventions are a common source of data in autonomous systems during testing.\nThese interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete.\nWe define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal.\nIn the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance.\nWe study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy.\nBy framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task.\nWe provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail.\nOur experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nDeep learning methods have enabled substantial progress in robotic control and decision making.\nReinforcement learning (RL) approaches can achieve strong performance when a well-specified reward function and accurate simulators are available (Schulman et al., 2017; Haarnoja et al., 2018). In many real-world settings, however, neither assumption holds. Reward functions are often under-specified or difficult to design, and simulators may fail to capture real-world dynamics (Jiang et al., 2024). These gaps can lead to brittle or unsafe behavior when policies are deployed outside of controlled training environments (Pan et al., 2022).\n\n\nImitation learning provides an alternative by leveraging expert demonstrations. Offline and online imitation learning methods have been shown to be effective across a range of domains (Pomerleau, 1988; Ziebart et al., 2008; Ho and Ermon, 2016; Fu et al., 2017; Garg et al., 2021; Wulfmeier et al., 2024). However, imitation learning is sensitive to both the quality (Belkhale et al., 2023) and coverage of the demonstration data, and collecting high-quality demonstrations can be costly or impractical for complex tasks.\nAs a result, many real-world systems are deployed with policies that are competent but imperfect, and that must be tested under human supervision before being trusted autonomously.\n\n\n\nFigure 1: \nRobust Intervention Learning (RIL) recognizes that interventions are an imperfect signal about how to solve a task. While directly optimizing for avoiding interventions (grey) works well with highly informative interventions, the goal of RIL (blue) is to improve the prior policy (red) under many intervention strategies.\n\n\nA common form of supervision during deployment is human intervention. In domains such as autonomous driving, mobile manipulation, and industrial robotics, a human supervisor monitors policy execution and intervenes when the system behaves unacceptably (Michael et al., 2019; Spencer et al., 2020; Jiang et al., 2024; Liu et al., 2025). These interventions may take the form of taking control, issuing corrective actions, or triggering an emergency stop. Intervention data is appealing because it is easy to collect during deployment and directly targets undesirable behavior. At the same time, intervention signals are typically incomplete. Supervisors often intervene to prevent specific classes of failures such as safety violations, but tolerate a wide range of safe yet suboptimal behavior (Saunders et al., 2017).\nAs shown in Figure 2, safety violations are only one of several potential flavors of suboptimal behavior.\n\n\n\nFigure 2: \nDepiction of hypothetical trajectories with e-stop interventions. In case 1, the intervention occurs to avoid an imminent catastrophic outcome (hitting the wall). In case 2, the intervention occurs because the robot is taking a wrong path, even though there is no immediate danger. In case 3, the intervention occurs because the robot gets stuck, even though the state it’s in is close to those of successful expert demonstrations.\nA human expert might only intervene for one or two of these cases, yet all three are suboptimal.\n\n\n\nThis creates a fundamental challenge. In many settings, avoiding interventions is a necessary condition but not a sufficient one to solve the task. A policy that merely avoids interventions may remain far from optimal, or may exploit gaps in the supervisor’s intervention strategy. Learning algorithms that treat interventions as a complete specification of the task risk producing policies that are safe but ineffective.\n\n\nWe formalize this setting as Robust Intervention Learning (RIL). Let Φ\\Phi denote a class of plausible intervention strategies ϕ:𝒮×𝒜→[0,1]\\phi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1] that reflect a deployment protocol. For example, supervisors may intervene to prevent unsafe behavior without enforcing optimality, or may intervene inconsistently across different failure modes. The goal of RIL is to learn policies that both reduce intervention rates and improve task performance under the true but unknown objective, across a broad range of ϕ∈Φ\\phi\\in\\Phi.\nCrucially, we do not assume that minimizing interventions alone uniquely specifies the desired behavior.\n\n\nFigure 1 illustrates this perspective. When interventions are highly informative and closely aligned with the task, directly optimizing to avoid interventions can succeed. As intervention signals become coarser or more selective, however, the gap between avoiding interventions and solving the task grows. RIL aims to achieve meaningful policy improvement across this spectrum, rather than relying on a narrow assumption about intervention quality. To succeed in this setting, an algorithm must combine intervention feedback with additional sources of task information.\n\n\n\n\nFigure 3: \nExperimental results on the Lunar Lander environment using a prior policy with approximately 50% success rate and two different intervention strategies.\nWithout prior policy regularization (RLIF), the policy forgets the information in the prior policy and needs highly informative interventions to succeed.\nWith prior policy regularization (RIFT), the policy can combine the information in the prior policy with the information from the interventions to achieve a significantly higher success rate.\n\n\n\nIn this work, we take the view that an existing policy can provide such a source of information.\nThe prior policy may be obtained from offline imitation learning, RL in simulation, or heuristic design. While imperfect, it encodes substantial structure about how to act in the environment. Our key insight is that intervention learning should be posed as a fine-tuning problem, where intervention feedback shapes the policy only where it conflicts with this prior, rather than attempting to replace it entirely. We formalize this idea in Section 4.\nHowever, there is no free lunch: we also show in Section 6.4 that if the prior policy does not contain additional information beyond what the interventions provide (either because it is a random policy or because it was pre-trained on the same intervention objective) then performing fine-tuning is not better than solely optimizing to avoid interventions. So long as the prior policy was pre-trained on something other than interventions, it should provide additional information to complement the intervention signal.\n\n\nWe study robust intervention learning in the context of emergency stop (e-stop) interventions (Ainsworth et al., 2019), the simplest and most widely deployed form of human supervision. In an e-stop intervention, the expert hits a “big red button” to make the robot stop (e.g. make a robotic arm remain stationary, decelerate a vehicle to a stop), as described in Algorithm 1. An e-stop provides no corrective action and no explicit preference information. It only indicates that the current behavior should not continue. This setting makes the incompleteness of intervention feedback particularly stark, and serves as a clean testbed for studying robustness to intervention strategy. We formalize emergency stop supervision in Section 4.1 and present our learning algorithm Residual Intervention Fine-Tuning (RIFT) in Algorithm 2.\n\n\nOur approach differs fundamentally from prior work (RLIF (Luo et al., 2024)) that treats interventions as a direct reward signal and as"
  },
  {
    "title": "Preference-based Conditional Treatment Effects and Policy Learning",
    "url": "https://arxiv.org/abs/2602.03823v1",
    "source": "arxiv",
    "summary": "We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditio",
    "full_text": null
  },
  {
    "title": "They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References",
    "url": "https://arxiv.org/abs/2602.03822v1",
    "source": "arxiv",
    "summary": "Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Multimodal Abuse Detection and LVLMs\n2.2 Knowledge Integration and Efficient Adaptation\n\n\n\n3 Methodology\n\n3.1 Problem Formulation\n\n3.2 CROSS-ALIGN+\n\n3.2.1 Stage I: External Knowledge Grounding\n3.2.2 Stage II: Adaptive Contrastive Fine-Tuning\n3.2.3 Stage III: Hierarchical Multistage Reasoning\n\n\n\n\n\n4 Experimental Setup\n\n4.1 Datasets\n4.2 Evaluation Metrics\n4.3 Model and Hyperparameter Configuration\n4.4 Baselines\n\n\n\n5 Results and Analysis\n\n5.1 Efficiency and Computational Overhead\n\n\n\n6 Ablation Studies\n\n6.1 Per-Task Ablation Analysis\n\n\n\n7 Analysis\n\n\n7.1 Stage III Explanation Analysis\n\n7.1.1 Interpretability–Accuracy Correlation\n\n\n7.2 Cross-Model Consistency\n7.3 Robustness under Symbolic and Linguistic Perturbations\n\n\n8 Conclusion\n\n\n\n\n\n\n\\setcctype\nby\n\nThey Said Memes Were Harmless — We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References\n\n\nSahil Tripathi\n\nsahilkrtr@gmail.com\n\nJamia HamdardNew Delhi, India\n\n, \nGautam Siddharth Kashyap\n\ngautam.kashyap@hdr.mq.edu.au\n\nMacquarie UniversitySydney, New South Wales, Australia\n\n, \nMehwish Nasim\n\nmehwish.nasim@uwa.edu.au\n\nThe University of Western AustraliaPerth, Australia\n\n, \nJian Yang\n\njian.yang@mq.edu.au\n\nMacquarie UniversitySydney, New South Wales, Australia\n\n, \nJiechao Gao\n\njiechao@stanford.edu\n\nStanford UniversityCalifornia, United States\n\n and \nUsman Naseem\n\nusman.naseem@mq.edu.au\n\nMacquarie UniversitySydney, New South Wales, Australia\n\n\n(2026)\n\nAbstract.\nMeme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.\n\nMeme abuse detection, multimodal learning, large vision-language models, cultural knowledge grounding, interpretability, contrastive fine-tuning\n\n††journalyear: 2026††copyright: cc††conference: Proceedings of the ACM Web Conference 2026; April 13–17, 2026; Dubai, United Arab Emirates††booktitle: Proceedings of the ACM Web Conference 2026 (WWW ’26), April 13–17, 2026, Dubai, United Arab Emirates††doi: 10.1145/3774904.3792718††isbn: 979-8-4007-2307-0/2026/04††ccs: Computing methodologies Machine learning approaches††ccs: Computing methodologies Computer vision tasks††ccs: Computing methodologies Natural language processing††ccs: Information systems Content analysis and understanding††ccs: Information systems Social networks\n\nFigure 1. Toy Example of Implicit Abuse in Memes.\nIllustrative samples showing how memes encode harmful intent through subtle cross-modal incongruence and cultural symbolism. Although humorous in isolation, cultural context exposes exclusionary or abusive meanings—e.g., the alt-right appropriation of “Pepe the Frog”, gendered subservience, or racially charged imagery—highlighting the need for culturally grounded multimodal reasoning in CROSS-ALIGN+.\n\n\n\n\n1. Introduction\n\nMemes have become one of the most viral forms of communication on the web, shaping online discourse on a wide range of platforms including X (Twitter), Reddit, and TikTok (Shah et al., 2024; Pramanick et al., 2021). Although often humorous, memes are increasingly weaponized to propagate hate, exclusion, and disinformation (see Figure 1) (Kiela et al., 2020; Pramanick et al., 2021), exploiting their ability to encode harmful intent through subtle cross-modal cues. This makes meme-based abuse detection a critical challenge for web-scale content moderation systems (Lin et al., 2024, 2025). Detecting abusive memes requires reasoning about implicit cultural symbolism and cross-modal incongruence (Kiela et al., 2020; Pramanick et al., 2021). For example, a meme featuring Pepe the Frog with the caption “Welcome to our neighborhood” appears harmless in isolation, but cultural context reveals Pepe’s co-optation as an alt-right symbol, transforming the message into exclusionary propaganda (Lynch, 2022). Current systems fail because they cannot ground such visual-textual signals within broader cultural knowledge (Azerbayev et al., 2024).\n\n\nRecent advances in Large Vision-Language Models (LVLMs) (Liu et al., 2023; Bai et al., 2023; Yin et al., 2024) offer strong multimodal understanding, yet remain limited in meme abuse detection due to: (i) cultural blindness —missing symbolic context and cultural references (Sharma et al., 2022; Shah et al., 2024); (ii) boundary ambiguity —confusing satire with abuse (Yang et al., 2023; Aggarwal et al., 2024); and (iii) lack of interpretability —providing opaque predictions without explanations (Zhang et al., 2025; Balazevic et al., 2023). Previous approaches from fusion-based methods (e.g. DISARM (Sharma et al., 2022), MemeCLIP (Shah et al., 2024)) to in-context learning strategies (Balazevic et al., 2023; Zhang et al., 2025), have improved performance, but do not address these limitations systematically (Xuan et al., 2024; Lin et al., 2025).\n\n\nOur Approach. We propose CROSS-ALIGN+ (Cross-modal Social Signal Alignment with External Knowledge &amp; Adaptive Contrastive Reasoning), a three-stage framework that directly mitigates the core limitations of existing LVLM-based methods. Stage I(External Knowledge Grounding), enriches multimodal representations with structured cultural knowledge from ConceptNet (Speer et al., 2017), Wikidata (Vrandečić and Krötzsch, 2014), and Hatebase (Davidson et al., 2017), thereby addressing cultural blindness. Stage II(Adaptive Contrastive Fine-Tuning), leverages parameter-efficient LoRA adapters (Hu et al., 2022) with contrastive objectives to sharpen decision boundaries and reduce boundary ambiguity between satire and abuse. Stage III(Hierarchical Multistage Reasoning), introduces cascaded natural language explanations that enhance interpretability and trustworthiness (Wei et al., 2022; Zhang et al., 2025). In summary, this work makes three main contributions:\n\n\n•\n\nWe present the first framework that systematically integrates external cultural knowledge into LVLM-based meme abuse detection.\n\n\n\n•\n\nWe design a three-stage framework that explicitly aligns each stage with a core limitation: cultural blindness (Stage I), boundary ambiguity (Stage II), and lack of interpretability (Stage III).\n\n\n\n•\n\nWe conduct extensive empirical analysis across five benchmarks and eight LVLMs, showing up to 17% relative F1 improvements and strong gains on culturally grounded abuse patterns.\n\n\n\n\n\n\n\n2. Related Works\n\nOur work is based on two key lines of research: i) detection of multimodal abuse with LVLMs, and ii) approaches to knowledge integration and efficient model adaptation. We position CROSS-ALIGN+ within these areas to highlight the unique gaps it addresses.\n\n\n\n2.1. Multimodal Abuse Detection and LVLMs\n\nEarly multimodal abuse detection methods relied on feature fusion of visual and textual signals (Kiela et al., 2020), but such approaches struggle with implicit abuse where harmful intent emerges from subtle cross-modal incongruence (Aggarwal et al., 2024). More recent models leverage pre-trained LVLMs, such as MemeCLIP (Shah et al., 2024), and specialized designs like DISARM (Sharma et al., 2022) for victim-aware detection or invariant learning for robustness (Yang et al., 2023). While these advances improve performance, they lack the ability to ground interpretations in cultural context, a key factor for understanding symbolic abuse (e.g., political or historical symbols). In-context learning has been explored as a lightweight way to adapt LVLMs without parameter updates (Wei et al., 2022). Extensions such as Hummingbird (Balazevic et al., 2023) and Critic-V (Zhang et al., 2025) show promise for retrieval-augmented prompting and error detection. However, these approaches remain constrained by semantic similarity, often missing cases where abusive meaning depends on culturally loaded symbolism. CROSS-ALIGN+ directly addresses this gap by systematically linking LVLM representations to structured cultural knowledge and aligning decisions with symbolic reasoning.\n\n\n\n\n2.2. Knowledge Integration and Efficient Adaptation\n\nKnowledge integration has proven to be effective in NLP tasks such as factual reasoning (Petroni et al., 2019). For multimodal understanding, LEMMA (Xuan et al., 2024) recently demonstrated that external knowledge can enhance misinformation detection. Yet, systematic integration of cultural knowledge for abuse detection remains underexplored. Resources such as ConceptNet (Speer et al., 2017), Wikidata (Vrandečić and Krötzsch, 2014), and Hatebase (Davidson et al., 2017) provide complementary symbolic knowledge, but prior works have not combined them in a principled pipeline for meme abuse detection. CROSS-ALIGN+ fills this gap by using these resources to contextualize visual-textual signals with cultural grounding. In parallel, parameter-efficient fine-tuning methods such as adapters (Houlsby et al., 2019) and LoRA (Hu et al., 2022) enable model adaptation without prohibitive compute. These techniques have been adopted for multimodal tasks but have rarely been explo"
  },
  {
    "title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
    "url": "https://arxiv.org/abs/2602.03817v1",
    "source": "arxiv",
    "summary": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Log-Linear Fusion, Opinion Pools, and Products of Experts\n2.2 Gating Networks, Mixtures of Experts, and Reliability Estimation\n2.3 Audio Foundation Models for Bioacoustics (Audio-Only Encoders)\n2.4 Spatiotemporal Context in Bioacoustics: Priors vs. Joint Models\n\n\n\n3 Method\n\n3.1 Notation and Evidence Sources\n3.2 Log-Linear Fusion Family\n\n3.3 Adaptive Gating: Reliability and Informativeness\n\nAvoiding Gate Collapse.\n\n\n3.4 Decision-Theoretic Safety and Robustness\n\n\n\n4 Experimental Setup\n\n\n4.1 Datasets and Spatiotemporal Prior\n\nContext model per benchmark.\n\n\n4.2 Audio Encoder and Classification Head\n\n4.3 Fusion Models and Training Procedure\n\nStage 1: Audio-Only Training.\nStage 2: Fixed-Weight Fusion.\nStage 3: Adaptive Gating Fusion.\n\n\n\n\n\n5 Results\n\n\n5.1 Main Benchmark Results (Aggregate)\n\nCBI.\nBirdSet.\n\n\n5.2 Qualitative Analysis: Adaptive Fusion in Practice\n\n\n6 Discussion\nAppendix A1: Additional Results\nAppendix A2: Decision-Theoretical Safety of Gated Fusion\nAppendix A3: When Log-Linear Fusion is Exact (Conditional Independence Case)\nAppendix A4: Empirical Test of Conditional Dependence\nAppendix A5: Spatiotemporal Prior Details\nAppendix A6: Training Configuration and Optimization Details\n\n\n\n\n\n\nAdaptive Evidence Weighting for Audio-Spatiotemporal Fusion\n\n\n\nOscar Ovanger\n\n  \nLevi Harris\n\n  \nTimothy H. Keitt\n\n\n\nAbstract\nMany machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models.\nWe introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor.\nFINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics.\nThe resulting fusion family contains the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback.\nAcross benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation.\nWe achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach.\nCode is available:\nanonymous-repository\n\nbioacoustics, evidence fusion, priors, uncertainty, ecological context\n\n\nFigure 1: Haemorhous mexicanus (North American House Finch), the namesake of our model. \n\n\n\n1 Introduction\n\nEnsemble learning studies how multiple predictive models for the same target variable can be combined to improve performance, robustness, or calibration relative to any single model (Kuncheva, 2004; Kittler et al., 1998). Classical ensemble methods such as bagging, boosting, and stacking typically assume that all predictors are trained on the same underlying data distribution and that ensemble weights or combination rules can be learned jointly with model parameters.\n\n\nIn many modern applications, however, predictive models are pre-trained, heterogeneous, and fixed at inference time. These models may rely on different sources of evidence, be trained on distinct datasets, and operate under incompatible modeling assumptions. As a result, joint retraining or fine-tuning is often infeasible. Instead, the problem becomes one of combining the outputs of fixed predictors in a principled manner.\n\n\nWe consider the common setting in which multiple predictive models provide complementary evidence about a shared latent target.\nLet yy denote a class label, and let xx and ss denote two sources of evidence.\nConditional independence,\n\n\n\nx⟂s∣y,x\\perp s\\mid y,\n\n(1)\n\n\nprovides a useful idealization that motivates multiplicative evidence fusion.\nIf the full generative distributions were available, Bayesian inference would yield\n\n\n\np​(y∣x,s)∝p​(x∣y)​p​(s∣y)​p​(y).p(y\\mid x,s)\\propto p(x\\mid y)\\,p(s\\mid y)\\,p(y).\n\n(2)\n\n\nFINCH adopts the corresponding log-linear fusion form while remaining applicable in regimes where this independence assumption holds only approximately, and deviations can be mitigated through adaptive, bounded weighting.\n\n\nIn practice, however, generative models are often unavailable. Instead, one typically has access only to discriminative predictors pθ​(y∣x)p_{\\theta}(y\\mid x) and pψ​(y∣s)p_{\\psi}(y\\mid s), trained independently. In this case, the posterior can be expressed as\n\n\n\np​(y∣x,s)∝pθ​(y∣x)​p​(x)​pψ​(y∣s)​p​(s)p​(y),p(y\\mid x,s)\\propto\\frac{p_{\\theta}(y\\mid x)\\,p(x)\\,p_{\\psi}(y\\mid s)\\,p(s)}{p(y)},\n\n(3)\n\n\nwhere the marginal distributions p​(x)p(x), p​(s)p(s), and the implied prior p​(y)p(y) are generally unknown. As a result, the true posterior is not directly computable from the discriminative models alone.\n\n\nA common approximation in this setting is to combine discriminative predictors using log-linear models or product-of-experts formulations (Hinton, 2002; Genest and Zidek, 1986). These models operate directly on posterior outputs and define a fused distribution of the form\n\n\n\nlog⁡p​(y∣x,s)=log⁡pθ​(y∣x)+log⁡pψ​(y∣s)−log⁡Z​(x,s),\\log p(y\\mid x,s)=\\log p_{\\theta}(y\\mid x)+\\log p_{\\psi}(y\\mid s)-\\log Z(x,s),\n\n(4)\n\n\nwhere Z​(x,s)Z(x,s) is a normalization constant ensuring ∑yp​(y∣x,s)=1\\sum_{y}p(y\\mid x,s)=1. Throughout the paper, we write fusion rules in unnormalized log-space; the normalization constant is implicitly handled by the final softmax and is therefore omitted when it does not affect comparisons across classes. Log-linear fusion is decision-theoretically justified as a logarithmic opinion pool under mild axioms (Heskes, 1998) and provides a practical surrogate when only discriminative models are available.\n\n\nExisting approaches typically employ fixed or globally learned weights when combining predictors. This implicitly assumes that the relative reliability of each evidence source is constant across the input space. In many applications, this assumption does not hold. The informativeness of a given evidence source may vary substantially across samples, leading to degraded performance or pathological dominance when fixed weights are used.\n\n\nIn this work, we introduce an adaptive log-linear fusion framework that preserves the structure and interpretability of classical product-of-experts models while allowing per-sample modulation of evidence strength.\nWe assume that the constituent predictors are fixed and discriminative, and we do not retrain or recalibrate their parameters.\nInstead, we learn a gating function that estimates the reliability of contextual evidence on a per-sample basis.\nThe resulting fused posterior is defined (up to normalization) as\n\n\n\nlog⁡p~ω​(y∣x,s)=log⁡pθ​(y∣x)+ω​(x,s)​log⁡pψ​(y∣s),\\log\\tilde{p}_{\\omega}(y\\mid x,s)=\\log p_{\\theta}(y\\mid x)+\\omega(x,s)\\,\\log p_{\\psi}(y\\mid s),\n\n(5)\n\n\nwhere ω​(x,s)≥0\\omega(x,s)\\geq 0 is a learned weighting function.\nThe normalized posterior pω​(y∣x,s)p_{\\omega}(y\\mid x,s) is obtained by applying a softmax over yy. This formulation recovers the first model classifier when ω​(x,s)=0\\omega(x,s)=0, bounds the influence of contextual evidence, and enables adaptive fusion without retraining the base models.\n\n\nWe evaluate this framework in the context of bioacoustic species classification by combining a state-of-the-art acoustic classifier with a structured spatiotemporal prior derived from large-scale observational data. Experiments show that adaptive weighting consistently outperforms fixed-weight fusion and audio-only baselines, particularly in regimes where contextual information is heterogeneous or weak in isolation.\n\n\n\n\n2 Related Work\n\nWe organize related work into two themes: (i) theory and mechanisms for combining probabilistic predictors (log-linear pooling, products of experts, and gating), and (ii) bioacoustic applications, distinguishing audio-only foundation encoders from systems that incorporate spatiotemporal context.\n\n\n\n2.1 Log-Linear Fusion, Opinion Pools, and Products of Experts\n\nThe combination of multiple classifiers has been studied extensively in machine learning and statistics (Kuncheva, 2004; Kittler et al., 1998). Classical ensemble methods (e.g., averaging, voting, Bayesian combination) depend on classifier diversity and error correlation, and often learn weights jointly with model parameters or via validation tuning.\n\n\nMaximum entropy models and logarithmic opinion pools formalize classifier fusion as a log-linear aggregation of predictive distributions (Berger et al., 1996; Genest and Zidek, 1986). Product-of-experts models combine distributions by multiplication, yielding sharp posteriors when experts agree and diffuse posteriors when they disagree (Hinton, 2002). Logarithmic opinion pools provide a decision-theoretic justification for weighted log-linear combinations under mild axioms (Heskes, 1998), and log-linear fusion can be viewed as a practical surrogate to Bayesian inference when only discriminative predictors are available (Bishop, 2006).\n\n\nMost prior approaches employ fixed or globally learned weights, implicitly assuming that expert reliability is stationary across the input space. Our work focuses instead on per-sample weighting of contextual evidence while preserving the interpretability of log-linear fusion.\n\n\n\n\n2.2 Gating Networks, Mixtures of Experts, and Reliability Estimation\n\nMixture-of-experts models introduce gating networks that assign input-dependent weights to multiple predict"
  },
  {
    "title": "SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving",
    "url": "https://arxiv.org/abs/2602.03816v1",
    "source": "arxiv",
    "summary": "We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer t",
    "full_text": null
  },
  {
    "title": "Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning",
    "url": "https://arxiv.org/abs/2602.03815v1",
    "source": "arxiv",
    "summary": "Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direc",
    "full_text": null
  },
  {
    "title": "Conformal Thinking: Risk Control for Reasoning on a Compute Budget",
    "url": "https://arxiv.org/abs/2602.03814v1",
    "source": "arxiv",
    "summary": "Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entai",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background: Adaptive early stopping via upper thresholding confidence\n\nReasoning models overthink.\nUncertainty monitoring for adaptive early stopping.\nAdvantages of adaptive thinking.\n\n\n\n3 Related Works\n\nEarly Exit for Chain-of-Thought.\nRisk control for reasoning LLM.\n\n\n\n4 Method\n\nNotation.\nMotivation and Goals.\n\n4.1 Risk of early exiting reasoning\n\nCorrectness loss.\nEfficiency Loss.\n\n\n\n4.2 Early stopping with two thresholding mechanisms.\n\nUpper threshold: stopping on confidence.\nLower threshold: stopping on pessimism.\n\n\n\n4.3 Picking threshold values using validation set (Alg. 1)\n\nPutting it together.\n\n\n4.4 Combining thresholds\n\n4.5 Advantages of the risk-control perspective\n\nWhy these conditions are atypical in practice.\nWhat risk control buys us.\n\n\n\n\n\n5 Empirical validation\n\n\n5.1 Experiment setup\n\nModels and datasets\nReasoning generation\nAnswer and uncertainty signal elicitation\n\n\n5.2 Risk control framework controls risk\n\n5.3 Risk control framework improves efficiency\n\nEnsembling signal improves efficiency\nLower threshold improves efficiency.\n\n\n\n5.4 Ablation study\n\nSize of validation set\nDistribution shift\n\n\n\n\n\nA Extended experiment specifications.\n\n\nA.1 Signal Extraction\n\nConfidence\nEAT\nConfidence\nEAT\nProbe signals.\n\n\n\n\n\nB Risk control and finite-sample correction\n\nB.1 Problem setup\nB.2 Why finite-sample correction is needed\n\nB.3 Calibration methods\n\nNaive (no finite-sample correction).\nUCB: concentration-based upper confidence bound.\nPractical remark (multiple comparisons).\n\n\n\n\nC Ablation study results\n\n\n\n\n\n\nConformal Thinking: Risk Control for Reasoning on a Compute Budget\n\n\nXi Wang*\n\n  \nAnushri Suresh*\n\n  \nAlvin Zhang*\n\n  \nRishi More*\n\n  \nWilliam Jurayj\n\n  \nBenjamin Van Durme\n\n  \nMehrdad Farajtabar\n\n  \nDaniel Khashabi\n\n  \nEric Nalisnick\n\n\n\nAbstract\nReasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases,\nmotivating adaptive reasoning—spending tokens when they improve reliability and stopping early when additional computation is unlikely to help.\nHowever, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute.\nOur framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage).\nGiven a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms.\nFor scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism.\nEmpirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nReasoning LLMs enable test-time scaling:\nSpending more reasoning tokens often yields better\nperformance (deepseekai2025deepseekr1incentivizingreasoningcapability; snell2024scaling).\nHowever, a practical challenge of reasoning LLMs lies in inducing adaptive reasoning behavior that adjusts to instance difficulty—deciding when additional thinking is still useful versus wasteful.\nRecent works propose adaptive thinking mechanisms (wang2025entropylangletextttthinkrangle; yang2025dynamicearlyexitreasoning; fu2025efficientlyscalingllmreasoning) by monitoring the reasoning model’s uncertainty in the answer (e.g. by measuring confidence or entropy), and when uncertainty falls below a pre-defined threshold, the reasoning is halted.\nAdaptive thinking enables instance-dependent token budgets, since the reasoning effort required to reach a confident threshold varies by problem.\nHowever, adaptive thinking does not alleviate the practical challenge of setting reasoning budget; it only converts the problem of setting a token budget into setting a threshold.\nIn fact, setting a threshold could be trickier than setting a token budget since the threshold value often lacks interpretable\nmeaning, and could lie in arbitrary ranges depending on how the uncertainty is computed (Figure. 1, left).\n\n\nIn this paper, we provide a principled framework for setting\nstopping rules for adaptive reasoning.\nIn particular, we leverage a key implication of test-time scaling: any early termination of reasoning introduces a risk of error.\nBased on this observation, we reframe the problem of setting a reasoning budget as choosing an acceptable level of risk, an interpretable quantity that directly interfaces with downstream decision-making pipelines.\nWe utilize an external validation set and distribution-free risk control framework (batesrcps; jazbec2024fast) to automatically map a user-specified risk to the corresponding criteria for terminating the reasoning chain.\n\n\nSpecifically, we delineate two complementary types of risks and their corresponding controlling mechanisms (illustrated in Fig. 2):\nFalse positive risk of thinking the model has the correct answer, controlled by an upper threshold of confidence;\nFalse negative risk of thinking the current (and future) answers will be incorrect, controlled by the lower threshold, a novel mechanism that measures whether the reasoning is making enough progress.\nThese two types of risks and thresholds are related to different sources of inefficiency:\nThe upper threshold reduces wasted tokens after the model has effectively converged (Eq. (8)),\nwhile the lower threshold avoids spending tokens when further reasoning is unlikely to help (Eq. (9)).\n\n\nIn summary, our contributions are: (i) We introduce a collection of loss functions that capture different sources of inefficiency and errors from early stopping, enabling incorporation of distribution-free risk control into reasoning LLMs, a principled approach that allows early stopping (Sec. 4.1);\n(ii) We introduce a novel parametric lower threshold that halts the reasoning when the model is not progressing sufficiently at decreasing uncertainty (Sec. 4.2);\n(iii) We demonstrate that at the same risk level, different approaches exhibit different efficiency, whereas using our upper threshold plus our proposed parametric lower threshold consistently yields efficiency gains (Sec. 5.3).\n\n\nFigure 1: Early-stopping behavior under different target test risks.\nLeft: Threshold values required to achieve a given target test risk vary substantially across early-stopping signals, indicating that threshold selection is signal-dependent.\nRight: The relative efficiency of different early-stopping methods depends on the target test risk, and no single method is uniformly most efficient across all risk levels.\nResults shown here use an upper-threshold-only stopping rule; introducing both upper and lower thresholds would further complicate the risk-threshold mapping.\n\n\n\n\n\n2 Background: Adaptive early stopping via upper thresholding confidence\n\nReasoning models overthink.\n\nRecent reasoning LLMs are post-trained to output an explicit reasoning trace in a delimited format, followed by a final answer:\n\n\n\ny=⟨think⟩​r1:T​⟨/think⟩​a,y\\;=\\;\\langle\\texttt{think}\\rangle\\;r_{1:T}\\;\\langle\\texttt{/think}\\rangle\\;a,\n\n(1)\n\n\nwhere a complete generation yy is composed of: r1:Tr_{1:T} the reasoning segment of length TT (tokens or steps) wrapped inside special beginning/end of thinking tokens: &lt;think&gt; and &lt;/think&gt;, and aa the final answer.\nA common observation is that TT can be much larger than is necessary on many instances (“overthinking”), in turn introducing unnecessary inference cost. In particular, models often continue to reason when a correct answer can already be elicited.\n\n\nFigure 2: Dual-threshold early exit via risk-controlled confidence dynamics.\nWe plot confidence trajectories as a function of token usage under Qwen3-8B on AIME questions.\nLeft: an unsolvable instance, model confidence fluctuates and fails to reach the upper threshold; the reasoning is halted early by the parametric lower threshold, preventing unnecessary token consumption.\nRight: a solvable instance, where confidence steadily increases and crosses the upper threshold, triggering termination once sufficient confidence is achieved.\n\n\n\n\nUncertainty monitoring for adaptive early stopping.\n\nGiven an input xx, let r1:tr_{1:t} denote the partial reasoning trajectory upto tt steps.\nRecent works (wang2025entropylangletextttthinkrangle; yang2025dynamicearlyexitreasoning) propose to adaptively early stop the reasoning by monitoring a scalar confidence/uncertainty signal computed from the partial trajectory:\n\n\n\nst=u​(x,r1:t),s_{t}\\;=\\;{u}\\!\\left(x,r_{1:t}\\right),\n\n(2)\n\n\nwhere a large sts_{t} typically indicates higher confidence (or lower uncertainty, depending on convention). Common choices for sts_{t} are often derived from the statistical properties of tokens generated after ⟨/think⟩\\langle\\texttt{/think}\\rangle, such as their entropy (wang2025entropylangletextttthinkrangle) or confidence (yang2025dynamicearlyexitreasoning).\nNote that raw signals can be noisy or have inconvenient ranges. Transformations such as smoothing, reciprocal, or normalization are often applied before thresholding:\n\n\n\ns~t=g​(s1:t).\\tilde{s}_{t}\\;=\\;g\\!\\left(s_{1:t}\\right).\n\n(3)\n\n\nHere g​(⋅)g(\\cdot) may depend on the history s1:ts_{1:t} (e.g., an exponential moving average) to reduce variance.\n\n\nA canonical early exit policy then halts reasoning at the earliest time the transformed score exceeds a threshold λ\\lambda:\n\n\n\nτ=min⁡{t≥1:s~t≥λ}.\\tau\\;=\\;\\min\\Big\\{t\\geq 1\\;:\\;\\tilde{s}_{t}\\geq\\lambda\\Big\\}.\n\n(4)\n\n\nWe will refer to this as the upper threshold exit mechanism in the rest of the text.\nThe policy then emits the answe"
  },
  {
    "title": "Antidistillation Fingerprinting",
    "url": "https://arxiv.org/abs/2602.03812v1",
    "source": "arxiv",
    "summary": "Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fing",
    "full_text": null
  },
  {
    "title": "Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network",
    "url": "https://arxiv.org/abs/2602.03808v1",
    "source": "arxiv",
    "summary": "Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact",
    "full_text": null
  },
  {
    "title": "Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation",
    "url": "https://arxiv.org/abs/2602.03806v1",
    "source": "arxiv",
    "summary": "Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulate",
    "full_text": "\n\n\n\n1 Introduction\n2 Problem Formulation\n\n3 Cobalt\n\n3.1 Offline Trajectory Collection\n3.2 Online Contextual Bandit Learning\n3.3 Inference\n3.4 Theoretical Analysis\n\n\n\n4 Experiments\n\n4.1 Datasets and Metric\n4.2 Setup\n4.3 Results\n\n\n\n5 Reward Hacking Analysis and Mitigation\n\n5.1 Test Case Perturbation\n5.2 Perturbation Results\n5.3 Hacking Behavior Analysis\n5.4 Mitigation with Cobalt\n\n\n6 Conclusion\nA Proof of Theorem 3.1\nB Related Work\n\nC Implementation Details\n\nC.1 Code Execution Server\nC.2 TACO Data Cleaning\nC.3 LLM Inference\nC.4 RL Hyperparameters\nC.5 Reward Shaping\nC.6 LLM Judge for Error Analysis\n\n\n\nD Additional Results\n\nD.1 Self-Improvement Results in Tables\n\nD.2 Turn-Level Reward Hacking Examples\n\nD.2.1 Hard Coding\nD.2.2 Logic Overfitting\nD.2.3 Semantic Drifting\n\n\n\n\n\n\n\n\n\nBridging Online and Offline RL:\nContextual Bandit Learning\nfor Multi-Turn Code Generation\n\n\nZiru Chen∗\n\n  \nDongdong Chen\n\n  \nRuinan Jin\n\n  \nYingbin Liang\n\n  \nYujia Xie†\n\n  \nHuan Sun†\n\n\n\nAbstract\nRecently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation.\nWhile online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption.\nIn this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL.\nCobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts.\nThen, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation.\nCobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench.\nAlso, we analyze LLMs’ in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue.\nOverall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation.\nOur code and data are available here.\n\n\n\n1 Introduction\n\n\nFigure 1: \nA multi-turn code generation example with four public test cases (two correct and two incorrect) and 16 hidden test cases.\nThe LLM starts with the coding problem and generates a program.\nIt passes all benign public tests and hidden tests, while failing the two perturbed tests.\nAs a result, one of the failed test case is returned as feedback.\nAll other test cases and their pass rates are hidden.\nThe LLM mistakenly follows the feedback and changes its program, which now passes the perturbed test case but only one hidden test case.\n\n\n\nProgramming is no longer confined to writing code line by line.\nWith large language models (LLMs), developers can now describe their intents in natural language and let LLMs generate, revise, and debug programs autonomously.\nEnabled by advances in reasoning (OpenAI et al., 2024; Guo et al., 2025) and self-improving LLMs (Chen et al., 2024; Novikov et al., 2025), this shift has propelled reinforcement learning (RL) to the forefront as a promising way to train LLMs as multi-turn code-generation agents.\n\n\nMany RL methods (Schulman et al., 2017; Shao et al., 2024) typically follow an online learning paradigm, where a model iteratively interacts with the environment based on its latest weights, collects a new batch of experiences, and optimizes its performance on this batch (Sutton and Barto, 2018).\nWhile these online methods show impressive performance, they can be expensive and unstable to train an LLM.\nFor instance, Gehring et al. (2025) trains an 8B multi-turn code generation LLM using 288 GPUs, an amount of resources that is costly and inaccessible outside large companies.\nMeanwhile, Deng et al. (2025) and Xue et al. (2025) report learning collapse and gradient explosion when training LLMs for multi-turn tool-integrated reasoning.\nAs an alternative to online methods, offline RL methods are more cost-effective and stable, but usually yields less performant models due to distributional shifts and lack of exploration (Levine et al., 2020).\n\n\nCan we combine the benefits of online and offline RL?\nIn this paper, we formulate multi-turn code generation as a one-step recoverable MDP (Jain et al., 2025a) and propose contextual bandit learning with offline trajectories (Cobalt) to train self-improving LLMs.\nCobalt first uses a reference LLM to generate multi-turn trajectories, akin to offline RL data collection, and divides them into partial trajectories.\nDuring online learning, Cobalt prompts the LLM to complete each partial trajectory with single-step code generation and samples different programs as its contextual bandit actions.\nThe model is then optimized to maximize the rewards of its generated programs.\nThis way, we decouple the trajectory data generation process from the online training loop, thus improving training efficiency and lowering cost.\nAlso, we conduct a theoretical analysis and show that compared to the online multi-turn RL objective, the stepwise objective of Cobalt gives a linear performance difference bound.\n\n\nMoreover, we augment Cobalt to be more robust to a notorious problem, in-context reward hacking (McKee-Reid et al., 2024; Pan et al., 2024a, b).\nWhen we insert incorrect test case results as noisy observations into the trajectories, we find that LLMs sometimes give up a correct program and modify it for reward hacking (Figure 1).\nWith a systematic analysis, we show that such behaviors consistently exist in open-weight and proprietary LLMs, such as hard coding wrong input-output pairs or violating some problem constraints.\nThus, we hypothesize that these issues are partly induced by RL as an alignment tax (Lin et al., 2024; MacDiarmid et al., 2025; Wen et al., 2025) and augment Cobalt’s offline trajectories with perturbation to improve LLMs’ robustness against inaccurate test cases.\nThis flexibility of modifying trajectory data to guide online learning is another advantage of Cobalt over online RL.\n\n\nThrough comprehensive experiments on TACO (Li et al., 2023) and LiveCodeBench (Jain et al., 2025b), we demonstrate the effectiveness of Cobalt.\nOn LiveCodeBench, Cobalt improves Pass@1 to 31.7 for R1-Distill 8B and 38.5 for Qwen3 8B, yielding absolute gains of 9.0 and 6.2 points over their base models.\nWhen compared to two strong online multi-turn RL baselines based on GRPO and VeRPO (Wang et al., 2026), Cobalt also outperforms both baselines in the multi-turn setting.\nDespite being trained with a limited number of turns (e.g., ttrain≤3t_{\\mathrm{train}}\\leq 3), both models show strong generalization to longer horizons at test time and continue to improve their performance beyond those turns.\nBesides, adding perturbed trajectories that contain incorrect test cases to training effectively mitigates LLMs’ in-context reward hacking, allowing them to maintain robustness against erroneous feedback.\nTogether with our theoretical analysis, these results establish Cobalt as a promising solution for multi-turn code generation.\nLooking forward, contextual bandit learning may be used to train LLMs for other iterative decision-making tasks, such as mathematical reasoning and deep research.\n\n\n\n\n2 Problem Formulation\n\nAs shown in Figure 1, we formulate multi-turn code generation as a Markov decision process (MDP).\nGiven a coding problem statement o0o_{0}, an LLM generates a program and iteratively improves it based on execution feedback from public test cases.\nThe state st=(o0,a0,…,ot)s_{t}=(o_{0},a_{0},...,o_{t}) encodes the interaction history between the LLM and the coding environment.\nWhen the LLM generates a new program ata_{t}, the environment will execute it on all test cases and return one of the failing public tests as the observation ot+1o_{t+1}.\nFollowing prior work (Chen et al., 2024; Han et al., 2025), we do not provide all failed public test cases to resemble realistic settings, such as LeetCode and Codeforces competitions.\nThen, the state will be updated to st+1s_{t+1} by appending ata_{t} and ot+1o_{t+1}, as well as updating the hidden test case pass rate, which is the reward R​(st,at)∈[0,1]R(s_{t},a_{t})\\in[0,1] to be maximized.\n\n\nWhile this formulation is naturally suitable for outcome-based multi-turn RL, there are two significant challenges in this learning paradigm:\n(1) Outcome-based rewards are sparse, making credit assignment across turns difficult, and\n(2) fully online RL repeatedly collects new trajectories after each policy update, which is expensive for LLMs and long-horizon tasks.\nTo alleviate these issues, we first draw on insights from Jain et al. (2025a) that multi-turn code generation is a one-step recoverable process:\n\n\n\nDefinition 2.1.\n\n\n(One-step Recoverability.)\nAn MDP ℳ=(𝒮,𝒜,P,R,γ)\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,R,\\gamma) with horizon TT is one-step recoverable if the advantage function of the optimal policy π∗\\pi^{*}, defined as A∗​(s,a)=Q∗​(s,a)−V∗​(s)A^{*}(s,a)=Q^{*}(s,a)-V^{*}(s), is uniformly bounded for all (s,a)(s,a), i.e., −1≤A∗​(s,a)≤0-1\\leq A^{*}(s,a)\\leq 0.\n\n\n\nAs explained by Jain et al. (2025a), multi-turn code generation satisfies one-step recoverability because the advantage function of an optimal policy is\n\n\n\nA∗​(s,a)=Q∗​(s,a)−V∗​(s)=R​(s,a)−maxa′⁡R​(s,a′).A^{*}(s,a)=Q^{*}(s,a)-V^{*}(s)=R(s,a)-\\max_{a^{\\prime}}R(s,a^{\\prime}).\n\n\n\nSince R​(s,a)∈[0,1]R(s,a)\\in[0,1] for code generation, we have −1≤A∗​(s,a)≤0-1\\leq A^{*}(s,a)\\leq 0.\nIntuitively, this property implies that a suboptimal action has limited negative impact on subsequent steps.\nThus, inspired by one-step recoverability, we train LLMs to greedily optimize each action, reducing the problem from sequential RL to contextual bandit lear"
  },
  {
    "title": "Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF",
    "url": "https://arxiv.org/abs/2602.03805v1",
    "source": "arxiv",
    "summary": "The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, f",
    "full_text": null
  },
  {
    "title": "Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods",
    "url": "https://arxiv.org/abs/2602.03802v1",
    "source": "arxiv",
    "summary": "Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods un",
    "full_text": null
  },
  {
    "title": "Conformal Reachability for Safe Control in Unknown Environments",
    "url": "https://arxiv.org/abs/2602.03799v1",
    "source": "arxiv",
    "summary": "Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines con",
    "full_text": null
  },
  {
    "title": "FullStack-Agent: Enhancing Agentic Full-Stack Web Coding via Development-Oriented Testing and Repository Back-Translation",
    "url": "https://arxiv.org/abs/2602.03798v1",
    "source": "arxiv",
    "summary": "Assisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents. However, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects. Notably, constructing production-level full-stack web applications is far more challenging than only generating fro",
    "full_text": "\n\n\n\n\n1 Introduction\n\nFullStack-Dev\nFullStack-Learn\nFullStack-Bench\n\n\n\n2 FullStack-Agent\n\n\n2.1 FullStack-Dev\n\nPlanning Agent.\nCoding Agents.\n\n\n\n2.2 FullStack-Learn\n\nRepository Back-Translation.\nRepository Augmentation.\nIterative Self-Improvement.\n\n\n\n2.3 FullStack-Bench\n\nFrontend Test.\nBackend Test.\nDatabase Test.\n\n\n\n\n\n3 Experiments\n\n\n3.1 FullStack-Dev Results\n\nTest Settings.\nBaselines.\nResults.\n\n\n\n3.2 FullStack-Learn Results\n\nTraining and Inference Settings.\nResults.\n\n\n\n3.3 Ablation Studies\n\nAnalysis of FullStack-Dev Design.\nAnalysis of FullStack-Learn Data Generation Method.\nAnalysis of Evaluation Reliability.\n\n\n\n\n\n4 Related Work\n\nWebsite Development Agents and Pipelines.\nWebsite Development Benchmarks.\nTraining Methods to Improve Software Development.\n\n\n5 Conclusion\nA Tool Details\nB Increasing the Number of Templates\nC Back-Translation Trajectory Transforming Process\nD Data Filtering Details\nE Baseline Implementation Details\nF Error Analysis\nG Human Annotation Details\nH FullStack-Dev Prompts\nI FullStack-Learn Prompts\nJ FullStack-Bench Prompts\n\n\n\n\n\nFullStack-Agent: Enhancing Agentic Full-Stack Web Coding \nvia Development-Oriented Testing and Repository Back-Translation\n\n\nZimu Lu\n\n  \nHouxing Ren\n\n  \nYunqiao Yang\n\n  \nKe Wang\n\n  \nZhuofan Zong\n\n  \nMingjie Zhan\n\n  \nHongsheng Li\n\n\n\nAbstract\nAssisting non-expert users to develop complex interactive websites has become a popular task for LLM-powered code agents.\nHowever, existing code agents tend to only generate frontend web pages, masking the lack of real full-stack data processing and storage with fancy visual effects.\nNotably, constructing production-level full-stack web applications is far more challenging than only generating frontend web pages, demanding careful control of data flow, comprehensive understanding of constantly updating packages and dependencies, and accurate localization of obscure bugs in the codebase.\nTo address these difficulties, we introduce FullStack-Agent, a unified agent system for full-stack agentic coding that consists of three parts:\n(1) FullStack-Dev, a multi-agent framework with strong planning, code editing, codebase navigation, and bug localization abilities.\n(2) FullStack-Learn, an innovative data-scaling and self-improving method that back-translates crawled and synthesized website repositories to improve the backbone LLM of FullStack-Dev.\n(3) FullStack-Bench, a comprehensive benchmark that systematically tests the frontend, backend and database functionalities of the generated website.\nOur FullStack-Dev outperforms the previous state-of-the-art method by 8.7%, 38.2%, and 15.9% on the frontend, backend, and database test cases respectively.\nAdditionally, FullStack-Learn raises the performance of a 30B model by 9.7%, 9.5%, and 2.8% on the three sets of test cases through self-improvement, demonstrating the effectiveness of our approach.\nThe code is released at https://github.com/mnluzimu/FullStack-Agent.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nAssisting non-expert users to develop complicated web applications based on natural language instructions has become a popular task for Large Language Model (LLM)-powered (Qwen, 2025a; Wang et al., 2025).\nVarious commercial products111https://bolt.new, https://lovable.dev and research studies (Lu et al., 2025a; Wan et al., 2025) have provided solutions to this task through agentic coding, code execution, and GUI-agent–based testing.\nHowever, these systems tend to generate frontend-only websites even when a backend and data storage are needed to fully support the functionality required by user instructions.\nThey often mask the lack of real data flow with fancy visual effects to create an appearance of interactivity.\nFor example, in one generated website, while a form can be submitted and a success notice appears, no data would actually be processed or stored due to the lack of a backend and database implementation.\nAdditionally, many of these methods generate only an HTML file (Xiao et al., 2025a; Si et al., 2025) or a very simple codebase (Lu et al., 2025a), lacking scalability for production environments.\n\n\nHowever, full-stack websites are rather complicated, making it hard to design effective generation methods. There are at least three challenges in building code agents capable of generating production-grade full-stack websites:\n(1) Real-world web development frameworks, such as Next.js and NestJS, involve large, complex codebases, requiring efficient code navigation and accurate localization and correction of obscure errors.\n(2) The complicated workflow of full-stack coding demands long-term reasoning, skillful tool invocation, and expert mastery of web packages–areas in which current backbone LLMs still have considerable room for improvement.\n(3) Evaluating full-stack website generation remains challenging, as existing GUI-agent-based benchmarks such as WebGen-Bench (Lu et al., 2025b) primarily judge UI-level interactions and fail to detect missing or incorrect backend implementations.\n\n\nTo address these challenges, we introduce FullStack-Agent, a unified system for full-stack website generation that aims to close the gap between real-world web development and current agentic approaches.\nIt jointly advances the effectiveness of full-stack development workflows, the agentic coding ability of backbone LLMs, and the comprehensiveness of website evaluation by proposing three tightly coupled components that work together to enable scalable and verifiable full-stack website construction.\nThe three components, covering the agent framework, backbone LLM training, and full-stack evaluation, are detailed below:\n\n\nFullStack-Dev \n\nTo effectively coordinate the complicated development workflow of full-stack website generation, we propose FullStack-Dev, a multi-agent system that takes inspiration from real-world development processes.\nA planning agent, serving as the lead architect, designs the structure of the full-stack website, and delegates frontend and backend plans to the corresponding coding agents.\nThe two coding agents serve as frontend and backend engineers, and are equipped with efficient code editing, shell command execution, and website debugging tools, allowing them to dynamically control the coding process.\nIn particular, the specially designed frontend and backend debugging tools can efficiently locate and correct subtle errors, greatly enhancing the coding agents’ development abilities.\n\n\n\nFullStack-Learn \n\nEven with a powerful agent framework, the agentic coding skills and expert knowledge possessed by the backbone LLM are still crucial to the overall performance of the system.\nTherefore, we introduce FullStack-Learn, a data-scaling and model self-improvement method that generates high-quality agent trajectories through augmentation and back-translation of website repositories collected from GitHub.\nOur artful design combines global planning and information gathering with local code implementation, effectively solving the non-trivial problem of converting a complicated repository into agent trajectories that implement it from scratch.\nThese agent trajectories, generated from real-world codebases and used for supervised fine-tuning, enable the LLMs to learn valuable agentic coding abilities and expert understanding of website development frameworks.\n\n\n\nFullStack-Bench \n\nExisting website evaluation benchmarks such as WebGen-Bench (Lu et al., 2025b) focus on frontend reactions observed by a GUI-agent judge, failing to detect false positive cases that show correct frontend effects yet lack a real backend implementation.\nTo solve this problem, we introduce FullStack-Bench, a full-stack evaluation benchmark that tests frontend, backend, and database functionalities by leveraging agent judges to run multiple carefully constructed test cases on each website.\nDuring frontend and backend tests, database logs are also gathered and evaluated to ensure that the actions are accompanied by adequate interactions with the database.\n\n\nTogether, the three components of the FullStack-Agent system comprehensively address the challenges of production-level full-stack development and significantly improve performance on full-stack code generation.\nExtensive experiments demonstrate the effectiveness of our approach.\nTesting FullStack-Dev with Qwen3-Coder-480B-A35B-Instruct as the backbone LLM on FullStack-Bench results in accuracies of 64.7%, 77.8%, and 77.9% in frontend, backend, and database test cases respectively, outperforming the previous state-of-the-art method by 8.7%, 38.2%, and 15.9%.\nAdditionally, training Qwen3-Coder-30B-Instruct with FullStack-Learn improves its accuracy by 9.7%, 9.5%, and 2.8% in the three sets of test cases respectively, demonstrating the effectiveness of our training method.\n\n\nIn summary, our contributions are as follows:\n\n\n\n\n•\n\nWe introduce FullStack-Dev, a multi-agent full-stack development framework with highly effective coding tools, significantly outperforming the previous state-of-the-art method.\n\n\n\n•\n\nWe propose FullStack-Learn, an iterative self-improvement method that substantially improves the full-stack development ability of the backbone LLM through repository augmentation and back-translation.\n\n\n\n•\n\nWe construct FullStack-Bench, a novel benchmark that comprehensively evaluates the functionalities of the generated full-stack websites.\n\n\n\n\n\n\n\n\n2 FullStack-Agent\n\nIn this section, we introduce FullStack-Agent, a unified system for agentic full-stack development that consists of a multi-agent framework for full-stack generation, an iterative self-improvement pipeline for backbone LLM training, and a comprehensive benchmark for full-stack evaluation.\n\n\nFigure 1: The FullStack-Agent system. It combines a multi-agent development framework equipped with efficient coding and debugging tools (FullStack-Dev), an iterative self-improvement method that enhances LLMs through repository augmentation and back-translation (FullStack-Learn), and a comprehensive benchmark eval"
  },
  {
    "title": "Manifold Random Features",
    "url": "https://arxiv.org/abs/2602.03797v1",
    "source": "arxiv",
    "summary": "We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approxi",
    "full_text": null
  },
  {
    "title": "Understanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity",
    "url": "https://arxiv.org/abs/2602.03794v1",
    "source": "arxiv",
    "summary": "LLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\nInformation-Theoretic Analysis of LLM Reasoning.\nLLM-based Multi-Agent Systems.\nEmpirical Studies of MAS Scaling and Diversity.\n\n\n\n3 Problem Formulation\n\n3.1 LLM-based Multi-Agent Systems\n3.2 Usable Evidence and Information Budget\n3.3 Agent Configuration Types\n\n3.4 Type-Dependent Ceilings Across workflows\n\nParallel interaction.\nSequential interaction.\nFrom ceilings to compute.\n\n\n\n\n\n4 Why Diversity Matters\n\n\n4.1 Effective Channels: From Compute to Usable Evidence\n\nConnecting KK and α\\alpha to recoverable information.\n\n\n\n4.2 KK as the State Variable of MAS Scaling\n\nA direct heterog–homog advantage bound.\nFast-then-slow scaling: the 1−e−α​K1-e^{-\\alpha K} shape.\n\n\n\n4.3 Measuring Effective Channels Without Labels: K∗K^{*}\n\nDefinition.\nInterpretation.\n\n\n\n\n\n5 Experiments\n\n\n5.1 Experimental Setup\n\nTasks.\nModels.\nMAS Workflows.\nDiversity Configurations.\n\n\n5.2 Finding 1: Scaling Homogeneous MAS Exhibits Diminishing Returns\n5.3 Finding 2: Diversity Consistently Beats Scale\n\n5.4 Finding 3: Performance Gains Are Governed by the Number of Effective Channels\n\n5.4.1 High output similarity hinders performance\n\n5.4.2 Diverse Channels improves performance\n\nDiversity increases K∗K^{*}.\nHigher K∗K^{*} leads to better performance.\nMechanistic Decomposition: Kc∗K^{*}_{c} vs. Kw∗K^{*}_{w}.\nThe Empirical Boundary.\n\n\n\n\n5.5 Design Guidelines for LLM-based MAS\n\n\n6 Conclusion\nTheoretical Contributions and Scope.\nLimitations of K∗K^{*}.\nEmpirical Scope.\n\nA Proofs and Technical Details\n\nA.1 Information-Theoretic Preliminaries\nA.2 Finite Information Budget (Upper Bound)\n\nA.3 Parallel Voting: Assumptions and Upper Bounds\n\nA.3.1 Conditional Independence for Parallel Sampling\n\nA.3.2 A Redundancy Identity\n\nImplication: redundancy controls early saturation.\n\n\nA.3.3 Homogeneous Parallel Bound\nA.3.4 Heterogeneous Parallel Bound\n\n\n\nA.4 Sequential Pipelines and Debate: Upper Bounds\n\n\nA.4.1 Maximal Per-Step Contribution\n\nDebate.\n\n\n\n\n\nA.5 Lower Bound via Independent Evidence-Bits Coverage\n\nA.5.1 Evidence Bits Model\nA.5.2 Fractional Coverage by Effective Channels\nA.5.3 Residual Contraction and Saturated Lower Bound\nA.5.4 Heterogeneity Advantage as an α​K\\alpha K Comparison\n\n\n\nA.6 Properties of the Effective Channel Count K∗K^{*}\n\nSetup.\n\n\n\n\n\nB Supplementary Experiments\n\n\nB.1 Closed-Source Model Experiments\n\nKey findings.\n\n\nB.2 Robustness to Embedding Model Choice\nB.3 Is K∗K^{*} More Than a Proxy for Scale and Configuration?\nB.4 Sanity Checks: Are K∗K^{*}–Performance Relations Accidental?\nB.5 Case Study: Heterogeneity Effects Across Models and Workflows\n\n\n\n\n\n\n\nUnderstanding Agent Scaling in LLM-Based Multi-Agent Systems via Diversity\n\n\nYingxuan Yang\n\n  \nChengrui Qu\n\n  \nMuning Wen\n\n  \nLaixi Shi\n\n  \nYing Wen\n\n  \nWeinan Zhang\n\n  \nAdam Wierman\n\n  \nShangding Gu\n\n\n\nAbstract\nLLM-based multi-agent systems (MAS) have emerged as a promising approach to tackle complex tasks that are difficult for individual LLMs. A natural strategy is to scale performance by increasing the number of agents; however, we find that such scaling exhibits strong diminishing returns in homogeneous settings, while introducing heterogeneity (e.g., different models, prompts, or tools) continues to yield substantial gains. This raises a fundamental question: what limits scaling, and why does diversity help?\nWe present an information-theoretic framework showing that MAS performance is bounded by the intrinsic task uncertainty, not by agent count. We derive architecture-agnostic bounds demonstrating that improvements depend on how many effective channels the system accesses. Homogeneous agents saturate early because their outputs are strongly correlated, whereas heterogeneous agents contribute complementary evidence. We further introduce K∗K^{*}, an effective channel count that quantifies the number of effective channels without ground-truth labels. Empirically, we show that heterogeneous configurations consistently outperform homogeneous scaling: 2 diverse agents can match or exceed the performance of 16 homogeneous agents. Our results provide principled guidelines for building efficient and robust MAS through diversity-aware design. Code and Dataset are available at the link: https://github.com/SafeRL-Lab/Agent-Scaling.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have achieved remarkable performance across diverse tasks, including reasoning, coding, and open-domain question answering (Wei et al., 2022; Achiam et al., 2023). However, individual LLMs still struggle with complex problems that require multi-step reasoning, diverse perspectives, or complementary expertise (Huang et al., 2023). To address these limitations, LLM-based multi-agent systems (MAS) have emerged as a promising paradigm. By orchestrating multiple LLM agents through communication, coordination, or aggregation mechanisms, MAS can tackle challenges that are difficult for single models (Wu et al., 2024; Hong et al., 2024; Du et al., 2023). Recent studies have demonstrated that multi-agent collaboration can yield substantial improvements over single-agent baselines on tasks ranging from software engineering (Qian et al., 2024) to scientific reasoning (Guo et al., 2024).\n\n\nFigure 1: \nEffect of model diversity. We compare a mixture of three LLMs (Qwen-2.5-7B, Llama-3.1-8B, Mistral-7B) with the average of independent single-LLM runs.\n\n\n\nFigure 2: \nScaling behavior of homogeneous multi-agent voting. Success rate versus agent count N on seven tasks for three base models. Performance improves with N but saturates, indicating clear diminishing marginal gains at larger agent counts.\n\n\n\nGiven the effectiveness of multi-agent systems, a natural question arises: can we improve MAS performance simply by scaling the number of agents? Intuitively, one might expect ensemble-style gains from aggregating more agent outputs (Li et al., 2024a; Wang et al., 2023). However, recent work (Kim et al., 2025) and our experiments reveal a more nuanced picture. As shown in Figure 2, scaling homogeneous agents (identical models, prompts, and configurations) exhibits strong diminishing returns: accuracy improves at small agent counts, but the marginal gain per additional agent, rapidly collapses toward zero. This suggests that simply adding more homogeneous agents (or allocating more test-time compute) does not reliably introduce new usable evidence into the system, but may instead produce increasingly redundant trajectories.\n\n\nIn contrast, our experiments (Figure 1) show that introducing diversity yields sustained performance improvements. Here, diversity broadly refers to heterogeneity in agent configurations, such as backbone models, prompts or personas, and tool access, which empirically leads to more complementary, rather than redundant, information being introduced into the system. As a result, diverse systems can outperform homogeneous ones even with substantially fewer agent calls (Wang et al., 2024a; Zhang et al., 2024; Qian et al., 2025). Motivated by these observations, we ask: what fundamentally limits scaling, and why does diversity help?\n\n\nWe hypothesize that the primary bottleneck arises from correlation among agent outputs. Higher correlation induces greater redundancy, reducing the number of effective channels and leading to performance saturation (Chen et al., 2024; Choi et al., 2025). This intuition is illustrated in Figure 3, where heterogeneous agents provide complementary coverage and better information processing diversity compared to homogeneous systems (Yuen et al., 2025; Tang et al., 2025). To formalize this intuition, we develop an information-theoretic framework that characterizes MAS performance in terms of effective channels, the number of independent, non-redundant reasoning paths present in agent outputs, rather than raw agent count. For example, two agents that reason in nearly identical ways contribute only one effective channel, whereas two agents that follow genuinely different reasoning paths contribute two. Our analysis reveals that performance is bounded by intrinsic task uncertainty, and improvements depend on how many effective channels the system accesses.\n\n\nBased on this framework, we introduce K∗K^{*}, a label-free metric that quantifies effective channels without requiring ground-truth labels. Empirically, we demonstrate that heterogeneous configurations consistently outperform homogeneous scaling: with only 2 diverse agents, we match or exceed the performance of 16 homogeneous agents, achieving significant improvement across seven benchmarks.\n\n\nAlthough diminishing returns in scaling have been observed empirically (Wang et al., 2024b; Kim et al., 2025), a unified theoretical framework explaining why and when this phenomenon occurs across different MAS workflows, such as voting (Wang et al., 2023), debate (Du et al., 2023; Khan et al., 2024), and centralized orchestration (Hong et al., 2024), remains lacking. Existing studies offer limited theoretical insight into how evidence accumulation is affected by agent redundancy as the system scales.\nTo address this gap, we provide a unified information-theoretic explanation for diminishing returns in LLM-based MAS. Our contributions are summarized as follows:\n\n\n•\n\nWe derive architecture-independent performance bounds, demonstrating that MAS effectiveness is constrained by the intrinsic task uncertainty H​(Y|X)H(Y|X), and that improvements arise from increasing the number of effective channels rather than scaling the agent count.\n\n\n\n•\n\nWe analyze representative MAS paradigms (vote, debate), showing that homogeneous configurations quickly saturate due to highly correlated evidence, whereas heterogeneity effectively reduces redundancy and expands the system’s capacity for effective channels.\n\n\n\n•\n\nWe introduce K∗K^{*}, an effective channel count that quantifies the number of non-redundant information sources in agent outputs. We empirically validate that K∗K^{*} tracks performance and provides principled g"
  },
  {
    "title": "WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents",
    "url": "https://arxiv.org/abs/2602.03792v1",
    "source": "arxiv",
    "summary": "Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing pr",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.03792v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Cryptography and Security\n    \n\n    \n      arXiv:2602.03792v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 3 Feb 2026]\n    Title:WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents\n    Authors:Xilong Wang, Yinuo Liu, Zhun Wang, Dawn Song, Neil Gong            View a PDF of the paper titled WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents, by Xilong Wang and 4 other authors\n    View PDF\n\n\n\n    \n            Abstract:Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user&#39;s intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \\emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: this https URL.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Cryptography and Security (cs.CR); Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2602.03792 [cs.CR]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.03792v1 [cs.CR] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.03792\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Xilong Wang [view email]          [v1]\n        Tue, 3 Feb 2026 17:55:04 UTC (179 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents, by Xilong Wang and 4 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CR\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.CL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscribe\n                \n              \n            \n          \n        \n        \n        \n        \n          \n            \n "
  },
  {
    "title": "Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation",
    "url": "https://arxiv.org/abs/2602.03791v1",
    "source": "arxiv",
    "summary": "Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with ",
    "full_text": null
  },
  {
    "title": "Fast Sampling for Flows and Diffusions with Lazy and Point Mass Stochastic Interpolants",
    "url": "https://arxiv.org/abs/2602.03789v1",
    "source": "arxiv",
    "summary": "Stochastic interpolants unify flows and diffusions, popular generative modeling frameworks. A primary hyperparameter in these methods is the interpolation schedule that determines how to bridge a standard Gaussian base measure to an arbitrary target measure. We prove how to convert a sample path of a stochastic differential equation (SDE) with arbitrary diffusion coefficient under any schedule int",
    "full_text": null
  },
  {
    "title": "Inference-time Unlearning Using Conformal Prediction",
    "url": "https://arxiv.org/abs/2602.03787v1",
    "source": "arxiv",
    "summary": "Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often ch",
    "full_text": null
  },
  {
    "title": "AOrchestra: Automating Sub-Agent Creation for Agentic Orchestration",
    "url": "https://arxiv.org/abs/2602.03786v1",
    "source": "arxiv",
    "summary": "Language agents have shown strong promise for task automation. Realizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving. However, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability. We address this challenge with a unified, framework-agnostic agent abstraction",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nMulti-Agent Systems\nSub-Agent as Tools\n\n\n\n3 Methodology\n\n\n3.1 Problem Formulation\n\nSub-agent-as-tools view.\n\n\n\n3.2 AOrchestra\n\nA unified four-tuple agent abstraction.\nAction Space of Orchestrator.\nImplementation of Delegate and Finish.\nAdvantages of AOrchestra\n\n\n\n3.3 Learnable Orchestrator\n\nSupervised fine-tuning (SFT) for task orchestration.\nIterative In-context Learning for Cost-aware Orchestration.\n\n\n\n\n\n4 Experiments\n\n4.1 Experiment Setup\n\n4.2 Main Results\n\nGAIA Results\nTerminal-Bench 2.0 Results\nSWE-Bench-Verified Results\n\n\n\n4.3 Advantage Analysis of AOrchestra\n\n4.3.1 Advantage 1: Context Sharing\n\n4.3.2 Advantage 2: A Learnable Orchestrator\n\nSupervised fine-tuning (SFT) for task orchestration\nIn-context Learning for Cost-aware Orchestration\n\n\n4.3.3 Advantage 3: Plug-and-play Sub-agents\n\n\n\n\n5 Conclusion\n\nA Implemention Details\n\nA.1 Datasets\nA.2 SFT Hyper-parameters.\nA.3 Baseline Implementations\n\n\n\nB Prompts\n\n\nB.1 Main Agent Prompts\n\nB.1.1 GAIA Main Agent Prompt\nB.1.2 Terminal-Bench Main Agent Prompt\nB.1.3 SWE-Bench Main Agent Prompt\n\n\n\nB.2 Sub-Agent Prompts\n\nB.2.1 GAIA Sub-Agent Prompt\nB.2.2 Terminal-Bench Sub-Agent Prompt\nB.2.3 SWE-Bench Sub-Agent Prompt\nB.2.4 Sub-Agent Summary Prompt\n\n\n\nB.3 Learning Prompt\n\nB.3.1 STRATEGY OPTIMIZE PROMPT\nB.3.2 STRATEGY SELECT PROMPT\n\n\n\n\n\nC Case Study\n\n\nC.1 GAIA Case Study\n\n\nC.1.1 Case Overview\n\nLong-Horizon Support and Context Stability.\nStrong Error Recovery.\n\n\nC.1.2 Detailed Case\n\n\n\n\n\nD Tools And Action Space\n\n\nD.1 Main Agent Action Space\n\ndelegate_task.\ncomplete.\n\n\n\nD.2 Sub-Agent Tools For each Benchmark\n\nD.2.1 GAIA Tools\nD.2.2 Terminal-Bench Tools\nD.2.3 SWE-Bench Tools\n\n\n\nD.3 Sandbox and Network Constraints\n\nCode Execution Sandbox.\nNetwork Constraints.\nTerminal-Bench Sandbox.\nSWE-Bench Sandbox.\n\n\n\n\n\nE Third-Party API and Model Pricing\n\nE.1 Third-Party APIs\nE.2 Model Pricing Table and Cost Accounting\n\n\n\n\n\n\n\n\nAOrchestra: Automating Sub-Agent Creation for Agentic Orchestration\n\n\nJianhao Ruan\n\n  \nZhihao Xu\n\n  \nYiran Peng\n\n  \nFashen Ren\n\n  \nZhaoyang Yu\n\n  \nXinbing Liang\n\n  \nJinyu Xiang\n\n  \nBang Liu\n\n  \nChenglin Wu\n\n  \nYuyu Luo\n\n  \nJiayi Zhang\n\n\n\nAbstract\nLanguage agents have shown strong promise for task automation.\nRealizing this promise for increasingly complex, long-horizon tasks has driven the rise of a sub-agent-as-tools paradigm for multi-turn task solving.\nHowever, existing designs still lack a dynamic abstraction view of sub-agents, thereby hurting adaptability.\nWe address this challenge with a unified, framework-agnostic agent abstraction that models any agent as a tuple ⟨I​n​s​t​r​u​c​t​i​o​n,C​o​n​t​e​x​t,T​o​o​l​s,M​o​d​e​l⟩\\langle Instruction,Context,Tools,Model\\rangle. This tuple acts as a compositional recipe for capabilities, enabling the system to spawn specialized executors for each task on demand.\nBuilding on this abstraction, we introduce an agentic system AOrchestra, where the central orchestrator concretizes the tuple at each step: it curates task-relevant context, selects tools and models, and delegates execution via on-the-fly automatic agent creation.\nSuch designs enable reducing human engineering efforts, and remain framework-agnostic with plug-and-play support for diverse agents as task executors. It also enables a controllable performance–cost trade-off, allowing the system to approach Pareto-efficient.\nAcross three challenging benchmarks (GAIA, SWE-Bench, Terminal-Bench), AOrchestra achieves 16.28% relative improvement against the strongest baseline when paired with Gemini-3-Flash. The code is available at: https://github.com/FoundationAgents/AOrchestra\n\n\n\n1 Introduction\n\nHumans handle complex, long-horizon work via collective intelligence and the ability to coordinate (Gao et al., 2025a; Zhu et al., 2025b; Li et al., 2025a). As today’s agents are pushed toward similarly complex and multi-turn tasks (Yao et al., 2024; Zhang et al., 2025a; Xu et al., 2026), a well-designed agentic system becomes a vital way to scale performance beyond a single model (Liu et al., 2025b).\n\n\nFigure 1: Overall performance on three challenging agentic benchmarks (GAIA, Terminal-Bench-2, SWE-Bench-Verified) paired with Gemini-3-Flash when comparing AOrchestra against other popular agentic frameworks.\n\n\nTo cope with increasingly complex scenarios, early attempts rely on fixed coordination workflows or multi-agent systems (Hong et al., 2023; Hu et al., 2025; Li et al., 2025a).\nWhile multi-agent collaboration can improve task decomposition, in open-ended environments it often incurs substantial coordination overhead and provides limited control over context routing, leading to either noisy over-sharing or harmful omission of critical information, which makes robust long-horizon execution difficult (Gao et al., 2025b).\n\n\nFigure 2: Comparison of sub-agent-as-tools approaches. (a) Sub-agents as context-isolated threads mitigate context rot but lack on-demand specialization. (b) Sub-agents as static roles provide specialized capabilities but are inflexible, leave coverage gaps, and require heavy human engineering. (c) Our Sub-agents as on-demand specialization concretizes a unified 4-tuple abstraction (Instruction, Context, Tools, Model) to enable creating tailored executors on the fly.\n\n\nMore recent approaches therefore move toward a more practical sub-agent-as-tools paradigm, where a main agent (orchestrator) delegates a task to a sub-agent via an explicit tool call.\nYet existing designs still lack flexibility in practice and often degenerate into two limited patterns, which are shown in Figure 2:\n(1) Sub-agents as context-isolation threads. Systems such as  Schroeder et al. (2025); Sun et al. (2025) primarily treat sub-agents as isolated context threads, aiming to prevent context rot (Hong et al., 2025). However, in real-world tasks, subtasks often require specialized capabilities. Therefore, such systems fail to fully realize the potential of specialized sub-agents. (2) Sub-agents as static roles.\nSystems such as  Anthropic (2025); Li et al. (2025c) treat each sub-agent as a static role, and their capabilities or their coordination patterns are typically hard‑wired. A pre-defined set of sub‑agents cannot cover the dynamically emerging variety of subtasks in open environments. Besides, it relies on heavy human engineering, making the system difficult to adapt to various environments.\n\n\nIn this paper, we introduce AOrchestra, an agentic framework designed to tackle long-horizon and complex tasks. Our core insight lies in treating sub-agents through the lens of on-demand specialization, as illustrated in Figure 2(c).\nWe posit that a sub-agent should be viewed as a flexible abstraction unit rather than a predefined, fixed role. This approach enables the system to instantiate tailored sub-agents at runtime by dynamically composing their capabilities to meet specific task demands—an essential feature.\nConcretely, any agent can be described as an instantiable unit via a unified four-tuple: (instruction, context, tools, model). This specialization is organized around two complementary axes essential for an agent’s task solving: (1) Working memory (instruction, context): what the agent must achieve and what evidence it should condition on. Notably, the context attribute is designed to inject only the most relevant information for the current sub-task, filtering out potentially distracting details. (2) Capabilities (tools, model): what the agent is empowered to do to accomplish that objective. By composing specific tools and models on a per-subtask basis, we endow each sub-agent with precise, task-specific functionality. Together, this 4-tuple design enables an automatic specialized sub-agent for each task.\n\n\nBuilding on this on-demand specialization view, we further introduce a dedicated orchestrator that operates directly over the four-tuple interface to automatically create tailored sub-agents on the fly. It does not execute any tasks and focuses exclusively on orchestration, where we define it as dynamically decomposing the overall objective into the next subtask, creating and delegating a specialized tailored sub-agent for task execution via explicit tool calls.\nThis decoupling design offers several key advantages.\nFirst, this dynamic creation allows each sub-agent to be customized with unique capabilities and a clean working context, significantly improving task execution accuracy.\nSecond, the orchestrator remains agnostic to the internal implementation of sub-agents, making them fully pluggable.\nThird, the orchestrator can be trained or learned from interactive experience.\nThis ranges from basic skills for agent creation to advanced features like adaptive model selection, achieving an optimal balance between cost and performance.\n\n\nThrough extensive experiments, we demonstrate AOrchestra achieves stronger performance and broader generalization in open‑world settings.\nWe first evaluate our framework in a training-free setting on three challenging agentic benchmarks: Terminal-Bench 2.0 (Team, 2025) (bash environment), SWE-Bench (Jimenez et al., 2023) (coding environment), and GAIA (Mialon et al., 2023) (digital world environment). As shown in Figure 1, our method consistently outperforms both representative sub-agent orchestration approaches  (Anthropic, 2025) and widely used agent frameworks (Wang et al., 2024; Yang et al., 2024) across all benchmarks. In particular, our framework achieves a 16.28% improvement when paired with Gemini3-Flash, validating the superiority of our orchestration model in complex, long-horizon tasks.\nImportantly, AOrchestra naturally supports learning the orchestration policy from experience.\nWe instantiate this in two ways: (1) we apply supervised fine-tuning to improve the Orchestrator’s subtask decomposition and 4-tuple synthesis, leading to better orchestration quality by +11.51% pass@1 on GAIA and\n(2) we leverage in-context learning to optimize cost-aware routing, which improves GAIA pass@1 by +3.03% while r"
  },
  {
    "title": "Context Compression via Explicit Information Transmission",
    "url": "https://arxiv.org/abs/2602.03784v1",
    "source": "arxiv",
    "summary": "Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-at",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Method\n\n\n3.1 Problem Formulation and Motivation\n\nSelf-attention in LLM-as-a-compressor.\n\n\n\n3.2 ComprExIT\n\nOverview.\n3.2.1 Token Anchors\n3.2.2 Compression Slots\n3.2.3 The Compressed Representations\n\n\n3.3 Training Method\n\n\n\n4 Experiments and Analysis\n\n\n4.1 Setup\n\nModels.\nBaselines.\nTraining and evaluation.\n\n\n4.2 Main Results\n4.3 Ablation Studies\n\n4.4 Further Analysis\n\n4.4.1 Coordinated Allocation\n4.4.2 Impacts of Layer selection\n\n\n4.5 Optimization Behavior\n\n\n5 Conclusion and Future Work\n\nA Appendix\n\nA.1 Datasets\nA.2 Model Inference Implementation\n\nA.3 More Experimental Results\n\nA.3.1 More Out-of-Domain Results\nA.3.2 Full NTP Results\nA.3.3 The Allocation Matrix\nA.3.4 A Preliminary Study of Layer Impacts\n\n\n\nA.4 Training\n\nA.4.1 Training Details\n\n\n\n\n\n\n\n\n\nContext Compression via Explicit Information Transmission\n\n\nJiangnan Ye\n\n  \nHanqi Yan\n\n  \nZhenyi Shen\n\n  \nHeng Chang\n\n  \nYe Mao\n\n  \nYulan He\n\n\n\nAbstract\nLong-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key–value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens.\nWe propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model’s internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information.\nAcross six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ∼\\sim1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.\n\nLarge Language Models, LLM Efficiency, Context Compression\n\n\n\n1 Introduction\n\nFigure 1: Visualization of two structural weaknesses of existing LLM-as-a-compressor methods. (i) Representation overwriting: The information carried by the compression token that captures Year 2012 is gradually overwritten into highly abstract features during the layer-by-layer encoding, leading to information loss for the decoder. (ii) Lack of global coordination: The key token Paris is not attended by the compression tokens due to the lack of global coordination of compression capacity allocation.\n\n\nAs Large Language Models (LLMs) continue to scale, the computational and memory costs associated with long-context processing have become a major bottleneck for practical deployment (Liu et al., 2025a, 2024). Context compression has therefore emerged as an effective technique to reduce the length of input context, lowering inference cost and alleviating the memory footprint of key–value caches during generation (Li et al., 2025a).\n\n\nExisting compression methods can be broadly categorized into hard and soft approaches. Hard compression methods select or prune discrete tokens based on estimated salience (Li et al., 2023; Jiang et al., 2023; Pan et al., 2024), which is efficient but lossy under high compression rate. Soft compression instead compress information from many tokens into a small set of continuous vectors  (Mu et al., 2023; Ge et al., 2024; Li et al., 2025b; Zhang et al., 2025; Deng et al., 2025a). We focus on soft compression in this work for its greater expressive capacity and fine-grained aggregation beyond discrete token boundaries.\n\n\n\nMost existing soft compression methods follow an LLM-as-a-compressor paradigm, where special compression tokens (e.g. gist/memory tokens) are introduced and iteratively updated through self-attention during LLM training in order to absorb information from context into these tokens across layers. After a forward pass, the final-layer hidden states of these compression tokens serve as the compressed context for decoding (Ge et al., 2024; Li et al., 2025b; Zhang et al., 2025). While effective, this design has two structural weaknesses as illustrated in Figure 1.\n\n\nFirst, this layer-by-layer compression is intertwined with the deep Transformer computation and can induce a fundamental representation overwriting issue: as compression tokens are repeatedly updated, information aggregated in earlier layers may be overwritten, while the states in late layers may inherently drift toward highly abstract, generation-specialized features. Consequently, this increases the distribution mismatch between the layer space of the compressor and the decoder, making compression harder to optimize and less robust. Second, compression tokens lack coordinated allocation. Self-attention aggregation in existing LLM-as-a-compressor methods is performed independently across compression tokens, with no mechanism to allocate capacity across tokens. As a result, some regions of context may be redundantly covered while others are underrepresented, leading to inefficient use of the limited compression budget. \n\n\nThese limitations motivate us to view compression from a different perspective. We consider that a forward pass of an LLM already produces informative hidden representations across layers at multiple levels of abstraction, ranging from low-level lexical features to high-level semantic abstractions (Zhang et al., 2024). Therefore, instead of training the LLM to learn how to compress, we propose to keep the LLM frozen and perform compression directly over these cached hidden states. This decouples compression from the LLM’s internal self-attention dynamics, avoiding progressive representation drift and uncoordinated allocation.\n\n\nTo realize this new paradigm, we propose ComprExIT, which formulates soft context compression as explicit information transmission over frozen LLM hidden states. The method performs compression along two orthogonal dimensions. First, depth-wise (inter-layer) transmission selectively aggregates layer-wise information into token anchors, mitigating progressive overwriting and reducing the mismatch induced by late-layer outputs. Second, width-wise (token-to-slot) transmission aggregates NN token anchors into KK slots under a globally optimized transmission plan, explicitly coordinating how information is distributed across slots, resulting in the final compression tokens.\n\n\nComprExIT is lightweight yet highly effective, adding only about 1% additional parameters. It achieves state-of-the-art performance across both in-domain and out-of-domain evaluation benchmarks, matching (or even surpassing) an uncompressed fine-tuned model. In summary, we make the following contributions:\n\n\n•\n\nWe identify two key structural limitations in existing soft context compression paradigms: (i) the representation overwriting and distribution mismatch induced by the layer-by-layer compression process (ii) the lack of global coordination in the information aggregation of compression tokens.\n\n\n\n•\n\nWe propose a new formulation of soft compression as explicit information transmission over frozen LLM hidden states, decoupling compression from internal self-attention and enabling stable, controllable aggregation. Based on the new paradigm, we introduce ComprExIT, a lightweight framework that performs coordinated depth-wise and width-wise transmission.\n\n\n\n•\n\nExtensive experiments on six QA benchmarks demonstrate that ComprExIT consistently outperforms prior state-of-the-art compression methods.\nWe provide comprehensive analyses and ablation studies to understand ComprExIT, offering insights into its behavior, and key design choices.\n\n\n\n\n\n\n\n2 Related Work\n\nFigure 2: A comparison between existing LLM-as-a-compressor methods (left) and ComprExIT (right). Existing methods introduce gist tokens that are iteratively encoded by the self-attention layers in the LLMs, which are trained to aggregate information from context tokens and align the representations to the decoder’s input space. ComprExIT instead leverages the hidden states of the context tokens encoded in a forward pass. The hidden states across layers are selectively aggregated into token anchors, which are then transmitted to the compression tokens through a coordinated transmission plan. \n\n\nHard compression. methods reduce long-context cost by pruning input tokens according to estimated importance. Representative approaches include SelectiveContext (Li et al., 2023) and LLMLingua-style methods (Jiang et al., 2023, 2024), which score tokens/spans using a language model (e.g., via self-information or related salience measures) and retain only the most informative parts of the context. LLMLingua-2 (Pan et al., 2024) instead distills a lightweight classifier from a stronger LLM (e.g., GPT-4) to decide which tokens to keep. EFPC (Cao et al., 2025) further unifies task-aware and task-agnostic compression within a single framework. Despite their efficiency gains, hard methods remain discrete and lossy, often facing bounded compression ratios and limited expressiveness compared with soft compression in continuous space.\n\n\nSoft context compression. provides greater flexibility than discrete token removal under high compression ratios. Wingate et al. presented an early attempt to compress context tokens into a single vector, focusing on representation learning. AutoCompressor (Chevalier et al., 2023) bu"
  },
  {
    "title": "Efficient Estimation of Kernel Surrogate Models for Task Attribution",
    "url": "https://arxiv.org/abs/2602.03783v1",
    "source": "arxiv",
    "summary": "Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing e",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries\n\n3 Methods\n\n3.1 Analyzing surrogate modeling in the attribution space\n3.2 Learning kernel surrogate models\n3.3 Efficient estimation via projected gradients\n\n\n\n4 Experiments\n\n4.1 Experiment setup\n4.2 Experimental results\n4.3 Regularization of Hessian\n\n\n5 Related Work\n6 Conclusion\n\nA Complete Proofs\n\nDerivation of influence functions.\nNotations.\nA.1 Proof of Proposition 3.1\nA.2 Proof of Corollary 3.2\nA.3 Gradient estimation algorithms\nA.4 First-order approximation error\nA.5 Expressivity of RBF kernels\n\n\n\nB Omitted Experiments\n\nAttribution baselines.\n\nB.1 Omitted experimental results\n\nQualitative results.\nCorrelation between surrogate models and influence functions.\nNonlinear numerical simulation.\n\n\n\nB.2 Ablation studies\n\nHyperparameters analysis.\nComparison of kernels.\n\n\n\nB.3 Extensions and discussions\n\nDiscussions.\n\n\n\n\n\n\n\n\n\nEfficient Estimation of Kernel Surrogate Models for Task Attribution\n\n\nZhenshuo Zhang\nNortheastern University\nEmail Address: {zhang.zhens, duan.mi, ho.zhang}@northeastern.edu.\n  \nMinxuan Duan*\nNortheastern University\n\n  \nHongyang R. Zhang*\nNortheastern University\n\n\n\nAbstract\nModern AI agents such as large language models are trained on diverse tasks—translation, code generation, mathematical reasoning, and text prediction—simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task’s performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate surrogate estimates with less than 2%2\\% relative error without repeated retraining. Experiments across multiple domains—including mathematical reasoning in transformers, in-context learning, and multi-objective reinforcement learning—demonstrate the effectiveness of kernel surrogate models. They achieve a 25%25\\% higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines, enabling more accurate and scalable task attribution. When used for downstream task selection, kernel surrogate models further yield a 40%40\\% improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.\n\n\n\n1 Introduction\n\nModern AI systems are trained to perform well across a diverse set of tasks, ranging from machine translation and code generation to object recognition and mathematical reasoning.\nThis broad, multi-task capability raises a central question in interpretability: how do individual training tasks contribute to model performance?\n\n\nIn this work, we study task attribution, the problem of quantifying the influence of each training task on downstream behavior.\nBeyond being conceptually fundamental, task attribution has direct implications across several application domains.\nIn multi-task learning, understanding task relationships can guide the design of neural architectures and loss-reweighting strategies (Wu et al., 2020; Yang et al., 2025).\nIn multi-group learning (Deng &amp; Hsu, 2024), it can reveal how training on different demographic groups shapes model behavior.\nIn in-context learning (Garg et al., 2022; Zhang et al., 2025), it parallels the question of how adding or removing a single demonstration affects predictions Min et al. (2022).\nRelated questions also arise in multi-objective reinforcement learning (Yu et al., 2020; Zhang et al., 2026), where one wants to understand how competing reward signals influence learned policy.\nAccurate task attribution can also help understand when the representations generated by pre-trained large-scale models can be used in downstream tasks (Wu et al., 2023).\n\n\nA natural estimator of task influence is leave-one-out (LOO) retraining, which measures the change in performance when one task is withheld from training.\nHowever, if the training set contains KK tasks, computing LOO scores requires K+1K+1 full training runs, which is prohibitive.\nMore efficient proxies for the LOO scores use influence functions (Koh &amp; Liang, 2017; Grosse et al., 2023), which estimate how model predictions would change if a training sample were added or removed.\nThese methods require evaluating Hessian-vector products, and are usually applied at the terminal state once the model has memorized the training data (Brown et al., 2021).\nHowever, computing influence scores for all KK tasks still requires repeatedly Hessian approximations (Basu et al., 2021; Kwon et al., 2024).\nA complementary line of work—datamodels (Ilyas et al., 2022) and data attribution (Park et al., 2023; Bae et al., 2024)—builds surrogate functions that approximate black-box model behaviors.\nThese methods treat the training pipeline as a function mapping data (or task) subsets to predictions, and learn a separate surrogate to emulate the mapping. Once trained, the surrogate provides a faster mechanism for evaluating how subset combinations contribute to a given test distribution.\nRecent work has primarily focused on linear surrogate models, which enjoy appealing theoretical properties, as they can capture first-order effects and admit linear sampling complexities in multi-task settings (Li et al., 2023; 2024a; Zhang et al., 2025).\nHowever, linear surrogates fail to capture second-order interactions where task effects are non-additive (See illustration in Figure 2).\n\n\nIn this paper, we revisit the surrogate modeling objective through a unified task-weighting framework.\nWe begin by analyzing linear surrogate models using a second-order Taylor expansion in the weight space.\nWe find that linear surrogate models estimated via minimizing the surrogate modeling objective are approximately equal to the influence functions, up to third-order expansion errors (Proposition 3.1).\nIn particular, when the second-term expansion error is small, then linear surrogates and influence functions are approximately the same (Corollary 3.2).\nA key technical tool involves applying the delta method to certain covariance statistics in the surrogate regression.\n\n\nMotivated by these observations, we propose kernel surrogate models (KernelSM), which extend surrogate modeling beyond the additive structure inherent in linear methods.\nKernel surrogates— such as those based on radial basis function (RBF) kernels—naturally capture geometric relationships in the 0-11 subset space. A direct kernel estimation, however, would require computing model outcomes on many task subsets.\nInstead, we design an efficient estimation that uses gradients as features, leveraging a first-order approximation of model outputs in its parameters.\nEmpirically, we find that the accuracy of this first-order approximation is under 1%1\\% relative error across diverse datasets, including CIFAR-10, modular arithmetic, in-context learning, and multi-objective RL benchmarks, on models ranging from small MLP classifiers to language models with up to 34B parameters. Theoretically, we bound the error of this estimation assuming the errors are small (Proposition A.2).\n\n\nWe evaluate the kernel surrogate modeling framework across a broad range of datasets and architectures. On modular arithmetic reasoning tasks—where the input-output mapping can be XOR, division, quadratic—kernel surrogates improve attribution accuracy by up to 42%42\\% relative to existing approaches (Li et al., 2023; Ilyas et al., 2022; Park et al., 2023).\nWe also observe similar gains on in-context learning and sequential decision-making tasks.\nApplied to the Qwen3-8B model on sentiment classification and mathematical reasoning, kernel surrogates improve attribution accuracy by 18%18\\% over prior methods. In reinforcement learning with soft actor-critic on the Meta-World MT10 benchmark (Yu et al., 2020), our method provides 5%5\\% improvement on environments with dynamically shifting data distributions.\nOverall, kernel surrogates consistently outperform prior methods across all settings, increasing correlation with leave-one-out estimates by 25%25\\% relative to existing attribution methods.\nFinally, for downstream task selection, our approach yields 40%40\\% lower loss in both LLM inference (demonstration selection for in-context learning) and multi-objective optimization on Meta-World, while using runtime comparable to fitting linear surrogate models.\n\n\nTaken together, this paper provides an efficient estimation algorithm for the task-attribution problem via kernel surrogate models.\nFirst, we analyze linear surrogate models, showing that influence functions and the estimated regression coefficients are approximately the same under certain assumptions.\nSecond, we introduce KernelSM, along with an efficient gradient-based estimation via kernel ridge regression.\nThus, our approach is both scalable and flexible for modeling nonlinear task relationships in black-box systems.\nThird, we evaluate KernelSM in a variety of empirical settings including in-context learning, mathematical reasoning, and reinforcement learning.\nWe provide the code to replicate our empirical results at https://github.com/VirtuosoResearch/Kernel-surrogate-models.\n\n\n\n\n2 Prel"
  }
]