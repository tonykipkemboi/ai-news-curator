[
  {
    "title": "Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment",
    "url": "https://arxiv.org/abs/2602.12281v1",
    "source": "arxiv",
    "summary": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We f",
    "full_text": null
  },
  {
    "title": "UniT: Unified Multimodal Chain-of-Thought Test-time Scaling",
    "url": "https://arxiv.org/abs/2602.12279v1",
    "source": "arxiv",
    "summary": "Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and makin",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Works\n\n3 Method\n\n3.1 Multimodal Chain-of-Thought Data\n3.2 Training and Inference\n3.3 Budget Forcing for Test-time Scaling\n\n\n\n4 Experiments\n\n4.1 Experiment Settings\n4.2 Compositional Generation and Editing\n4.3 Multi-Turn Editing\n4.4 Visual Reasoning\n4.5 Qualitative Results\n\n\n\n5 Discussion\n\n5.1 Sequential vs. Parallel Scaling\n5.2 Ablation on Cognitive Behaviors\n5.3 Data Quality Analysis\n5.4 Failure Cases\n\n\n6 Conclusion\n\nA Data Synthesis Pipeline\n\nA.1 Pipeline Architecture\nA.2 Model Components\nA.3 Example Trajectory\nA.4 Training Data Statistics\nA.5 VLM Prompt Design\n\n\nB Additional Qualitative Results\nC Generalization Preservation\nD Scaling Beyond C=10C{=}10\nE Failure Analysis\n\n\n\n\n\n\n1]Stanford University\n2]Meta Superintelligence Labs\n3]Nanyang Technological University\n\\metadata[Paper]https://ai.meta.com/research/publications/unit-unified-multimodal-chain-of-thought-test-time-scaling\n\\correspondenceLeon Liangyu Chen at\n\nUniT: Unified Multimodal Chain-of-Thought Test-time Scaling\n\n\nLeon Liangyu Chen\n\n‚ÄÉ‚ÄÉ\nHaoyu Ma\n\n‚ÄÉ‚ÄÉ\nZhipeng Fan\n\n‚ÄÉ‚ÄÉ\nZiqi Huang\n\n‚ÄÉ‚ÄÉ\nAnimesh Sinha\n\n‚ÄÉ‚ÄÉ\nXiaoliang Dai\n\n‚ÄÉ‚ÄÉ\nJialiang Wang\n\n‚ÄÉ‚ÄÉ\nZecheng He\n\n‚ÄÉ‚ÄÉ\nJianwei Yang\n\n‚ÄÉ‚ÄÉ\nChunyuan Li\n\n‚ÄÉ‚ÄÉ\nJunzhe Sun\n\n‚ÄÉ‚ÄÉ\nChu Wang\n\n‚ÄÉ‚ÄÉ\nSerena Yeung-Levy\n\n‚ÄÉ‚ÄÉ\nFelix Juefei-Xu\n\n[\n\n[\n\n[\n\nliangyuc@stanford.edu\n\n\n(February 11, 2026)\n\nAbstract\nUnified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge.\nWe introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.\n\n\n\n\n\n\n\nFigure 1: \nMultimodal chain-of-thought enables test-time scaling through emergent cognitive behaviors. We propose the UniT framework for unified multimodal models, which induces subgoal decomposition for compositional tasks and unlocks content understanding and memory for multi-turn editing. Controlling the number of test-time images, chain-of-thought sequential scaling outperforms best-of-N parallel scaling across generation and reasoning benchmarks.  User input \n Model output \n\n\n\n1 Introduction\n\nUnified multimodal models (deng2025emerging; wu2025janus; zhou2024transfusion) aim to merge vision, language, and more modalities into a single architecture capable of both understanding and generation. Unlike modular pipelines where separate models handle perception, verification, and generation, unified models handle all modalities within one coherent conversation, enabling richer cross-modal grounding, continuous contextual tracking, and seamless interleaving of understanding and generation. However, despite this potential, existing unified models still operate mostly in a single-pass mode: they produce an output once, without explicit mechanisms for evaluating, reflecting on, or refining their predictions. This limitation becomes fundamental for tasks that intrinsically require multi-step reasoning and self-correction, such as compositional generation, multi-turn editing, and complex visual reasoning - settings where both humans and AI naturally benefit from extended reasoning.\n\n\nRecent advances in language models have demonstrated that test-time scaling (TTS)‚Äîallocating additional computational resources during inference through extended chain-of-thought reasoning, verification, and iterative refinement (jaech2024openai; guo2025deepseek; snell2024scaling)‚Äîenables substantial performance gains on complex reasoning tasks in mathematics (cobbe2021training), coding (chen2021evaluating), and logic (srivastava2023beyond). Early work on multimodal chain-of-thought has shown similar benefits for single-round visual understanding and generation (visualcot; fang2025got; xiao2025mindomni; huang2025interleaving; chern2025thinking). Yet, test-time scaling for unified multimodal models remains largely unexplored. The challenge is nontrivial: test-time scaling requires capabilities that currently scatter across specialized models (image generation models for generation, vision-language models for verification, image editing models for refinement). Bridging this gap requires a unified framework that systematically integrates data synthesis, model training, and inference mechanisms for multimodal test-time scaling. This motivates the central question:\n\n\nHow to enable scalable multimodal inference that allows unified models to iteratively generate, reflect, and refine?\n\n\nWe introduce UniT, a unified framework for multimodal chain-of-thought test-time scaling. Scalable multimodal inference requires the tight integration of three components: (i) Agentic data synthesis to induce cognitive behaviors through multi-round trajectories. We develop an automated pipeline (Fig. 2) where vision-language models iteratively critique and image editing models refine generated images with explicit chain-of-thought reasoning. This naturally produces training data exhibiting three critical cognitive behaviors (gandhi2025cognitive): verification‚Äîevaluating outputs against instructions; subgoal decomposition‚Äîbreaking complex instructions into sequential planning steps; content memory‚Äîmaintaining understanding of visual content across rounds through unified multimodal context. (ii) Unified model training to enable the model to internalize multimodal reasoning patterns. We collect approximately 12K multi-round trajectories and fine-tune the Bagel unified multimodal model (deng2025emerging) for 700 H100 hours, enabling it to perform both understanding and refinement without switching models. (iii) Multimodal test-time scaling at inference with flexible computational budget. The trained model performs all reasoning, generation, and refinement iteratively through explicit multimodal chain-of-thought thinking, allocating more rounds to more challenging tasks.\n\n\nThe synergy of these components enables the model to act as a single, coherent multimodal reasoner capable of self-evaluation and iterative improvement. The UniT framework exhibits strong test-time scaling behavior (Fig. 1) with emergent capabilities. Most notably, models trained on shorter reasoning trajectories (averaging 3.6 rounds) effectively generalize to longer inference chains at test time (averaging 4.7 rounds) (Fig. 5), echoing patterns previously seen only in text-only models (snell2024scaling). Furthermore, chain-of-thought sequential scaling substantially outperforms best-of-N parallel sampling, achieving comparable performance with 2.5√ó\\times less computational cost (Fig. 1). This demonstrates that iterative refinement with explicit reasoning provides more efficient use of inference compute than parallel sampling. Critically, UniT achieves 5.56% improvement on CompBench multi-object editing, 2.95 human preference scores on ImgEdit multi-turn editing, and 10.34% on OneIG instruction following compared to single-pass generation. Moreover, it improves out-of-distribution visual reasoning on MIRA by 53.33%, establishing multimodal chain-of-thought test-time scaling as a unified paradigm that benefits both generation and comprehension.\n\n\nWe summarize our contributions as follows:\n\n\n‚Ä¢\n\nUnified multimodal test-time scaling. We propose UniT, a unified framework for multimodal chain-of-thought test-time scaling, integrating agentic data synthesis, unified model training, and test-time scaling mechanisms.\n\n\n\n‚Ä¢\n\nEmergent extrapolation to longer reasoning chains. We demonstrate that models trained on shorter trajectories generalize to longer inference chains at test time, extrapolating beyond the training distribution.\n\n\n\n‚Ä¢\n\nBroad improvements across multimodal tasks. UniT achieves substantial gains on compositional generation/editing, multi-turn editing, and visual reasoning, establishing chain-of-thought test-time scaling as a unified paradigm for both generation and understanding tasks.\n\n\n\n\n\nFigure 2: Agentic framework for synthesizing chain-of-thought training data. Starting from a user prompt, an image generation model generates an initial image. A vision-language model then performs verification - evaluating whether the output satisfies the prompt. When unsatisfactory, the VLM engages in explicit subgoal decomposition through thinking tokens, planning concrete improvements, and rewriting editing instructions. This iterative loop continues until verification succeeds, generating multi-turn reasoning trajectories that teach unified models to refine outputs through test-time computation. The explicit reasoning traces of the three models capture how cognitive behaviors emerge from the interplay between generation, verification, and planning. \n\n\n\n\n2 Related Works\n\nTest-time scaling. Test-time scaling allocates additional computation during inference to improve model performance. We distin"
  },
  {
    "title": "AttentionRetriever: Attention Layers are Secretly Long Document Retrievers",
    "url": "https://arxiv.org/abs/2602.12278v1",
    "source": "arxiv",
    "summary": "Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed Attention",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Long Document Retrieval\n2.2 Context Window Length Extension\n2.3 Attention Mechanism Interpretation\n\n\n3 Observations\n\n4 Method\n\n4.1 Overview\n4.2 Attention for Sentence Scoring\n4.3 Sentence Embedding for Multi-view Similarity Search\n4.4 Entity-based Retrieval\n\n\n5 Dataset Construction\n\n6 Experiments\n\n6.1 Experimental Setup\n6.2 Main Results\n\n\n7 Conclusion\n\nA Details of LongBench-v2-Retrieval\n\nA.1 Comparison with Existing Long Document Retrieval Datasets\nA.2 Example Queries\n\n\nB Detailed Attention Analysis\nC Full Evaluation Results on Retrieval Datasets\nD Comparison of Efficiency on Single-document Retrieval Datasets\nE Evaluation Results on Question Answering Datasets\nF Ablation Studies\nG Prompts\n\n\n\n\n\nAttentionRetriever: Attention Layers are Secretly Long Document Retrievers\n\n\n\nDavid Jiahao Fu1,\nLam Thanh Do1,\nJiayu Li1,\nKevin Chen-Chuan Chang1\n1University of Illinois Urbana-Champaign\n\n{jiahaof4, lamdo, jiayul11, kcchang}@illinois.edu\n\n\n\nAbstract\nRetrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.\n\n\n\nAttentionRetriever: Attention Layers are Secretly Long Document Retrievers\n\n\n\n\n\nDavid Jiahao Fu1,\nLam Thanh Do1,\nJiayu Li1,\nKevin Chen-Chuan Chang1\n\n1University of Illinois Urbana-Champaign\n\n{jiahaof4, lamdo, jiayul11, kcchang}@illinois.edu\n\n\n\n\n\n\n1 Introduction\n\nRecent advancements in Large Language Models (LLMs) OpenAI (2023, 2025); Dubey et al. (2024); Jiang et al. (2023) have demonstrated strong capabilities in natural language understanding, but recent studies Liu et al. (2024); Maharana et al. (2024); Lu et al. (2024) have shown that LLMs still struggle to perform well on long document processing tasks due to the lost-in-the-middle problem and limited context window length, while the quadratic complexity of the attention mechanism makes processing long documents very expensive, especially for large models with hundreds of billions of parameters. In recent years, retrieval-augmented generation (RAG) Lewis et al. (2020) techniques have been widely applied to address this issue by selecting relevant information from the document with a retrieval model, which improves performance by removing distracting information and decreases processing time by shortening the input length.\n\n\nIn the RAG pipeline, the retrieval model needs to perform long document retrieval, which requires the model to find a subset D‚Ä≤D^{\\prime} from a user-provided long document DD such that |D‚Ä≤|‚â™|D||D^{\\prime}|\\ll|D| and D‚Ä≤D^{\\prime} is sufficient and necessary to answer the input query qq. However, existing retrieval models are not tailored for long document retrieval and overlook the following three types of dependencies in long documents:\n\n\n\n\n‚Ä¢\n\nContextual dependency. Since long documents are generally coherent, context is often required to resolve issues like coreference and word ambiguity, which are crucial for determining the relevance of chunks. For example, in a document discussing Chicago, the author might use \"the city\" to refer to \"Chicago\", but this reference is clear only if the context is provided.\n\n\n\n‚Ä¢\n\nCausal dependency. The query may involve intermediate answers from the document that are needed to reach the final answer. For the same document about Chicago, an example query would be \"What was the population of Chicago when the Great Fire happened?\", where the intermediate answer \"the Great Fire happened in 1871\" is needed to find the chunk containing the final answer.\n\n\n\n‚Ä¢\n\nQuery dependency. Text chunks providing background information, like the one containing \"the Great Fire happened in 1871\" in the previous example, are also important to answering the query and should be retrieved. However, these chunks might receive low similarity scores because they are not very relevant to the query, which asks about \"the population of Chicago\". Therefore, it is necessary to accurately decide the scope of retrieval in long document retrieval tasks.\n\n\n\n\n\nModeling the first two dependencies requires a more advanced retrieval model that can build context-aware representations and update the embeddings as additional contextual information is available. We found that the attention layers in transformer models perfectly match both requirements. Since attention layers calculate the representations of each token by aggregating information from other tokens, they are essentially cross-encoders that embed contextual information into the representation of each token, providing more abundant semantic information compared to existing embedding-based retrieval models. Furthermore, as the representations are propagated through layers, they are also dynamically adjusted based on contextual information gathered in previous layers to encode causal dependencies.\n\n\nIt is also intuitive to use attention layers as retrievers because the attention operations are essentially calculating similarity scores. In each attention layer of the transformer model, the attention score assigned to the jj-th token by the ii-th token is calculated as the weighted dot product between the key vector kjk_{j} and the query vector qiq_{i}, which is identical to similarity calculation of embedding models. Moreover, attention computation is performed on two sets of embeddings qq and kk, which allows the attention layers to perform a broader range of tasks other than semantic similarity search by adjusting the embeddings.\n\n\nHowever, training a transformer model for retrieval is very expensive. Ye et al. (2025) found that the last layer in the Qwen-2 model shows high retrieval accuracy without any additional training, implying the possibility of directly employing pretrained LLMs to estimate relevance with attention scores. However, their experiments were very limited and the findings might not generalize to other attention layers and other LLMs. More importantly, since pretrained LLMs suffer from accuracy and efficiency issues with long context, it is also crucial to find out whether attention scores can be efficiently calculated and still achieve high retrieval accuracy with long context.\n\n\nTherefore, we conducted careful analysis (Section 3) to verify if attention layers in pretrained LLMs can be effective and efficient training-free retrievers. Our analysis showed that attention scores are more precise than outputs in collecting relevant information and suffer less from the lost-in-the-middle problem, validating the effectiveness of using LLMs for retrieval tasks despite that they face various issues in long context tasks. Moreover, existing attention approximation methods for LLMs can be directly applied to this approach, making it possible to process long documents with arbitrary lengths. Furthermore, we found that employing pretrained LLMs with around 3 billion parameters as retrievers is already able to achieve impressive performance, which eliminates the need for using larger LLMs and improves efficiency.\n\n\nHowever, attention scoring alone is still insufficient to model the third dependency. Background information is still unlikely to receive high attention scores in layers with high retrieval accuracies because it is already embedded into the representations before these layers. To obtain a better estimate of the scope of retrieval, it is essential to additionally consider text chunks that are not immediately relevant but provide background information. Since each piece of background information typically focuses on one entity, and it should be included as part of the retrieval result only if the entity is relevant to the input query, we believe an entity graph structure could help to determine the retrieval scope precisely by connecting text chunks through entities and finding the entities relevant to the query during retrieval to discover hidden background information. In contrast to knowledge graphs, entity graphs are much easier and more efficient to construct, without the need to extract relationships between entities.\n\n\nFigure 1: Overview of AttentionRetriever.\n\n\nBased on these findings and analysis, we proposed AttentionRetriever, a novel retrieval model that builds context-aware embeddings with pretrained LLMs and decides the scope of retrieval through entity-based retrieval, as summarized in Figure 1. During retrieval, we leverage LLMs to process the long document and the query together, adopting the attention maps at layers that show high retrieval accuracies to estimate the relevance score of each text segment, which is combined with embedding-based similarity scoring for more precise retrieval. To decide the scope of retrieval, we find the desired entities by ranking them by the scores of sentences containing the entities. We eventually obtain the final output by collecting all text chunks that contain the highest ranked entities and sentences.\n\n\nTo better evaluate and compare the performance and efficiency of our proposed method and baselines on extremely long documents, we also constructed a new long document retrieval dataset consisting of different types of documents with an average length of over 100,000 words and various types of queries (Section 5). To the best of our knowledge, this is the first retrieval dataset that features documents with lengths exceeding the context"
  },
  {
    "title": "Agentic Test-Time Scaling for WebAgents",
    "url": "https://arxiv.org/abs/2602.12276v1",
    "source": "arxiv",
    "summary": "Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dyn",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nInference-Time Scaling and Test-Time Compute.\nTool-Using Agents and Long-Horizon Tasks.\n\n\n\n3 From Static to Dynamic Inference-Time Scaling\n\n\n3.1 Experimental Setup\n\nAction Clustering and Vote Distributions.\n\n\n\n3.2 Static Baseline: Majority Voting\n\nObservation: Majority Vote Yields Diminishing Returns.\nTakeaway.\n\n\n\n3.3 From Voting to Arbitration\n\nObservation: Arbitration Improves Over Majority Vote.\nArbiter Scaling.\nObservation: Arbitration is not Uniformly Beneficial.\nDeeper Aggregation Methods.\n\n\n3.4 DeepConf-Style Confidence Filtering\n\n\n\n4 Using Vote-Derived Uncertainty as a Test-Time Signal\n\n\n4.1 Analyzing Action Distributions\n\nUncertainty profiles differ between successful and failed trajectories.\nUncertainty can predicts when arbitration helps versus hurts.\n\n\n4.2 CATTS: Confidence-Aware Test-Time Scaling\n4.3 Results\n\n4.4 Discussion\n\nRegime 1: Redundancy (high consensus).\nRegime 2: Contention (genuine uncertainty).\n\n\n\n\n5 Conclusion\n\nA Agent Design Decisions\n\nA.1 Action Space\nA.2 Error Handling and Validation\nA.3 Conversation Format\n\n\n\nB Agent Prompts\n\nB.1 Base Agent (ReAct Executor)\nB.2 Semantic Deduplicator\nB.3 Arbiter\nB.4 External Planner (Plan-and-Act)\n\n\n\nC Benchmark Characteristics and LLM-as-Judge Reliability\n\nBenchmark difficulty comparison.\nLLM-as-judge reliability.\n\n\nD Semantic Deduplication Ablation\nE Plan-and-Act Scaling Results\nF Complete Arbiter Scaling Results\n\nG RSA and PlanRSA Full Results\n\nAnalysis.\n\n\nH Threshold Sensitivity Analysis\nI Vote Distribution Analysis\nJ Failure Node Examples\n\n\n\n\n\nAgentic Test-Time Scaling for WebAgents\n\n\nNicholas Lee\n\n‚ÄÉ‚ÄÉ\nLutfi Eren Erdogan\n\n‚ÄÉ‚ÄÉ\nChris Joseph John\n\n‚ÄÉ‚ÄÉ\nSurya Krishnapillai\n\n‚ÄÉ‚ÄÉ\nMichael W. Mahoney\n\n‚ÄÉ‚ÄÉ\nKurt Keutzer\n\n‚ÄÉ‚ÄÉ\nAmir Gholami\n\n\n\nAbstract\nTest-time scaling has become a standard way to improve performance and boost reliability of neural network models.\nHowever, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns.\nIn this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents.\nWe first conduct an empirical study of inference-time scaling for web agents.\nWe find that uniformly increasing per-step compute quickly saturates in long-horizon environments.\nWe then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions.\nWe show that uncertainty statistics derived from the agent‚Äôs own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation.\nBased on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite\nand GoBrowse by up to 9.1% over React while using up to 2.3√ó\\times fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge language models (LLMs) are increasingly used not only to produce text, but to take actions: they can call tools, navigate websites, operate software, and execute multi-step procedures in interactive environments.\nIn these settings, an agent must repeatedly choose a next action, e.g., clicking a button, typing into a form, or issuing a search query, based on the current observation and the history of what it has already done (Patil et al., 2024; Schick et al., 2023; Erdogan et al., 2024).\nThe sequential nature of these tasks makes reliability much more challenging than simpler single-shot question answering: a single poor decision can send the trajectory into an unrecoverable state, and small per-step error rates can compound over many steps (Erdogan et al., 2025).\n\n\nA widely used strategy for improving LLM capabilities at test time is test-time scaling (Snell et al., 2024; Brown et al., 2024; Cobbe et al., 2021; Zelikman et al., 2022). The core idea is to spend compute at test-time generating more tokens, rather than spending this compute during pretraining. For single-shot reasoning tasks, this can yield large gains because different samples can explore different reasoning paths: even if the first attempt fails, additional attempts often contain a correct solution, and majority voting (Wang et al., 2023) or verification (Shinn et al., 2023)) can exploit this diversity.\n\n\nAn important question that remains to be answered is: What does inference-time scaling look like for multi-step, tool-using agents?\nA direct analogue of the single-shot recipe is to consider each step separately, treating each step the same way one would treat an ordinary reasoning task.\nAt each step tt, instead of generating one action, we can sample NN candidate actions from the base model and then choose which action to execute.\nRepeating this procedure throughout a trajectory yields a simple and appealing ‚Äúknob‚Äù for scaling up compute (Wang et al., 2023; Venkatraman et al., 2025).\n\n\nHowever, a naive strategy, i.e., uniform scaling,\nwhere we always sample the same number of candidates and apply the same selection rule at every step, runs into two issues:\n\n\n1.\n\nWasted computation on easy steps.\nIn many cases, a majority of steps are obvious based on the state and goal (e.g., continuing a form fill, clicking submit, etc.). Therefore, performing test time scaling for these easy steps results in wasted compute and higher inference cost.\n\n\n\n2.\n\nHigh-Variance Decisions.\nUsing majority voting when the votes exhibit very high variance and voting results are close is often not helpful. This particularly happens when the model has to solve a hard step, or when there are multiple plausible actions that can compete. In these cases, simply sampling more actions and taking a majority vote can be ineffective, especiallly when votes spread across many distinct options with no clear winner.\n\n\n\n\n\nRecent work typically uses deeper rollouts to iteratively refine and improve solutions (Venkatraman et al., 2025; Zhang et al., 2024; Muennighoff et al., 2025).\nThe most common pattern is to have an additional LLM that takes the current state and the list of candidate actions and then chooses the best one.\nWe refer to this reranker-style mechanism as an¬†arbiter.\n\n\nWhile using additional LLMs can help with difficult decisions, they are also prone to overthinking (Cuadron et al., 2025), where even though the samples all agree on the best action, the additional selection step can override the consensus and choose a harmful action.\nIn other words, extra compute is not automatically beneficial; it matters where and how we spend it inside the loop.\n\n\nIn this work, we study this design space systematically in order to understand where current inference-time scaling techniques are helpful and where they are harmful, in agentic settings.\nWe begin by adapting several common inference-time scaling techniques, such as best-of-NN sampling and voting, reranking/aggregation via additional rollouts (Venkatraman et al., 2025), and confidence-aware filtering methods (Fu et al., 2025), to long-horizon web agents.\nWe then analyze where these methods help, where they fail, and what signals predict those regimes.\nThe outcome of this analysis is a simple principle: Inference-time compute should be allocated where it is likely to change the decision.\nCritically, we find that the distribution of answers generated at each step can be used as a measure for how likely the task will succeed (Section¬†4).\n\n\nUsing this insight, we present CATTS (Confidence-Aware Test-time Scaling), where we use the uncertainty at each step in order to dynamically allocate additional compute when necessary.\nWe evaluate on WebArena-Lite and GoBrowse, and we find that CATTS achieves consistent improvements while using 2√ó\\times fewer tokens than uniform scaling.\nThis is accomplished\nby concentrating compute on uncertain and difficult steps, rather than spending it uniformly across all decisions.\n\n\nFigure 1: Comparing agentic inference-time scaling methods. Visual comparison of selection strategies at each agent step. Left: Majority Voting samples NN candidates and selects the most frequent action via argmax over vote distribution pt‚Äã(a)p_{t}(a). Center-Left: Arbiter samples NN candidates and uses an additional LLM call to reason over candidates and select the best action. Center-Right: CATTS conditionally invokes the arbiter only when vote-derived uncertainty (entropy HtH_{t} or margin Œît\\Delta_{t}) exceeds threshold œÑ\\tau, otherwise falls back to majority voting.\n\n\n\n\n\n2 Related Work\n\nInference-Time Scaling and Test-Time Compute.\n\nVast amounts of recent work have explored how to improve the performance and reliability of LLMs by spending additional test-time compute.\nSelf-consistency decoding (Wang et al., 2023) demonstrated substantial gains on reasoning tasks by sampling multiple chain-of-thought traces (Wei et al., 2022; Kojima et al., 2022) and taking a majority vote over final answers, treating it as an ensemble problem. Subsequent work has explored richer aggregation strategies including ranked voting and diversity-aware selection (Wang et al., 2025; Naik et al., 2023; Wan et al., 2024), although several studies highlight fundamental limits when sampled outputs share correlated errors (Byerly and Khashabi, 2024; Turpin et al., 2023).\n\n\nAlternative approaches allocate compute via structured search over reasoning steps (Yao et al., 2023a; Besta et al., 2024; Zhou et al., 2024; Kim et al., 2024) or use confidence-aware filtering to gate computation (Fu et al., 2025; Kadavath et al., 2022). Recent work formalizes compute-optimal policies that trade off parallel sampling and sequential refinement (Snell et al., 2024), connecting to adaptive computation ideas"
  },
  {
    "title": "On-Policy Context Distillation for Language Models",
    "url": "https://arxiv.org/abs/2602.12275v1",
    "source": "arxiv",
    "summary": "Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We de",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nContext Distillation\nOn-Policy Distillation\nSelf-Distillation\n\n\n\n3 Method\n\n\n3.1 Teacher Model Configurations\n\nTeacher-Student Distillation (œÄteacher‚â†œÄŒ∏\\pi_{\\mathrm{teacher}}\\neq\\pi_{\\theta})\nSelf-Distillation (œÄteacher=œÄŒ∏\\pi_{\\mathrm{teacher}}=\\pi_{\\theta})\n\n\n\n\n\n4 Experiments\n\n\n4.1 Evaluation Tasks\n\n\n4.1.1 Experiential Knowledge Distillation\n\nDatasets\n\n\n\n4.1.2 System Prompt Distillation\n\nDatasets\n\n\n\n\n\n4.2 Setup\n\nModels\nTraining\nEvaluation\n\n\n\n4.3 Results\n\nExperiential Knowledge Consolidation\nSystem Prompt Distillation\n\n\n4.4 Effect of Model Size\n4.5 On-Policy Context Distillation Mitigates Forgetting\n4.6 Teacher-Student Distillation vs. Self-Distillation\n4.7 Importance of Learning from Experiential Knowledge\n\n\n5 Conclusion\n\nA Experiential Knowledge Distillation Details\n\nA.1 Prompt Templates\nA.2 Dataset Details\nA.3 Training Details\nA.4 Experiential Knowledge Accumulation\nA.5 Experiential Knowledge Examples\n\n\n\nB System Prompt Distillation Details\n\nB.1 System Prompts\nB.2 Training Details\n\n\n\n\n\n\n\nOn-Policy Context Distillation for Language Models\n\n\nTianzhu Ye¬†¬†¬†¬†¬†¬†¬†¬†Li Dong11footnotemark: 1\nXun Wu¬†¬†¬†¬†¬†¬†Shaohan Huang¬†¬†¬†¬†¬†¬†Furu Wei \n¬†Microsoft Research \n¬†https://aka.ms/GeneralAI\n\n¬†Equal contribution.\n\n\nAbstract\nContext distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.\n\n\n\n1 Introduction\n\nLarge language models (LLMs) exhibit remarkable in-context learning capabilities, allowing them to adapt their behavior based on the information provided in the prompt without parameter updates [4, 7].\nBy prepending instructions, few-shot demonstrations, or retrieved documents to the input, users can steer model behavior without updating parameters.\nHowever, in-context knowledge is transient. In other words, valuable insights generated or retrieved during a session are lost once the context is reset, requiring the model to ‚Äúre-learn‚Äù from the prompt every time.\n\n\nA natural question arises: Can we internalize transient in-context knowledge into the model‚Äôs permanent parameters?\nContext distillation [2, 19] addresses this by training a student model to mimic the behavior of a context-conditioned teacher, effectively compressing the context into the student‚Äôs weights. Once trained, the student can reproduce the teacher‚Äôs context-aware behavior without requiring the context at inference time, effectively ‚Äúinternalizing‚Äù the context.\n\n\nDespite its appeal, existing context distillation methods face a fundamental limitation: they rely on off-policy training with forward Kullback-Leibler (KL) divergence minimization on a fixed dataset. However, this off-policy approach suffers from distinct drawbacks.\nFirst, it induces exposure bias, where the student is trained on teacher-generated or ground-truth data but must generate its own autoregressive sequences at inference time.\nSecond, minimizing forward KL encourages mode-covering behavior, causing the student to assign probability mass to all teacher-generated tokens, often resulting in ‚Äúhallucinations‚Äù or overly broad distributions when the student lacks the capacity to fully model the teacher‚Äôs complex, context-aware distribution [9].\n\n\nIn this work, we propose On-Policy Context Distillation (OPCD), a method that bridges on-policy distillation [9, 12, 1] with context distillation to internalize in-context knowledge more effectively. The key is that the student model learns from its own generation trajectories rather than those of the teacher.\nSpecifically, OPCD samples responses from the student model (without context), then computes the reverse KL divergence between the student‚Äôs token distributions and those of a context-conditioned teacher at each position along the student‚Äôs trajectory. This on-policy approach ensures that the student learns to correct its own mistakes and align its generation distribution with the teacher‚Äôs context-aware behavior.\n\n\nWe demonstrate the effectiveness of OPCD on two important applications. First, we introduce experiential knowledge distillation, where a model extracts transferable knowledge from its historical solution traces and internalizes this accumulated experience into its parameters. We show that models can progressively improve by accumulating experiential knowledge from solved problems, and that OPCD successfully consolidates this knowledge without requiring the extended context at inference time. Second, we apply OPCD to system prompt distillation, enabling models to internalize beneficial behaviors encoded in externally optimized prompts for specialized tasks such as medical question answering and safety classification.\n\n\nOur experiments span mathematical reasoning, text-based games, and domain-specific tasks with optimized system prompts. Across all settings, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities and relieving catastrophic forgetting. We further demonstrate that OPCD enables effective teacher-student distillation, where smaller student models can internalize experiential knowledge from larger teachers. In contrast, directly injecting teacher-generated knowledge into smaller model contexts degrades performance.\n\n\n\n\n2 Related Work\n\nContext Distillation\n\nContext distillation compresses in-context knowledge into model parameters, eliminating the inference overhead of context processing [2, 19]. While prior methods rely on off-policy forward KL minimization, they suffer from exposure bias due to the mismatch between teacher-guided training and autoregressive inference. In contrast, our method employs on-policy sampling, allowing the student to learn from its own trajectories and bridging the gap between training and deployment distributions.\n\n\n\nOn-Policy Distillation\n\nOn-policy distillation methods [9, 12, 1] mitigate exposure bias by training students on their own generated trajectories. By minimizing the reverse KL divergence¬†[9], these approaches promote mode-seeking behavior, compelling the student to focus on the teacher‚Äôs high-likelihood regions and avoiding the mode-averaging issues of standard forward KL.\n[23] has extended this to black-box settings.\nOur work adapts the on-policy distillation paradigm specifically for the problem of context internalization, allowing a model to efficiently consolidate transient in-context knowledge into its permanent weights.\n\n\n\nSelf-Distillation\n\nRecent research has increasingly explored self-distillation mechanisms in which a model improves by learning from its own output or a conditioned version of itself.\n[25] demonstrates that a model can bootstrap its reasoning capabilities by iteratively training self-generated solutions that lead to correct answers.\nCloser to our approach, concurrent works¬†[26, 11, 17, 15] utilize on-policy self-distillation conditioning on privileged information (such as ground-truth solutions, environmental feedback, or demonstrations) to supervise the model sharing the same weights.\nIn comparison, the teacher model in our framework can be a different model or the same model, and it can be updated simultaneously or kept frozen. This allows us to adapt to various training scenarios and objectives, whereas self-distillation methods typically focus on a single model learning from itself without the flexibility of incorporating external knowledge or different training dynamics.\n\n\n\n\n\n3 Method\n\nFigure 1: Overview of on-policy context distillation (OPCD). Given a context and an input prompt, the student model generates a response without the context. It is then trained to minimize the reverse KL divergence to the teacher model that conditions on the context. The student internalizes the contextual information with on-policy learning.\n\n\nWe present On-Policy Context Distillation (OPCD), a method that internalizes in-context knowledge into model parameters by bridging on-policy distillation¬†[9, 12, 1] with context distillation¬†[2, 19]. Our approach enables models to consolidate contextual information (such as experience knowledge or instructions) directly into their weights.\nThe fundamental goal is to compress a specific prompt or context cc into the parameters Œ∏\\theta of a student model œÄŒ∏\\pi_{\\theta}, such that the student can replicate the behavior of a context-aware teacher œÄteacher\\pi_{\\mathrm{teacher}} without requiring the context at inference time.\n\n\nFormally, given an input xx, we minimize the divergence between the student distribution œÄŒ∏(‚ãÖ‚à£x)\\pi_{\\theta}(\\cdot\\mid x) and the teacher distribution œÄteacher(‚ãÖ‚à£c,x)\\pi_{\\mathrm{teacher}}(\\cdot\\mid c,x), where the teacher has access to the guiding context cc prepended to the input.\nOPCD optimizes the reverse Kullback-Leibler (KL) divergence¬†[9] between the student and teacher distributions using on-policy sampling.\n\n\nWe decompose sequence-level divergence into the sum of token-level divergences. The loss function is defined as:\n\n\n\n‚Ñí(Œ∏)=ùîº(x,c)‚àºùíü,y‚àºœÄŒ∏(‚ãÖ‚à£x)[1|y|‚àët=1|y|DKL(œÄŒ∏(‚ãÖ‚à£x,y&lt;t)‚à•œÄteacher(‚ãÖ‚à£c,x,y&lt;t"
  },
  {
    "title": "Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage",
    "url": "https://arxiv.org/abs/2602.12274v1",
    "source": "arxiv",
    "summary": "Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribut",
    "full_text": null
  },
  {
    "title": "Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs",
    "url": "https://arxiv.org/abs/2602.12273v1",
    "source": "arxiv",
    "summary": "We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed lea",
    "full_text": null
  },
  {
    "title": "MonarchRT: Efficient Attention for Real-Time Video Generation",
    "url": "https://arxiv.org/abs/2602.12271v1",
    "source": "arxiv",
    "summary": "Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong result",
    "full_text": null
  },
  {
    "title": "Creative Ownership in the Age of AI",
    "url": "https://arxiv.org/abs/2602.12270v1",
    "source": "arxiv",
    "summary": "Copyright law focuses on whether a new work is \"substantially similar\" to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been ge",
    "full_text": null
  },
  {
    "title": "CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use",
    "url": "https://arxiv.org/abs/2602.12268v1",
    "source": "arxiv",
    "summary": "AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.12268v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.12268v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 12 Feb 2026]\n    Title:CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use\n    Authors:Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang            View a PDF of the paper titled CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use, by Zhen Zhang and 13 other authors\n    View PDF\n\n\n\n    \n            Abstract:AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn&#39;s intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: this https URL.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.12268 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.12268v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.12268\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Zhen Zhang [view email]          [v1]\n        Thu, 12 Feb 2026 18:55:09 UTC (1,701 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use, by Zhen Zhang and 13 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is commi"
  },
  {
    "title": "Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data",
    "url": "https://arxiv.org/abs/2602.12267v1",
    "source": "arxiv",
    "summary": "Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. ",
    "full_text": null
  },
  {
    "title": "T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization",
    "url": "https://arxiv.org/abs/2602.12262v1",
    "source": "arxiv",
    "summary": "Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation",
    "full_text": null
  },
  {
    "title": "Think like a Scientist: Physics-guided LLM Agent for Equation Discovery",
    "url": "https://arxiv.org/abs/2602.12259v1",
    "source": "arxiv",
    "summary": "Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step re",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Symbolic Regression\n2.2 LLM-Based SR\n\n\n\n3 Physics-Aware Equation Discovery Agent\n\n3.1 Overview\n3.2 Agent Architecture\n\n3.3 Toolset\n\nPython code interpreter.\nVisual subagent.\nSymmetry discovery.\nPySINDy.\nPySR.\n\n\n\n3.4 Implementation Details\n\nExperience log.\nWorkspace.\nStopping criteria.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Datasets\n\nLLM-SRBench.\nDifferential equations (DiffEq).\n\n\n4.2 Evaluation criteria\n4.3 Baselines\n4.4 LSR-Transform Results\n4.5 DiffEq Results\n\n\n5 Discussion\n\nA Implementation Details\n\nA.1 System Prompt\n\nA.2 Tool Implementations and Specifications\n\nPython code interpreter.\nVisual subagent.\nSymmetry discovery.\nPySINDy.\nPySR.\n\n\n\n\n\nB Experiment Details\n\n\nB.1 Baselines and Common Setups\n\nLLM-SR.\nPySR.\n\n\nB.2 Differential Equation Datasets\n\n\n\nC Additional Results\n\nC.1 Long-Term Prediction in DiffEq Systems\nC.2 An Instance of KeplerAgent Reasoning Trace\n\n\n\n\n\n\n\nThink like a Scientist: Physics-guided LLM Agent for Equation Discovery\n\n\nJianke Yang\n\n‚ÄÉ‚ÄÉ\nOhm Venkatachalam\n\n‚ÄÉ‚ÄÉ\nMohammad Kianezhad\n\n‚ÄÉ‚ÄÉ\nSharvaree Vadgama\n\n‚ÄÉ‚ÄÉ\nRose Yu\n\n\n\nAbstract\nExplaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations.\nWe introduce\nKeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nExplaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. From Kepler‚Äôs laws to the Navier‚ÄìStokes equations, many major discoveries can be viewed as instances of ‚Äúequation discovery‚Äù: given observations of a system, infer a compact mathematical expression that captures its underlying structure and dynamics. Symbolic regression (SR) including evolutionary search (Schmidt and Lipson, 2009) and sparse regression (Brunton et al., 2016) formalizes this problem as a search over symbolic expressions, aiming to find formulas that both fit the data and are interpretable. This makes SR an attractive tool for scientific discovery, where interpretability and extrapolation are as important as predictive accuracy.\n\n\nFigure 1: Our KeplerAgent orchestrates physics-based tools and is capable of discovering different types of equations from data.\n\n\nHowever, algorithmic SR methods place a heavy configuration burden on the user. To obtain a correct and interpretable equation, a practitioner must make many intertwined design choices: the function library (polynomial degree, inclusion of rational or transcendental functions), sparsity regularization and thresholds, allowed mathematical operators, and stopping criteria, etc. If the configuration is too restrictive (e.g., a low-degree polynomial library), the true equation may be outside the hypothesis space and cannot be recovered. If it is too expressive (e.g., a very rich function set with a large number of operators), the hypothesis space becomes enormous, making the equation search intractable.\nIn practice, experts mitigate these issues by injecting prior knowledge: dimensional analysis, symmetries, conserved quantities, or qualitative behaviors such as saturation or periodicity. However, this requires deep domain expertise and many manual iterations from configuring and running to inspecting and refining, limiting the accessibility and scalability of algorithmic SR-based equation discovery.\n\n\nRecent works have shown that Large language models (LLMs) can automate some of this laborious configuration workflow thanks to their broad scientific knowledge and reasoning abilities. This has led to a new wave of LLM-based methods for discovering symbolic equations. For instance, LLM-SR (Shojaee et al., 2025a) represents equations as numerical programs and uses an LLM to synthesize program ‚Äúskeletons‚Äù that define candidate equation structures. These skeletons are then completed and refined via evolutionary search and numerical optimization. By leveraging LLM‚Äôs prior knowledge and code generation ability, LLM-SR explores promising regions of expression space more efficiently and has achieved strong performance on established SR benchmarks.\n\n\nDespite their advantages over purely algorithmic SR, current LLM-based equation discovery methods lack explicit reasoning about the structure or additional physical properties of the underlying system. They use brute-force to map data to equations by iteratively scoring the LLM-proposed candidate expressions.\nHuman scientists, on the contrary, rarely jump straight from raw data to a final closed-form equation. They first probe the system to uncover structural properties, such as symmetries, conserved quantities, dimensional constraints, etc. These properties are then used to reshape the problem. Scientists choose variables, coordinates, and candidate function families that respect the identified structural constraints. Only after this process has substantially narrowed the plausible hypothesis space do they search for specific equations and test them through simulation and extrapolation.\nUnfortunately, none of the existing LLM-based approaches follow the reasoning process of human scientists and directly operate over these intermediate steps. This makes the discovery process brittle and opaque, and it underutilizes the rich domain knowledge from a pretrained LLM and the wealth of physics-based numerical and algorithmic tools that can uncover structure from data as additional inputs.\n\n\nMoreover, current LLM-based evaluations (Shojaee et al., 2025b) are still narrow in terms of experimental domains. Many benchmarks focus on scalar algebraic relations or one-dimensional ODEs, while comprehensive tests on richer dynamical systems are rare, e.g., systems governed by coupled ODEs and PDEs. These systems often possess additional physical structure, such as spatial and phase-space symmetries, conservation laws, etc. These properties provide powerful levers for reducing the search space, and thus create a natural setting to test whether an LLM behaves more like a human scientist: first inferring and using such structure to constrain hypotheses, rather than directly guessing equations from data.\n\n\nIn this work, rather than using LLMs as monolithic equation guessers, we propose to use LLMs as agents that orchestrate physics-based tools to emulate the multi-step workflow used by human scientists.\nWe introduce KeplerAgent, a physics-guided LLM agent framework for equation discovery. Given observational data from a system, an LLM agent can call tools to estimate intermediate structure, such as candidate symmetries, relevant functional terms and operators, among other constraints and patterns derived from data. The agent then translates these structural findings into concrete configuration decisions for symbolic regression tools such as PySINDy (de Silva et al., 2020) and PySR (Cranmer, 2023). By interleaving these tool calls, the agent can refine both the hypothesis space and candidate equations iteratively, which can be especially helpful when the naive search space is intractably large.\n\n\nWe evaluate our approach across a suite of benchmarks that span algebraic equations, systems of ODEs and PDEs, with a focus on cases where physical structure meaningfully constrains the space of admissible equations. Empirically, KeplerAgent recovers ground-truth equations more frequently than both direct LLM-based baselines and standalone SR tools with standard configurations, and it produces models that better predict the target variables or future states of the underlying physical systems.\n\n\nIn summary, our contributions are:\n\n\n‚Ä¢\n\nPhysics-guided agentic framework. We propose KeplerAgent that orchestrates physics-based tools for structure discovery and SR packages to emulate the multi-step reasoning workflow of human scientists.\n\n\n\n‚Ä¢\n\nAutomatic configuration of SR backends. We show how intermediate structural information can be translated into concrete configuration decisions for PySINDy and PySR, substantially reducing the effective search space.\n\n\n\n‚Ä¢\n\nMulti-domain evaluation. We provide a systematic evaluation across diverse domains, including dynamical systems governed by ODEs and PDEs, demonstrating improved symbolic and numerical accuracies over state-of-the-art classical and LLM-based SR baselines.\n\n\n\n\n\n\n\n2 Related Works\n\n\n2.1 Symbolic Regression\n\nSymbolic regression (SR) seeks to recover an explicit symbolic expression that maps inputs to outputs from data, rather than learning a black-box predictor. Early SR systems are largely based on genetic programming (GP), which evolves populations of expression trees under selection pressure from a fitness objective (Schmidt and Lipson, 2009; Gaucel et al., 2014). These GP-based methods, implemented in software such as Eureqa (Dubƒç√°kov√°, 2011) and PySR (Cranmer, 2023), are shown to rediscover classical laws from experimental data in areas like physics (Cranmer et al., 2020), materials science (Wang et al., 2019), and bioinformatics (Christensen et al., 2022).\n\n\nIn parallel, sparse regression methods focus on dynamical systems. SINDy (Brunton et al., 2016) assumes that the right-hand side of an ODE o"
  },
  {
    "title": "On the implicit regularization of Langevin dynamics with projected noise",
    "url": "https://arxiv.org/abs/2602.12257v1",
    "source": "arxiv",
    "summary": "We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, ",
    "full_text": null
  },
  {
    "title": "Is Online Linear Optimization Sufficient for Strategic Robustness?",
    "url": "https://arxiv.org/abs/2602.12253v1",
    "source": "arxiv",
    "summary": "We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller's manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In ",
    "full_text": null
  },
  {
    "title": "A technical curriculum on language-oriented artificial intelligence in translation and specialised communication",
    "url": "https://arxiv.org/abs/2602.12251v1",
    "source": "arxiv",
    "summary": "This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an acc",
    "full_text": null
  },
  {
    "title": "Community Concealment from Unsupervised Graph Learning-Based Clustering",
    "url": "https://arxiv.org/abs/2602.12250v1",
    "source": "arxiv",
    "summary": "Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastruct",
    "full_text": null
  },
  {
    "title": "\"Sorry, I Didn't Catch That\": How Speech Models Miss What Matters Most",
    "url": "https://arxiv.org/abs/2602.12249v1",
    "source": "arxiv",
    "summary": "Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically divers",
    "full_text": null
  },
  {
    "title": "ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction",
    "url": "https://arxiv.org/abs/2602.12247v1",
    "source": "arxiv",
    "summary": "Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadt",
    "full_text": "\n\n\n\n\n1 Introduction\n\n\n1.1 Related Work\n\nDocument understanding and enterprise document IE benchmarks\nStructured output generation and constrained decoding\nNested JSON extraction and evaluation methodology\nPDF processing and upstream document parsing\n\n\n\n\n\n2 The ExtractBench Dataset\n\n2.1 Design Philosophy\n\n2.2 Domains and Complexity Dimensions\n\nSchema complexity dimensions.\n\n\n2.3 Annotation Process\n\n\n\n3 A Principled Evaluation Methodology\n\n3.1 The Evaluation Challenge\n\n3.2 Schema-Driven Evaluation\n\nMetric library.\n\n\n3.3 Missing Value Semantics\n3.4 Semantic Array Matching\n3.5 Open-Source Release\n\n\n\n4 Experiments\n\n4.1 Main Results\n4.2 Analysis: Why and How Models Fail\n4.3 Structured Output Mode\n\n\n5 Conclusion\n\nA Ethical Considerations\n\nData Privacy and Licensing\nLimitations\n\n\nB Additional Tables and Figures\n\nC Evaluation Configs\n\nstring_exact\nstring_case_insensitive\nstring_fuzzy\nstring_semantic\ninteger_exact\nnumber_exact\nnumber_tolerance\nboolean_exact\narray_llm\n\n\nD Extraction Prompt\n\n\n\n\n\nExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction\n\n\nNick Ferguson\n\nnick.ferguson@collab.contextual.ai\n\n, \nJosh Pennington\n\njosh@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n, \nNarek Beghian\n\nnarek.beghian@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n, \nAravind Mohan\n\nara@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n, \nDouwe Kiela\n\ndouwe@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n, \nSheshansh Agrawal\n\nsheshansh@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n and \nThien Hang Nguyen\n\nthien.nguyen@contextual.ai\n\nContextual AIMountain ViewCAUSA\n\n\n(2026)\n\nAbstract.\nUnstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats.\nLLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount.\nHowever, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination.\nWe address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric.\nBaseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.\n\nstructured extraction, benchmark, JSON Schema, document understanding, large language models, evaluation methodology\n\n‚Ä†‚Ä†copyright: acmlicensed‚Ä†‚Ä†journalyear: 2026‚Ä†‚Ä†doi: XXXXXXX.XXXXXXX‚Ä†‚Ä†conference: Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining; August 9‚Äì13, 2026; Jeju, Korea‚Ä†‚Ä†isbn: 978-1-4503-XXXX-X/2026/08‚Ä†‚Ä†ccs: Computing methodologies¬†Natural language processing‚Ä†‚Ä†ccs: Information systems¬†Information extraction\n\n\n1. Introduction\n\nFigure 1. Overview of the financial data schema structure showing (a) main sections, (b) key metric fields, (c) JSON schema reference example, (d) growth metric definition with 17 fields, and (e) leaf type definitions.\n\n\nEnterprises run on structured data, but much of what they need starts as unstructured documents ‚Äì contracts, filings, reports. As LLMs move into production automation, structured extraction becomes a core requirement, not a niche task.\n\n\nAt first glance, extracting JSON from a document appears straightforward: provide the PDF and a JSON Schema and ask a model to populate it. However, in practice, real enterprise schemas are not simple (see for example Figure 1). They mix many field types (identifiers, numbers, free text, optional fields), are deeply nested, and array types create unpredictable complexity. There are two immediate challenges. First, long documents and large schemas force models to produce long, precisely structured outputs, which are brittle to truncation and formatting errors. Second, syntactically valid output can be wrong in subtle ways. Correctness is field-dependent: IDs need exact match, quantities need tolerance, names need semantic comparison, and arrays demand careful handling of ordering and per-item matching.\n\n\nExisting evaluations benchmark and methodology isolate only parts of the problem. There are 2 main gaps today:\n\n\n‚Ä¢\n\nGap¬†1 (Benchmark): Document understanding benchmarks focus on span- or entity-level extraction with relatively small schema. Structured output benchmarks test schema conformance under clean text prompts but not extraction correctness. Nested JSON extraction benchmarks use text inputs with modest schema breadth. No existing benchmark jointly covers extraction from PDFs, enterprise-scale JSON schemas, and fine-grained key-level evaluation.\n\n\n\n‚Ä¢\n\nGap¬†2 (Methodology): Existing frameworks apply a single recursive comparison that treats every field identically ‚Äì exact equality for primitives, position-dependent matching for arrays of objects, and set overlap for arrays of scalars. This conflates a minor formatting difference in a name with a wrong identifier, penalizes correctly extracted arrays that differ only in order, and draws no distinction between a missing field and an explicit null. Reliable evaluation requires the schema to define not just what to extract but how each field should be scored.\n\n\n\nWe introduce ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The dataset consists of (PDF, JSON Schema, gold JSON) triplets across 5 high-value domains, spanning schema complexity from tens to hundreds of fields. Additionally, we propose and open-source a schema-driven evaluation framework: the schema specifies not only what to extract but how each field should be scored e.g., exact match for identifiers, numeric tolerance for quantities, semantic equivalence for free text.\n\n\nOur key finding is that frontier models fail at enterprise-scale PDF to JSON tasks.\nEvaluations of frontier models on ExtractBench reveal sharp degradation as schema breadth grows, reaching 0% valid output on a 369-field financial reporting schema across all tested frontier models. This exposes a failure mode hidden by benchmarks limited to small schemas: frontier models become unreliable when required to generate long, deeply structured JSON. Valid JSON also does not imply correct extraction ‚Äì on one domain, models achieve 90% valid output but only a 12.5% pass rate.\n\n\nOur paper makes four contributions:\n\n\n‚Ä¢\n\nExtractBench dataset: (PDF, JSON Schema, gold JSON) triplets with human-annotated gold labels across multiple domains and a wide range of schema complexities.\n\n\n\n‚Ä¢\n\nSchema-driven evaluation: an extensible framework for nested structured extraction with per-field metrics, semantic array matching, and explicit missing/null handling.\n\n\n\n‚Ä¢\n\nFrontier baselines: evaluations of frontier models from OpenAI, Anthropic, and Google under a consistent extraction setup.\n\n\n\n‚Ä¢\n\nComplexity analysis: empirical characterization of failure modes as a function of schema depth, breadth, and array structure, identifying schema breadth as the dominant predictor of reliability.\n\n\n\nBoth the dataset and evaluation framework are released at https://github.com/ContextualAI/extract-bench.\n\n\nPaper organization.\nSection¬†1.1 reviews related benchmarks and structured output evaluation. Section¬†2 introduces the ExtractBench dataset. Section¬†3 presents our schema-driven evaluation methodology. Section¬†4 reports baseline results and analyzes performance as a function of schema complexity.\n\n\n\n1.1. Related Work\n\nExtractBench addresses a gap at the intersection of document understanding, structured output generation, and hierarchical JSON evaluation. We briefly review each area.\n\n\nDocument understanding and enterprise document IE benchmarks\n\nDocument understanding benchmarks span form understanding, receipt parsing, and document VQA, including FUNSD¬†(Jaume et al., 2019),\nSROIE¬†(Huang et al., 2019),\nCORD¬†(Park et al., 2019),\nand DocVQA¬†(Mathew et al., 2021).\nLong-document settings and layout variability are emphasized by Kleister¬†(Stanis≈Çawek et al., 2021)\nand visually-rich benchmarks such as ChartQA¬†(Masry et al., 2022)\nand InfographicVQA¬†(Mathew et al., 2022).\nClosest to our setting are enterprise-focused PDF benchmarks such as RealKIE¬†(Townsend et al., 2024)\nand VRDU¬†(Wang et al., 2023), which foreground OCR noise, sparse annotations, and hierarchical entities. However, these benchmarks typically evaluate span/entity extraction with F1-style metrics and comparatively small field sets (e.g., RealKIE: 3‚Äì28 fields; VRDU: ‚àº\\sim12 entities), rather than end-to-end extraction into large hierarchical JSON instances governed by JSON Schema. While these benchmarks address document understanding, they do not target schema-conformant JSON outputs.\n\n\n\nStructured output generation and constrained decoding\n\nA parallel line of work studies generating schema-conformant structured outputs from language models, often via constrained decoding. JSONSchemaBench¬†(Geng et al., 2025)\nprovides a large-scale evaluation of real-world JSON Schemas and shows that constrained decoding frameworks differ substantially in coverage and reliability.\nRepresentative systems include Outlines¬†(Willard and Louf, 2023),\nXGrammar¬†(Dong et "
  },
  {
    "title": "Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces",
    "url": "https://arxiv.org/abs/2602.12245v1",
    "source": "arxiv",
    "summary": "Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short artic",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Minimal Background\n\nJEPA (Latent Representations and Induced Energies).\nQRL (Goal-Reaching values are Quasimetrics).\nValue-Guided Action Planning with JEPA World Models.\nIntrinsic Energies and the Least-Action Principle.\n\n\n3 Intrinsic Energy Functions and Quasimetrics\n\n4 Intrinsic-Energy JEPA and the QRL hypothesis class\n\nGoal-reaching control as intrinsic energy.\nRelation to value-guided JEPA planning.\n\n\n\n5 Discussion and scope\n\nWhat this note does not claim.\nWhy the least-action form is not ad hoc.\nBeyond RL.\n\n\n\n\n\n\n\n\nIntrinsic-Energy Joint Embedding Predictive\nArchitectures Induce Quasimetric Spaces\n\n\n\n\nAnthony Kobanda1,2,\nWaris Radji2\n1Ubisoft La Forge, Bordeaux, France,\n2Inria Scool,\nUniversit√© de Lille, France\n\n\n\nAbstract\nJoint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings,\ninducing a scalarcompatibility energy in a latent space.\nIn contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values(cost-to-go) that support reaching goals under asymmetric dynamics.\nIn this short article,\nwe connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima ofaccumulated local effort over admissible trajectories between two states.\nUndermild closure and additivity assumptions, any intrinsic energy is a quasimetric.\nIn goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ;\ninversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.\n\n\n\n1 Introduction\n\nJoint-Embedding Predictive Architectures (JEPAs) (LeCun, 2022; Assran et al., 2023) recently emerged as a compelling self-supervised paradigm for representation learning in which modelspredict target embeddings from context embeddings rather than reconstructing raw observations.\nFrom a control viewpoint, the resulting training score can be interpreted as a scalar compatibilityenergy between inputs : low energy meaning compatible,\nwhile high energy means incompatible.\n\n\nIndependently, Quasimetric Reinforcement Learning (QRL) (Wang et al., 2023)\nrepresents goal-conditioned control through the geometry of a learned cost-to-go.\nA central fact is that reaching costs induce a directed notion of distance :\ngoing from ss to gg can be easy, while returning from gg to ss may be difficult or impossible, and multi-step composition should be consistent.\nHence, QRL formalizes this by targeting function classes with quasimetric structure (Wang and Isola, 2022), and proposes a learning framework and objectives designed for that regime.\n\n\nThis short article addresses a narrow structural question :\n\nUnder what conditions does a JEPA-induced energy behave like a cost-to-go ?\n\nA key insight is that sequential compositionality is not a modelling preference, but usually required when reasoning (Johnson-Laird, 2010; Plaat et al., 2025).\nIf a state uu (e.g., a position, a proposition, a concept) is accessible\n(e.g., reachable, demonstrable, explainable) on the way from xx to yy, then an energy meant to support multi-step reasoning should satisfy a consistency inequality of the form :\n\n\n\nE‚Äã(x,g)‚â§E‚Äã(x,u)+E‚Äã(u,g),E(x,g)\\;\\leq\\;E(x,u)+E(u,g),\n\n\n\nwhich is precisely a triangle inequality.\n\n\nIn physics and optimal control, the canonical way to obtain such a global, compositional energy is to define it intrinsically via a least-action principle: the energy between two states is the infimum of accumulated local effort over admissible trajectories connecting them (De Sapio et al., 2008; Ho, 2018; Terekhovich, 2018).\nThis construction yields triangle inequality by definition (via concatenation), and it naturally produces asymmetry whenever admissibility or local effort is directed.\n\n\nOur main message is simple :\n\nIf a JEPA is defined according to an intrinsic\n(least-action) energy,\nthen it induces a quasimetric space.\n\nIn goal-conditioned tasks, costs-to-go can be interpreted as intrinsic energies, hence such Intrinsic-Energy JEPAs fall into the same quasimetric value-functions class that QRL is designed to model.\n\n\n\n\n2 Minimal Background\n\nJEPA (Latent Representations and Induced Energies).\n\nJoint-Embedding Predictive Architectures learn representations by predicting target embeddings from context embeddings (LeCun, 2022).\nGiven a context encoder fœïf_{\\phi} and a target encoder fœï¬Øf_{\\bar{\\phi}}, JEPA forms\nzx=fœï‚Äã(x),zy=fœï¬Ø‚Äã(y),z_{x}=f_{\\phi}(x),z_{y}=f_{\\bar{\\phi}}(y),\nand a predictor pŒ∏p_{\\theta} produces a predicted target embedding z^y=pŒ∏‚Äã(zx;c)\\hat{z}_{y}=p_{\\theta}(z_{x};\\,c), where cc denotes a conditioning.\n\n\nTraining uses a comparator function D‚Äã(‚ãÖ,‚ãÖ)D(\\cdot,\\cdot) in the embedding space :\n\n\n\n‚ÑíJEPA‚Äã(œï,œï¬Ø,Œ∏)=ùîº(x,y)‚àºùíü‚Äã[D‚Äã(z^y,sg‚Äã(zy))],\\mathcal{L}_{\\text{JEPA}}(\\phi,\\bar{\\phi},\\theta)~=~\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\\!\\left[\\,D\\!\\left(\\hat{z}_{y},\\texttt{sg}(z_{y})\\right)\\right],\n\n\n\nEven when the comparator form is fixed, the model learns an induced energy landscape over inputs through the learned representation and predictor: pairs (x,y)(x,y) that are compatible are precisely those for which the latent prediction error is small. This motivates interpreting JEPA training as learning a scalar energy (compatibility) between xx and yy, in line with the energy-based perspective on JEPA.\n\n\n\nQRL (Goal-Reaching values are Quasimetrics).\n\nQuasimetric Reinforcement Learning studies goal-conditioned control through directed distances that compose over time (Wang et al., 2023).\nIn reaching-cost problems, the optimal value function is V‚ãÜ‚Äã(s,g)V^{\\star}(s,g), usually negative (up to sign conventions), and satisfies a triangle inequality in (s,g)(s,g), yielding a quasimetric structure :\n\n\n\nd‚ãÜ‚Äã(s,g)‚âú‚àíV‚ãÜ‚Äã(s,g),d‚ãÜ‚Äã(s,g)‚â§d‚ãÜ‚Äã(s,w)+d‚ãÜ‚Äã(w,g),d^{\\star}(s,g)\\;\\triangleq\\;-V^{\\star}(s,g),\\qquad d^{\\star}(s,g)\\leq d^{\\star}(s,w)+d^{\\star}(w,g),\n\n\n\nwith d‚ãÜ‚Äã(s,g)=+‚àûd^{\\star}(s,g)=+\\infty for unreachable pairs. QRL leverages this structure by learning dŒ∏d_{\\theta} within quasimetric function classes (Wang and Isola, 2022), enforcing local constraints from observed transitions and using the triangle inequality to propagate these constraints to long horizons; the resulting objectives come with recovery guarantees under suitable data coverage conditions. For our purposes, the key takeaway is that QRL treats the goal-conditioned value as a directed geometry.\n\n\n\nValue-Guided Action Planning with JEPA World Models.\n\nValue-guided JEPA planning uses JEPA-like world models for control by shaping representation spaces so that an embedding-space cost aligns with a goal-reaching value, enabling planning via minimizing a representation-space objective (Destrade et al., 2025).\nTheir contribution is primarily algorithmic and empirical: learning representations that make planning effective.\nOur focus is orthogonal and structural: we do not propose planners or additional experiments.\nInstead, we isolate a hypothesis-class condition on the energy itself under which a JEPA-induced energy necessarily satisfies quasimetric inequalities, directly connecting JEPA energies to the quasimetric value viewpoint formalized in QRL.\n\n\n\nIntrinsic Energies and the Least-Action Principle.\n\nIn physics and control,\nit is standard to define a global cost between configurations as the infimum of an action functional,\ntypically an integral of a local effort along admissible trajectories, following a least-action viewpoint (Siburg, 2004).This perspective is used in modern ML to endow learned representations with physically meaningful structure\n(Greydanus et al., 2019; Cranmer et al., 2020),\nand more recently to motivate learning from variational constructions\n(Guo and Sch√∂lkopf, 2025).\nIn optimal control, path-integral methods also build directly on trajectory functionals\nof the form ‚à´0T‚Ñì‚Äã(x‚Äã(t),u‚Äã(t))‚Äãùëët\\int_{0}^{T}\\ell(x(t),u(t))\\,dt, optimizing them to produce actions, which makes the ‚Äúleast accumulated effort over trajectories‚Äù interpretation operational in control pipelines\n(Asmar et al., 2023; Zhai et al., 2025).\nIn this article, we adopt the same high-level principle but in a representation learning setting: we interpret the JEPA score as an energy and restrict attention to energies that admit an intrinsic (least-action) representation.\n\n\n\n\n\n3 Intrinsic Energy Functions and Quasimetrics\n\n\nDefinition 1 (Quasimetric).\n\n\nA function d:ùí≥√óùí≥‚Üí‚Ñùd:\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R} is a quasimetric if,\nfor all x,y,z‚ààùí≥x,y,z\\in\\mathcal{X} :(i : reflexivity) d‚Äã(x,x)=0d(x,x)=0 ,\n(ii : non-negativity) d‚Äã(x,y)‚â•0d(x,y)\\geq 0 ,\n(iii : Identity of indiscernibles) if d‚Äã(x,y)=0d(x,y)=0 then x=yx=y,\n(iv : triangular inequality) d‚Äã(x,z)‚â§d‚Äã(x,y)+d‚Äã(y,z)d(x,z)\\leq d(x,y)+d(y,z).\n\n\n\n\nDefinition 2 (Intrinsic (Least-Action) Energy).\n\n\nLet ùí≥\\mathcal{X} be a path-connected state space.\nFor x,y‚ààùí≥x,y\\in\\mathcal{X}, let Œì‚Äã(x‚Üíy)\\Gamma(x\\!\\to\\!y) denote a set of admissible C1C^{1} trajectories Œ≥:[0,T]‚Üíùí≥\\gamma:[0,T]\\to\\mathcal{X}, with Œ≥‚Äã(0)=x\\gamma(0)=x and Œ≥‚Äã(T)=y\\gamma(T)=y, ‚àÄT&gt;0\\forall T&gt;0.\nLet L:T‚Äãùí≥‚Üí‚Ñù¬Ø+L:\\mathrm{T}\\mathcal{X}\\to\\bar{\\mathbb{R}}^{+} be a local effort density, veryfing L‚Äã(x,v)‚â•c‚ãÖ‚à•v‚à•L(x,v)\\geq c\\cdot\\lVert v\\rVert for some norm ‚à•‚ãÖ‚à•\\lVert\\cdot\\rVert and a constant c&gt;0c&gt;0.\nDefining the action of a trajectory as :\n\n\n\nùñ†ùñºùóç‚Äã(Œ≥)=‚à´0TL‚Äã(Œ≥‚Äã(t),Œ≥Àô‚Äã(t))‚Äãùëët.\\mathsf{Act}(\\gamma)\\;=\\;\\int_{0}^{T}L(\\gamma(t),\\dot{\\gamma}(t))\\,dt.\n\n\n\nthe intrinsic energy is\nE‚Äã(x,y)=infŒ≥‚ààŒì‚Äã(x‚Üíy)ùñ†ùñºùóç‚Äã(Œ≥),E(x,y)\\;=\\;\\operatorname*{inf}_{\\gamma\\in\\Gamma(x\\to y)}\\mathsf{Act}(\\gamma),\nwith E‚Äã(x,y)=+‚àûE(x,y)=+\\infty if Œì‚Äã(x‚Üíy)=‚àÖ\\ \\Gamma(x\\to y)=\\emptyset.\n\n\n\n\nRemark 1 (Physics and Control Grounding).\n\n\nThe least-action form is standard: it defines global energies from local effort and yields Euler‚Äì"
  },
  {
    "title": "Federated Gaussian Process Learning via Pseudo-Representations for Large-Scale Multi-Robot Systems",
    "url": "https://arxiv.org/abs/2602.12243v1",
    "source": "arxiv",
    "summary": "Multi-robot systems require scalable and federated methods to model complex environments under computational and communication constraints. Gaussian Processes (GPs) offer robust probabilistic modeling, but suffer from cubic computational complexity, limiting their applicability in large-scale deployments. To address this challenge, we introduce the pxpGP, a novel distributed GP framework tailored ",
    "full_text": null
  },
  {
    "title": "Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications",
    "url": "https://arxiv.org/abs/2602.12241v1",
    "source": "arxiv",
    "summary": "Latency-critical speech applications (e.g., live transcription, voice commands, and real-time translation) demand low time-to-first-token (TTFT) and high transcription accuracy, particularly on resource-constrained edge devices. Full-attention Transformer encoders remain a strong accuracy baseline for automatic speech recognition (ASR) because every frame can directly attend to every other frame, ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Motivation\n\n\n2.1 Full-attention encoders: accurate, but latency-heavy\n\nTime-to-first-token (TTFT).\n\n\n2.2 Sliding-window attention encoders: streaming-friendly latency\n\n\n\n3 Approach\n\n3.1 Audio preprocessor\n\n3.2 Encoder\n\nNo positional embeddings (ergodic encoder).\nProvisional vs. finalized encoder states.\n\n\n3.3 Adapter\n3.4 Decoder\n\n\n\n4 Evaluation &amp; Results\n\n\n4.1 Experimental setup\n\n\n4.1.1 Training.\n\nData.\nTokenizer &amp; optimization.\n\n\n4.1.2 Implementation.\n\n4.1.3 Benchmarks.\n\nWord error rate (WER).\nTime-to-first-token (TTFT).\nResponse latency.\nCompute cost.\n\n\n\n\n4.2 Results\n\n\n5 Discussion &amp; Conclusion\n\n\n\n\n\nMoonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications\n\n\nManjunath Kudlur\n\n‚ÄÉ‚ÄÉ\nEvan King\n\n‚ÄÉ‚ÄÉ\nJames Wang\n\n‚ÄÉ‚ÄÉ\nPete Warden\n\n\n\nAbstract\nLatency-critical speech applications‚Äîincluding live transcription, voice\ncommands, and real-time translation‚Äîdemand low time-to-first-token (TTFT) and\nhigh transcription accuracy, particularly on resource-constrained edge devices.\nFull-attention Transformer encoders remain a strong accuracy baseline for\nautomatic speech recognition (ASR) because every frame can directly attend to\nevery other frame, which resolves otherwise locally ambiguous acoustics using\ndistant lexical context. However, this global dependency incurs quadratic\ncomplexity in sequence length, inducing an inherent\n‚Äâ‚Äúencode-the-whole-utterance‚Äù‚Äâ latency profile. For streaming use cases,\nthis causes TTFT to grow linearly with utterance length as the encoder must\nprocess the entire prefix before any decoder token can be emitted. To better\nmeet the needs of on-device, streaming ASR use cases we introduce Moonshine v2,\nan ergodic streaming-encoder ASR model that employs sliding-window\nself-attention to achieve bounded, low-latency inference while preserving strong\nlocal context. Our models achieve state of the art word error rates across\nstandard benchmarks, attaining accuracy on-par with models 6x their size while\nrunning significantly faster. These results demonstrate that carefully designed local\nattention is competitive with the accuracy of full attention at a fraction of\nthe size and latency cost, opening new possibilities for interactive speech\ninterfaces on edge devices.\n\nKeywords\n\n\nMoonshine AI\n\n\n\n1 Introduction\n\nModern automatic speech recognition (ASR) systems are separated into two\ndeployment paradigms: cloud-based models that leverage server-scale compute and\nedge models that run locally on resource-constrained devices. While cloud ASR\ncan achieve excellent accuracy by utilizing large models and extensive\ncomputational resources, edge ASR is essential for applications where network\nconnectivity is unreliable or unavailable, such as offline voice assistants,\nmedical dictation in remote settings, real-time captioning for accessibility,\nand privacy-sensitive voice commands on mobile devices. Edge deployment also\neliminates network round-trip latency and reduces privacy concerns by keeping\naudio data on-device.\n\n\nIn edge use cases, latency and transcription quality are the two key‚Äîand often\ncompeting‚Äîconstraints. Achieving human-perceivable real-time performance\nrequires minimizing time-to-first-token (TTFT) and maintaining low per-token\nlatency, while simultaneously delivering word error rates (WERs) competitive\nwith cloud-based alternatives. Balancing these competing objectives on devices\nwith limited memory, compute, and power budgets remains a central challenge in\npractical ASR deployment.\n\n\nExisting edge ASR models leverage a full-attention encoder architecture, which\nallows every frame to directly attend to every other frame in a sequence of\nspeech audio. This enables powerful contextual disambiguation as it resolves\nlocally ambiguous acoustics using distant lexical information that occurs\nearlier or later in a chunk of speech audio. However, full attention also\nintroduces quadratic complexity in sequence length and imposes an inherent\n‚Äúencode-the-whole-utterance‚Äù latency profile: in streaming scenarios, the\nencoder must process the entire prefix (or wait for the complete utterance)\nbefore decoder tokens can be emitted, resulting in high TTFT that scales\nlinearly with utterance length. In practical applications, this reduces system\nresponsiveness and limits interactivity.\n\n\nIn this paper, we introduce Moonshine v2, a family of ergodic streaming encoder\nASR models designed specifically for latency-critical edge applications.\nMoonshine v2 models employ sliding-window attention in a position-free encoder\nto enable low-latency streaming inference while maintaining state-of-the-art\naccuracy on standard benchmarks. We train three variants of increasing size‚Äî\ntiny, small, and medium‚Äîand show that the models achieve transcription quality and\nspeed on-par with models 6x their size while running significantly faster (i.e., Whisper Large v3).\nWe release the models under a permissive license, encouraging community adoption for on-device,\nlatency-critical ASR applications.\n\n\nThe paper is structured as follows. Section¬†2 analyzes the\nlatency-accuracy trade-offs inherent in full-attention encoders and motivates\nour sliding window approach. Section¬†3 details the Moonshine v2\narchitecture, including the audio preprocessor, sliding-window encoder, adapter,\nand decoder components. Section¬†4 presents our experimental\nsetup and benchmark results across standard ASR datasets. Finally,\nSection¬†5 discusses implications and future directions for\nergodic streaming ASR.\n\n\n\n011223344556677889910100100100200200300300400400500500600600700700Audio processed (s)TTFT (ms)\n\n\n1\n\n\nFigure 1: Illustrative time-to-first-token (TTFT) for a full-attention\nencoder as a function of audio length, for processors with different peak\nthroughput (TOPS). The estimate includes both a linear non-attention term and\na quadratic self-attention mixing term. The dotted horizontal line shows a 0.1¬†TOPS\nsliding-window encoder with w=20w=20 frames.\nThe dashed line indicates a 250¬†ms one-way delay limit often used as a\npractical upper bound for acceptable interactive voice in private\nnetworks¬†(Cisco Systems, 2026).\n\n\n\n\n2 Motivation\n\nThis section motivates the need for low-latency, streaming-friendly encoder\narchitectures in ASR, highlighting the trade-offs between recognition accuracy\nand time-to-first-token (TTFT) latency in current models.\n\n\n\n2.1 Full-attention encoders: accurate, but latency-heavy\n\nMany high-accuracy ASR systems rely on encoder architectures that use full\nself-attention over the entire input sequence. For example,\nWhisper¬†(Radford et al., 2022) uses a Transformer\nencoder with global attention, and NVIDIA‚Äôs Parakeet models build on\nFastConformer-style encoders¬†(Rekesh et al., 2023).\n\n\nFull attention helps accuracy because it lets each frame incorporate evidence\nfrom any other frame, enabling global disambiguation (e.g., long-range\ncoarticulation, speaker/style consistency, and resolving locally ambiguous\nacoustics using distant lexical context). This ability to integrate long-range\ncontext is one reason these models achieve strong recognition accuracy. However,\nthis same global dependency that enables superior accuracy also creates a\nfundamental latency bottleneck for streaming applications.\n\n\nTime-to-first-token (TTFT).\n\nFor latency-critical ASR, a key metric is TTFT: the wall-clock time from audio\narrival to the first emitted text token. With a full-attention encoder, TTFT\ngrows with the amount of audio that must be encoded before decoding can start.\nMoreover, even with a fixed model size, the attention mixing work grows\nquadratically with sequence length.\n\n\nFigure¬†1 illustrates this effect for a\n100M-parameter encoder processing 50¬†Hz features (Whisper-style). We estimate\nencoder compute as opstotal‚Äã(N)=6‚ÄãP‚ÄãT+4‚Äãd‚ÄãL‚ÄãT2\\mathrm{ops}_{\\mathrm{total}}(N)=6PT+4dLT^{2} with\nT=50‚ÄãNT=50N frames, and convert operations to time assuming a peak throughput of XX\nTOPS (i.e., X‚ãÖ1012X\\cdot 10^{12} ops/s). The plotted curves show the resulting TTFT\n(ms) versus audio duration for several hardware budgets. We also include a\nconstant TTFT line for sliding-window attention at 0.1¬†TOPS using\nopstotal‚Äã(N)=6‚ÄãP‚ÄãT+4‚Äãd‚ÄãL‚ÄãT‚Äãw\\mathrm{ops}_{\\mathrm{total}}(N)=6PT+4dLTw with w=20w=20 frames (matching the\nMoonshine v2 streaming lookback+lookahead window).\n\n\nWe plot only 0.1‚Äì1¬†TOPS because our focus is edge deployment (phones and\nsmaller devices), where achievable throughput is often in the 10s‚Äì100s of GOPS.\nA simple sanity check is\npeak MAC/cycle ‚âà\\approx (instr/cycle)√ó\\times(MAC/instr), e.g., an Arm Cortex-A55 might reach ‚âà16\\approx 16¬†MAC/cycle; at 2.31¬†GHz this is ‚âà37\\approx 37¬†GMAC/s (‚âà74\\approx 74¬†GOPS). Even when edge devices advertise multi-10s of TOPS, sustaining 1¬†TOPS in practice is difficult due to memory bandwidth and thermals, so we focus on the 0.1‚Äì1¬†TOPS regime. The horizontal line at 250¬†ms marks a commonly used one-way delay limit for acceptable interactive voice in private networks¬†(Cisco Systems, 2026).\n\n\nA key takeaway is that even a very strong edge-class budget of 500¬†GOPS\n(‚âà0.5\\approx 0.5¬†TOPS) crosses the 250¬†ms threshold at roughly 4.1¬†s of audio in\nthis model, making ‚Äúresponsive‚Äù first-token latency impractical for longer\nutterances without streaming.\n\n\nFor sliding-window attention, we show only the 0.1¬†TOPS line because it already\nfalls below the 250¬†ms voice-delay limit; higher-throughput hardware would\nreduce the line further.\n\n\n\n\n\n2.2 Sliding-window attention encoders: streaming-friendly latency\n\nA natural way to reduce TTFT is to replace full self-attention with\nsliding-window self-attention, where each frame attends only to a bounded\nlocal neighborhood. With a fixed window size ww, the attention mixing cost\nbecomes linear in sequence length (ùí™‚Äã(T‚Äãw)\\mathcal{O}(Tw) rather than\nùí™‚Äã(T2)\\mathcal{O}(T^{2})), and‚Äîcrucially for streaming‚Äîthe encoder can emit usable\nrepresentations incrementally as soon as the required local context has arrived.\n\n\nIn a causal sliding-window encoder, the repre"
  },
  {
    "title": "Olmix: A Framework for Data Mixing Throughout LM Development",
    "url": "https://arxiv.org/abs/2602.12237v1",
    "source": "arxiv",
    "summary": "Data mixing -- determining the ratios of data from different domains -- is a first-order concern for training language models (LMs). While existing mixing methods show promise, they fall short when applied during real-world LM development. We present Olmix, a framework that addresses two such challenges. First, the configuration space for developing a mixing method is not well understood -- design",
    "full_text": null
  },
  {
    "title": "Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision",
    "url": "https://arxiv.org/abs/2602.12236v1",
    "source": "arxiv",
    "summary": "Neuromorphic vision systems based on spiking neural networks (SNNs) offer ultra-low-power perception for event-based and frame-based cameras, yet catastrophic forgetting remains a critical barrier to deployment in continually evolving environments. Existing continual learning methods, developed primarily for artificial neural networks, seldom jointly optimize accuracy and energy efficiency, with p",
    "full_text": null
  },
  {
    "title": "Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation",
    "url": "https://arxiv.org/abs/2602.12235v1",
    "source": "arxiv",
    "summary": "Efficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility -- and when compression begins to erase task-releva",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nLong-context modeling and compression\nSoft compression in RAG\nMotivation for overflow detection\n\n\n\n3 Methodology\n\n3.1 Problem Setup\n3.2 Context Complexity Measures\n\n3.3 Token Saturation Statistics\n\nHoyer‚Äôs sparsity\nSpectral entropy\nKurtosis\n\n\n\n3.4 Attention Features: Query-conditioned Overflow Signals\n\nMean attention to xRAG tokens\nAttention ratios\nAttention entropy\n\n\n\n3.5 Overflow Detection Methods\n\nFeature-based classification\nLearned probing on vector representations\nRepresentation extraction\nClassifier architectures\n\n\n\n\n\n4 Results\n\n\n4.1 Experimental Setup\n\nEvaluation protocol\n\n\n\n4.2 Main Results\n\n\n4.2.1 RQ1: Characterizing overflow in compressed representations\n\nSaturation statistics distinguish token types but not overflow\nContext complexity provides minimal signal\n\n\n\n4.2.2 RQ2: Efficient overflow detection without full LLM inference\n\nOverflow is detectable immediately after compression\nLLM processing provides no additional signal\n\n\n\n4.2.3 RQ3: The necessity of modeling query-context interactions\n\nJoint representations substantially outperform single-source models\n\n\n\n\n\n\n5 Conclusion\nA Classifiers Ablation Study\nB Features Ablation Study\nC Saturation Statistics\nD Hyperparameters\n\n\n\n\n\nDetecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation\n\n\n\nJulia Belikova1,2,\nDanila Rozhevskii1,\nDennis Svirin1,4,\n\nKonstantin Polev2,\nand Alexander Panchenko1,3\n1Skoltech,\n2Sber AI Lab,\n3AIRI\n\n4Institute for Information Transmission Problems of the Russian Academy of Sciences\n\n\nCorrespondence: {julia.belikova, a.panchenko}@skol.tech\n\n\n\nAbstract\nEfficient long-context processing remains a crucial challenge for contemporary large language models (LLMs), especially in resource-constrained environments. Soft compression architectures promise to extend effective context length by replacing long token sequences with smaller sets of learned compressed tokens. Yet, the limits of compressibility ‚Äì and when compression begins to erase task-relevant content ‚Äì remain underexplored. In this paper, we define token overflow as a regime in which compressed representations no longer contain sufficient information to answer a given query, and propose a methodology to characterize and detect it.\nIn the xRAG soft-compression setting, we find that query-agnostic saturation statistics reliably separate compressed from uncompressed token representations, providing a practical tool for identifying compressed tokens but showing limited overflow detection capability. Lightweight probing classifiers over both query and context xRAG representations detect overflow with 0.72 AUC-ROC on average on HotpotQA, SQuADv2, and TriviaQA datasets, demonstrating that incorporating query information improves detection performance. These results advance from query-independent diagnostics to query-aware detectors, enabling low-cost pre-LLM gating to mitigate compression-induced errors.\n\n\n\nDetecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation\n\n\n\n\n\nJulia Belikova1,2,\nDanila Rozhevskii1,\nDennis Svirin1,4,\n\nKonstantin Polev2,\nand Alexander Panchenko1,3\n\n1Skoltech,\n2Sber AI Lab,\n3AIRI\n\n4Institute for Information Transmission Problems of the Russian Academy of Sciences\n\n\nCorrespondence: {julia.belikova, a.panchenko}@skol.tech\n\n\n\n\n\n\n1 Introduction\n\nLarge language models (LLMs) remain computationally constrained when processing long contexts, even as architectural advances and extended context windows become widely available¬†(Vaswani et al., 2017; Liu et al., 2024). In retrieval-augmented generation (RAG), this limitation is particularly acute: retrieved evidence must be aggressively compressed or truncated, creating a tension between efficiency and faithfulness¬†(Lewis et al., 2020; Aushev et al., 2025). Soft compression architectures address this by mapping long contexts into dense vectors that can be directly consumed by the model, dramatically reducing token count while preserving global semantics¬†(Liao et al., 2025).\n\n\nHowever, the same mechanism that enables extreme compression also introduces a critical failure mode. As more information is packed into a fixed-dimensional compressed token, its representation can enter token overflow: it no longer carries sufficient task-relevant signal for the query and effectively behaves like noise, silently degrading downstream performance. Recent work on trainable tokens shows that individual embeddings have substantial theoretical capacity, but also that practical limits depend strongly on architecture, training, and input complexity¬†(Kuratov et al., 2025). Yet, current compression systems are typically evaluated only via end-task metrics, offering little insight into when a single compressed token crosses from informative to overflowed states.\n\n\nThis paper investigates token overflow in soft compression architectures. We ask: (RQ1) How can we characterize overflow in compressed representations? (RQ2) Can overflow be detected efficiently, without full LLM inference, using lightweight diagnostics? (RQ3) Is overflow detectable from compressed tokens alone, or does it require modeling query-context interactions?\n\n\nTo address these questions, we:\n\n\n‚Ä¢\n\nformalize token overflow and propose a methodology advancing from query-independent to query-aware detection approaches;\n\n\n\n‚Ä¢\n\ndemonstrate that saturation statistics reliably distinguish compressed tokens from standard tokens, providing a practical tool for identifying compressed representations, but show limited overflow detection capability;\n\n\n\n‚Ä¢\n\nshow that attention patterns during generation provide moderate overflow signal but require LLM forward passes;\n\n\n\n‚Ä¢\n\ndevelop learned probing classifiers operating on joint query-context representations that achieve strong overflow detection without LLM inference (Table¬†1), showing that incorporating query information improves detection performance;\n\n\n\n\n\nAlthough our experiments focus on the xRAG architecture, the methodology is general and we expect it to yield similar results when applied to other setups. The source code is publicly available online111https://github.com/s-nlp/overflow-detection.\n\n\n\n\n2 Related Work\n\nLong-context modeling and compression\n\nEfficient long-context processing has been tackled through architectural changes¬†(Beltagy et al., 2020; Zaheer et al., 2020; Dai et al., 2019) and explicit compression. Context compression is systematized into hard, soft, and hybrid paradigms¬†(Liao et al., 2025): hard compression selects token subsets with strict information bottlenecks; soft compression maps contexts into dense vectors accessible via attention; hybrid methods combine both approaches. Our work analyzes the failure modes of soft compression, asking when compressed vectors fail to carry useful task information.\n\n\n\nSoft compression in RAG\n\nRetrieval-augmented generation (RAG) frameworks extend LLMs with external corpora¬†(Lewis et al., 2020), motivating compression of retrieved passages. Several soft compression methods have been proposed: AutoCompressors¬†Chevalier et al. (2023) learn summary vectors by training the model to reconstruct compressed context through attention; ICAE¬†Ge et al. (2024) employs in-context autoencoding to compress sequences into memory slots with combined reconstruction and language modeling objectives (‚àº\\sim4√ó\\times compression, 1% parameters).\nWe focus our experiments on xRAG¬†(Cheng et al., 2024), utilizing it not merely as a baseline, but as a representative projector-based compression paradigm. Unlike autoencoder-based methods that compress context via complex recurrence or reconstruction objectives, xRAG treats dense retrieval embeddings as a distinct modality. It employs a lightweight projector to map these embeddings directly into the LLM‚Äôs input space. This architectural choice isolates the compression mechanism from the complexities of extensive parameter fine-tuning (&lt;&lt;0.1% parameters), allowing us to study the interactions between the projector and the frozen LLM in a controlled setting. By decoupling the retrieval representation from the generative process, xRAG provides clean access to pre- and post-projection states, making it an ideal testbed for analyzing signal degradation and token saturation without the confounders of end-to-end model adaptation.\n\n\n\nMotivation for overflow detection\n\nDetecting information overflow ‚Äì where input complexity exceeds compressed token capacity ‚Äì is critical for optimizing RAG pipelines. It enables adaptive chunking, allowing systems to dynamically resize input segments based on semantic density rather than arbitrary fixed lengths. Furthermore, early overflow detection facilitates computational pruning: identifying and discarding saturated representations immediately after projection prevents wasteful LLM inference on degraded context.\nDespite progress across methods, most evaluations treat compressed vectors as black boxes and focus on downstream metrics¬†(Ge et al., 2024; Cheng et al., 2024). Recent work shows single vectors can theoretically encode thousands of tokens, yet practical capacity depends on architecture and complexity¬†(Kuratov et al., 2025). In contrast, we operationalize capacity limits through overflow detection in xRAG, advancing from query-independent saturation statistics to query-aware learned probing.\n\n\n\n\n\n3 Methodology\n\nOur goal is to characterize and detect token overflow in soft compression architectures across tasks and context regimes. We focus on xRAG-style compressors attached to frozen LLM backbones, studying how compressed token properties change as context complexity increases and downstream quality degrades. We employ a spectrum of detection approaches with increasing query-awareness: from query-agnostic saturation statistics, through query-conditioned attention patterns, to fully query-aware learned probing classifiers.\n\n\nOur methodology is motivated by the observation that the same compressed representation may be "
  },
  {
    "title": "Categorical Flow Maps",
    "url": "https://arxiv.org/abs/2602.12233v1",
    "source": "arxiv",
    "summary": "We introduce Categorical Flow Maps, a flow-matching method for accelerated few-step generation of categorical data via self-distillation. Building on recent variational formulations of flow matching and the broader trend towards accelerated inference in diffusion and flow-based models, we define a flow map towards the simplex that transports probability mass toward a predicted endpoint, yielding a",
    "full_text": null
  },
  {
    "title": "Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser",
    "url": "https://arxiv.org/abs/2602.12229v1",
    "source": "arxiv",
    "summary": "Diffusion alignment adapts pretrained diffusion models to sample from reward-tilted distributions along the denoising trajectory. This process naturally admits a Sequential Monte Carlo (SMC) interpretation, where the denoising model acts as a proposal and reward guidance induces importance weights. Motivated by this view, we introduce Variance Minimisation Policy Optimisation (VMPO), which formula",
    "full_text": null
  },
  {
    "title": "Bandit Learning in Matching Markets with Interviews",
    "url": "https://arxiv.org/abs/2602.12224v1",
    "source": "arxiv",
    "summary": "Two-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as \\textit{low-cost hints} that reveal partial preference information ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Model\n\n2.1 Extended Action Space for Firms‚Äô Uncertainty\n\n\n3 Algorithmic Paradigms and Preliminaries\n4 Centralized Learning\n\n5 Decentralized Learning\n\n5.1 Strategic Firm‚Äôs Rejection Policy\n5.2 Coordinated Decentralized Algorithm with Only Vacancy ùí±‚Äã(t)\\mathcal{V}(t) as Feedback\n5.3 Coordination-Free Decentralized Algorithm with Anonymous Hiring Changes ùí±+‚Äã(t)\\mathcal{V}^{+}(t) as Feedback\n\n\n6 Conclusion\n\nA General Notation, Lemmas, and Observations for the Regret Analysis\n\nA.1 Top-kk Ground-Truth and Estimated Preferences\nA.2 Rounds of Top-kk Alignment\n\n\nB Rejection Variables Update Pseudo-Codes (Algorithms¬†1,¬†2,3, and¬†7)\n\nC Deferred Concepts from the Model (Section¬†2) and Preliminaries (Section¬†3)\n\nC.1 Definition of Local Observations\nC.2 Definition of Convergence\n\n\n\nD Deferred Concepts and Proofs from Section¬†4\n\nD.1 Pseudocode for the Centralized Algorithm\nD.2 Proof of an Auxiliary Lemma\nD.3 Proof of Lemma¬†4.3\nD.4 Proof of Theorem¬†4.1\nD.5 Optimality: Proof of Proposition¬†D.2\n\n\n\nE Deferred Concepts and Proofs from Section¬†5.2\n\nE.1 Necessary Coordination under ùí±‚Äã(t)\\mathcal{V}(t) compared to ùí±+‚Äã(t)\\mathcal{V}^{+}(t)\nE.2 Committing to a Perfect Matching¬†Lemma¬†E.2\nE.3 Counting Updating Stages via Triggered Events\nE.4 Bounding the Number of Updating Stages in Œ±\\alpha-Reducible Markets\nE.5 Proof Sketch of Theorem¬†5.1 for the agent a1a_{1}\nE.6 Proof of Theorem¬†5.1\n\nE.7 Extension of Algorithm¬†2‚Äôs Analysis to General Markets\n\nE.7.1 An Upper Bound on the Expected Number of Updating Stages\nE.7.2 Proof of Convergence to a Stable Matching\n\nE.7.3 Crucial Observations\n\n(i)\n(ii)\n(iii)\n\n\nE.7.4 Proof of¬†Theorem¬†5.2\n\n\n\n\n\nF Deferred Concepts and Proofs from Section¬†5.3\n\nF.1 Crucial Structural Observation\nF.2 Proof Sketch of¬†Theorem¬†5.3 for the agent a1a_{1}\nF.3 Proof of¬†Theorem¬†5.3\n\nF.4 Extension of Coordination-Free Algorithm¬†3 to General Markets\n\nF.4.1 Impossibility of Guaranteeing Convergence to the Agent-Optimal Stable Matching\nF.4.2 Moving from k=2k=2 to k=3k=3\nF.4.3 Necessity of applying to more than one firm per round for coordination-free decentralized learning\nF.4.4 Extended¬†Algorithm¬†3\nF.4.5 Crucial Theorems, Lemmas, and Observations\nF.4.6 Proof of¬†Theorem¬†5.4\n\n\n\n\n\nG Discussion on Incentive Compatibility in Decentralized Learning\n\nG.1 Incentive compatibility in matching markets with a unique stable matching\n\nG.2 Incentive compatibility in matching markets with multiple stable matchings\n\nAgents.\nFirms.\n\n\n\n\n\nH Optimality of the Results\n\nH.1 Optimality of the Centralized Bounds¬†Theorem¬†4.1\nH.2 Optimality of the Coordinated Decentralized Bounds¬†Theorem¬†5.2 and¬†Theorem¬†5.1\nH.3 Optimality of the Coordination-Free Decentralized Bounds¬†Theorem¬†5.3 and¬†Theorem¬†5.4\n\n\n\nI Improvement over Prior Work\n\nI.1 Applicability of Strategic Firm‚Äôs Rejection Policy¬†Algorithm¬†1\nI.2 Improvements with Limited Firm-Side Feedback\n\n\nJ Future Directions and Open Questions\n\n\n\n\n\nBandit Learning in Matching Markets with Interviews\n\n\n\nAmirmahdi Mirfakhar\n\n‚ÄÉ‚ÄÉ\nXuchuang Wang\n\n‚ÄÉ‚ÄÉ\nMengfan Xu\n\n‚ÄÉ‚ÄÉ\nHedyeh Beyhaghi\n\n‚ÄÉ‚ÄÉ\nMohammad Hajiesmaili \n\nUniversity of Massachusetts Amherst\n\n\n\nAbstract\nTwo-sided matching markets rely on preferences from both sides, yet it is often impractical to evaluate preferences. Participants, therefore, conduct a limited number of interviews, which provide early, noisy impressions and shape final decisions. We study bandit learning in matching markets with interviews, modeling interviews as low-cost hints that reveal partial preference information to both sides. Our framework departs from existing work by allowing firm-side uncertainty: firms, like agents, may be unsure of their own preferences and can make early hiring mistakes by hiring less preferred agents. To handle this, we extend the firm‚Äôs action space to allow strategic deferral (choosing not to hire in a round), enabling recovery from suboptimal hires and supporting decentralized learning without coordination.\nWe design novel algorithms for (i) a centralized setting with an omniscient interview allocator and (ii) decentralized settings with two types of firm-side feedback. Across all settings, our algorithms achieve time-independent regret, a substantial improvement over the O‚Äã(log‚Å°T)O(\\log T) regret bounds known for learning stable matchings without interviews. Also, under mild structured markets, decentralized performance matches the centralized counterpart up to polynomial factors in the number of agents and firms.\n\n\n\n1 Introduction\n\nTwo-sided matching markets¬†[19] provide a foundational model for labor markets¬†[20], school choice¬†[1], and online platforms¬†[9]. A market comprises two sides (e.g., nn agents and mm firms), each with preferences over the other side. A matching is stable if no agent‚Äìfirm pair would both prefer to deviate from their current assignments¬†[7].\nIn many markets, preferences are not known a priori: participants learn them through interaction, and any given participant can only evaluate a small fraction of potential partners. This motivates bandit learning in matching markets¬†[14, 17, 15, 10, 8], where agents repeatedly apply to firms, observe outcomes, and aim to learn a stable matching while minimizing regret. Most existing work assumes (i) only agents are uncertain and (ii) firms follow fixed, known preferences and always hire among applicants. These assumptions simplify decentralized dynamics, but do not capture the following two pervasive features of real markets.\n\n\nTwo missing ingredients: interviews and uncertain firms.\nFirst, participants typically engage in low-cost preliminary interactions (interviews, visits, screening calls) before submitting final applications. Such interactions produce noisy side-observations of match quality that can accelerate learning. This resembles bandits with hints¬†[5, 18] and, more broadly, algorithms with predictions¬†[16]. However, interviews create a key structural constraint absent from existing hinted-bandit models: they simultaneously reveal information and restrict feasible actions, since an agent may apply only to an interviewed firm¬†[13, 3].\n\n\nSecond, firms may also be uncertain about their preferences (i.e., their rankings must be learned). Early misrankings can cause firms to reject strong candidates, slowing convergence to stability and increasing regret. To model how firms hedge against uncertainty in practice, we allow a firm to defer hiring in a round (remain vacant) rather than commit to an applicant it currently deems suboptimal. We emphasize that this is a modeled action option (not a game-theoretic manipulation): it expands the firm action space in a way that can stabilize decentralized learning dynamics and is consistent with evidence that firm-side hiring deferrals can shape stable outcomes in static/non-learning settings¬†[11].\n\n\nModel at a glance.\nWe study two-sided bandit learning with interviews and uncertainty on both sides. In round tt, every agent aa selects kk firms to interview. Each interviewed pair (a,f)(a,f) yields stochastic, noisy feedback to both sides with unknown means Œºa,f\\mu_{a,f} and Œºf,a\\mu_{f,a}, representing their expected utilities from matching with each other. After interviewing, each agent applies to one of its interviewed firms; a firm may accept its most-preferred applicant or defer and remain vacant. When a match occurs, both sides receive stochastic rewards drawn from the corresponding distributions.\n\n\nWe evaluate performance via agent regret: the gap between the expected reward of the agent‚Äôs stable match and the agent‚Äôs accrued reward over TT rounds. We study both a centralized setting with an interview allocator and decentralized settings without it. A central challenge in decentralization is the information revealed to agents: we restrict attention to minimal public firm-side signals revealed to all agents‚Äîeither vacancy-only feedback (whether a firm filled its position) or anonymous hiring-change feedback (whether a firm‚Äôs hire changed), without identities. As shown in Appendix¬†5.2, such coarse firm-side signal is necessary for decentralized algorithms especially in unstructured markets.\n\n\nA guiding question.\nExisting bandit-matching work without interviews typically yields regret that scales at least logarithmically with the horizon, often O‚Äã(log‚Å°T)O(\\log T), even under stronger observability assumptions¬†[14, 17, 15]. Interviews add side-observations before committing to an application, suggesting that learning could be dramatically faster. This leads to our central question:\nCan a market leverage a small number of interviews per round to learn a stable matching with horizon (time)-independent regret, in a decentralized manner with only anonymous firm-side signals, even when firms themselves are uncertain and may defer hiring?\nAnswering this question requires addressing two challenges.\n\n\n‚ñ∂\\blacktriangleright Interviews as hints under competition and stability:\nInterviews provide side-observations, but also impose an action constraint (apply only to interviewed firms). Unlike existing bandits-with-hints results in single-agent¬†[23, 4] and multi-agent¬†[18], our environment couples learners through competition: agents‚Äô choices interact and can create persistent collisions, which impede stability and inflate regret. Moreover, in our decentralized setting, agents do not observe match identities or firm preference list; they only see vacancy or anonymous hiring-change signals.\n\n\n‚ñ∂\\blacktriangleright Uncertain firms and controlled deferral:\nWhen firms learn their rankings, early misrankings can induce rejections of strong applicants, creating feedback loops that redirect agents and slow convergence. Allowing firms to defer can mitigate premature commitments, but it must be carefully controlled: deferral should not itself become a source of regret, and it must support convergence to stable outcomes.\n\n\nTable 1: Performance of the algorithms: (‚àó)(*) centralized, (‚Ä†)(\\dagger) coordinated and (‚Ä°)(\\ddagger) coordination-free decentralized; in all de"
  },
  {
    "title": "Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training",
    "url": "https://arxiv.org/abs/2602.12222v1",
    "source": "arxiv",
    "summary": "Supervised fine-tuning (SFT) is computationally efficient but often yields inferior generalization compared to reinforcement learning (RL). This gap is primarily driven by RL's use of on-policy data. We propose a framework to bridge this chasm by enabling On-Policy SFT. We first present \\textbf{\\textit{Distribution Discriminant Theory (DDT)}}, which explains and quantifies the alignment between da",
    "full_text": null
  },
  {
    "title": "The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics",
    "url": "https://arxiv.org/abs/2602.12218v1",
    "source": "arxiv",
    "summary": "Determining whether neural models internalize physical laws as world models, rather than exploiting statistical shortcuts, remains challenging, especially under out-of-distribution (OOD) shifts. Standard evaluations often test latent capability via downstream adaptation (e.g., fine-tuning or high-capacity probes), but such interventions can change the representations being measured and thus confou",
    "full_text": null
  }
]