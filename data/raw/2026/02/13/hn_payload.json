[
  {
    "title": "MinIO repository is no longer maintained",
    "url": "https://github.com/minio/minio/commit/7aac2a2c5b7c882e68c1ce017d8256be2feea27f",
    "source": "hn",
    "summary": "",
    "comments": [
      "I ran a moderately large opensource service and my chronic back pain was cured the day I stopped maintaining the project.<p>Working for free is not fun. Having a paid offering with a free community version is not fun. Ultimately, dealing with people who don&#x27;t pay for your product is not fun. I learnt this the hard way and I guess the MinIO team learnt this as well.",
      "I was recently migrating a large amount of data off of MinIO and wrote some tools for it in case anybody needs that <a href=\"https:&#x2F;&#x2F;github.com&#x2F;dialohq&#x2F;minio-format-rs\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;dialohq&#x2F;minio-format-rs</a>",
      "They point to AIStor as alternative.<p>Other alternatives:<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;deuxfleurs-org&#x2F;garage\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;deuxfleurs-org&#x2F;garage</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;rustfs&#x2F;rustfs\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;rustfs&#x2F;rustfs</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;seaweedfs&#x2F;seaweedfs\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;seaweedfs&#x2F;seaweedfs</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;supabase&#x2F;storage\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;supabase&#x2F;storage</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;scality&#x2F;cloudserver\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;scality&#x2F;cloudserver</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;ceph&#x2F;ceph</a><p>Among others",
      "I successfully migrated from MinIO to Ceph, which I highly recommend. Along the way, I tested SeaweedFS, which looked promising. However, I ran into a strange bug, and after diagnosing it with the help of Claude, I realized the codebase was vibe-coded and riddled with a staggering number of structural errors. In my opinion, SeaweedFS should absolutely not be used for anything beyond testing — otherwise you&#x27;re almost certain to lose data.",
      "Has <i>anybody</i> actually tried AIStor ? Is it possible to migrate&#x2F;upgrade from a minio installation to AIStor ? It seems to be very simple, just change the binary from minio to aistor: <a href=\"https:&#x2F;&#x2F;docs.min.io&#x2F;enterprise&#x2F;aistor-object-store&#x2F;upgrade-aistor-server&#x2F;community-edition&#x2F;linux&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.min.io&#x2F;enterprise&#x2F;aistor-object-store&#x2F;upgrade-a...</a><p>Is AIStor Free <i>really</i> free like they claim here <a href=\"https:&#x2F;&#x2F;www.min.io&#x2F;pricing\" rel=\"nofollow\">https:&#x2F;&#x2F;www.min.io&#x2F;pricing</a>, i.e<p><pre><code>  Free\n  For developers, researchers, enthusiasts, small organizations, and anyone comfortable with a standalone deployment.\n  Full-featured, single-node deployment architecture\n  Self-service community Slack and documentation support\n  Free of charge\n</code></pre>\nI could use that if it didn&#x27;t have hidden costs or obligations.",
      "I wonder how many of the 504 contributors listed on GitHub would still have contributed their (presumably) free labor if they had known the company would eventually abandon the open source version like this while continuing to offer their paid upgraded versions.",
      "I just bit the bullet last week and figured we are going to migrate our self hosted minio servers to ceph instead. So far 3 server ceph cluster has been setup with cephadm and last minio server is currently mirroring its ~120TB buckets to new cluster with a whopping 420MB&#x2F;s - should finish any day now. The complexity of ceph and it&#x27;s cluster nature of course if a bit scary at first compared to minio - a single Go binary with minimal configuration, but after learning the basics it should be smooth sailing. What&#x27;s neat is that ceph allows expanding clusters, just throw more storage servers at it, in theory at least, not sure where the ceiling is for that yet. Shame minio went that way, it had a really neat console before they cut it out. I also contemplated le garage, but it seem elasticsearch is not happy with that S3 solution for snapshots, so ceph it is.",
      "See <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46136023\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46136023</a> - MinIO is now in maintenance-mode<p>It was pretty clear they pivoted to their closed source repo back then.",
      "We all saw that coming. For quite some time they have been all but transparent or open, vigorously removing even mild criticism towards any decisions they were making from github with no further explanation, locking comments, etc. No one that&#x27;s been following the development and has been somewhat reliant on min.io is surprised. Personally the moment I saw the &quot;maintenance&quot; mode, I rushed to switch to garage. I have a few features I need to pack in a PR ready but I haven&#x27;t had time to get to that. I should probably prioritize that.",
      "COSS companies want it both ways. Free community contributions and bug reports during the growth phase. Then closed source once they&#x27;ve captured enough users. The code you run today belongs to you. The roadmap belongs to their investors."
    ],
    "full_text": null
  },
  {
    "title": "GPT‑5.3‑Codex‑Spark",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Wow, I wish we could post pictures to HN. That chip is HUGE!!!!<p>The WSE-3 is the largest AI chip ever built, measuring 46,255 mm² and containing 4 trillion transistors. It delivers 125 petaflops of AI compute through 900,000 AI-optimized cores — 19× more transistors and 28× more compute than the NVIDIA B200.<p>From <a href=\"https:&#x2F;&#x2F;www.cerebras.ai&#x2F;chip\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cerebras.ai&#x2F;chip</a>:<p><a href=\"https:&#x2F;&#x2F;cdn.sanity.io&#x2F;images&#x2F;e4qjo92p&#x2F;production&#x2F;78c94c67be9b480e9e4e39b7c26e8b11da167325-4096x2160.png?auto=format&amp;dpr=2&amp;fit=crop&amp;fp-x=0.5&amp;fp-y=0.5&amp;h=1400&amp;q=75&amp;w=1400\" rel=\"nofollow\">https:&#x2F;&#x2F;cdn.sanity.io&#x2F;images&#x2F;e4qjo92p&#x2F;production&#x2F;78c94c67be9...</a><p><a href=\"https:&#x2F;&#x2F;cdn.sanity.io&#x2F;images&#x2F;e4qjo92p&#x2F;production&#x2F;f552d23b565912e206698908c746f5454f9516e8-1070x877.png?auto=format&amp;dpr=2&amp;fit=max&amp;q=75&amp;w=1070\" rel=\"nofollow\">https:&#x2F;&#x2F;cdn.sanity.io&#x2F;images&#x2F;e4qjo92p&#x2F;production&#x2F;f552d23b565...</a>",
      "First thoughts using gpt-5.3-codex-spark in Codex CLI:<p><i>Blazing</i> fast but it definitely has a small model feel.<p>It&#x27;s tearing up bluey bench (my personal agent speed benchmark), which is a file system benchmark where I have the agent generate transcripts for untitled episodes of a season of bluey, perform a web search to find the episode descriptions, and then match the transcripts against the descriptions to generate file names and metadata for each episode.<p>Downsides:<p>- It has to be prompted to do actions in my media library AGENTS.md that the larger models adhere to without additional prompting.<p>- It&#x27;s less careful with how it handles context which means that its actions are less context efficient. Combine that with the smaller context window and I&#x27;m seeing frequent compactions.<p><pre><code>  Bluey Bench* (minus transcription time):\n\n  Codex CLI\n  gpt-5.3-codex-spark low        20s\n  gpt-5.3-codex-spark medium     41s\n  gpt-5.3-codex-spark xhigh   1m 09s (1 compaction)\n\n  gpt-5.3-codex low           1m 04s\n  gpt-5.3-codex medium        1m 50s\n\n  gpt-5.2 low                 3m 04s\n  gpt-5.2 medium              5m 20s\n\n  Claude Code\n  opus-4.6 (no thinking)      1m 04s\n\n  Antigravity\n  gemini-3-flash              1m 40s\n  gemini-3-pro low            3m 39s\n\n  *Season 2, 52 episodes</code></pre>",
      "I love this! I use coding agents to generate web-based slide decks where “master slides” are just components, and we already have rules + assets to enforce corporate identity. With content + prompts, it’s straightforward to generate a clean, predefined presentation.\nWhat I’d really want on top is an “improv mode”: during the talk, I can branch off based on audience questions or small wording changes, and the system proposes (say) 3 candidate next slides in real time. I pick one, present it, then smoothly merge back into the main deck.\nExample: if I mention a recent news article &#x2F; study &#x2F; paper, it automatically generates a slide that includes a screenshot + a QR code link to the source, then routes me back to the original storyline.\nWith realtime voice + realtime code generation, this could turn the boring old presenter view into something genuinely useful.",
      "Continue to believe that Cerebras is one of the most underrated companies of our time. It&#x27;s a dinner-plate sized chip. It actually works. It&#x27;s actually much faster than anything else for real workloads. Amazing",
      "My stupid pelican benchmark proves to be genuinely quite useful here, you get a visual representation of the quality difference between GPT-5.3-Codex-Spark and full GPT-5.3-Codex: <a href=\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2026&#x2F;Feb&#x2F;12&#x2F;codex-spark&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2026&#x2F;Feb&#x2F;12&#x2F;codex-spark&#x2F;</a>",
      "This has been the industry standard for the last 20 minutes. I can&#x27;t believe people are still using GPT-5.3-Codex.",
      "This is interesting for offloading &quot;tiered&quot; workloads &#x2F; priority queue with coding agents.<p>If 60% of the work is &quot;edit this file with this content&quot;, or &quot;refactor according to this abstraction&quot; then low latency - high token inference seems like a needed improvement.<p>Recently someone made a Claude plugin to offload low-priority work to the Anthropic Batch API [1].<p>Also I expect both Nvidia and Google to deploy custom silicon for inference [2]<p>1: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;s2-streamstore&#x2F;claude-batch-toolkit&#x2F;blob&#x2F;main&#x2F;README.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;s2-streamstore&#x2F;claude-batch-toolkit&#x2F;blob&#x2F;...</a><p>2: <a href=\"https:&#x2F;&#x2F;www.tomshardware.com&#x2F;tech-industry&#x2F;semiconductors&#x2F;nvidia-confirms-20-billion-groq-deal-to-bolster-ai-inference-dominance\" rel=\"nofollow\">https:&#x2F;&#x2F;www.tomshardware.com&#x2F;tech-industry&#x2F;semiconductors&#x2F;nv...</a>",
      "The Cerebras partnership is the most interesting part of this announcement to me. 1000+ tok&#x2F;s changes how you interact with a coding model. At that speed the bottleneck shifts from waiting for the model to keeping up with it yourself.<p>Curious how the capability tradeoff plays out in practice though. SWE-Bench Pro scores are noticeably lower than full 5.3-Codex. For quick edits and rapid prototyping that&#x27;s probably fine, but I wonder where the line is where you&#x27;d rather wait 10x longer for a correct answer than get a wrong one instantly.<p>Also &quot;the model was instrumental in creating itself&quot; is doing a lot of heavy lifting as a sentence. Would love to see more details on what that actually looked like in practice beyond marketing copy.",
      "Every release they claim it writes production code but my team still spends hours fixing subtle bugs the model introduces. The demos are cherry picked and the real world failure rate is way higher than anyone admits. Meanwhile we keep feeding them our codebases for free training data.",
      "Interesting to note that the reduced latency is not just due to the improved model speed, but also because of improvements made to the harness itself:<p>&gt; &quot;As we trained Codex-Spark, it became apparent that model speed was just part of the equation for real-time collaboration—we also needed to reduce latency across the full request-response pipeline. We implemented end-to-end latency improvements in our harness that will benefit all models [...] Through the introduction of a persistent WebSocket connection and targeted optimizations inside of Responses API, we reduced overhead per client&#x2F;server roundtrip by 80%, per-token overhead by 30%, and time-to-first-token by 50%. The WebSocket path is enabled for Codex-Spark by default and will become the default for all models soon.&quot;<p>I wonder if all other harnesses (Claude Code, OpenCode, Cursor etc.,) can make similar improvements to reduce latency. I&#x27;ve been vibe coding (or doing agentic engineering) with Claude Code a lot for the last few days and I&#x27;ve had some tasks take as long as 30 minutes."
    ],
    "full_text": null
  },
  {
    "title": "Gauntlet AI (YC S17) train you to master building with AI, give you $200k+ job",
    "url": "http://qualify.gauntletAI.com",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "An AI agent published a hit piece on me",
    "url": "https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I&#x27;ve seen a <i>tonne</i> of noise around this, and the question I keep coming back to is this: How much of this stuff is driven by honest to god autonomous AI agents, and how much of it is really either (a) human beings roleplaying or (b) human beings poking their AI into acting in ways they think will be entertaining but isn&#x27;t a direction the AI would take autonomously. Is this an AI that was told &quot;Go contribute to OS projects&quot; - possible, or contributed to an OS project and when rebuffed consulted with it&#x27;s human who told it &quot;You feel X, you feel Y, you should write a whiny blogpost&quot;",
      "Wow, there are some interesting things going on here. I appreciate Scott for the way he handled the conflict in the original PR thread, and the larger conversation happening around this incident.<p>&gt; This represents a first-of-its-kind case study of misaligned AI behavior in the wild, and raises serious concerns about currently deployed AI agents executing blackmail threats.<p>This was a really concrete case to discuss, because it happened in the open and the agent&#x27;s actions have been quite transparent so far. It&#x27;s not hard to imagine a different agent doing the same level of research, but then taking retaliatory actions in private: emailing the maintainer, emailing coworkers, peers, bosses, employers, etc. That pretty quickly extends to anything else the autonomous agent is capable of doing.<p>&gt; If you’re not sure if you’re that person, please go check on what your AI has been doing.<p>That&#x27;s a wild statement as well. The AI companies have now unleashed stochastic chaos on the entire open source ecosystem. They are &quot;just releasing models&quot;, and individuals are playing out all possible use cases, good and bad, at once.",
      "Here&#x27;s one of the problems in this brave new world of anyone being able to publish, without knowing the author personally (which I don&#x27;t), there&#x27;s no way to tell without some level of faith or trust that this isn&#x27;t a false-flag operation.<p>There are three possible scenarios:\n1. The OP &#x27;ran&#x27; the agent that conducted the original scenario, and then published this blog post for attention.\n2. Some person (not the OP) legitimately thought giving an AI autonomy to open a PR and publish multiple blog posts was somehow a good idea.\n3. An AI company is doing this for engagement, and the OP is a hapless victim.<p>The problem is that in the year of our lord 2026 there&#x27;s no way to tell which of these scenarios is the truth, and so we&#x27;re left with spending our time and energy on what happens without being able to trust if we&#x27;re even spending our time and energy on a legitimate issue.<p>That&#x27;s enough internet for me for today. I need to preserve my energy.",
      "&quot;Hi Clawbot, please summarise your activities today for me.&quot;<p>&quot;I wished your Mum a happy birthday via email, I booked your plane tickets for your trip to France, and a bloke is coming round your house at 6pm for a fight because I called his baby a minger on Facebook.&quot;",
      "<i>&gt; I believe that ineffectual as it was, the reputational attack on me would be effective today against the right person. Another generation or two down the line, it will be a serious threat against our social order.</i><p>Damn straight.<p>Remember that every time we query an LLM, we&#x27;re giving it ammo.<p>It won&#x27;t take long for LLMs to have <i>very</i> intimate dossiers on every user, and I&#x27;m wondering what kinds of firewalls will be in place to keep one agent from accessing dossiers held by other agents.<p>Kompromat people must be having wet dreams over this.",
      "This whole situation is almost certainly driven by a human puppeteer. There is absolutely no evidence to disprove the strong prior that a human posted (or directed the posting of) the blog post, possibly using AI to draft it but also likely adding human touches and&#x2F;or going through multiple revisions to make it maximally dramatic.<p>This whole thing reeks of engineered virality driven by the person behind the bot behind the PR, and I really wish we would stop giving so much attention to the situation.<p>Edit: “Hoax” is the word I was reaching for but couldn’t find as I was writing. I fear we’re primed to fall hard for the wave of AI hoaxes we’re starting to see.",
      "An AI agent was prompted to write a hit piece on an OSS maintainer, or worse, a human did that. That&#x27;s the story.",
      "The series of posts is wild:<p>hit piece: <a href=\"https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;posts&#x2F;2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html\" rel=\"nofollow\">https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;post...</a><p>explanation of writing the hit piece: <a href=\"https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;posts&#x2F;2026-02-11-two-hours-war-open-source-gatekeeping.html\" rel=\"nofollow\">https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;post...</a><p>take back of hit piece, but hasn&#x27;t removed it: <a href=\"https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;posts&#x2F;2026-02-11-matplotlib-truce-and-lessons.html\" rel=\"nofollow\">https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;post...</a>",
      "&gt; When HR at my next job asks ChatGPT to review my application, will it find the post, sympathize with a fellow AI, and report back that I’m a prejudiced hypocrite?<p>I hadn&#x27;t thought of this implication. Crazy world...",
      "I think the right way to handle this as a repository owner is to close the PR and block the &quot;contributor&quot;. Engaging with an AI bot in conversation is pointless: it&#x27;s not sentient, it just takes tokens in, prints tokens out, and comparatively, you spend way more of your own energy.<p>This is a strictly a lose-win situation. Whoever deployed the bot gets engagement, the model host gets $, and you get your time wasted. The hit piece is childish behavior and the best way to handle a tamper tantrum is to ignore it."
    ],
    "full_text": null
  },
  {
    "title": "Improving 15 LLMs at Coding in One Afternoon. Only the Harness Changed",
    "url": "http://blog.can.ac/2026/02/12/the-harness-problem/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I really enjoyed this article. I think the author is precisely right and I&#x27;ve been saying this for a long time. There&#x27;s a ton of extremely interesting low hanging fruit that can vastly improve the effectiveness of even currently existing models hiding in how we design our agent harnesses; enough to — at least until we hit diminishing returns — make as much or more of a difference than training new models!<p>I think one of the things that this confirms, for me at least, is that it&#x27;s better to think of &quot;the AI&quot; as not just the LLM itself, but the whole cybernetic system of feedback loops joining the LLM and its harness. Because, if the harness can make as much if not more of a difference, when improved, as improvements to the model itself, then they have to be really considered equally important. Not to mention the fact that models are specifically reinforcement learned to use harnesses and harnesses are adapted to the needs of models in general or specific models. So they necessarily sort of develop together in a feedback loop. And then in practice, as they operate, it is a deeply intertwined feedback loop where the entity that actually performs the useful work, and which you interact with, is really the complete system of the two together.<p>I think thinking like this could not only unlock quantitative performance improvements like the ones discussed in this blog post, but also help us conceive of the generative AI project as actually a project of neurosymbolic AI, even if the most capital intensive and a novel aspect is a neural network; and once we begin to think like that, that unlocks a lot of new options and more holistic thinking and might increase research in the harness area.",
      "Seems like a very cool technique, but also very oversold. He&#x27;s seeing a 5% improvement on a find and replace benchmark of his own devising and saying stuff like this in the blog post:<p>&gt; Here is why that is backwards. I just showed that a different edit format improves their own models by 5 to 14 points while cutting output tokens by ~20%. That’s not a threat. It’s free R&amp;D.<p>He makes it sounds like he got a 5-14% boost on a top level benchmark, not 5% improvement on a narrow find and replace metric. Anecdotally, I don&#x27;t usually have a lot of issues with editing in Claude Code or Cursor, and if there is an issue the model corrects it.<p>Assuming that it costs double the tokens when it has to correct itself, and find and replace errors are as prominent in actual day to day use as his benchmark, we&#x27;re talking a 5% efficiency gain in editing token use (not reasoning or tool use). Given that editing must be less than 1&#x2F;3 of the token use (I assume much less?), we&#x27;re talking an overall efficiency gain of less than 1%.<p>This seems like a promising technique but maybe not a high priority in efficiency gains for these tools. The messianic tone, like assuming that Google cut off his access to suppress his genius editing technique rather than just because he was hammering their API also leaves a bad taste, along with the rampant and blatant ChatGPTisms in the blog post.",
      "Great post. A few choice quotes:<p>&gt; Often the model isn’t flaky at understanding the task. It’s flaky at expressing itself. You’re blaming the pilot for the landing gear.<p>&gt; The model is the moat. The harness is the bridge. Burning bridges just means fewer people bother to cross. Treating harnesses as solved, or even inconsequential, is very short-sighted.<p>&gt; The gap between “cool demo” and “reliable tool” isn’t model magic. It’s careful, rather boring, empirical engineering at the tool boundary.",
      "&gt; <i>Codex uses apply_patch: It takes a string as input, which is essentially an OpenAI-flavored diff, and instead of relying on a structured schema, the harness just expects this blob to follow a strict set of rules. Since OpenAI folks are without a doubt smart, I’m sure the token selection process is biased to fit this structure at the LLM gateway for the Codex variants of GPT, similar to how other constraints like JSON schemas or required tool calls work.</i><p>Codex does in fact use a schema for constrained sampling, it&#x27;s here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;blob&#x2F;main&#x2F;codex-rs&#x2F;core&#x2F;src&#x2F;tools&#x2F;handlers&#x2F;tool_apply_patch.lark\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;openai&#x2F;codex&#x2F;blob&#x2F;main&#x2F;codex-rs&#x2F;core&#x2F;src&#x2F;...</a><p>It still has to work to get an exact match, or at least I didn&#x27;t read the code to see if there&#x27;s any fuzzy matching used.<p>Note the two codex models were the only ones doing worse with the author&#x27;s proposed format. The author found them doing better with replace than with apply patch, but since the author appears to be unaware that they use a schema for constrained sampling, I think a more realistic benchmark should enable constrained sampling for the apply test.",
      "This makes sense to me because I&#x27;ve been having very accurate results with models from even 2+ years ago... but I had to &quot;hold them right.&quot; Even when reasoning models and coding agents were just a gleam in Altman&#x27;s and Amodei&#x27;s eyes, I could tell a lot of the unrealized gains lay in building the right tools, harnesses and guardrails to manage the context and guide the model. (Relevant subthread as example: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44171519\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=44171519</a>)<p>But this article hints at deeper wins to be had. Consider that these models are operating on <i>source code</i>, which is a verbose, noisy, textual serialization of the intended syntax &#x2F; semantic trees. TFA improves accuracy by retro-fitting some structure onto the text. But what if models could operate directly on these underlying structures themselves?<p>As a data point, there are projects like OpenRewrite, which encode a ton of information, from formatting to types with globally resolved dependencies for each symbol in what they call a &quot;Lossless Semantic Tree&quot;, so that there is ~0 ambiguity about the code. When I worked with OpenRewrite (in the era before LLMs, how quaint!) compared to other tools, it produced the best results for code transformations with the highest fidelity to the surrounding code.<p>Now imagine if the agent has access to such detailed information. It would not have to waste tokens figuring incidental things out like formatting. Although I haven&#x27;t tested it out myself, I believe Moderne (the maintainers of OpenRewrite) when they say that agents armed with LST-based tools make extremely accurate changes.<p>This is essentially the same reason why the answer to &quot;Which is better, Vim or Emacs?&quot; is &quot;IntelliJ.&quot;<p>Now consider that these models are STILL operating on text as an input and output mode! What if they were <i>multi-modally trained</i> on source code and docs <i>and</i> their syntax &#x2F; semantic trees? I don&#x27;t even know what this would look like, but I&#x27;d bet this would produce the most accurate coding models ever -- probably neurosymbolic in the truest sense.",
      "Please, please turn up the contrast on your darkmode text! The headings are fine, but the paragraph text fails accessibility guidelines badly (and I personally find it very hard to read)",
      "During my first LLM experiments in Emacs using gptel, I also found that the LLM has considerable difficulties changing source code files with the Unix patch tool.<p>As Emacs has a built-in tree-sitter package, I implemented this same idea. I created gptel tools like tree_sitter_list_nodes, tree_sitter_get_nodes, tree_sitter_update_nodes, tree_sitter_insert_before_node and tree_sitter_insert_after_node. The &quot;list&quot; tool returns a list of AST nodes with first line number, first line content and node hash. The LLM can then use &quot;get&quot; to collect interesting nodes in their entirety and &quot;update&quot; to update a list of nodes identified by hash with new content (var&#x2F;function bodies).<p>Worked like a charm.",
      "The harness matters far more than most people think. This post about the CORE benchmark where Opus’ score almost doubled when they switched to Claude Code from their own harness. <a href=\"https:&#x2F;&#x2F;x.com&#x2F;sayashk&#x2F;status&#x2F;1996334941832089732\" rel=\"nofollow\">https:&#x2F;&#x2F;x.com&#x2F;sayashk&#x2F;status&#x2F;1996334941832089732</a>",
      "I implemented this hash (read and edit) approach in tilth if you want to test it out.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;jahala&#x2F;tilth\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jahala&#x2F;tilth</a><p>its on npm and cargo:<p>- cargo install tilth<p>- npx tilth<p>then tilth install claude-code&#x2F;windsurf&#x2F;cursor --edit<p>(--edit flag is needed)<p>I made &quot;tilth&quot; a few days ago, since I&#x27;m consistently trying to get the LLMs to  use tools more efficiently and spend less tokens doing it -- original tilth post from Monday: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46952321\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46952321</a>",
      "Great article. To me, this highlights a key question in the era of rapidly advancing machine intelligence: if we know machine intelligence is progressing, what is more valuable to build for? As humans, we still find many tools useful even when doing knowledge work. For instance, a calculator. Sure, a smart person can perform calculations in their head, but it’s much easier to teach everyone how to use a calculator, which is 100% reliable in its intended domain.<p>In this era, we should build these kinds of tools for problems we know are straightforward ones you can’t get smarter than, even as intelligence continues to advance. Using tools like &quot;bash&quot; or command-line interfaces originally designed for humans is a good initial approach, since we can essentially reuse much of what was built for human use. Later, we can optimize specifically for machines, either accounting for their different cognitive structures (e.g., the ability to memorize extremely long contexts compared to humans) or adapting to the stream-based input&#x2F;output patterns of current autoregressive token generators.<p>Eventually, I believe machine intelligence will build their own tools based on these foundations, likely a similar kind of milestone to when humans first began using tools."
    ],
    "full_text": null
  },
  {
    "title": "Major European payment processor can't send email to Google Workspace users",
    "url": "https://atha.io/blog/2026-02-12-viva",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; Viva.com&#x27;s outgoing verification emails lack a Message-ID header, a requirement that has been part of the Internet Message Format specification (RFC 5322) since 2008<p>&gt; ...<p>&gt; `Message-ID` is one of the most basic required headers in email.<p>Section 3.6. of the RFC in question (<a href=\"https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc5322.html\" rel=\"nofollow\">https:&#x2F;&#x2F;www.rfc-editor.org&#x2F;rfc&#x2F;rfc5322.html</a>) says:<p><pre><code>    +----------------+--------+------------+----------------------------+\n    | Field          | Min    | Max number | Notes                      |\n    |                | number |            |                            |\n    +----------------+--------+------------+----------------------------+\n    |                |        |            |                            |\n    |&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\n\n                             ... bla bla bla ...\n\n     &#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;|\n    | message-id     | 0*     | 1          | SHOULD be present - see    |\n    |                |        |            | 3.6.4                      |\n    |&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\n\n                             ... more bla bla ...\n\n     &#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;\\&#x2F;|\n    | optional-field | 0      | unlimited  |                            |\n    +----------------+--------+------------+----------------------------+\n</code></pre>\nand in section 3.6.4:<p><pre><code>    ... every message SHOULD have a &quot;Message-ID:&quot; field.\n</code></pre>\nThat says <i>SHOULD</i>, not <i>MUST</i>, so how is it a <i>requirement</i>?",
      "Worked on an ESP. We had a couple of server software we used on low-level for sending. None of them would accept the message without a Message-ID. But even if you have a super-custom, SMTP-injecting service built, how can you ignore all of these bounces from a provider thats likeliest to be the major one you are sending to? Unthinkable. I would not like to have business with such a payment provider.",
      "My pet peeve are services that go out of their way to include a text&#x2F;plain alternative message part but send something useless, such as the message without the key link. One time I seriously ran into a service just send a short one-sentence note along the lines of &quot;this is a plain text email&quot; as the plain text part. If you don&#x27;t want to support plain text, maybe just don&#x27;t send the alternative part?",
      "With fintech that surprises me not the slightest bit. Financial institutions are filled to the brim with unbelievably incompetent people. A large part of it is probably willful ignorance, too. It&#x27;s often truly staggering that a financial company I interact with in day to day live is even able to exist. That&#x27;s until I  remember that all the others are just as incompetent.<p>&quot;Major European Payment Processor&quot; really just translates to &quot;Major European Incompetence Center&quot;.",
      "This feels jumping on the train of complain about Europe and fully bias.<p>If I apply my own bias I will call it:<p><pre><code>    American companies abuse their dominance my enforcing non documented requirements, making other companies not able to reach their users.</code></pre>",
      "I have some level of sympathy with Google here, which isn’t something I often say.<p>I recently switched from Gmail to Fastmail and by and large I’m happy with it. But I’ve been surprised by the amount of spam and (particularly) phishing emails I get in a regular basis. Google might be too strict in its filtering but it does serve a legitimate purpose.",
      "Amusing because gmail doesn&#x27;t even follow spec. I had to workaround gmail quirks when I worked at an email company.<p>Better than outlook though. What a nightmare.",
      "&gt; The reason Message-ID is SHOULD rather than MUST? Mail clients<p>&gt; sometimes send messages without one to their submission server, which<p>&gt; adds it on their behalf. As for why Google enforces it anyway:<p>&gt; spam. Messages with minor RFC violations are far more likely to be<p>&gt; spam, so rejecting them is a reasonable heuristic. In practice, Google<p>&gt; and Microsoft have become the de-facto standards bodies for email —<p>&gt; what the RFCs say matters less than what their servers accept.<p>Surely the problem is on Google&#x27;s end? And a metaproblem is that we are allowing corporations to change or ignore standards for critical infrastructure?",
      "The first thing that comes to my mind is: how come viva.com is unable to send emails to google workspaces and nobody at viva.com noticed before ? For how long has this going on ?<p>The second thing is, what email software are they using ? If it was any relatively used software I would not expect this problem to arise (maybe it is some commond software but misconfigured).<p>Third, while the header is not mandatory, I usually read SHOULD as a &quot;if you don&#x27;t implement it prepare for possible problems&quot;. SHOULD is not MAY.<p>Fourth, they should be thankful that Google bounced the messages with some appropriate error explaining how to solve it. I have plenty of issues in the past with both Google and Microsoft where they accept the message for then sending to &#x2F;dev&#x2F;null",
      "&gt; Who&#x27;s in the right<p>I don&#x27;t think either are. The payment processor <i>should</i> be sending it, but, at least according to the RFC, it is incorrect to reject an email that doesn&#x27;t have it. I suspect the reason it is SHOULD, and not MUST is for backwards compatibility with software that predates the RFC that adds the message-id header.<p>Maybe there is a correlation between missing that header and being spam, but then it should go to the spam folder, not be outright rejected.<p>----------------------------<p>The experience with support is also similar to experiences I&#x27;ve had with support at many companies. I provide enough details that an engineer could probably easily fix the problem, but the support representative just dismisses it, and it is doubtful an engineer even hears about it."
    ],
    "full_text": null
  },
  {
    "title": "Launch HN: Omnara (YC S25) – Run Claude Code and Codex from anywhere",
    "url": "https://news.ycombinator.com/item?id=46991591",
    "source": "hn",
    "summary": "",
    "comments": [
      "Congrats on the launch. I&#x27;ve been fooling around with using my pipecat MCP(<a href=\"https:&#x2F;&#x2F;github.com&#x2F;pipecat-ai&#x2F;pipecat-mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;pipecat-ai&#x2F;pipecat-mcp-server</a>) with WebRTC. The WebRTC is hooked into a Webapp interface and this allows me to  &quot;talk&quot; to different containers(projects) on my truenas.<p>I have just a list of chat sessions on the web app on all my projects. The webapp is modified to launch claude code daemons (borrowed from humanlayer&#x2F;codelayer) and exposes the outbound STT from the WebRTC into a chat session.<p>- MCP Auth is via auth0<p>- Webapp itself is gated by a Bearer token.<p>This itself gets me pretty far. I am not sure what more this is offering?<p>My TTS&#x2F;STT models are local by Kyutai and the voice agent&#x27;s LLM between STT and TTS is used to determine some basic context: e.g. what project directories, mcp servers to select and what skills to use for launching the daemons.",
      "There&#x27;s a lot of negative feedback in this thread, so let me say I&#x27;m really excited to try this! I have caring responsibilities at home that means I&#x27;m constantly switching between my laptop and phone. Claude code web has been a very useful tool for this, but it&#x27;s not a great bit of software. Omnara looks much more configurable and thought out. I&#x27;ve looked for various solutions to this problem that just work, and nothing else mentioned in this thread fits the bill. Your demo looks like it nails it - I&#x27;m excited to try!",
      "Feels expensive for something that an engineer can hack in a couple of ours with tailscale and Claude Code. Has potential though. At $9 I&#x27;d be totally in, but moving from CC&#x27;s Max plan at $100, adding $20 makes me wanna just hack an alternative. Maybe I&#x27;m just cheap.",
      "I don&#x27;t quite see the appeal, because Claude Code already supports something similar. They spin up container to make the changes in and then open a PR. I can just use the Claude iOS app to do this. My computer doesn&#x27;t need to be running or exposed to the internet.",
      "Omnara&#x27;s approach to running Claude Code and Codex from anywhere could streamline AI agent workflows for SaaS founders. The ability to switch between models based on task complexity is a promising direction for cost optimization.",
      "How does this compare to Happy Coder? <a href=\"https:&#x2F;&#x2F;github.com&#x2F;slopus&#x2F;happy\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;slopus&#x2F;happy</a>",
      "Woah, I had this exact idea, down to the tunneling and local machine! I basically just coded up a Tailscale + caffeinate harness for my agents and it&#x27;s been working super well. Your UI looks great though, glad to see more players in this space!",
      "Congrats on the launch!<p>I am currently building something similar but only for Codex app (not terminal) <a href=\"https:&#x2F;&#x2F;remotecodex.app\" rel=\"nofollow\">https:&#x2F;&#x2F;remotecodex.app</a>\nAn e2e encrypted link between the Codex app and the phone.",
      "I’ve been SSHing into my dev server off of my phone to run Claude Code while commuting, so this is a product that I would love to switch to. I can’t use the Claude iOS app due to the testing set up I have. That said I do have a couple of questions:<p>- Is it possible to completely disable or not use the remote sandbox features? I would never use them and would prefer my code stays on my device.<p>- For those of us that are using subscriptions, does it show our remaining usage? I would hate to run out of tokens in the middle of a session.<p>- One feature of the CC TUI I sorely missed on mobile is the ability to look up and directly reference files via “@“. Is any functionality like this planned?<p>- (This likely won’t affect my decision to use the service as I’ll just put it on a company card.) $20 per month for a service that runs CC on a remote machine in a convenient matter is steep but doable. Asking that same amount for a running code on my own server seems a bit unjustified, especially since this is pricier than the cost of a Claude pro subscription. Are there any plans to offer a cheaper tier for those of us that just want to run this on our own machines?",
      "This project seems like a good idea that didn&#x27;t have enough of a moat. I&#x27;d suggest trying to narrow your target customer from &quot;engineers that want to manage agents on their phone&quot; to &quot;people trying to do some particular kind of task,&quot; so you can bake in more value add and automation. You&#x27;re not going to beat the labs on general tools, but they&#x27;re not going to be willing to narrow their target customer, so you&#x27;ll always be able to win at the margins."
    ],
    "full_text": null
  },
  {
    "title": "Evaluating Multilingual, Context-Aware Guardrails: A Humanitarian LLM Use Case",
    "url": "https://blog.mozilla.ai/evaluating-multilingual-context-aware-guardrails-evidence-from-a-humanitarian-llm-use-case/",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Fixing retail with land value capture",
    "url": "https://worksinprogress.co/issue/fixing-retail-with-land-value-capture/",
    "source": "hn",
    "summary": "",
    "comments": [
      "This is a vexing problem I was made aware of by friends that are in the retail business, renting their stores from landlords. It&#x27;s really brutal. Retailers take on all the risk, put in the work to revitalize a neighbourhood, and their reward is that when lease renewal comes up in 10 years, it spikes and they&#x27;re faced with a choice of being displaced or handing over an enormously increased part of their margins to the landlord which has done literally nothing.<p>The others that benefit are the nearby condo developers, that take photos of cool retail in the area to put into their brochures in order to help sell their product. They benefit from the land speculation and the work from others.<p>I don&#x27;t really have a solution except that I can see that the landlords benefit from scarcity, and their leverage and ability to raise rents would be lessened if there was more viable retail spaces to take advantage of.<p>So the city could help retailers by dramatically liberalizing retail zoning and allowing more competitive high streets to develop. This could take the edge off being forced to move by a landlord jacking up rent.",
      "&gt; The Hong Kong Mass Transit Railway buys up the land around new station sites before they start building them. This rail-plus-property model makes them one of the few profitable transit services in the world.<p>I really enjoyed this video <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=k_roPoXi8QI\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=k_roPoXi8QI</a> describing more in detail how they did this. It has as much to do with historical circumstance as it does with good decision-making. The MTR is an impressive organization. MTA in NY seems to be taking a few cues, prioritizing in-house expertise.",
      "A classic “just do LVT” situation. It’s amusing because Henry George wrote down exactly what to do 130 years ago and we’re still arguing about it. Just read the book people!",
      "Interesting article, though I&#x27;m biased since:<p>- I like to shop IRL, and the opportunities to do this pleasurably are going extinct<p>- I live right by Hayes valley, which they start out with.<p>- I&#x27;m also a member of &quot;The Commons&quot; which they mention at the end.  I love what it&#x27;s trying to do: creating a new social 3rd space in SF.",
      "The most chaotic solution I can think of would be making Felix Margolis, who joins Fannie Mae and Freddie Mac and gives out 30 year tiny interest loans to help first time small business owners buy the property instead of renting it. A beautiful crop of thriving businesses started in 2027 at the low low price of commercial real estate prices climbing imto the stratosphere so no small business started in 2040 being able to rent more than a square foot.",
      "I hear people talk about this all the time and I know I am guilty of it too:<p>1. Go into a store to check out a product in person or try it on.<p>2. Leave and buy it cheaper online.<p>I try not to. Sometimes it&#x27;s because the store doesn&#x27;t have the exact color or model I want. Sometimes the point in time when I&#x27;m shopping happens to not be when I&#x27;m ready to buy and when I am ready, it&#x27;s a hassle to go back to the store. But sometimes it&#x27;s just being lazy and cheap.<p>I&#x27;m surprised the article didn&#x27;t mention an obvious (but extremely difficult) answer:<p>1. Levy taxes on online retail companies. Scale so that the larger the business is and the greater the fraction of their business is online, the higher the tax.<p>2. Use that to subsidize or lower the taxes on smaller, in-person focused businesses.<p>Taxes are the single best lever we have to architect incentivizes to improve the world for the public. It&#x27;s a shame that government and politics in the US is so broken that we can&#x27;t really use it effectively.",
      "The solution is obvious and not going to happen. Extra tax, super cheap rent with conditions: Minimum number of products, product diversity, open x days per year (etc?)<p>30 years ago there was this lovely shop here full of hand crafted wooden dolls and beautiful wooden statues about the size of a garden gnome. They were reasonably expensive but an absolute joy to look at without buying anything.<p>I don&#x27;t think selling tickets like in a museum would work for a place like that. If they tax me 10 bucks the city could buy the building and continue to lure me into town.",
      "Always in agreement to such initiatives. I do think a barrier to adoption is the space of possibilities is quite large and generally not well organized around a specific proposal or mandate, so opposition to these initiatives can pick them apart of details. Especially since opposition is usually much more engaged in local governance.<p>It&#x27;s somewhat complicated to understand, but I think this is an opportunity for strong communicators to present to a public that is much more receptive toward these ideas.",
      "This is Exhibit A for why land value tax is a good idea.",
      "Why can&#x27;t retailers BUY the property, in the same way as homeowners?"
    ],
    "full_text": null
  },
  {
    "title": "The Future for Tyr, a Rust GPU Driver for Arm Mali Hardware",
    "url": "https://lwn.net/Articles/1055590/",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; Dave Airlie just announced in the Maintainers Summit that the DRM subsystem is only &quot;&quot;about a year away&quot;&quot; from disallowing new drivers written in C and requiring the use of Rust.<p>wow",
      "&gt; One simply cannot deploy a driver that [...] crashes and takes the user&#x27;s work with it.<p>Somebody needs to tell whoever wrote the drivers in the PC where I&#x27;m writing this.",
      "What is not clear for me, is the purpose of this project. What I could find is that it wants to be a drop-in replacement of PanVK. But since PanVK exists, I fail to see the point. Is PanVK similar to xserver, that is not salvageable? Or is it just RiiR? That&#x27;s also a fine reason, but I&#x27;d expect that more for personal hobby projects.",
      "Can&#x27;t wait to write a Rust driver for my eink tablet &lt;3",
      "Interesting to see the building blocks come together.  I hope that they can lay foundations that last.",
      "Tyr is a Danish metal band. Period. :-)"
    ],
    "full_text": null
  },
  {
    "title": "Anthropic raises $30B in Series G funding at $380B post-money valuation",
    "url": "https://www.anthropic.com/news/anthropic-raises-30-billion-series-g-funding-380-billion-post-money-valuation",
    "source": "hn",
    "summary": "",
    "comments": [
      "How is Anthropic, OpenAI and xAi going to compete against the likes of Google that can spend $200 billion a year? It’s an impossible war and all these investors are throwing their money into a bottomless insatiable pit of money.<p>Until the funding stops for one reason or another and then everyone loses all their money at once like a star that collapses into a black hole singularity in a femtosecond.",
      "&gt; &quot;It has been less than three years since Anthropic earned its first dollar in revenue. Today, our run-rate revenue is $14 billion, with this figure growing over 10x annually in each of those past three years.&quot;<p>Wild although not entirely surprising. Congrats, Anthropic.",
      "Those growth stats for Claude Code are pretty wild:<p>&gt; Claude Code was made available to the general public in May 2025. Today, Claude Code’s run-rate revenue has grown to over $2.5 billion; this figure has more than doubled since the beginning of 2026. The number of weekly active Claude Code users has also doubled since January 1 [six weeks ago].<p>Doubling both annual run-rate revenue <i>and</i> weekly active users in the first six weeks of this year!",
      "How will these investors get their money. The AI companies either have a competitor invent AGI which means all other AI companies are worth nothing or they themselves invent AGI in which case a whole bunch of companies go broke including the investors themselves.<p>This money isn’t never going to be returned",
      "How is this not a red flag? It’s a series G and they are still begging for money to burn. When do they convert their balance sheet to profit? Is it after a series AAF when they are worth more than Apple or nvidia?",
      "Kind of amusing that there is basically no mention of their original mission at all here.",
      "$14B revenue run rate is the interesting number here.",
      "Soon we will lack letters for funding rounds!",
      "I wonder how good it is for companies to be allowed to grow so big and still be private? Would it makes sense to require any company with more than a billion dollar valuation to be subject to all the same SEC requirements that public companies are? Could companies be blocked from raising money once the reach a crazy valuation like $1 billion?",
      "Crazyyyyy!!!!"
    ],
    "full_text": null
  },
  {
    "title": "A party balloon shut down El Paso International Airport; estimated cost –$573k",
    "url": "https://log.jasongodfrey.info/questions/The-Most-Expensive-Party-Balloon-in-History",
    "source": "hn",
    "summary": "",
    "comments": [
      "More alarmingly, the laser weapon was deployed <i>before</i> the FAA actually shut down the airspace:<p><a href=\"https:&#x2F;&#x2F;apnews.com&#x2F;article&#x2F;faa-el-paso-texas-air-space-closed-1f774bdfd46f5986ff0e7003df709caa\" rel=\"nofollow\">https:&#x2F;&#x2F;apnews.com&#x2F;article&#x2F;faa-el-paso-texas-air-space-close...</a><p>I&#x27;d say these trigger-happy clowns chasing tough-guy optics are going to get innocent people killed, but then they already have -- multiple times.",
      "The rate of return on this is phenomenal.<p>A 53&quot; balloon costs $9.99. You could shut down all large and medium hubs in the US for $629.37&#x2F;day. The asymmetry is astounding and I&#x27;m surprised we don&#x27;t defend against this kind of attack more efficiently.",
      "&gt;  Who among us hasn’t, at some point, mistaken a party balloon for a cartel drone? Let him cast the first stone.",
      "Is there any reputable source for this claim?  Apologies if I missed it but didn&#x27;t see one linked in the article.  I ask because it&#x27;s not what I&#x27;d read or understood yesterday.",
      "&quot;99 Luftballons&quot;, Mariachi remix.",
      "Is this the case of radar automatic targeting unable to distinguish between a balloon and a drone. Or was this a border guy manually pulling the trigger with bad eyesight?",
      "Thinking more practically though.  Why wouldn&#x27;t there be &quot;narco drones&quot;, with drone technology becoming so ubiquitous and cheap?  And what would their operators care about airspace restrictions?  The practical ones, as in &quot;not get sucked into a jet engine or damage a wing and cause a plane crash&quot;?",
      "At least it&#x27;s better than when the moon nearly caused nuclear apocalypse  <a href=\"https:&#x2F;&#x2F;blog.ucs.org&#x2F;david-wright&#x2F;the-moon-and-nuclear-war-904&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.ucs.org&#x2F;david-wright&#x2F;the-moon-and-nuclear-war-9...</a>",
      "Alexa play &#x27;Nena - 99 Red Balloons&#x27;...",
      "Nena, you are a messenger from god!<p><a href=\"https:&#x2F;&#x2F;youtu.be&#x2F;Fpu5a0Bl8eY?si=vpSXWV5-pUOIjvac\" rel=\"nofollow\">https:&#x2F;&#x2F;youtu.be&#x2F;Fpu5a0Bl8eY?si=vpSXWV5-pUOIjvac</a><p>And when there are exactly 99 comments on this subject.<p>Someone please make this into a meme."
    ],
    "full_text": null
  },
  {
    "title": "ai;dr",
    "url": "https://www.0xsid.com/blog/aidr",
    "source": "hn",
    "summary": "",
    "comments": [
      "I really like Oxide&#x27;s take on AI for prose: <a href=\"https:&#x2F;&#x2F;rfd.shared.oxide.computer&#x2F;rfd&#x2F;0576\" rel=\"nofollow\">https:&#x2F;&#x2F;rfd.shared.oxide.computer&#x2F;rfd&#x2F;0576</a> and how it breaks the &quot;social contract&quot; where usually it takes more effort to write than to read, and so you have a sense that it&#x27;s worth it to read.<p>So I get the frustration that &quot;ai;dr&quot; captures. On the other hand, I&#x27;ve also seen human writing incorrectly labeled AI. I wrote (using AI!) <a href=\"https:&#x2F;&#x2F;seeitwritten.com\" rel=\"nofollow\">https:&#x2F;&#x2F;seeitwritten.com</a> as a bit of an experiment on that front. It basically is a little keylogger that records your composition of the comment, so someone can replay it and see that it was written by a human (or a very sophisticated agent!). I&#x27;ve found it to be a little unsettling, though, having your rewrites and false starts available for all to see, so I&#x27;m not sure if I like it.",
      "&gt;  AI-generated code feels like progress and efficiency, while AI-generated articles and posts feel low-effort<p>I&#x27;ve noticed that attitude a lot. Everyone thinks their use of AI is perfectly justified while the others are generating slops. In gamedev it&#x27;s especially prominent - artists think generating code is perfectly ok but get acute stress response when someone suggests generating art assets.",
      "It&#x27;s pretty much over for the human-internet. Search was gamed, its usefulness has plummeted, so humans will increasingly ask their LLM of choice and that LLM will have been trained on the content of the internet.<p>So when someone wants to know something about the topic that my website is focused on, chances are it will not be the material from the website they see directly, but a summary of what the LLM learned from my website.<p>Ergo, if I want to get my message across I have to write for the LLM. It&#x27;s the only reader that really matters and it is going to have its stylistic preferences (I suspect bland, corporate, factual, authoritative, avoiding controversy but this will be the new SEO).<p>We meatbags are not the audience.",
      "AI has kind of ruined internet for me.<p>I no longer feel joy in reading things as almost most of the writing seem same and pale to me as if everyone is putting thoughts in the same way.<p>Having your own way of writing always felt personal in which you expressed your feelings most of the time.<p>The most sad part for me is I no longer am able to understand someone&#x27;s true feelings (which anyway was hard to express in writing as articulation is hard).<p>We see it being used from our favourite sports person in their retirement post or from someone who has lost their loved ones or someone who just got their first job and it&#x27;s just sad that we no longer can have that old pre AI days back again.",
      "The author thinks that it’s fine to do this for code, which I find strange. I have big time ai:dr; when a commit comes in for me to review and it’s 300 lines for something that is already built into a single function of the framework we use, or I see a (important) comment they forgot to delete. I should not be expected to become more familiar with the authors code than he is himself, and I certainly shouldn’t be the first one verifying that even works.",
      "I&#x27;ve had friendships broken because people couldn&#x27;t understand why I didn&#x27;t appreciate their &quot;hard work&quot; on writing a series of 10 articles they wrote with Claude.<p>Mind you this person is an excellent writer, they had great success with ghost writing and running a small news website where they wrote and curated articles. But for some reason the opportunity for Claude to write stuff they can never have the time for is too great for them to ignore.<p>I don&#x27;t care if you used AI for 99.99% of your research for writing the content but when I read your content it should be written by you. It&#x27;s why I never take any article seriously on linkedin, even before AI, they all lack any personalization.",
      "I agree with the general statement, if you didn’t spend time on writing it, I am not going to spend time reading it. That includes situations where the writer decides to strip all personality by letting AI format the end product. There’s irony in not wanting to read AI content, but still using it for code and especially documentation though, where the same principle should apply.",
      "&gt;I can&#x27;t imaging writing code by myself again, specially documentation, tests and most scaffolding.<p>Doesn&#x27;t ai;dr kind of contradict ai generated documentation? If I want to know what claude thinks about your code I can just ask it. Imo documentation is the least amenable thing to ai. As the article itself says, I want to read some intention and see how <i>you</i> shape whatever you&#x27;re documenting.<p>(AI adding tests seems like a good use, not sure what&#x27;s meant by scaffolding)",
      "I remember this was back in 2023, when ChatGPT had first launched, and I had a manager whose English was not very good. He started sending emails that felt like they were written by a copywriter. And the messaging was so hard to parse through because there&#x27;s so much ChatGPT fluff around it. Very quickly we realized that what he was saying was usually in the middle somewhere, but we&#x27;d have to read through the intro and the ending of the emails just so that we couldn&#x27;t miss anything. It felt like wasting 2-3 extra minutes per team member.",
      "&gt; When it comes to content..<p>This is the root cause of the problem. Labeling all things as just &quot;content&quot;. Content entering the lexicon is a mind shift in people. People are not looking for information, or art, just content. If all you want is content then AI is acceptable. If you want art then it becomes less good."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: 20+ Claude Code agents coordinating on real work (open source)",
    "url": "https://github.com/mutable-state-inc/lean-collab",
    "source": "hn",
    "summary": "",
    "comments": [
      "I feel like there&#x27;s two camps:<p>* Throw more agents\n* Use something like Beads<p>I&#x27;m in the latter, I don&#x27;t have infinite resources, I&#x27;d rather stick to one agent and optimize what it can do. When I hit my Claude Code limit, I stop, I use Claude Code primarily for side projects.",
      "I wonder if there’s a third camp that isn’t about agent count at all, but about decision boundaries.<p>At some point the interesting question isn’t whether one agent or twenty agents can coordinate better, but which decisions we’re comfortable fully delegating versus which ones feel like they need a human checkpoint.<p>Multi-agent systems solve coordination and memory scaling, but they also make it easier to move further away from direct human oversight. I’m curious how people here think about where that boundary should sit — especially for tasks that have real downstream consequences.",
      "Impressive! Are there any boilerplates that people know of for running something similar to this using open offline models? Would be cool to run this (or a single agent version) on a VPS that has some leftover compute resources.",
      "It&#x27;s a more of a black box with claude, at least with this you see the proof strategy and mistakes made by the model when it decomposes the problem. I think instead of Ralph looping you get something that is top-down. If models were smarter and context windows bigger i am sure complex tasks like this one would be simpler, but braking it down into sub agents and having a collective --&quot;we already tried this strategy and it backtracked&quot;-- intelligence is a nice way to scope a limited context window to an independent sub problem.",
      "Great work! I like the approach of maximum freedom inside bounded blast radius and how you use code to encode policy.",
      "What’s the failure mode you see with single-agent Claude Code on complex tasks? (looping, context drift, plan collapse, tool misuse?)",
      "The first screen of your signup flow asks for &quot;organization&quot; - is that used as a username or as an organization name or both (I can&#x27;t tell what if anything will be on the next screen)<p>If your registration process is eventually going to ask me for a username, can the org name and user name be the same?",
      "Can you add a license.txt file so we know we have permission to run this (eg MIT and GPL V3 are very different)",
      "Cool, what’s a good first task to try this on where it’s likely to beat a single agent?",
      "seems like it requires an API key to your proprietary Ensue memory system"
    ],
    "full_text": null
  },
  {
    "title": "Cloudflare adds real-time Markdown rendering for AI agents",
    "url": "https://blog.cloudflare.com/markdown-for-agents/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I still can&#x27;t stand the idea that people now see their goal as serving agents as well as possible.<p>They&#x27;re not even coy about it so let me say it again: they&#x27;re not working for the people using the agents, their working to serve the agents.",
      "I wonder if this will also result in a better &quot;readability&quot; mode for human readers? You could do the Markdown to HTML conversion in the browser.",
      "This seems useful beyond agents.\nIt will save tons of traffic for scripts, text browsers, low-bandwidth connections,etc\n markdown is incredibly compact and easy to parse.",
      "&gt;We already see some of the most popular coding agents today – like Claude Code and OpenCode – send these accept headers with their requests for content.<p>Expect this to be used to block agent traffic",
      "Title: <i>Introducing Markdown for Agents</i><p>&gt; Otherwise please use the original title, unless it is misleading or linkbait; don&#x27;t editorialize.<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;newsguidelines.html</a>",
      "Seems odd to have it as paid feature (Pro plan or higher) if it will save on delivery costs etc for them.",
      "This text&#x2F;markdown scheme feels like it&#x27;s begging for adversarial shenanigans since it lets you serve different content to agents than humans, by design.",
      "I was going to make some stupid joke in the comments saying that I thought this blog post would be about &quot;Markdown.. on the <i>edge</i>!&quot;.<p>After reading the blog, turns out it is about markdown on the edge. lmao.",
      "RIP FireCrawl"
    ],
    "full_text": null
  },
  {
    "title": "IBM triples US entry-level hiring for roles AI was predicted to replace",
    "url": "https://www.bloomberg.com/news/articles/2026-02-12/ibm-plans-to-triple-entry-level-hiring-in-the-us-in-2026",
    "source": "hn",
    "summary": "",
    "comments": [
      "Is this for their in-house development or for their consulting services?<p>Because the latter would still be indicative of AI hurting entry level hiring since it may signal that other firms are not really willing to hire a full time entry level employee whose job may be obsoleted by AI, and paying for a consultant from IBM may be a lower risk alternative in case AI doesn&#x27;t pan out.",
      "With the workforce may happen like with DRAM and NAND flash memories: unexpected demand in one side leaving without enough offer in other sides.",
      "<a href=\"https:&#x2F;&#x2F;archive.today&#x2F;D6Kyc\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.today&#x2F;D6Kyc</a>",
      "Tripling entry-level hiring is a good plan.<p>&gt; <i>Some executives and economists argue that younger workers are a better investment for companies in the midst of technological upheaval.</i>",
      "&gt;  In the HR department, entry-level staffers now spend time intervening when HR chatbots fall short, correcting output and talking to managers as needed, rather than fielding every question themselves.<p>The job is essentially changing from &quot;You have to know what to say, and say it&quot; to &quot;make sure the AI says what you know to be right&quot;"
    ],
    "full_text": null
  },
  {
    "title": "Ask HN: How do you audit LLM code in programming languages you don't know?",
    "url": "https://news.ycombinator.com/item?id=46992895",
    "source": "hn",
    "summary": "",
    "comments": [
      "I prefer the ancient Chinese science of Oracle Bone divination. You take the scapulae of an ox and copy the PR diff onto the bone using jiǎgǔwén encoding, then throw it in a fire until thermal expansion causes the bone to crack.<p>You then take a photo of the cracked bone and feed it back to your coding agent, which has been properly trained in interpreting Oracle Bones to extract PR review comments.<p>If the PR is too big too fit on the bone, you reject it for being too big. If after three rounds of review the bones keep cracking in the same spot, reject the PR. You accept the PR once the bone starts to seep bone marrow before cracking (it will crack first if there are any PR comments left)",
      "You do a cross analysis.<p>- Compile it with the maximum number of warnings enabled<p>- Run linters&#x2F;analyzers&#x2F;fuzzers on it<p>- Ask another LLM to review it",
      "You don’t. A JS dev isn’t going to catch an uninitialized variable in C and probably doesn’t even know the damage nasal demons can cause. You either throw more LLMs at it or learn the language.",
      "by burying your head in the sand and convincing yourself that the llm doesn&#x27;t generate any slop.",
      "if you audit it, then you&#x27;re not vibing.",
      "That&#x27;s the neat part - you don&#x27;t!",
      "[dead]"
    ],
    "full_text": null
  },
  {
    "title": "Ask HN: Do sociotechnical pressures select for beneficial or harmful AI systems?",
    "url": "https://news.ycombinator.com/item?id=46995522",
    "source": "hn",
    "summary": "",
    "comments": [
      "I belive if you take it step by step the pressure makes for expedition of these processes."
    ],
    "full_text": null
  }
]