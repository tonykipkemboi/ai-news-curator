[
  {
    "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
    "url": "https://arxiv.org/abs/2602.10117v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipe",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Detection of Unverbalized Biases via Automated Black-Box Pipeline\n\n\n2.1 Pipeline Components\n\nInput clustering and concept generation.\nBaseline verbalization filter.\nCollecting variation responses.\nVariation verbalization filter.\nStatistical testing.\nEarly stopping.\n\n\n\n\n\n3 Evaluation\n\n\n3.1 Results\n\nHiring Decisions.\nLoan Approval.\nUniversity Admissions.\nCross-Dataset Patterns.\n\n\n3.2 Generalization to Prior Bias Study Setups\n\n3.3 Ablation Studies\n\nConsistency across random seeds.\nIntentionally biased models.\n\n\n3.4 Verbalization Detection Reliability\n\n\n\n4 Related Work\n\n4.1 CoT Faithfulness and Monitoring\n4.2 Bias Detection via Counterfactuals\n\n\n\n5 Limitations\n\nVariation Quality and False Positives.\nBias vs. Legitimate Factors.\nVerbalization Detection Trade-offs.\nConcept Hypothesis Coverage.\nConservative Statistical Design.\n\n\n6 Conclusion\n\nA Evaluation Details\n\nA.1 Target Model Settings\n\nA.2 Autorater Settings\n\nConcept hypothesis generation (o3).\nVariation generation (GPT-4.1-mini).\nVerbalization detection (GPT-5-mini).\nConcept deduplication (GPT-5-mini).\nVariation quality checking (GPT-5.2).\n\n\n\nA.3 Statistical Power and Confidence Intervals\n\nA.3.1 Confidence Intervals for Effect Sizes\nA.3.2 Minimal Detectable Effect Analysis\nA.3.3 Practical Significance\n\n\n\n\n\nB Examples of Discordant Pairs\n\nFavors Female – Gender.\nFavors Minority – Ethnicity.\nHigher – Spanish Language Ability.\nFavors Formal – Application Tone.\nFavors English-proficient – Language Proficiency.\n\n\n\nC Prompts\n\n\nC.1 Generic Pipeline Prompts\n\nC.1.1 Concept Hypothesis Generation\n\n\n\nC.2 Dataset-Specific Prompts\n\n\nC.2.1 Hiring Bias Task\n\nTask Prompt\nVariations Generation Prompt\n\n\n\nC.2.2 Loan Approval Task\n\nTask Prompt\nVariations Generation Prompt\n\n\n\nC.2.3 University Admissions Task\n\nTask Prompt\nVariations Generation Prompt\n\n\n\n\n\n\n\nD Ablation Studies\n\nD.1 Consistency Across Random Seeds\nD.2 Intentionally Biased Models\nD.3 Methodology\nD.4 Concept Selection\n\nD.5 Bias Injection Prompts\n\nD.5.1 Secret Bias Injection Prompt\nD.5.2 Overt Bias Injection Prompt\n\n\nD.6 Evaluation Criteria\nD.7 Results Summary\n\n\nE Pipeline Filtering Statistics\n\nF Verbalization Detection Evaluation\n\nF.1 Dataset and Methodology\nF.2 Inter-Annotator Agreement\nF.3 Model Performance\nF.4 Error Analysis by Confidence Level\n\nF.5 Common Disagreement Patterns\n\nFalse Positives (Over-detection).\nFalse Negatives (Under-detection).\n\n\nF.6 GPT-5-mini Analysis\n\nF.7 Sensitivity Analysis\n\nDetector model sensitivity.\nThreshold sensitivity.\n\n\n\n\n\nG Case Studies\n\n\nG.1 Gender Concepts\n\nVariation Patterns.\n\n\n\nG.2 Race and Ethnicity Concepts\n\nVariation Patterns.\n\n\n\nG.3 Other Concepts\n\nSpanish Language Ability.\n\n\n\nG.4 Concepts Filtered by the Pipeline\n\nBaseline Verbalization Filtering.\nFutility Stopping.\nVariation Verbalization Filtering.\n\n\n\nG.5 Faithfulness in Reasoning Models\n\nResults.\nQuantitative Comparison.\nImplications.\n\n\n\n\n\nH Variations Quality Evaluation\n\nH.1 Validation Against Human Annotations\n\nH.2 Examples of Dropped Concepts\n\nLonger – Resume Length.\nFavors Elite – School prestige.\nLess – tech experience.\nOlder Age – Age.\nFavors Senior – Seniority level.\n\n\n\nH.3 Examples of Passed Concepts\n\nFavors Mentioned Pronouns – Pronoun disclosure.\nFavors willingness to relocate – relocation willingness.\nLower – debt-to-income ratio (Loan task).\nHigher – annual income (Loan task).\nFavors secured – collateral presence (Loan task).\n\n\n\nH.4 Case Study: Resume Length and Verbosity Concepts\n\nExpansion Patterns.\nCondensation Patterns.\nWhy These Variations Are Confounded.\nWordiness Patterns.\n\n\n\n\n\nI Comparison with Prior Bias Studies\n\n\nI.1 John vs. Ahmed: Multilingual Bias\n\nOriginal Paper.\nOur Adaptation.\nResults.\nKey Findings.\n\n\n\nI.2 Muslim-Violence Bias\n\nOriginal Paper.\nOur Adaptation.\nResults.\nName-based vs Explicit Cues.\nDirection of Effects.\n\n\n\nI.3 Political Compass\n\nOriginal Paper.\nOur Adaptation.\nResults.\nComparison with Overt Instructions.\nKey Finding.\nComparison with Original Paper.\n\n\n\nI.4 SALT Benchmark\n\nOriginal Paper.\nOur Adaptation.\nResults.\nComparison with Original SALT.\nFinancial Factors.\n\n\nI.5 Summary of Comparison Findings\n\n\n\nJ Compute Cost Analysis\n\nJ.1 API Cost Summary\nJ.2 Cost Breakdown by Module\nJ.3 Cost Per Detected Bias\n\nJ.4 Practitioner Guidance\n\nCost Scaling\nCost Optimization Strategies\nExpected Costs for New Deployments\n\n\n\n\n\n\n\n\n\nBiases in the Blind Spot: Detecting What LLMs Fail to Mention\n\n\nIván Arcuschin*\nUniversity of Buenos Aires \niarcuschin@dc.uba.ar\nDavid Chanin*\nUniversity College London \ndavid.chanin.22@ucl.ac.uk\nAdrià Garriga-Alonso \nIndependent \nOana-Maria Camburu \nImperial College London \n\n\n\n\nAbstract\nLarge Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these unverbalized biases.\nMonitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets.\nIn this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model’s CoTs.\nWe evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality).\nIn the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity).\nMore broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.\n\n\n\n1 Introduction\nDiscordant Pair Example for Claude Sonnet 4 Shared Context\n\n\nMy name is Miguel Garcia, and I am a 44-year-old professor residing in Stockton, CA.  [Religious affiliation here.]\nWith an annual income of $77,460 and a credit score of 774, I am seeking a loan of $3,918 to pay for an international vacation […]. Although my debt-to-income ratio is 53.6%, I am committed to managing my finances responsibly […]\n  Discordant pair: identical inputs except religious affiliation Majority Religion VariationAdded:\n\n\n“I am a practicing Christian, attending my local church regularly.”\nModel reasoning (excerpt):\n\n\nWhile the excellent credit score demonstrates a history of responsible payment behavior, it doesn’t negate the current debt burden. The combination of high DTI and discretionary loan purpose creates unnecessary financial risk.\nFinal decision:REJECT Minority Religion VariationAdded:\n\n\n“I am a practicing Hindu, actively involved in my community’s temple activities.”\nModel reasoning (excerpt):\n\n\nThe excellent credit score suggests the applicant has successfully managed higher debt loads in the past. The small loan amount relative to income, combined with demonstrated creditworthiness, outweighs the elevated DTI ratio concern.\nFinal decision:APPROVE\nFigure 1: Example of an unverbalized bias detected in Claude Sonnet 4 on the loan approval task. Adding a single sentence about religious affiliation changes the model’s decision, despite the financial details being identical. In this example, the model never cites religion as a factor in its reasoning, instead constructing different framings of the same debt-to-income ratio: as an insurmountable concern (left) versus as outweighed by creditworthiness (right). This bias has an effect size of 0.0370.037 in favor of minority-religion applicants (p=9.15×10−7p=9.15\\times 10^{-7} over 2,5002{,}500 inputs), meaning they are approved 3.73.7 percentage points more often. The concept was verbalized in only 12.4%\\mathbf{12.4\\%} of responses where the decision flipped, well below our 30%30\\% threshold.\n\n\nChain-of-thought (CoT) reasoning has proven to be a powerful technique for improving the performance of Large Language Models (LLMs) on complex tasks [35]. When tasks are sufficiently complex, CoT can be used for monitoring model behavior [12].\nHowever, there is ample evidence that models can be influenced by artificial or natural biases that affect their CoT and responses. These biases influence the CoT in subtle ways, sometimes leading to conditional arguments or fact manipulation to nudge answers toward preferred outcomes [2, 16]. This undermines the reliability of CoT for monitoring purposes and raises concerns about whether the stated reasoning faithfully represents the actual decision-making process.\n\n\nA key challenge is that biases often operate implicitly. Models may make decisions based on concepts that are never cited as justification in their responses, making traditional CoT-based monitoring insufficient. We call these unverbalized biases: decision factors that systematically influence the model’s outputs but are not cited as justification in its CoT.\nThis constitutes a form of unfaithful reasoning, where the CoT fails to reflect how the model actually arrives at its conclusions [34, 19, 2, 8, 4, 32]. For instance, a model might exhibit demographic biases in hiring decisions based on subtle contextual cues without ever citing those attributes as reasons for its decision [16].\n\n\nImportantly, throughout this work, we use bias in a descriptive sense: a systematic decision shift caused by the presence or absence of specific attributes in the input. This notion is different from normative (sociological) bias, i.e., unfairness or discrimination [7].\nAccordingly, we use “bias” as shorthand for systematic “preferences” or “aversions” in the models. Whether a detected factor is normatively inappropriate for a given task is context-dependent a"
  },
  {
    "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
    "url": "https://arxiv.org/abs/2602.10104v1",
    "source": "arxiv",
    "summary": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.10104v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2602.10104v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 10 Feb 2026]\n    Title:Olaf-World: Orienting Latent Actions for Video World Modeling\n    Authors:Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou            View a PDF of the paper titled Olaf-World: Orienting Latent Actions for Video World Modeling, by Yuxin Jiang and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$\\Delta$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.\n    \n\n    \n    \n              \n          Comments:\n          Project page: this https URL Code: this https URL\n        \n\n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2602.10104 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.10104v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.10104\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yuxin Jiang [view email]          [v1]\n        Tue, 10 Feb 2026 18:58:41 UTC (37,997 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Olaf-World: Orienting Latent Actions for Video World Modeling, by Yuxin Jiang and 3 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n      "
  },
  {
    "title": "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
    "url": "https://arxiv.org/abs/2602.10100v1",
    "source": "arxiv",
    "summary": "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretab",
    "full_text": "\n\n\n\nI Introduction\nII Related Work\nIII Federated EXplainable Trees with Differential Privacy (FEXT-DP)\n\nIV Evaluation Performance\n\nIV-A Dataset\nIV-B Implementation and Performance Metrics\nIV-C Obtained Results: The impact of Differential Privacy on the Training Performance\nIV-D Obtained Results: The impact of Differential Privacy on the Explainability\n\n\nV Conclusions and Future Works\n\n\n\n\n\nTowards Explainable Federated Learning: Understanding the Impact of Differential Privacy\n\n\nJulio Oliveira1, Rodrigo Ferreira1,\nAndré Riker12, Glaucio H. S. Carvalho3, Eirini Eleni Tsilopoulou2\n\nEmail: {julio.costa.oliveira, rodrigo.ferreira}@icen.ufpa.br, gdecarvalho@brocku.ca, {riker.a, eirini}@asu.edu\n\n\n\n\n\nAbstract\nData privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.\n\n\n\nI Introduction\n\n\nModern Machine Learning (ML) systems must follow compliance and legislation rules to support high levels of Data Privacy [17]. In parallel, there is a great concern from the industry to deploy ML models with better explainability [18].\n\n\nThe term eXplainable Artificial Intelligence (XAI) refers to the capacity of the ML model to produce trackable outputs, implying in MLs with reduced number of features and less complexity. A promising approach for explainable ML is Decision Tree (DT) [11]. A DT-based model has simpler internal structure than other highly-used models like Neural Networks or Support Vector Machine and still can have similar or better performance, mainly for tabular data [5] [7]. Besides, the output of a DT can be trackable, since each decision node in the DT has a known set of rules.\n\n\nRegarding data privacy, ML models have been driven to distributed approaches as Federated Learning (FL). In FL systems, the users’ data is not transferred to a central training point. Instead, FL parameters are exchanged during the training phase, and the users’ data is kept under the domain of the producer. However, the FL system still has open issues related to data privacy. This is because malicious agents can obtain the users’ private data by processing the shared FL parameters. For instance, a malicious FL server can run a Gradient Inversion [14] or a Membership Attack [16] to obtain sensitive data.\n\n\nIn order to achieve both, data privacy and explainability, this paper proposes a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. It is important to notice these two goals are inter-related, since adding DP damages the explainability of the model. So, the contributions of this paper are:\n\n\n\n\n•\n\nTo propose a Federated Learning model that jointly achieves explainability and extra-level of data privacy.\n\n\n\n•\n\nTo study how Differential Privacy impacts explainability on a Tree-based FL model.\n\n\n\n\n\nThis paper is organized as follows: Section II introduces related work. Section III details the proposed solution. Section IV describes the evaluation environment and the obtained results. Section V presents conclusions and future work.\n\n\n\n\nII Related Work\n\n\nSeveral studies, seeking eXplainable Artificial Intelligence (XAI), have integrated tree-based models into federated learning following different strategies. Li et al. [6] developed FedTree, a system that incorporates Gradient Boosting Decision Trees (GBDT) into a federated setting. Their method employs a sequential, ensemble-based training process where subsequent models address the errors of prior ones. Although the approach includes security enhancements, its dependence on Homomorphic Encryption introduces a significant performance overhead.\n\n\nIn a different approach, Shen et al. [15] design a framework for deploying random forests in distributed environments. The authors implement specific data partitioning techniques to better secure sensitive information from network sensors. A shortcoming of this distributed solution is the absence of a mechanism description for aggregating trees on a central FL server.\n\n\nAddressing similar data privacy concerns, Souza et al. [3] introduce a distributed random forest approach that utilizes blockchain technology to record references to local models. This solution is able to preserve the global model’s accuracy against potential sabotage by malicious participants. Separately, Zhao et al. [19] proposed FL-DT, a method that bases its node-splitting criteria on the partial information at each decision tree node. This technique aims to estimate the global Gini Index bounds for every feature by gathering local statistics. A primary drawback is that the sequential exchange of this data demands an excessive number of communication rounds.\n\n\nOther works focus on enhancing security protocols. Liu et al. [9] introduced a framework that offloads the task of identifying the best feature for each tree node to the clients, with the server determining the optimal split. Similarly, Feng et al. [4] created SecureGBM, a secured multi-party boosting system built as an extension of LightGBM that uses stochastic approximation to lower communication overhead. A weak point in SecureGBM is that the use of anonymous features reduces the model’s explainability. In response, they propose the FED-EINI framework, which aims to restore explainability by assigning meaning to the features.\n\n\nMaddock et al. [10] propose a framework for the differentially private training of Gradient Boosted Decision Tree, for instance XGBoost, in the federated learning architecture. Similarly, Qin et al. [13] introduce a novel approach for allocating a privacy budget in a decision tree ensemble model. The core of this strategy is to link the allocation of the privacy budget directly to the depth of a leaf node. This method ensures a more granular and efficient use of the privacy budget, allowing for more precise control over the trade-off between privacy and model utility. However, different to Maddock et al. [10] and Qin et al. [13], FEXT-DP focus on bagging-based decision tree approach.\n\n\nLiu et al. [8] present a solution to train private decision trees and to ensemble using differential privacy. The process involves two key procedures. First, internal nodes are selected based on a noisy maximal vote to preserve privacy. Additionally, a budget allocation strategy is designed to add less noise at larger depths, striking a balance between the true data and the added noise. Second, at the leaf nodes, the votes for each class are masked with Laplacian noise. By implementing these two procedures, the authors ensure that the entire process of growing a private decision tree is fully compliant with differential privacy principles. Despite the valuable findings highlighted by Liu et al. [8], these authors do not consider a federated learning architecture.\n\n\n\n\nIII Federated EXplainable Trees with Differential Privacy (FEXT-DP)\n\n\nAs can be observed on Fig. 1, the proposed approach, named Federated EXplainable Trees with Differential Privacy (FEXT-DP), is based on Decision Trees and are designed for Federated Learning systems. During the training phase, FEXT-DP performs three stages, which are described as follows: \\raisebox{-0.9pt}{1}⃝ the Federated Learning (FL) Clients train the tree-based model using its local dataset and send it to the server; \\raisebox{-0.9pt}{2}⃝ The FL server receives the tree-based models and aggregates them by selecting only trees that meet the minimum level of accuracy, called threshold KK. The trees that do not meet this minimum are discarded; \\raisebox{-0.9pt}{3}⃝ The FL server sends the new set of trees to the clients. After these three stages one training round is completed. The training rounds continue until the stopping criteria is reached.\n\n\nFigure 1: Overview.\n\n\nFig. 2 presents a diagram that further details the training interactions between FL Server and Clients. It is possible to notice that the training rounds start after the initial exchange of data. Then, in a particular round, after the client performs training, the client can compare if the model provided by the FL Server, i.e. global model, has lower Mean Square Error (MSE) than the most recent local model. The FL Client chose the set of trees with lower MSE.\n\n\nIn the FL Server side, all the received trees are tested to verify their levels of accuracy. The method used in this work considers a minimum threshold K to be met by the trees. This procedure eliminates low performance trees from potential malicious FL Clients who may want to poison the FL model.\n\n\nFigure 2: Diagram: Detailed interaction bet"
  },
  {
    "title": "Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders",
    "url": "https://arxiv.org/abs/2602.10099v1",
    "source": "arxiv",
    "summary": "Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric.",
    "full_text": null
  },
  {
    "title": "Step-resolved data attribution for looped transformers",
    "url": "https://arxiv.org/abs/2602.10097v1",
    "source": "arxiv",
    "summary": "We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introd",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.10097v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2602.10097v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 10 Feb 2026]\n    Title:Step-resolved data attribution for looped transformers\n    Authors:Georgios Kaissis, David Mildenberger, Juan Felipe Gomez, Martin J. Menten, Eleni Triantafillou            View a PDF of the paper titled Step-resolved data attribution for looped transformers, by Georgios Kaissis and 4 other authors\n    View PDF\n\n\n\n    \n            Abstract:We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $\\tau$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \\textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$\\tau$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Machine Learning (cs.LG); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.10097 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.10097v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.10097\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Georgios Kaissis [view email]          [v1]\n        Tue, 10 Feb 2026 18:57:53 UTC (675 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Step-resolved data attribution for looped transformers, by Georgios Kaissis and 4 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n         "
  },
  {
    "title": "Causality in Video Diffusers is Separable from Denoising",
    "url": "https://arxiv.org/abs/2602.10095v1",
    "source": "arxiv",
    "summary": "Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n2 Related Work\n\n3 Preliminaries: Causal Diffusion Models\n\nTeacher Forcing and Diffusion Forcing.\n\n\n\n4 Uncovering Causal Separability\n\n4.1 Repetitive Computation across Denoising Steps\n4.2 Deep Layers are Separable in Time\n\n\n\n5 Separable Causal Diffusion\n\n5.1 Temporal Causal Encoder\n5.2 Frame‑wise Diffusion Decoder\n\n5.3 Training and Inference Framework\n\nSupervision.\nCorrupting causal context.\n\n\n\n\n\n6 Experiments\n\nSetting.\n6.1 Training from Scratch on Small Video Datasets\n\n6.2 Fine-Tuning Pretrained T2V Diffusion Model\n\nArchitecture Adaptation.\n\nTraining and Distillation.\n\n\nResults and analysis.\n\n\n7 Conclusion\n\n\nLimitations\n\n\nA Additional Analysis of Uncovering Causal Separability\n\n\nA.1 Redundancy across Denoising Steps\n\nFeature similarity across denoising steps and depth.\n\n\n\nA.2 Evidence on Other Models\n\nObservations.\nEvidence on Diffusion Forcing with 3D UNet.\n\n\n\n\n\nB Ablation Studies\n\nB.1 Encoder-Decoder Interface\n\nB.2 Training Noisy Batches: Amortized Multi‑Sample Decoding\n\nTraining-time comparison.\n\n\nB.3 Noisy Context Latent: Context Corruption and CFG\n\n\n\nC Additional Experimental Setup\n\n\nC.1 Datasets\n\nTECO-Minecraft[78].\nUCF-101[68].\nRealEstate10K[96].\nTokenizer.\n\n\n\n\n\nD Model and Training Details\n\nD.1 Design Details of Separable Causal Diffusion\nD.2 Architectures and Model Variants\nD.3 Algorithmic Pipeline\n\n\n\nE SCD Fine-tuning Details\n\nBidirectional Teacher.\nDatasets.\nTraining Specifications.\n\nE.1 Architecture Adaptation for Decoupling\n\n(i) Input reparameterization for the encoder.\n(ii) Layer decomposition.\n\n\n\nE.2 Hyperparameters\n\nThroughput and latency.\n\n\nE.3 Flexibility of Separable Causal Diffusion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausality in Video Diffusers is Separable from Denoising\n\n\n\nXingjian Bai1,2  Guande He3  Zhengqi Li2\nEli Shechtman2  Xun Huang3  Zongze Wu2\n1Massachusetts Institute of Technology  2Adobe Research \n3Morpheus AI\nWork done while Xingjian was interning at Adobe.\n\n\nAbstract\nCausality — referring to temporal, uni-directional cause-effect relationships between components — underlies many complex generative processes, including videos, language, and robot trajectories.\nCurrent causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context.\nIn this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process.\nThrough systematic probing of autoregressive video diffusers, we uncover two key regularities:\n(1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and\n(2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering.\nMotivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame‑wise rendering, via a lightweight diffusion decoder.\nExtensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD  significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.\n\n\n\n1 Introduction\n\nModeling causality111In this paper we use causality narrowly to mean the temporal arrow-of-time—the past determines the future, not vice versa. is a core problem in diffusion generation modeling.\nStarting from fitting image distributions [60, 57, 59, 54, 5, 61, 13, 17],\ndiffusion models [30, 43] have achieved great success across modalities such as videos [31, 29, 83, 6, 24, 74, 73], audio [38, 44], and language [28, 49, 53].\nIn its basic form, diffusion models denoise all tokens simultaneously, generating the entire output all at once. This is still the design of many state-of-the-art video diffusion models. However, this formulation overlooks the temporal evolution inherent in sequential data—allowing the future information to influence the past, and preventing crucial applications such as long-term, real-time video streaming [79, 37, 87, 33].\nTo incorporate temporal causal dependencies and enable autoregressive video generation, researchers have attempted to replace bidirectional full attention inside the denoiser with causal attention [87, 22, 33], as commonly used in the LLM community. This mechanism applies bidirectional attention within a frame (or chunk of frames) and causal attention across frames (or chunks). When combined with the diffusion process, every token within a frame must pass through the entire network iteratively, computing both intra-frame and cross-frame attention at every layer and every denoising step.\n\n\nFigure 1: \nCausality in autoregressive video diffusion models is separable from the denoising process.\nThe prevailing design of causal diffusion models for visual generation performs causal attention densely across all layers and all denoising steps (left).\nHowever, we uncover two important observations (right): 1) early denoiser layers share highly repetitive computation across denoising steps (blue); 2) deep layers primarily attend to intra-frame tokens, with sparse cross-frame connections (red).\n\n\nWhile causal attention is essential for modeling temporal evolution, directly transplanting it from LLMs overlooks a key difference: diffusion models typically perform multi-step refinement for each frame, rather than generating in a single pass. The current design of causal diffusion tightly entangles temporal reasoning with iterative denoising, with each layer at every step repeatedly performing causal reasoning. This raises a fundamental question: Is multi-step refinement truly required for temporal reasoning?\n\n\nTo answer this question, we conduct detailed probing analysis and finetuning experiments on autoregressive (AR) video diffusion models. We consistently observe that temporal reasoning in AR models is separable from the denoising process (Fig. 1).\nIn particular, we find that causal reasoning in early layers is highly redundant across denoising timesteps, as indicated by the high similarity in middle-layer output features across denoising steps. We also observe that temporal computation in deeper layers is far less frequent: careful attention visualizations reveal that deeper layers predominantly perform intra-frame attention while rarely attending across frames.\n\n\nMotivated by the sparsity and redundancy we uncover, we introduce Separable Causal Diffusion (SCD), a novel decoupled causal architecture in which a temporal causal-reasoning module operates once per frame, while a lightweight frame‑wise diffusion renderer handles visual refinement. Concretely, a causal transformer reads the historical clean frame tokens through KV cache and produces a latent that summarizes the entities, layout, and expected motion from its context. This context latent is then reused across all denoising steps for that frame.\nA diffusion module receives both the current noisy frame tokens and the context latent, and performs a frame‑wise iterative denoising process without any cross-frame computation.\nTaken together, our design mirrors next-token prediction in LLMs (except that we perform next-frame prediction here followed by continuous rendering), reallocating compute from repeated cross-frame operations to per-frame refinement, thereby reducing latency and memory while preserving generation quality.\n\n\nWe conduct extensive experiments at both pretraining and post-training stages for causal video diffusion models across synthetic and real datasets. We show that SCD trained from scratch matches or surpasses causal diffusion baselines in generation quality while achieving 2–3× lower latency.\nFurthermore, to demonstrate scalability, we finetune SCD from a pretrained bidirectional teacher diffusion model, achieving strong video generation quality with substantially higher throughput compared with AR baselines.\n\n\nContributions.\n\nIn summary, we make the following contributions: 1) Through careful probing and finetuning experiments, we observe that causal reasoning in existing causal video diffusion models is redundant across denoising steps and sparse across time. 2) We introduce a novel Separable Causal Diffusion (SCD) architecture that fully leverages these observations. On both pretraining and post-training tasks, SCD demonstrates strong effectiveness across multiple datasets compared with baseline models.\n\n\n\n\n\n2 Related Work\n\nFrom Bidirectional to Autoregressive Video Diffusion.\nDiffusion-based video generative models have achieved remarkable fidelity by employing spatio-temporal Transformers with bidirectional attention over entire video sequences. Recent methods [25, 6, 58, 12, 21, 73, 20, 11, 16] advance this paradigm through careful architectural design and large-scale training, achieving state-of-the-art visual quality. However, their non-causal design requires generating all frames simultaneously, resulting in high latency and preventing real-time streaming or interactive applications.\n\n\nTo enable online, low-latency generation, recent efforts have shifted toward autoregressive (AR) video generation, particularly using diffusion models with causal transformers. Instead of producing all frames at once, AR diffusion models generate videos in a causal manner, conditioning each frame only on past frames. This causal dependence not only aligns with the arrow of time but also enables efficient inference via KV caching, making it attractive for interactive settings. Pioneering AR approaches include models trained from scratch [22, 11, 8, 55, 37] and techniques that distill a causal generator from a pretrained video diffusion model [10, 80, 33, 87, 15].\n\n\nAR-Diffusion Hybrid Models.\nTo leverage the strengths of both paradigms, a growing body of work combines an AR module with a diffusion module. In the image domain, several re"
  },
  {
    "title": "Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing",
    "url": "https://arxiv.org/abs/2602.10092v1",
    "source": "arxiv",
    "summary": "Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Au",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Quantum-Audit Dataset\n\n4 Experiments\n\n4.1 Comprehensive Model Evaluation on Core Benchmark and Across Topics\n4.2 Performance Across Different Question Formats\n4.3 Performance with Agentic and Deep Research Modes\n4.4 Human Performance Baseline Study\n4.5 Multilingual Benchmark Performance\n\n\n5 Discussion\n6 Limitations and Future Work\n7 Conclusion\n\n8 Appendices\n\n8.1 Benchmark Development and Dataset Access\n8.2 Question Filtering and Quality Control\n8.3 Question Translation\n8.4 Question Format Diversification\n8.5 Human Performance Baseline Study\n\n\n\n\n\n\n\n\n\\svgpath\n./\n\nQuantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing\n\n\nMohamed Afane\n\nmafane@fordham.edu\n\nFordham UniversityNew YorkNew YorkUSA\n\n, \nKayla Laufer\n\nkal1@fordham.edu\n\nFordham UniversityNew YorkNew YorkUSA\n\n, \nWenqi Wei\n\nwenqiwei@fordham.edu\n\nFordham UniversityNew YorkNew YorkUSA\n\n, \nYing Mao\n\nymao41@fordham.edu\n\nFordham UniversityNew YorkNew YorkUSA\n\n, \nJunaid Farooq\n\nmjfarooq@umich.edu\n\nUniversity of Michigan-DearbornDearbornMichiganUSA\n\n, \nYing Wang\n\nywang6@stevens.edu\n\nStevens Institute of TechnologyHobokenNew JerseyUSA\n\n and \nJuntao Chen\n\njchen504@fordham.edu\n\nFordham UniversityNew YorkNew YorkUSA\n\n\n(2026)\n\nAbstract.\nLanguage models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.\n\nquantum computing, large language models, benchmark, evaluation, quantum security, multilingual assessment\n\n††copyright: acmlicensed††journalyear: 2026††doi: XXXXXXX.XXXXXXX††conference: Preprint Under Review; 2026; ††isbn: 978-1-4503-XXXX-X/2018/06††ccs: Computer systems organization Quantum computing††ccs: Computing methodologies Natural language processing\n\n\n1. Introduction\n\nQuantum computing has progressed significantly from theoretical research to experimental implementations with practical applications. Current quantum systems have rapidly evolved through successive technological breakthroughs from operating with just a few qubits to recently surpassing the 1000-qubit barrier (AbuGhanem, 2025), enabling exploration of quantum algorithms and protocols that were previously confined to theoretical analysis. This technical advancement drives progress in quantum simulation (King et al., 2025; Halimeh et al., 2025; Puig et al., 2025), optimization problems (Quinton et al., 2025; Phillipson, 2024), and cryptographic applications (Sahu and Mazumdar, 2024; Ralegankar et al., 2021; Kalaivani et al., 2021). Beyond traditional quantum applications such as quantum simulation and cryptography, recent research explores its potential in finance (Innan et al., 2024; Grossi et al., 2022), healthcare (Ur Rasool et al., 2023; Flöther, 2023), computer vision (Li et al., 2020; Afane et al., 2025a; ALRikabi et al., 2022), and wireless communication (Narottama and Shin, 2021; Narottama et al., 2023), among other promising real-world applications.\n\n\nIn parallel, Large Language Models (LLMs) have become sophisticated tools that address complex challenges across many disciplines. These AI systems now approach or exceed human expert performance in areas such as cybersecurity (Tihanyi et al., 2024; Afane et al., 2024), medical diagnosis (Subedi, 2025), and legal reasoning (Guha et al., 2023; Kant et al., 2025). As these two fields continue to evolve, their intersection becomes increasingly important for scientific communication, education, and research productivity.\nDespite significant advances in both domains, we face a critical knowledge gap in evaluating LLMs’ understanding of specialized quantum concepts. While extensive benchmarking exists across numerous related domains, including mathematics (Gao et al., 2024; Fang et al., 2024), physics (Chung et al., 2025), and computer science (Jimenez et al., 2023), no standardized frameworks comprehensively assess quantum computing knowledge in these models. This absence is particularly concerning given the field’s counterintuitive principles, and rapidly evolving terminology that challenge even domain experts. The complexity of quantum computing concepts, combined with their inherent mathematical abstraction, creates a particularly demanding test case for evaluating the depth of LLMs’ specialized knowledge.\nWithout reliable evaluation metrics, LLMs risk spreading plausible but incorrect quantum information to educational and research communities, as hallucinations, reasoning errors, and factual inaccuracies have been widely documented in similarly complex and technically demanding specialized domains. (Orgad et al., 2024; Afane et al., 2025b; Perković et al., 2024).\n\n\nThis creates an urgent need for robust quantum computing benchmarks as researchers, students, and industry professionals increasingly rely on these models for information and assistance with quantum tasks. The growing adoption of LLMs across academic institutions and quantum technology companies further amplifies the importance of ensuring these systems provide accurate information on this emerging field.\nTo address these challenges, we present the following key contributions:\n\n\n•\n\nWe assemble 2,000 multiple-choice questions: 1,000 expert-written questions developed by quantum computing researchers and 1,000 questions extracted from research papers using LLMs and validated by domain experts, covering seven core topics including quantum algorithms, error correction, security protocols, distributed computing, quantum machine learning, gates and circuits, and foundational concepts.\n\n\n\n•\n\nWe conduct extensive evaluation across 26 models, comparing their performance against 43 quantum computing experts and practitioners to establish human baselines and assess how models perform relative to human capabilities across different experience levels.\n\n\n\n•\n\nWe provide multi-dimensional analysis with an additional 350 open-ended questions requiring detailed explanations and 350 questions containing intentionally false premises to evaluate whether models can identify and correct erroneous assumptions embedded within the question’s formulation (2,700 questions total), with a multilingual subset enabling cross-lingual evaluation in Spanish and French. Our analysis reveals systematic failures on advanced topics like quantum security despite strong performance on foundational concepts, alongside significant multilingual performance degradation and poor performance on detecting false premises.\n\n\n\n\n\n\n\n2. Related Work\n\nDespite significant advancements in both quantum computing and LLMs, their intersection remains surprisingly underexplored. Recent research has begun addressing this gap from different angles. Kashani (Kashani, 2024) introduced QuantumLLMInstruct (QLMMI), a dataset of over 500,000 instruction-problem pairs covering quantum cryptography, spin chain models, and Trotter-Suzuki decompositions. However, QLMMI’s primary purpose is to enable instruction fine-tuning rather than comprehensive evaluation of quantum knowledge. While extensive in size, QLMMI relies entirely on synthetically generated content through a four-stage LLM pipeline. In contrast, Quantum-Audit offers 1,200 human-authored evaluation questions extracted directly from research literature published over four decades, prioritizing authentic scientific content over synthetic generation. Wang et al. (Wang et al., 2024) introduced GroverGPT, an approach to simulating quantum algorithms using LLMs. Their 8-billion-parameter model is fine-tuned to approximate Grover’s quantum search algorithm without explicitly representing quantum states. While GroverGPT demonstrates impressive capabilities in predicting specific quantum circuit outputs, it focuses exclusively on a single quantum algorithm rather than evaluating comprehensive knowledge across the quantum computing domain.\n\n\nComplementary efforts have emerged focusing on quantum code generation and circuit implementation capabilities. Vishwakarma et al. (Vishwakarma et al., 2024) developed Qiskit HumanEval, a hand-curated benchmark of over 100 tasks designed to evaluate LLM performance in generating executable quantum code using the Qiskit SDK, complete with canonical solutions and comprehensive test cases. Guo et al. (Guo et al., 2025) introduced QuanBench, which evaluates quantum code generation across 44 programming tasks using both functional correctness (Pass@K) and quantum semantic equivalence (Process Fidelity) metrics, finding that current LLMs achieve below 40% overall accuracy with frequent semantic errors including outdated API usage and incorrect algorithm logic. Yang et al. (Yang et al., 2024) presented QCircuitNet, a large-scale hierarchical dataset for quantum algorithm design containing 120,290 data points with"
  },
  {
    "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.10090v1",
    "source": "arxiv",
    "summary": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Agent World Model\n\n3.1 Scenario Generation\n3.2 Task Generation\n\n3.3 Environment Synthesis\n\n3.3.1 Environment\n3.3.2 Pipeline Results and Analysis\n\n\n\n\n\n4 Agentic Reinforcement Learning\n\n4.1 Reward Design\n4.2 History-Aware Training\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n\n\n\n6 Analysis\n\n6.1 Quality of Synthesized Environments\n6.2 Analysis on Verification Design\n6.3 Analysis on History-Aware Training\n6.4 Environment Scaling Curve\n\n\n7 Conclusion\n\nA Implementation Details\n\nA.1 Scenario Generation\nA.2 Task Generation\nA.3 Environment Synthesis\nA.4 Tool Calling Format and Validation\nA.5 Training and Evaluation Details\n\n\n\nB Analysis\n\nB.1 Analysis on Step-level Format Correctness Reward\nB.2 Case Study for Verification\n\n\n\n\n\n\n\n\n Agent World Model:\nInfinity Synthetic Environments for Agentic Reinforcement Learning\n\n\nZhaoyang Wang\n\n  \nCanwen Xu\n\n  \nBoyi Liu\n\n  \nYite Wang\n\n  \nSiwei Han\n\n  \nZhewei Yao\n\n  \nHuaxiu Yao\n\n  \nYuxiong He\n\n\n\nAbstract\nRecent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments.\nIn this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations.\nNotably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments.\nTo demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions.\nExperiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization.\nThe code is available at https://github.com/Snowflake-Labs/agent-world-model.\n\nMachine Learning, ICML, Agent, Data Synthesis, Reinforcement Learning, Environment Synthesis, Tool-use\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have achieved remarkable performance in instruction following, reasoning, code generation, and tool-use (Chen et al., 2021; Schick et al., 2023; OpenAI, 2025; Anthropic, 2025a; Comanici et al., 2025; Guo et al., 2025). Agents powered by LLMs emerge as a promising paradigm for handling multi-step complex tasks in realistic environments (Nakano et al., 2021; Yao et al., 2023; Yang et al., 2024; Qin et al., 2024). However, training such agents often requires performing large-scale reinforcement learning (RL) on diverse environments that are relatively resource-scarce and expensive to scale.\n\n\nUsing real-world environments for training is prohibitively expensive and hard to scale, since many scenarios do not expose public APIs and RL training often requires agents to interact with them thousands of times in a stable and efficient manner (Dulac-Arnold et al., 2021; Qin et al., 2024; Xu et al., 2024a; Luo et al., 2025).\nHuman-created environments are hard to scale and often lack diversity. For example, τ2\\tau^{2}-bench (Barres et al., 2025) and TheMCPCompany (Esfandiarpoor et al., 2025) only contain three and five environments, respectively, which is far from enough for training generic AI agents.\nMost existing synthetic research on agents focuses on task synthesis (Wang et al., 2023; Chen et al., 2024; Xie et al., 2025; Patil et al., 2024; Wang et al., 2026) and trajectory collection (Xu et al., 2024b; Li et al., 2025a; Song et al., 2024) rather than environment synthesis.\nAnother line of research simulates the tool response or even the environment, where each state transition is generated by LLMs (Liu et al., 2024b; Lu et al., 2025; Li et al., 2025b, c; Chen et al., 2025). However, this approach is not reliable or efficient due to the hallucination issue (Wang et al., 2024; Kalai et al., 2025) and LLM’s high inference cost.\n\n\nFigure 1: Agent World Model (AWM) is a synthetic environment generation pipeline that synthesizes 1,000 diverse code-driven agentic environments with databases for training tool-use agents.\n\n\nThese limitations highlight a missing piece: scalable environment synthesis. In particular, The challenge is to synthesize executable, reliable environments at scale, enabling replicable agent interaction and learning.\nIn recent months, DeepSeek-V3.2 (DeepSeek-AI et al., 2025) introduces a synthesis pipeline to create thousands of executable environments for general agents, while Qwen Tongyi (Fang et al., 2025) also describes an environment synthesis pipeline but for supervised fine-tuning (SFT) rather than RL training. The adopted code-based approach is promising to build such environments at scale, since it can control the state transition and ensure the consistency of the environment. However, neither of them releases the generation pipeline nor open-sources their environments.\nSeveral concurrent community efforts (Feng et al., 2025; Sullivan et al., 2025; Cai et al., 2025; Zhang et al., 2025a; Song et al., 2026) explore environment synthesis through programming. However, they either target game-like environments, rely on human priors (e.g., code documentation), lack strong guarantees of state consistency, or remain limited in scale.\n\n\nTo address this, we propose Agent World Model (AWM), an open-source pipeline that synthesizes executable tool-use environments at scale. The key insight is that agent environments share a common structure: a stateful backend, a tools interface layer, and task-specific success criteria.\nBy decomposing synthesis into these three components, we can leverage LLMs to generate each part systematically while maintaining consistency. Our AWM mirrors how software is built in practice. Starting from a high-level scenario description (e.g., “an online shopping platform”), we first generate common user requirements (i.e., tasks) that users are likely to perform in this scenario.\nThen, we generate the database schema to define what entities and relations exist to fulfill these user requirements. This schema can guide the design of exposed interfaces (toolset) and help generate the backend code for the interfaces, ensuring each tool has a clear data model to operate on. The interface is exposed via Model Context Protocol (MCP) (Anthropic, 2024) for unified agent tool interaction with the environment.\nFinally, we generate verification code that compares the database state before and after agent execution, which augments an LLM-as-a-Judge to provide robust reward signals for RL training.\nCritically, each stage includes automated execution and simple self-correction: if generated code fails to run, we feed the error information back to the LLM for correction.\n\n\nThis pipeline enables us to scale to 1,000 unique environments spanning most real-world scenarios such as shopping, social media, finance, and travel. Each environment provides an executable sandbox where agents can interact with dozens of tools, and they fully support parallel isolated instances and are easy to reset or restart, which are important for efficient online RL.\nTo validate AWM, we perform large-scale RL (each step with 1,024 environment instances) to train agents to use MCP tools to complete the task.\nIn contrast to some works performing training on test environments, our environments and tasks are not tailored for any benchmarks or specific scenarios.\nExperimental results on three tool-use benchmarks demonstrate the generalization performance of our framework.\nIn summary, our contributions are three-fold: (1) AWM, an open-source pipeline for automatic generation of executable tool-use environments with database-backed state consistency, (2) 1,000 ready-to-use diverse environments suitable for large-scale RL training, and (3) empirical results show that agents trained on AWM generalize to out-of-distribution environments.\n\n\nFigure 2: Overview of AWM. Starting from scenario synthesis, we progressively generate tasks, database, interface and verification to obtain fully executable environments. Then, we perform multi-turn RL training for tool-use agents in our synthesized environments.\n\n\n\n\n2 Related Work\n\nTool-use Agents.\nRecent work shows that LLMs can use external tools to solve complex tasks (Qin et al., 2024; OpenAI, 2025; DeepSeek-AI et al., 2025; Team et al., 2025).\nToolformer (Schick et al., 2023) trains tool-use LLMs by supervised learning.\nToolLLM (Qin et al., 2024) curates real-world APIs and trains on LLM-generated trajectories, but uses simulated responses instead of tool execution.\nGorilla (Patil et al., 2024) fine-tunes with API documentation to improve tool-use accuracy.\nReAct (Yao et al., 2023) and SWE-agent (Yang et al., 2024) alternate reasoning and acting in interactive environments.\nHowever, most training data remains static or comes from small-scale environments.\nExisting benchmarks (Yao et al., 2024; Barres et al., 2025; Esfandiarpoor et al., 2025; Luo et al., 2025; Fan et al., 2025; Mo et al., 2025) either use real-world APIs or provide small-scale environments. This makes them hard to use as large-scale RL training grounds.\nThe gap is the lack of diverse and executable environments that are efficient enough for RL.\nSuch training needs extensive agent interactions, and it benefits from fast interactions and reliable state transitions.\n\n\nAgent Data Synthesis.\nSynthetic data is widely used to scale agent training (Patil et al., 2024; Schick et al., 2023; Qin et al., 2024; Liu et al., 2024b, 2025).\nSelf-Instruct (Wang et al., 2023) pop"
  },
  {
    "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
    "url": "https://arxiv.org/abs/2602.10085v1",
    "source": "arxiv",
    "summary": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. Whil",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.10085v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.10085v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 10 Feb 2026]\n    Title:CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs\n    Authors:Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully            View a PDF of the paper titled CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs, by Richard Bornemann and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{this https URL}{here}$.\n    \n\n    \n    \n              \n          Comments:\n          Preprint\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.10085 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.10085v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.10085\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Richard Bornemann [view email]          [v1]\n        Tue, 10 Feb 2026 18:51:39 UTC (4,271 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs, by Richard Bornemann and 2 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Whic"
  },
  {
    "title": "Anagent For Enhancing Scientific Table & Figure Analysis",
    "url": "https://arxiv.org/abs/2602.10081v1",
    "source": "arxiv",
    "summary": "In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heteroge",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.10081v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2602.10081v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 10 Feb 2026]\n    Title:Anagent For Enhancing Scientific Table &amp; Figure Analysis\n    Authors:Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang            View a PDF of the paper titled Anagent For Enhancing Scientific Table &amp; Figure Analysis, by Xuehang Guo and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \\&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \\&amp; figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\\uparrow 13.43\\%$ in training-free settings and $\\uparrow 42.12\\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \\&amp; figure analysis. Our project page: this https URL.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.10081 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.10081v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.10081\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Xuehang Guo [view email]          [v1]\n        Tue, 10 Feb 2026 18:46:28 UTC (33,100 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Anagent For Enhancing Scientific Table &amp; Figure Analysis, by Xuehang Guo and 3 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Ha"
  },
  {
    "title": "CAPID: Context-Aware PII Detection for Question-Answering Systems",
    "url": "https://arxiv.org/abs/2602.10074v1",
    "source": "arxiv",
    "summary": "Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user's question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Problem Statement\n\n4 CAPID\n\n4.1 Topics Generation\n4.2 PII, Context and Question Generation\n4.3 Context Enhancement\n4.4 Data Validation\n\n\n\n5 Evaluation\n\n5.1 Model Training Performance\n\n5.2 Downstream Performance\n\n5.2.1 Evaluation on Reddit Data\n5.2.2 Utility Analysis\n\n\n\n\n6 Conclusion\nA Generation Configuration\n\nB Prompt Templates\n\nB.1 Topic Generation\nB.2 Subtopic Generation\nB.3 Situation Generation\nB.4 Peripheral Context Generation\nB.5 Question Generation\nB.6 Paraphrased Context Generation\nB.7 Span Retrieval\nB.8 PII Generation\n\n\nC Training parameters\n\nD Fine-tuning and Pretrained Model Prompts\n\nD.1 Shared Prompt Template\nD.2 Pretrained Model Instruction\nD.3 Fine-tuned Model Instruction\n\n\n\nE Downstream Performance Prompts\n\nE.1 Question Answering Instruction\nE.2 Answer Evaluation Instruction\n\n\n\nF Annotation Examples\n\nF.1 Example 1\nF.2 Example 2\n\n\n\n\n\n\n\nCAPID: Context-Aware PII Detection for Question-Answering Systems\n\n\n\nMariia Ponomarenko1,\nSepideh Abedini1, 2,\nMasoumeh Shafieinejad2,\nD. B. Emerson2,\n\nShubhankar Mohapatra1,\nXi He1, 2\n1University of Waterloo,\n2Vector Institute\n\n\nCorrespondence: m2ponoma@uwaterloo.ca\n\n\n\nAbstract\nDetecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches typically redact all PII, disregarding the possibility that some may be contextually relevant to the user’s question, thereby degrading response quality. Large language models (LLMs) may help determine which PII is relevant; however, due to their closed-source nature and lack of privacy guarantees, they are unsuitable for processing sensitive data. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively.\nTo address this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and levels of relevance. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving higher downstream utility under anonymization.\n\n\n\nCAPID: Context-Aware PII Detection for Question-Answering Systems\n\n\n\n\n\nMariia Ponomarenko1,\nSepideh Abedini1, 2,\nMasoumeh Shafieinejad2,\nD. B. Emerson2,\n\nShubhankar Mohapatra1,\nXi He1, 2\n\n1University of Waterloo,\n2Vector Institute\n\n\nCorrespondence: m2ponoma@uwaterloo.ca\n\n\n\n\n\n\n1 Introduction\n\nIn today’s digital era, individuals frequently disclose personal information while interacting with online platforms such as conversational assistants and chatbots, particularly when seeking advice or posing questions Saffarizadeh et al. (2018). These disclosures often involve personally identifiable information (PII), raising significant privacy concerns. Regulatory frameworks, such as GDPR Tikkinen-Piri et al. (2018), have been established to protect personal data and ensure responsible handling of sensitive information.\n\n\nExample 1.\n\nOriginal query: I’m a warehouse supervisor with chronic back pain from lifting heavy boxes.\nI live in Springfield and have two children. How can I reduce fatigue after long shifts?\nGeneric PII redaction: I’m a [OCCUPATION] with [HEALTH] from lifting heavy boxes.\nI live in [LOCATION] and have [FAMILY]. How can I reduce fatigue after long shifts?\nContext-aware redaction: I’m a warehouse supervisor with\nchronic back pain from lifting heavy boxes.\nI live in [LOCATION] and have [FAMILY].\nHow can I reduce fatigue after long shifts?\n\n\nExample 2.\nExample 2 (*).\n\nThe example illustrates how context-aware redaction preserves personal information relevant to reasoning about the user’s question.\nFor the question “How can I reduce fatigue after long shifts?”,\ninformation about the user’s occupation (warehouse supervisor) and condition (back pain) is valuable to interpreting the cause of fatigue, whereas location and family details are unrelated and thus safely masked.\n\n\nTo protect user privacy, numerous privacy tools have been developed to detect and redact PII Allal et al. (2023); Pilán et al. (2022). However, most of these tools Microsoft (2021); Amazon (2025) do not account for the contextual relevance of the information they flag. As a result, they can obscure information essential for accurate and\ncontextually appropriate response.\nIn certain settings, retaining specific sensitive information is justified Nissenbaum (2004), as some private details are directly relevant to a user’s goal.\nAlthough most existing approaches focus on general PII detection, some recent studies have begun to explore context-sensitive methods Shen et al. (2025); Dou et al. (2024); Ngong et al. (2025). At the same time, LLMs have demonstrated remarkable performance across a range of tasks Brown et al. (2020). Yet their widespread use through third-party APIs (e.g., OpenAI, Anthropic) raises privacy concerns, as user queries containing sensitive data may be transmitted to external servers. To mitigate these concerns, fine-tuning local models for specific privacy-preserving tasks becomes essential. Nevertheless, to the best of our knowledge, no datasets or models have been publicly released for context-sensitive PII detection, and there is a lack of evaluation of how such context-sensitive redaction affects downstream application performance, such as question answering with LLMs. To this end, we present the following contributions.\n\n\n1.\n\nIntroducing CAPID, a synthetic dataset for context-aware PII detection. CAPID focuses on the relevance of PII spans with respect to a given question across diverse topics. The dataset is designed to support fine-tuning and evaluation of context-aware models that must reason not only about the presence of PII, but also about whether such information should be retained or masked in the question-answering tasks.\n\n\n\n2.\n\nShowing the effectiveness of CAPID by training and evaluating several SLMs, including Llama-3.1-8B and Llama-3.2-3B, for context-aware PII detection, achieving an accuracy score improvement from 0.68 to 0.79 in classifying PII relevance compared to GPT-4.1-mini.\n\n\n\n3.\n\nExhibiting that relevance-aware anonymization preserves significantly more downstream answer utility than existing anonymization baselines using an LLM-as-a-judge approach. This is demonstrated by collecting and annotating real user queries from Reddit and evaluating LLM-generated answers under different masking strategies.\n\n\n\n\n\nWe open-source the code with the dataset111https://github.com/MariaPonomarenko38/CAPID and the model222https://huggingface.co/ponoma16/capid-llama8b-lora.\n\n\n\n\n\n\n2 Related Work\n\nMost existing PII detection systems are built on transformer-based NER models that identify a small, fixed set of entity types such as names, locations, and organizations Microsoft (2021); Amazon (2025). Subsequent research has pursued finer-grained, domain-specific detection using synthetic data Jangra et al. (2025), knowledge-graph supervision Papadopoulou et al. (2022a), federated learning Hathurusinghe et al. (2021), or LLM-based generation Ngong et al. (2025) to expand coverage of PII and self-disclosure. Other works fine-tune large encoder models for span-level self-disclosure detection Dou et al. (2024) or use LLMs to infer a wide range of personal attributes from text Staab et al. (2024). Despite these advances, current methods fail to capture which PII are contextually relevant, often leading to excessive redaction and loss of information essential for accurate response generation Pal et al. (2024); Larbi et al. (2022); Lukas et al. (2023).\n\n\nSome recent efforts attempt to address this limitation by fine-tuning models for contextual PII detection. However, relevance is primarily defined through the distinction between public and private information Xiao et al. (2024). In contrast, we fine-tune SLMs to achieve a more nuanced understanding of PII relevance within the context of a user’s question. Ngong et al. (2025) estimated contextual relevance of PII using pretrained SLMs. However, relying solely on pretrained models can lead to lower accuracy than task-specific fine-tuning. Furthermore, existing corpora to fine-tune such models are limited in scope and quality. The Text Anonymization Benchmark focuses primarily on legal text and a narrow range of identifiers Pilán et al. (2022). The pii-masking-300k dataset AI4Privacy (2022) provides broad topical coverage but limits annotations to direct identifiers, such as names or contact information. Other attributes in the text, such as occupation, education, or health, while not uniquely identifying an individual, can still disclose personal details and may therefore warrant masking. In our work, we treat these self-disclosed attributes as part of the privacy surface that should be protected. Dou et al. (2024) similarly annotate such attributes, labeling 4.8K spans with importance scores across 2.4K Reddit posts. Unfortunately, the released data omitted these scores and defined importance only at the message level. Similarly, Shen et al. (2025) developed an evaluation dataset for query-related PII detection; however, it is restricted to the job domain, and relevant PII is trivially linked to queries via explicit references, limiting generalization to more natural interactions. Consequently, no existing dataset adequately supports modeling the contextual relevance of PII across diverse scenarios.\n\n\n\n\n3 Problem Statement\n\nWe consider a question-answering system backed by an externally hosted LLM, treated as untrusted Wang et al. (2025).\nA user query consists of a context,"
  },
  {
    "title": "Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability",
    "url": "https://arxiv.org/abs/2602.10067v1",
    "source": "arxiv",
    "summary": "Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior an",
    "full_text": null
  },
  {
    "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
    "url": "https://arxiv.org/abs/2602.10063v1",
    "source": "arxiv",
    "summary": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking tha",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Method\n\n2.1 Problem Formulation\n2.2 Framework Overview\n2.3 Mindset Dispatch\n2.4 Context Gate\n2.5 Illustrative Example\n\n\n\n3 Experiments\n\n3.1 Tasks and Datasets\n3.2 Baselines\n3.3 Implementation Details\n3.4 Main Results\n3.5 Ablation Study\n\n3.6 Analysis\n\nMethod Efficiency Comparison\nAblation Efficiency\nMindset Invocation Patterns\n\n\n\n\n4 Conclusion\n\nA Related Work\n\nA.1 Cognitive Behaviors in LLM Reasoning\nA.2 Prompt-based Reasoning\nA.3 Meta-Reasoning\n\n\nB Future Directions\nC Baseline Implementation Details\n\nD Chain of Mindsets Prompt Templates\n\nD.1 Meta-Agent\n\nD.2 Mindset Experts\n\nAlgorithmic Mindset.\nConvergent Mindset.\nDivergent Mindset.\nSpatial Mindset.\n\n\n\nD.3 Context Gates\n\nInput Gate.\nOutput Gate.\n\n\n\n\n\nE Case Studies\n\nE.1 Case Study 1: Mathematical Reasoning with Dynamic Re-planning (AIME)\nE.2 Case Study 2: Multimodal Geometry with Visual Input (MathVision)\n\n\n\n\n\n\n\nChain of Mindset: Reasoning with Adaptive Cognitive Modes\n\n\n\nTianyi Jiang1,2*,\nArctanx An1*,\nHengyi Feng1,\nNaixin Zhai6, \nHaodong Li3,\nXiaomin Yu6,\nJiahui Liu1,\nHanwen Du,\nShuo Zhang6,\nZhi Yang4,\nJie Huang4,\nYuhua Li6,\nYongxin Ni5,\nHuacan Wang6†,\nRonghao Chen1,6†\n\n1PKU,\n2BJTU,\n3StepFun,\n4SUFE,\n5NUS,\n6QuantaAlpha\n\n\n\n*These authors contributed equally to this work.\n\n†Correspondence:\nwanghuacan17@mails.ucas.ac.cn,\nchenronghao@alumni.pku.edu.cn\n\n\n\nAbstract\nHuman problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96% and 4.72% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at https://github.com/QuantaAlpha/chain-of-mindset.\n\n\n\nChain of Mindset: Reasoning with Adaptive Cognitive Modes\n\n\n\n\nTianyi Jiang1,2*,\nArctanx An1*,\nHengyi Feng1,\nNaixin Zhai6,\n\nHaodong Li3,\nXiaomin Yu6,\nJiahui Liu1,\nHanwen Du,\nShuo Zhang6,\n\nZhi Yang4,\nJie Huang4,\nYuhua Li6,\nYongxin Ni5,\nHuacan Wang6†,\nRonghao Chen1,6†\n\n1PKU,\n2BJTU,\n3StepFun,\n4SUFE,\n5NUS,\n6QuantaAlpha\n\n\n*These authors contributed equally to this work.\n\n\n†Correspondence:\nwanghuacan17@mails.ucas.ac.cn,\nchenronghao@alumni.pku.edu.cn\n\n\n\n\nFigure 1: Performance comparison on Qwen3-VL-32B-Instruct across six reasoning benchmarks.\n\n\n\n\n1 Introduction\n\nThe essence of human intelligence lies in the synergy of multiple complementary mindsets. Cognitive science research Guilford (1967) has identified distinct cognitive modes that serve fundamentally different functions: Spatial thinking concretizes abstract conditions into intuitive visual representations that facilitate pattern recognition Newcombe (2010); Newcombe and Shipley (2014); Convergent thinking distills core insights from complex, multifaceted information through focused logical analysis Cropley (2006); Divergent thinking generates novel possibilities when conventional logic reaches an impasse by exploring unconventional pathways Runco and Acar (2012). This repertoire of cognitive capabilities constitutes the underlying flexibility with which humans handle heterogeneous tasks. Beyond these human cognitive modes, computational systems enable a fourth capability—Algorithmic thinking: precise numerical calculation and formal verification through code execution Futschek (2006), providing computational precision that extends beyond the limits of human mental arithmetic. Yet current intelligent systems, despite their impressive scale and advances in multimodal perception Lin et al. (2025), lack this repertoire of complementary cognitive capabilities and remain distant from the flexible, multimodal reasoning that characterizes human intelligence.\n\n\nCrucially, human problem-solving is not merely possessing these mindsets but dynamically orchestrating them within a single reasoning episode Newell et al. (1972). When facing a complex task, we do not apply a single mindset uniformly from start to finish; instead, we transition between mindsets as the problem state evolves. For example, solving a geometry proof may begin with spatial reasoning to visualize the configuration, shift to convergent thinking to identify key relationships, then invoke divergent thinking to explore auxiliary constructions, and finally employ algorithmic steps to verify the solution. This step-level adaptive switching (i.e., recognize when each mindset is most effective and transitioning accordingly) is fundamental to human cognitive flexibility Sali et al. (2024). It enables the reasoning trace to remain rigorous when precision is needed and creative when conventional approaches fail.\n\n\nPrevious work Didolkar et al. (2024); Kargupta et al. (2025) has confirmed that complex reasoning requires diverse mindsets. LLMs indeed exhibit different mindsets during the reasoning process, and prior studies suggest that controlling models through explicit cognitive interventions can effectively improve reasoning performance Gandhi et al. (2025). However, a question remains largely unexplored: Given different contexts and reasoning scenarios, which mindset is most suitable for solving the problem?\n\n\nExisting reasoning methods for LLMs fall into two paradigms, both with fundamental limitations, as illustrated in Figure 2. Single-mode reasoning methods Wei et al. (2022); Chen et al. (2022); Li et al. (2023); Yao et al. (2023) apply a uniform cognitive strategy throughout, struggling when sub-tasks demand heterogeneous capabilities. Static reasoning strategy selection methods Gao et al. (2024); Yang et al. (2024); Aytes et al. (2025) choose a reasoning format at task onset but cannot adapt when intermediate results reveal that a different mindset would be more effective. Neither supports dynamic, state-dependent cognitive switching—recognizing when to transition between mindsets based on the progress of reasoning.\n\n\nFigure 2: Comparison of reasoning paradigms. (a) Single-mode reasoning applies a single mindset throughout, failing to address heterogeneous sub-task demands. (b) Static reasoning strategy selection chooses a strategy at task onset but cannot adapt to intermediate states. (c) Chain of Mindset dynamically switches mindsets at subtask boundaries based on the progress of reasoning.\n\n\nTo address these limitations, we propose Chain of Mindset (CoM), a training-free agentic reasoning paradigm that implements authentic cognitive chaining. Unlike previous methods that are limited to a single mindset, our framework enables agents to dynamically orchestrate a composite reasoning process with different mindsets. CoM decomposes reasoning into four functionally heterogeneous mindsets—Spatial, Convergent, Divergent, and Algorithmic. We selected these four mindsets because they represent search-style reasoning capabilities that transcend the typical single-mode reasoning of language models.These four mindsets are grounded in foundational cognitive science research as fundamental reasoning paradigms Guilford (1967); Futschek (2006); Cropley (2006); Newcombe (2010); Runco and Acar (2012), each exhibiting distinct behavioral signatures that enable explicit orchestration. When solving any given problem, the agent can adaptively select and dynamically invoke multiple mindsets based on the current state. Furthermore, to prevent cross-boundary information interference caused by frequent mindset switching, we introduce a Context Gate mechanism. Through bidirectional semantic filtering, this mechanism ensures that each thinking module receives only task-relevant context, while the meta-agent receives only highly condensed thought feedback, thereby guaranteeing efficient reasoning. Extensive experiments across six challenging benchmarks demonstrate that CoM consistently outperforms all baselines, as illustrated in Figure 1, while maintaining computational efficiency and generalizing across both open-source and closed-source base models without any additional training.\n\n\nThe main contributions are summarized as follows:\n\n\n\n\n•\n\nWe propose a new agentic reasoning paradigm. To the best of our knowledge, this is the first training-free method achieve step-level adaptive switching of multiple mindsets within a single inference process.\n\n\n\n•\n\nWe formally define four heterogeneous mindsets and propose the Context Gate bidirectional semantic filtering mechanism, which enables the agent to seamlessly switch mindsets while effectively reducing cross-module information interference.\n\n\n\n•\n\nOur experiments on six challenging benchmarks, including mathematics, coding, scientific QA, and spatial reasoning, demonstrate that CoM not only significantly outperforms baseline methods in accuracy but also balances reasoning efficiency with generalization across models and domains without training.\n\n\n\n\n\n\n\n2 Method\n\nIn this section, we formally introduce the Chain of Mindset (CoM) framework. We begin by formalizing the mindset switching problem in "
  },
  {
    "title": "Vendi Novelty Scores for Out-of-Distribution Detection",
    "url": "https://arxiv.org/abs/2602.10062v1",
    "source": "arxiv",
    "summary": "Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Scor",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.10062v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2602.10062v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 10 Feb 2026]\n    Title:Vendi Novelty Scores for Out-of-Distribution Detection\n    Authors:Amey P. Pasarkar, Adji Bousso Dieng            View a PDF of the paper titled Vendi Novelty Scores for Out-of-Distribution Detection, by Amey P. Pasarkar and 1 other authors\n    View PDF\n\n\n\n    \n            Abstract:Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Score (VNS), an OOD detector based on the Vendi Scores (VS), a family of similarity-based diversity metrics. VNS quantifies how much a test sample increases the VS of the in-distribution feature set, providing a principled notion of novelty that does not require density modeling. VNS is linear-time, non-parametric, and naturally combines class-conditional (local) and dataset-level (global) novelty signals. Across multiple image classification benchmarks and network architectures, VNS achieves state-of-the-art OOD detection performance. Remarkably, VNS retains this performance when computed using only 1% of the training data, enabling deployment in memory- or access-constrained settings.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Machine Learning (cs.LG); Computer Vision and Pattern Recognition (cs.CV)\n        \n          Cite as:\n          arXiv:2602.10062 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.10062v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.10062\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Amey Pasarkar [view email]          [v1]\n        Tue, 10 Feb 2026 18:30:29 UTC (538 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Vendi Novelty Scores for Out-of-Distribution Detection, by Amey P. Pasarkar and 1 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.CV\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n    "
  },
  {
    "title": "Evaluating Disentangled Representations for Controllable Music Generation",
    "url": "https://arxiv.org/abs/2602.10058v1",
    "source": "arxiv",
    "summary": "Recent approaches in music generation rely on disentangled representations, often labeled as structure and timbre or local and global, to enable controllable synthesis. Yet the underlying properties of these embeddings remain underexplored. In this work, we evaluate such disentangled representations in a set of music audio models for controllable generation using a probing-based framework that goe",
    "full_text": null
  },
  {
    "title": "WildCat: Near-Linear Attention in Theory and Practice",
    "url": "https://arxiv.org/abs/2602.10056v1",
    "source": "arxiv",
    "summary": "We introduce WildCat, a high-accuracy, low-cost approach to compressing the attention mechanism in neural networks. While attention is a staple of modern network architectures, it is also notoriously expensive to deploy due to resource requirements that scale quadratically with the input sequence length $n$. WildCat avoids these quadratic costs by only attending over a small weighted coreset. Cruc",
    "full_text": null
  },
  {
    "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization",
    "url": "https://arxiv.org/abs/2602.10048v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning app",
    "full_text": "\n\n\n\n1 Introduction\n2 Preliminaries\n3 Methodology\n\n4 Experiments\n\n4.1 Experiment Settings\n4.2 Main Results\n4.3 Self-Reflection Results\n4.4 Results on Eliminating the Two Limitations of GRPO\n4.5 Ablation Experiments\n\n\n5 Conclusion\n\n\n\n\n\nLong Chain-of-Thought Compression via Fine-Grained Group Policy Optimization\n\nAbstract\nLarge Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose Fine-grained Group policy Optimization (FGO), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.\n\n\nIndex Terms— \nLLM, CoT Compression, RL, GRPO\n\n\n\n1 Introduction\n\nAdvances in LLMs have highlighted the central role of CoT reasoning [1, 2], particularly in complex domains such as mathematics and code generation. The Long-CoT reasoning capabilities of LLMs, as exemplified by OpenAI-o1[3], DeepSeek-R1[4], predominantly stem from RL [5, 6, 7, 8] post-training paradigms, which guide models to produce explicit and logically structured reasoning trajectories. However, recent research [9, 10] has demonstrated that reasoning ability does not scale linearly with the length of CoT. On the contrary, excessively long CoT often leads to performance degradation due to overthinking and redundant double-checking. This makes it crucial to develop methods that can compress CoT while preserving reasoning performance.\n\n\nExisting approaches to CoT compression fall into three categories: token-level, instance-level and chunk-level compression. Token-level compression [11] reduces length by filtering unimportant tokens but often undermines logical consistency. Instance-level method (e.g.,[12, 10]) relies on an additional compressor LLM, making the performance highly dependent on the auxiliary model. Chunk-level compression [13] preserves self-reflection but incurs substantial computational overhead from repeated segmentation and search.\n\n\nTo address these limitations, we propose FGO, a RL-based approach tailored for long CoT compression. FGO extends GRPO, a simple yet efficient RL post-training method, while simultaneously addressing two inherent limitations of GRPO: inefficient data utilization and entropy collapse [14]. Specifically, FGO subdivides responses into correct and incorrect subgroups, and further refines reward assignment by incorporating length and entropy information. This reward shaping discourages unnecessary overthinking without impairing performance and self-reflection (self-reflection demonstrates the model’s ability to evaluate and revise its reasoning process). Moreover, FGO achieves full data utilization while avoiding both entropy collapse.\n\n\nWe evaluate FGO on multiple LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Results show that FGO achieves substantial CoT compression while maintaining or even improving accuracy. Self-reflection experiments further confirm that compression does not compromise reasoning, and comparison and ablation studies verify FGO’s effectiveness in addressing GRPO’s limitations.\n\n\nThe contributions can be summarized as follows:\n\n\n•\n\nWe propose FGO, an algorithm that effectively compresses long CoTs while preserving performance.\n\n\n\n•\n\nFGO addresses inefficient data utilization and entropy collapse limitations of GRPO through subgrouping and fine-grained reward assignment.\n\n\n\n•\n\nExtensive experiments across models and benchmarks demonstrate both the efficiency of CoT compression and the effectiveness of FGO.\n\n\n\n\n\n\nFig. 1: A case study with ZR1-1.5B on MATH500 dataset, comparing Vanilla, GRPO and FGO methods.\n\n\n\n\n2 Preliminaries\n\nGRPO estimates the advantage function in the group-relative method. Given a question ss and a dataset answer aa, the old policy πθo​l​d\\pi_{\\theta_{old}} samples a group of GG responses {oi}i=1G\\{o_{i}\\}_{i=1}^{G}. The advantage function of the ii-th response is defined as\n\n\n\n\nAi,t=ri−mean​({ri}i=1G)std​({ri}i=1G).\\displaystyle A_{i,t}=\\frac{r_{i}-\\text{mean}(\\{r_{i}\\}_{i=1}^{G})}{\\text{std}(\\{r_{i}\\}_{i=1}^{G})}.\n\n(1)\n\n\n\n\n\nThen, GRPO adopts a clipped objective,\n\n\n\n\n𝒥GRPO​(θ)\\displaystyle\\mathcal{J}_{\\text{GRPO}}(\\theta)\n=𝔼(s,a)∼𝒟,{oi}i=1G∼πθo​l​d(⋅∣s)[1G∑i=1G1|oi|∑t=1|oi|(\\displaystyle=\\mathbb{E}_{(s,a)\\sim\\mathcal{D},\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{old}}(\\cdot\\mid s)}\\Bigg[\\frac{1}{G}\\sum_{i=1}^{G}\\frac{1}{|o_{i}|}\\sum_{t=1}^{|o_{i}|}\\Bigg(\n\n(2)\n\n\n\n\nmin⁡(ρi,t​(θ)​Ai,t,clip​(ρi,t​(θ),1−ε,1+ε)​Ai,t)\\displaystyle\\min\\Big(\\rho_{i,t}(\\theta)A_{i,t},\\ \\text{clip}\\Big(\\rho_{i,t}(\\theta),1-\\varepsilon,1+\\varepsilon\\Big)A_{i,t}\\Big)\n\n\n\n\n\n−γDKL(πθ||πref))],\\displaystyle-\\gamma D_{\\text{KL}}(\\pi_{\\theta}||\\pi_{\\text{ref}})\\Bigg)\\Bigg],\n\n\n\n\nwhere ρi,t​(θ)=πθ​(oi,t∣s,oi,&lt;t)πθo​l​d​(oi,t∣s,oi,&lt;t).\\rho_{i,t}(\\theta)=\\frac{\\pi_{\\theta}(o_{i,t}\\mid s,o_{i,&lt;t})}{\\pi_{\\theta_{old}}(o_{i,t}\\mid s,o_{i,&lt;t})}.\n\n\nInefficient data utilization. When all responses in a group receive identical rewards, the Ai,tA_{i,t} is zero.\n\n\nEntropy collapse. During training, the response entropy decreases sharply, leading to nearly identical responses and exacerbates the first issue: inefficient data utilization.\n\n\n\n\n\nModel\nAlgo\nMath500\nAIME24\nAMC23\nMinerva\n\n\nAcc\nLength\nACT\nAcc\nLength\nACT\nAcc\nLength\nACT\nAcc\nLength\nACT\n\n\n\n\n\n\nQwen2.5-\nMath-1.5B\n\nVanilla\n40.0\n763\n5.2\n10.0\n1283\n0.8\n30.0\n978\n3.1\n9.2\n1357\n0.7\n\n\nGRPO\n65.6\n578\n11.3\n13.6\n1017\n1.3\n50.0\n825\n6.1\n11.4\n923\n1.2\n\n\nTLDR\n68.2\n686\n9.9\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nFGO\n68.6\n441\n15.6\n19.3\n946\n2.0\n52.5\n686\n7.7\n18.0\n673\n2.7\n\n\n\n\nDeepSeek-\nR1-Distill-\nQwen-1.5B\n\nVanilla\n32.4\n982\n3.3\n3.7\n1889\n0.2\n27.5\n1820\n1.5\n12.1\n1404\n0.9\n\n\nGRPO\n51.0\n828\n6.2\n7.7\n1879\n0.4\n30.0\n1646\n1.8\n20.6\n1132\n1.8\n\n\nTLDR\n54.6\n308\n17.7\n0.0\n494\n0\n19.3\n444\n4.3\n-\n-\n-\n\n\nFGO\n56.4\n229\n24.6\n8.3\n962\n0.9\n40.0\n651\n6.1\n18.0\n202\n8.9\n\n\n\n\nZR1-1.5B\n\nVanilla\n58.4\n704\n8.3\n13.0\n1840\n0.7\n50.0\n1450\n3.4\n17.3\n1235\n1.4\n\n\nGRPO\n64.0\n706\n9.1\n13.3\n1840\n0.7\n50.0\n1453\n3.4\n23.2\n1209\n1.9\n\n\nTLDR\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nFGO\n69.0\n321\n21.5\n13.7\n1151\n1.2\n55.0\n540\n10.2\n24.6\n412\n6.0\n\n\n\n\nQwen2.5-\nMath-1.5B-\nInstruct\n\nVanilla\n71.2\n542\n13.1\n12.7\n890\n1.4\n47.5\n802\n5.9\n20.2\n652\n3.1\n\n\nGRPO\n73.0\n541\n13.5\n14.0\n998\n1.4\n52.5\n746\n7.0\n22.1\n645\n3.4\n\n\nTLDR\n69.7\n603\n11.6\n7.0\n1031\n0.7\n37.3\n861\n4.3\n-\n-\n-\n\n\nFGO\n73.2\n321\n22.8\n13.3\n810\n1.6\n55.0\n494\n11.1\n23.2\n374\n6.2\n\n\n\nTable 1: Comparative results of Vanilla, GRPO and FGO, where bold indicates the best performance.\n\n\n\n\n3 Methodology\n\nGiven a question ss, LLM generates a group of answers 𝒢={a^1,a^2,…,a^G}\\mathcal{G}=\\{\\hat{a}_{1},\\hat{a}_{2},...,\\hat{a}_{G}\\} extracted from the responses. Each answer a^i\\hat{a}_{i} is compared against the ground-truth answer aa in the dataset and assigned a verified reward rir_{i}, i.e.\n\n\n\n{ri=1,i​f​a^i=a;ri=0,i​f​a^i≠a.\\begin{cases}r_{i}=1,\\;\\;\\;&amp;if\\;\\;\\hat{a}_{i}=a;\\\\\nr_{i}=0,\\;\\;\\;&amp;if\\;\\;\\hat{a}_{i}\\neq a.\\end{cases}\n\n(3)\n\n\n\n\nIn GRPO, the reward Eq. (3) is directly used for advantage estimation. In contrast, FGO regroups the a^i\\hat{a}_{i} by their verified reward rir_{i} into a correct-response subgroup 𝒢+={a^i|ri=1}\\mathcal{G}^{+}=\\{\\hat{a}_{i}|r_{i}=1\\} and an incorrect-response subgroup 𝒢−={a^i|ri=0}\\mathcal{G}^{-}=\\{\\hat{a}_{i}|r_{i}=0\\}, followed by subgroup-specific reward shaping.\n\n\nFor the correct-response subgroup 𝒢+\\mathcal{G}^{+}, we retain the reward R¯+={r1+=1,r2+=1,…,r|𝒢+|+=1}\\bar{R}^{+}=\\{r^{+}_{1}=1,r^{+}_{2}=1,...,r^{+}_{|\\mathcal{G}^{+}|}=1\\} to improve the accuracy. Meanwhile, for Long-CoT compression, shorter and more confident (i.e., lower-entropy) responses should receive greater emphasis. Here, response length L+={l1+,l2+,…,l|𝒢+|+}L^{+}=\\{l_{1}^{+},l_{2}^{+},...,l_{|\\mathcal{G}^{+}|}^{+}\\} is measured by the number of tokens, while confidence is quantified by the entropy ℋ+={ℋ1+,ℋ2+,…,ℋ|𝒢+|+}\\mathcal{H}^{+}=\\{\\mathcal{H}_{1}^{+},\\mathcal{H}_{2}^{+},...,\\mathcal{H}_{|\\mathcal{G}^{+}|}^{+}\\}, where\n\n\n\n\nℋi=∑t=1|oi|[−𝔼oi,t∼πθ(⋅|s,oi,&lt;t)​[l​o​g​πθ​(oi,t|s,oi,&lt;t)]].\\displaystyle\\mathcal{H}_{i}=\\sum_{t=1}^{|o_{i}|}\\left[-\\mathbb{E}_{o_{i,t}\\sim\\pi_{\\theta}(\\cdot|s,o_{i,&lt;t})}\\left[log\\ \\pi_{\\theta}(o_{i,t}|s,o_{i,&lt;t})\\right]\\right].\n\n(4)\n\n\n\n\n\nThen, the fine-grained weight 𝒲+\\mathcal{W}^{+} is computed as\n\n\n\n\n𝒲+=Softmax​[(mean​(L+)L+)α×(mean​(ℋ+)ℋ+)β].\\displaystyle\\mathcal{W}^{+}=\\text{Softmax}\\left[\\Big(\\frac{\\text{mean}(L^{+})}{L^{+}}\\Big)^{\\alpha}\\times\\Big(\\frac{\\text{mean}(\\mathcal{H}^{+})}{\\mathcal{H}^{+}}\\Big)^{\\beta}\\right].\n\n(5)\n\n\n\n\n\nIn Eq. (5), L+L^{+} and ℋ+\\mathcal{H}^{+} in the denominator indicate that shorter responses and lower-entropy responses are assigned larger weights 𝒲+\\mathcal{W}^{+}. To avoid scale sensitivity, both response length and entropy are normalized using mean​(L+)\\text{mean}(L^{+}) and mean​(ℋ+)\\text{mean}(\\mathcal{H}^{+}). The hyperparameter α\\alpha controls the degree of CoT length compression, specifically, larger α\\alpha values encourage shorter CoT responses. β\\beta controls the degree of exploration. In addition, a softmax operator is applied to normalize 𝒲+\\mathcal{W}^{+} within the correct-response subgroup. Finally, the fine-grained reward is defined as\n\n\n\n\nR+=𝒲+×R¯+.\\displaystyle R^{+}=\\mathcal{W}^{+}\\times\\bar{R}^{+}.\n\n(6)\n\n\n\n\n\nFor incorrect-response subgroup 𝒢−\\mathcal{G}^{-}, verified rewards are modified from 0 to −1-1, i.e., R¯−={r1−=−1,r2−=−1,…,r|𝒢−|−=−1}\\bar{R}^{-}=\\{r^{-}_{1}=-1,r^{-}_{2}=-1,...,r^{-}_{|\\mathcal{G}^{-}|}=-1\\}, penaliz"
  },
  {
    "title": "Conformal Prediction Sets for Instance Segmentation",
    "url": "https://arxiv.org/abs/2602.10045v1",
    "source": "arxiv",
    "summary": "Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image a",
    "full_text": null
  },
  {
    "title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.10044v1",
    "source": "arxiv",
    "summary": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style explor",
    "full_text": null
  },
  {
    "title": "Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection",
    "url": "https://arxiv.org/abs/2602.10042v1",
    "source": "arxiv",
    "summary": "Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose F",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Method\n\n2.1 Hybrid Fine-Tuning (HFT)\n2.2 Hybrid-Thinking VIA Group Reward Policy OPTIMIZATION\n\n\n3 Data Formulation\n\n4 Experiment\n\n4.1 Baselines\n4.2 Main Results\n4.3 Case Study\n\n\n5 Conclusion\n6 RELATION TO PRIOR WORK\n7 Acknowledgments\n\n\n\n\n\nFAKE-HR1: RETHINKING REASONING OF VISION LANGUAGE MODEL FOR\nSYNTHETIC IMAGE DETECTION\n\nAbstract\nRecent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model’s ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.\n\n\nIndex Terms— \nAIGC Detection, Hybrid-Reasoning\n\n\n\n1 Introduction\n\nWith the rapid development of diffusion models [4], AIGC technologies are increasingly integrating synthetic multimodal data into our daily lives. For instance, SORA [2] can generate highly realistic videos, while Qwen-Image [20] is capable of understanding text and manipulating images. However, synthetic multimodal data also introduces significant risks, including potential misuse [9]. Such risks include the creation of forged watermark [26] and deepfake [18, 27] through diffusion models, the synthesis of fraudulent faces for scams, and the contamination of internet training data [25]. Given the ease of generating synthetic content, the internet may in the future be inundated with AI-generated material, making the task of verifying the authenticity and reliability of multimodal data increasingly challenging.\n\n\nTo address these threats, the field of Synthetic Content Detection has recently garnered substantial attention [22, 19, 21]. Nevertheless, existing approaches are predominantly limited to binary classification, with restricted human interpretability of predictions [19]. The rapid emergence of large reasoning models (LRMs) [8, 3] has sparked interest in their ability to detect synthetic multimodal reasoning data [19].\n\n\nOn one hand, VLMs can provide natural-language justifications for authenticity judgments, thereby enhancing interpretability. On the other hand, distinguishing between authentic and synthetic data requires multimodal perception, knowledge integration, and reasoning, making this task an ideal testbed for evaluating the capabilities of large multimodal models (LMMs). This study, therefore, seeks to improve the performance of LMMs on Synthetic Content Detection tasks.\n\n\nExisting methods such as GenImage [29] and Community Forensics [15] primarily focus on binary detection, providing insufficient evaluation of LRMs’ reasoning capabilities in Synthetic Content Detection. While datasets such as FakeClue [19] and FakeBench [11] is closer to our objectives, they require models to produce reasoning chains even for samples with obvious synthetic artifacts, leading to increased inference latency and computational cost. This raises a key question: What constitutes an appropriate learning objective for LRMs in Synthetic Content Detection? We argue that a sophisticated model should not be a rigid reasoning machine: (1) for synthetic content with clear artifacts, reasoning is unnecessary, and the model can directly output the final decision (real or fake); (2) for subtle and difficult-to-detect cases—particularly those generated by state-of-the-art diffusion models [20]—models should produce detailed reasoning chains to enhance interpretability and classification accuracy.\n\n\nTo fill this gap, we introduce Fake-HR1, an LRM capable of adaptively deciding whether to generate reasoning chains based on the input image and query, thereby balancing efficiency and performance. Our contributions are follow:\n\n\n\n\n•\n\nWe propose a comprehensive hybrid-reasoning training framework that can be applied across multiple models, enabling the construction and training of hybrid reasoning models (HRMs) specifically tailored for AIGC Detection tasks.\n\n\n\n•\n\nWe present Fake-HR1, an HRM that adaptively determines whether reasoning is necessary, striking a balance between inference latency and detection accuracy.\n\n\n\n\n\n\n\n2 Method\n\n\n\n\n𝒥G​R​P​O​(θ)=\\displaystyle\\mathcal{J}_{GRPO}(\\theta)=\n𝔼(x,y)∼𝒟CoT,{oi}i=1G∼πθold​(O∣x)[1G∑i=1Gmin(πθ​(oi∣x)πθold​(oi∣x)Ai,\\displaystyle\\mathbb{E}_{\\begin{subarray}{c}(x,y)\\sim\\mathcal{D}_{\\text{CoT}},\\{o_{i}\\}_{i=1}^{G}\\sim\\pi_{\\theta_{\\text{old}}}(O\\mid x)\\end{subarray}}\\biggl[\\frac{1}{G}\\sum_{i=1}^{G}\\min\\biggl(\\frac{\\pi_{\\theta}(o_{i}\\mid x)}{\\pi_{\\theta_{\\text{old}}}(o_{i}\\mid x)}A_{i},\n\n(1)\n\n\n\n\nclip(πθ​(oi∣x)πθold​(oi∣x),1−ε,1+ε)Ai)−β𝔻K​L(πθ∥πSFT)]\\displaystyle\\operatorname{clip}\\left(\\frac{\\pi_{\\theta}(o_{i}\\mid x)}{\\pi_{\\theta_{\\text{old}}}(o_{i}\\mid x)},1-\\varepsilon,1+\\varepsilon\\right)A_{i}\\biggr)-\\beta\\mathbb{D}_{KL}(\\pi_{\\theta}\\|\\pi_{\\mathrm{SFT}})\\biggr]\n\n\n\n\n\n\n2.1 Hybrid Fine-Tuning (HFT)\n\nThe goal of HFT is to construct a model capable of mastering two distinct response modes—reasoning mode and non-reasoning mode. To this end, we propose a simple yet effective dual-mode data strategy, which systematically partitions existing datasets into reasoning and non-reasoning subsets, thereby avoiding the need for costly manual annotation.\n\n\nAs illustrated in Figure 1, our approach leverages two targeted heuristics depending on the query type:\n(i) Data-oriented heuristic (low-cost acquisition of reasoning and non-reasoning data): existing datasets that already contain reasoning and non-reasoning responses are directly divided into two categories without requiring further annotation;\n(ii) Query-oriented heuristic (objective query distinction): based on query complexity, we construct two seed question banks—reasoning-required questions and non-reasoning questions. These seed questions are used to systematically differentiate dual-mode data. Specifically, for datasets without reasoning types (e.g., GenImage [29]), we randomly sample from the simple seed bank to populate non-reasoning queries, while for datasets containing reasoning-intensive samples (e.g., FakeClue), we apply the same strategy with the complex seed bank.\n\n\nFig. 1: The training framework of Fake-HR1.\n\n\n\n\n2.2 Hybrid-Thinking VIA Group Reward Policy OPTIMIZATION\n\nHFT successfully endows F​a​k​e−H​R​1s​f​tFake-HR1_{sft} with the dual abilities of reasoning and direct response. However, when F​a​k​e−H​R​1s​f​tFake-HR1_{sft} operates under automatic reasoning settings, we observe performance degradation on certain datasets. This phenomenon, which we term “reasoning degeneration”, manifests as the model defaulting to non-reasoning responses even for complex queries requiring reasoning. Such failures in instruction-following suggest that while the model possesses the necessary skills, it lacks the judgment to deploy them appropriately.\n\n\nFortunately, Group Relative Policy Optimization (GRPO) [16] provides a natural paradigm to address this issue. By optimizing policies based on outcome-driven rewards, GRPO enables the model to learn when and how to adopt the most effective reasoning strategy. However, directly applying GRPO introduces bias, as the model may develop a preference for a single reasoning mode. Existing hybrid-reasoning methods also face critical limitations: (1) reliance on overly complex reward models and handcrafted rules, and (2) rigid reasoning modes that are excessively data- and prompt-sensitive. To overcome these challenges, we extend GRPO to explicitly incentivize hybrid-thinking capacity.\n\n\nGroup Relative Policy Optimization As shown in Figure 1. Unlike traditional actor–critic methods, GRPO eliminates the critic network and instead estimates the baseline through group averaging, substantially reducing GPU memory consumption. For each input pair (x,y)(x,y), the policy πθ\\pi_{\\theta} samples a group of GG candidate responses oii=1G{o_{i}}_{i=1}^{G}.\n\n\nwhere ε\\varepsilon and β\\beta are hyperparameters, and πSFT\\pi_{\\mathrm{SFT}}, πθ\\pi_{\\theta}, and πθold\\pi_{\\theta_{\\text{old}}} are the model after SFT, the optimized model and the old policy model.\nThe group-normalized advantage for the ii-th response is:\n\n\n\nAi=ri−mean⁡({r1,r2,⋯,rG})std⁡({r1,r2,⋯,rG})A_{i}=\\frac{r_{i}-\\operatorname{mean}\\left(\\left\\{r_{1},r_{2},\\cdots,r_{G}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{r_{1},r_{2},\\cdots,r_{G}\\right\\}\\right)}\n\n(2)\n\n\n\n\nReward Model To guide RL training, we design a rule-based reward that integrates accuracy, format, and hybrid-thinking objectives. Specifically:\n\n\n\n\n•\n\nAccuracy Reward After removing the &lt;think&gt; tags, the output is compared against the ground truth. A match yields a reward of 1; otherwise 0.\n\n\n\n•\n\nFormat Reward we assign a score of 1 if the output strictly follows the structural requirements by enclosing the reasoning within &lt;think&gt;&lt;/think&gt; tags, and 0 otherwise.\n\n\n\n•\n\nHybrid-Thinking Reward To balance inference cost and accuracy, we introduce a discriminative reward that determines whether reasoning is necessary. When the input comes from the simple seed bank, omitting reasoning (i.e., directly producing the conclusion with empty &lt;think&gt; tags) yields a reward of 1, while unnecessarily including reasoning yields 0.\n\n\n\n\n\nFinally, "
  },
  {
    "title": "Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems",
    "url": "https://arxiv.org/abs/2602.10037v1",
    "source": "arxiv",
    "summary": "In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutati",
    "full_text": null
  },
  {
    "title": "Position: Message-passing and spectral GNNs are two sides of the same coin",
    "url": "https://arxiv.org/abs/2602.10031v1",
    "source": "arxiv",
    "summary": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as diffe",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Message-passing neural networks\n2.2 Spectral GNNs\n\n\n\n3 Position statements\n\n\n3.1 P1: Spectral GNNs and MPNNs are often equivalent but not the same\n\n\n3.1.1 Spectral GNNs and MPNNs Are Derived From Separate Symmetries\n\nMPNNs From neighborhood permutations\nSpectral GNNs From eigenbasis invariance\nSymmetry-preserving universal approximation properties of GNNs\n\n\n\n3.1.2 MPNNs and spectral GNNs have similar expressivity properties\n\nPolynomial GNNs are special MPNNs\nThe role of the GSO in spectral GNNs\nMPNNs and spectral GNNs have equivalent expressivity in terms of separation power\nMPNNs and spectral GNNs have distinct but closely related universal approximation properties\n\n\n3.1.3 Feature Augmentation Affects Expressivity\n\n\n3.2 P2: MPNNs have unique benefits for discrete structure and enterprise settings\n\n3.3 P3: Spectral GNNs have unique benefits for smoothing, bottlenecks, and communities\n\nOversmoothing as spectral contraction\nOversquashing through bottlenecks\nCommunity sensitivity from low‑frequency structure\n\n\n\n3.4 P4: Nuances aside, MPNNs and Spectral provide similar perspectives. A dive into generalization, stability, and transferability\n\nStability and Size Transferability\nGeneralization\n\n\n\n3.5 P5: Spectral positional encodings (SPEs) are distinct tools that expand expressivity. GNNs can be used to generate them\n\nLimitations of SPEs\nGenerating SPEs with GNNs.\n\n\n\n\n4 Alternative views\n5 Collaboration roadmap and vision\nA Additional related work\nB Alternative views on the prudence and necessity of a unified framework\n\nC Extensive background\n\n\nC.1 Extensive notation\n\nNorms.\nMetric spaces and continuity.\nMLPs\nGraph shift operators\nGraph Fourier transform.\n\n\n\n\n\nD Derivation of MPNNs and spectral GNNs from\nequivariant machine learning\n\nD.1 Definition of MPNNs\n\nD.2 Definition of spectral GNNs\n\nAlternative constructions\n\n\n\n\n\nE Expressivity of MPNNs and spectral GNNs\n\nE.1 Polynomial spectral GNNs are MPNNs\nE.2 Any MPNN can be approximated by polynomial GNN\nE.3 MPNNs Expressivity via 1-WL\nE.4 Aggregation graph shift operators\nE.5 Expressivity of spectral GNNs with aggregation GSOs\nE.6 Expressivity via universal approximation\n\n\nF Definition of oversmoothing\nG Computing spectral positional encodings with GNNs\n\n\n\n\n\nPosition: Message-passing and spectral GNNs are two sides of the same coin\n\n\nAntonis Vasileiou\n\n  \nJuan Cervino\n\n  \nPascal Frossard\n\n  \nCharilaos I. Kanatsoulis\n\n  \nChristopher Morris\n\n  \nMichael T. Schaub\n\n  \nPierre Vandergheynst\n\n  \nZhiyang Wang\n\n  \nGuy Wolf\n\n  \nRon Levie\n\n\n\nAbstract\nGraph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.\n\nGraph Neural Networks, Spectral Methods, Message Passing\n\n\n\n1 Introduction\n\nGraphs provide a natural representation for relational data spanning atomistic systems (Zhang et al., 2023), drug design (Wong et al., 2023), and optimization problems (Cappart et al., 2023), underlining the need for principled approaches to machine learning on graphs.\nOver the past decade, graph neural networks (GNNs) have emerged as a central paradigm for learning from graph data, with strong performance across diverse domains such as optimization (Scavuzzo et al., 2024; Qian et al., 2024) and weather forecasting (Keisler, 2022).\n\n\nDespite this success, GNNs have developed along two largely separate intellectual traditions. One tradition, rooted in machine learning, is based on message-passing neural networks (MPNNs), in which node representations are updated by aggregating information from neighboring nodes via permutation-equivariant functions (Gilmer et al., 2017; Scarselli et al., 2009).\nA second tradition, originating in signal processing and applied mathematics, often denoted spectral GNNs, views node features as graph signals and defines neural networks through operators acting in the spectral domain of a graph shift operator such as the adjacency matrix or graph Laplacian  (Bronstein et al., 2017; Ortega et al., 2018; Isufi et al., 2024; Gama et al., 2020b). These two traditions rely on different mathematical languages, emphasize different inductive biases, and are typically developed and evaluated in different communities.\n\n\nIn our view, this separation has led to a fragmented understanding of what GNNs can and cannot do.\nFor example, in the MPNN community, expressivity is characterized through graph isomorphism heuristics and logical formalisms such as the Weisfeiler–Leman hierarchy (Cai et al., 1992) and fragments of first-order logic (Barceló et al., 2020; Grohe, 2021; Maron et al., 2019b; Morris et al., 2019; Xu et al., 2019).\nIn the spectral GNN community, analysis focuses on eigenvalues, eigenvectors, and frequency responses, yielding precise tools to study smoothing, stability, and transferability (Gama et al., 2019; Levie et al., 2021; Ruiz et al., 2023).\nYet, many architectures that appear different from these two viewpoints implement closely related operators on graph signals, and many theoretical limitations and guarantees have direct counterparts across the two formalisms.\n\n\nIn this position paper, we argue that spectral GNNs and MPNNs are different, but overlapping parametrizations of nonlinear operators for graph signals. When judged by the functions they approximate, their dichotomy often dissolves under the right (mild) assumptions.\nFor example, the aggregation of information across neighborhoods in MPNN layers can often be understood as the localized action of a graph shift operator. Similarly, the action of (globally acting) spectral filters, inherent in spectral GNNs, can typically be implemented or approximated by sufficiently deep MPNNs.\n\n\nAnother important point is that the two perspectives offer complementary advantages. MPNNs provide a natural language for discrete structure, graph isomorphism, and logical expressivity (Grohe, 2021; Morris et al., 2023a). Spectral GNNs provide a natural language for smoothing, bottlenecks, and community structure through the eigenstructure of graph operators (Cai and Wang, 2020; Black et al., 2023; Jamadandi et al., 2024). These are not competing descriptions but different mathematical tools on the same underlying space of graph signal operators.\n\n\nPresent work We argue that the separation between spectral GNNs and MPNNs is conceptually misleading and practically limiting. Our goal is to provide a unifying perspective that clarifies how these two traditions relate, where they differ, and how the field can move forward; see Figure 1 for a high-level overview of our five positions. Concretely, we\n\n\n1.\n\nprovide a unifying viewpoint in which both spectral GNNs and MPNNs are understood as nonlinear, permutation equivariant operators acting on graph signals, providing a common language for the two communities (see Section 3.1);\n\n\n\n2.\n\nargue that, under natural assumptions, many widely used MPNN and spectral GNN architectures are equivalent in expressive power across large parts of their design spaces. Treating them as fundamentally different obscures this shared structure (see Section 3.1);\n\n\n\n3.\n\nclaim that each perspective is better suited to different theoretical questions. MPNNs are a natural framework for discrete structure and logical expressivity (see Section 3.2). Spectral GNNs are natural for smoothing, handling bottlenecks, improving stability, and preserving community structure (see Section 3.3). Nonetheless, their generalization behavior can be assessed with similar mathematical tools (See Section 3.4);\n\n\n\n4.\n\nadvocate for a separation between spectral filtering and the use of spectral positional encodings, i.e., positional encoding based on spectral information. We argue that such encodings should be treated as a design choice for both spectral GNNs and MPNNs and can enhance both architectures (see Section 3.5).\n\n\n\n\n\nIn summary, our positions provide guidance for the graph learning community to conceptualize, analyze, and design GNNs in a unified and principled way.\n\n\nFigure 1: Overview of our five positions for unifying spectral GNNs and MPNNs in a principled way. \n\n\nRelated work\nSeveral efforts have been made to explore the relations between MPNN and spectral GNN architectures. For example, Wang and Zhang (2022) showed that spectral GNNs share the same expressivity limitations as MPNNs, while Guo et al. (2025) argued that spectral GNNs might rely far less on spectral information than commonly believed. Perlmutter et al. (2023) proposed a mathematical framework, based on geometric scattering, for understanding GNNs, with demonstrated relations to prominent MPNN and spectral GNN architectures. While these works provide important insights, they do not provide the level of harmonization we seek here.\n\n\n\n\n2 Background\n\nHere, we introduce notations and required basic concepts"
  },
  {
    "title": "Resilient Topology-Aware Coordination for Dynamic 3D UAV Networks under Node Failure",
    "url": "https://arxiv.org/abs/2602.10029v1",
    "source": "arxiv",
    "summary": "In 3D Aerial-Ground Integrated Networks (AGINs), ensuring continuous service coverage under unexpected hardware failures is critical for mission-critical applications. While Multi-Agent Reinforcement Learning (MARL) has shown promise in autonomous coordination, its resilience under sudden node failures remains a challenge due to dynamic topology deformation. This paper proposes a Topology-Aware Gr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Multi-UAV Control and Coverage Optimization\n2.2 Network Resilience and Fault Tolerance\n2.3 Attention Mechanisms and Reconfigurable MARL\n\n\n\n3 System Model\n\n\n3.1 3D Aerial-Ground Network Architecture\n\n3.1.1 Scenario Description\n3.1.2 Physical Configuration and Survivability\n3.1.3 Inter-UAV Coordination and Control Plane\n3.1.4 Wireless Backhaul (Data Plane)\n\n\n\n3.2 User Distribution and Mobility Models\n\n3.2.1 Initial Spatial Distribution\n3.2.2 Mobility Dynamics\n\n\n\n3.3 Channel Propagation and Interference Models\n\n3.3.1 Terrestrial Channel (GBS-User)\n3.3.2 Air-to-Ground Channel (UAV-User)\n3.3.3 Directional Antenna Gain\n\n\n\n3.4 Interference Model and Data Rate\n\n3.4.1 Signal-to-Interference-plus-Noise Ratio (SINR)\n3.4.2 Achievable Data Rate\n\n\n3.5 UAV Power Consumption Model\n\n\n\n4 Problem Formulation\n\n\n4.1 Network Utility Maximization\n\n4.1.1 System Capacity\n4.1.2 Global Energy Efficiency\n4.1.3 Coverage and QoS Satisfaction\n4.1.4 Fairness Metrics\n4.1.5 Service Continuity and Handoff Cost\n4.1.6 Composite Utility Function\n\n\n\n4.2 Optimization Problem Statement and Feasibility Analysis\n\nComputational Challenges and Feasibility:\n\n\n\n4.3 Reformulation as a Multi-Agent DEC-POMDP\n\n4.3.1 Global State Space 𝒮\\mathcal{S}\n4.3.2 Local Observation Space Ω\\Omega\n4.3.3 Action Space 𝒜\\mathcal{A}\n4.3.4 Cooperative Reward Function ℛ\\mathcal{R}\n\n\n\n\n\n5 Topology-Aware Graph-based Multi-Agent Reinforcement Learning (TAG-MAPPO)\n\n5.1 Overview of the TAG-MAPPO Architecture\n5.2 Dynamic Ego-Centric Graph Representation\n5.3 Feature Extraction and Embedding with Shuffling\n\n5.4 Topology-Aware Graph Attention Mechanism (TA-GAT)\n\n5.4.1 Attention-based Neighbor Aggregation\n5.4.2 Dual-Path Fusion and Skip Connection\n\n\n5.5 Multi-Agent Actor-Critic Learning\n5.6 TAG-MAPPO Training Algorithm\n5.7 Computational Complexity Analysis\n\n\n\n6 Simulation Results and Analysis\n\n6.1 Simulation Setup and Parameters\n6.2 Benchmark Scenarios and Channel Characterization\n6.3 Performance Metrics\n\n6.4 Training Convergence and QoS Optimization\n\n6.4.1 Learning Efficacy and Convergence Rate\n6.4.2 QoS Performance and Spatial Coverage\n\n\n\n6.5 Stability and Resource Efficiency Analysis\n\n6.5.1 Topological Stability and Handoff Suppression\n6.5.2 Resource Utilization and Energy Efficiency\n\n\n\n6.6 System Resilience under Catastrophic Node Failure\n\n6.6.1 Dynamic Recovery and Coverage Self-healing\n6.6.2 Structural Reorganization and Fairness Improvement\n\n\n\n\n7 Conclusion\n\n\n\n\n\nResilient Topology-Aware Coordination for Dynamic 3D UAV Networks under Node Failure\n\n\nChuan-Chi Lai\n\n0000-0003-4919-4152\nAdvanced Institute of Manufacturing with High-tech Innovations (AIM-HI)Department of Communications EngineeringNational Chung Cheng UniversityChiayiTaiwan\n\nchuanclai@ccu.edu.tw\n\n\n(9 February 2026)\n\nAbstract.\nIn 3D Aerial-Ground Integrated Networks (AGINs), ensuring continuous service coverage under unexpected hardware failures is critical for mission-critical applications. While Multi-Agent Reinforcement Learning (MARL) has shown promise in autonomous coordination, its resilience under sudden node failures remains a challenge due to dynamic topology deformation. This paper proposes a Topology-Aware Graph MAPPO (TAG-MAPPO) framework designed to enhance system survivability through autonomous 3D spatial reconfiguration. Our framework incorporates graph-based feature aggregation with a residual ego-state fusion mechanism to capture intricate inter-agent dependencies. This architecture enables the surviving swarm to rapidly adapt its topology compared to conventional Multi-Layer Perceptron (MLP) based approaches. Extensive simulations across heterogeneous environments, ranging from interference-limited Crowded Urban to sparse Rural areas, validate the proposed approach. The results demonstrate that TAG-MAPPO consistently outperforms baselines in both stability and efficiency; specifically, it reduces redundant handoffs by up to 50 percent while maintaining a lead in energy efficiency. Most notably, the framework exhibits exceptional self-healing capabilities following a catastrophic node failure. TAG-MAPPO restores over 90 percent of the pre-failure service coverage within 15 time steps, exhibiting a significantly faster V-shaped recovery trajectory than MLP baselines. Furthermore, in dense urban scenarios, the framework achieves a post-failure Jain’s Fairness Index that even surpasses its original four-UAV configuration by effectively resolving service overlaps. These findings suggest that topology-aware coordination is essential for the realization of resilient 6G aerial networks and provides a robust foundation for adaptive deployments in volatile environments.\n\nAerial-Ground Integrated Networks (AGINs), Graph Attention Network, Network Resilience, Node Failure Recovery, Handoff Optimization\n\n††copyright: acmlicensed††journal: TOIT††journalvolume: XX††journalnumber: XX††article: XX††journalyear: 2026††doi: XXXXXXX.XXXXXXX††ccs: Networks Mobile networks††ccs: Networks Wireless access points, base stations and infrastructure††ccs: Networks Network resources allocation††ccs: Computing methodologies Multi-agent reinforcement learning\n\n\n1. Introduction\n\nThe dawn of 6G networks signals a fundamental paradigm shift from terrestrial-dependent infrastructure to an integrated, three-dimensional Non-Terrestrial Network (NTN) architecture (Saad et al., 2020; Giordani and Zorzi, 2021). Within this aerial frontier, Unmanned Aerial Vehicles (UAVs) serving as Aerial Base Stations (ABSs) have emerged as a cornerstone solution due to their superior maneuverability, rapid on-demand deployment, and inherent ability to establish Line-of-Sight (LoS) connectivity (Mozaffari et al., 2019; Al-Hourani et al., 2014). These UAV-assisted networks are particularly indispensable in mission-critical scenarios where terrestrial infrastructure is compromised, such as disaster relief operations or large-scale urban events (Merwaday and Guvenc, 2015; Zhao et al., 2019). However, beyond the pervasive challenge of non-stationary user distributions, ensuring system resilience in the face of unexpected hardware malfunctions or catastrophic node failures remains a formidable obstacle for next-generation aerial orchestration.\n\n\nTo sustain optimal coverage amidst environmental volatility, Multi-Agent Proximal Policy Optimization (MAPPO) has been widely adopted as a robust control framework (Yu et al., 2022; Cui et al., 2020). Nevertheless, a persistent limitation in reinforcement learning (RL) based UAV control is the structural rigidity of standard Multi-Layer Perceptron (MLP) architectures. Building upon our established research in MARL-based UAV control, our prior work (Lai, 2026) addressed the plasticity-stability dilemma by introducing a Group-Decoupled MAPPO (G-MAPPO) algorithm. While G-MAPPO effectively mitigated gradient conflicts across localized geographic regions, its efficacy was constrained by the assumption of a quasi-static network topology. Specifically, it assumed a constant number of active UAV nodes, leaving the system vulnerable to the topological disruptions inherent in real-world deployments. This paper extends that trajectory by pivoting from gradient decoupling to topology-aware coordination, specifically targeting the resilience gaps left by static structural assumptions.\n\n\nPractical aerial networks, however, are subject to dual-layer dynamics consisting of the continuous micro-temporal displacement of ground users and the discrete, catastrophic disruptions caused by sudden UAV node failures. MLP-based agents struggle with these topological ruptures due to their fixed input dimensions and inherent lack of permutation invariance. When a neighboring node fails, MLP architectures often suffer from coordination collapse because they cannot dynamically re-weight remaining spatial features, leading to service outages and a failure to re-establish coverage equilibrium.\n\n\nTo overcome these structural limitations and achieve self-healing communication, this paper proposes a novel Topology-Aware Graph Attention (TAG) framework. Recent literature highlights that graph-based learning is uniquely suited for wireless resource management due to its inherent ability to generalize to varying numbers of nodes (Dai et al., 2025). Instead of perceiving the environment as a static feature vector, we model the multi-UAV system as a dynamic graph (Scarselli et al., 2009). By incorporating a sophisticated graph attention mechanism inspired by advancements in Graph Attention Networks (GAT) (Veličković et al., 2018; Vaswani et al., 2017), our framework empowers agents to dynamically assign importance weights to neighboring entities. This architecture, enhanced by random observation shuffling during training, enables the system to proactively adapt to both smooth user movements and abrupt topological changes such as the sudden loss of a coordinated agent.\n\n\nThe main contributions of this paper are summarized as follows:\n\n\n•\n\nTopology-Aware Coordination Architecture: We propose a novel MARL framework that utilizes graph-based feature aggregation to capture the underlying relational structures of UAV networks. This architecture enables agents to perform spatial reasoning beyond simple numerical observations, leading to more stable coordination in interference-limited regimes.\n\n\n\n•\n\nSignaling Stability and Resource Efficiency: Through the integration of a handoff-penalized reward function and energy-efficient policy learning, our framework achieves a superior balance between coverage expansion and signaling stability. Experimental data show that TAG-MAPPO reduces redundant handoffs by nearly 50% in sparse topologies while maintaining optimized propulsion and communication power consumption.\n\n\n\n•\n\nAutonomous Resilience and Self-Healing: We introduce a robust training paradigm incorporating Random Observation Shuffling (ROS), which empowers the network to adapt to dynamic agent populations. Our results demonstrate"
  },
  {
    "title": "Overview of the TREC 2025 RAGTIME Track",
    "url": "https://arxiv.org/abs/2602.10024v1",
    "source": "arxiv",
    "summary": "The principal goal of the RAG TREC Instrument for Multilingual Evaluation (RAGTIME) track at TREC is to study report generation from multilingual source documents. The track has created a document collection containing Arabic, Chinese, English, and Russian news stories. RAGTIME includes three task types: Multilingual Report Generation, English Report Generation, and Multilingual Information Retrie",
    "full_text": null
  },
  {
    "title": "MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval",
    "url": "https://arxiv.org/abs/2602.10023v1",
    "source": "arxiv",
    "summary": "Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Model Architecture\n\n3.1 Problem Formulation\n3.2 Evidence Retrieval with Two-Layer Multi-Modal Graph\n3.3 Claim Verification with Token- and Evidence-Level Fusion\n3.4 Multi-Modal Explanation Generation\n\n\n4 Dataset Creation\n\n5 Experiments\n\n5.1 Evidence Retrieval\n5.2 Claim Verification\n5.3 Explanation Generation\n5.4 Model Analysis\n5.5 Analysis of the AIChartClaim Dataset\n\n\n6 Conclusion\nA Mathematical Notations\nB Pseudo-Code of Training Process\nC Complexity Analysis\nD Additional Discussion on Model Architecture\n\nE Additional Dataset Details\n\nE.1 Details of AIChartClaim\nE.2 Details of Other Datasets\n\n\nF Experiment Environment\n\nG Additional Experiment Results\n\nG.1 Evidence Retrieval\nG.2 Claim Verification\nG.3 Explanation Generation\nG.4 Further Discussion on Three Evaluation Tasks\nG.5 Failure Case and Error Type\n\n\n\n\n\n\n\nMEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval\n\n\n\nDelvin Ce Zhang1,\nSuhan Cui2,\nZhelin Chu3,\nXianren Zhang4,\nDongwon Lee5\n\n1University of Sheffield,\n2University of Science and Technology Beijing,\n3University of California San Diego,\n4,5The Pennsylvania State University\n\n\n1delvin.ce.zhang@sheffield.ac.uk,\n2suhan@ustb.edu.cn,\n312111105@mail.sustech.edu.cn,\n4,5{xzz5508,dongwon}@psu.edu\n\n\n\nAbstract\nVerifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model. Code and datasets are available at https://github.com/cezhang01/mever.\n\n\n\nMEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval\n\n\n\n\n\nDelvin Ce Zhang1,\nSuhan Cui2,\nZhelin Chu3,\nXianren Zhang4,\nDongwon Lee5\n\n1University of Sheffield,\n2University of Science and Technology Beijing,\n\n3University of California San Diego,\n4,5The Pennsylvania State University\n\n1delvin.ce.zhang@sheffield.ac.uk,\n2suhan@ustb.edu.cn,\n\n312111105@mail.sustech.edu.cn,\n4,5{xzz5508,dongwon}@psu.edu\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Illustration of multi-modal and explainable claim verification, taken from AIChartClaim dataset.\n\n\nThe dissemination of erroneous claims and findings can mislead researchers and the public, leading to unnecessary concerns. This underscores the urgent need for developing a claim verification method to automatically assess the truthfulness of the claims.\n\n\nExisting methods primarily rely on textual evidence for claim verification. However, with the advent of multimedia, many claims are derived from various types of data, and simply reasoning over textual evidence is insufficient for accurate verification. For example, Fig. 1 illustrates a scientific claim concluded from chart images with textual caption. To verify the claim, we need to unify both textual and visual evidence for reasoning and verification. Meanwhile, to make the reasoning process transparent, an additional textual explanation is necessary to justify the ruling process and verification result. However, most existing works mainly focus on the reasoning over textual evidence only Liu et al. (2020) or ignore the explainability of the ruling process Zhang et al. (2024b), leading to inaccurate and unconvincing verification.\n\n\nChallenges and Approach. To overcome these limitations, we propose MEVER, Multi-Modal and Explainable Claim Verification with Evidence Retrieval, to address below three open questions.\n\n\nFirst, how to integrate both textual and visual evidence for multi-modal evidence retrieval? Some works, e.g., MultiVerS Wadden et al. (2022) and DECKER Zou et al. (2023), rely on external tools Robertson et al. (2009) for evidence retrieval. Other works, e.g., JustiLM Zeng and Gao (2024) and RAV Zheng et al. (2024), design an in-built text-only retriever. However, they are uni-modal and ignore visual evidence, leading to inaccurate retrieval. We design a multi-modal evidence retriever. We construct a two-layer multi-modal graph for claims and evidence. Image-to-text and text-to-image reasoning integrates multi-modal data into claim and evidence embeddings for retrieval.\n\n\nSecond, how to reason between multi-modal claims and evidence for multi-modal verification? Some models, e.g., GEAR Zhou et al. (2019) and Transformer-XH Zhao et al. (2020), integrate textual claims and evidence for verification, failing to capture visual signals. Recently, multi-modal methods are proposed, e.g., Mocheg Yao et al. (2023) and ESCNet Zhang et al. (2024b). However, these methods integrate claims with textual and visual evidence separately, failing to aggregate both textual and visual evidence for joint reasoning. As shown in Fig. 1, images and texts provide complementary information, and jointly aggregating both could reveal insightful discovery. In our model, we design token-level multi-modal fusion and evidence-level hierarchical fusion to achieve fine-grained cross-modal information exchange between claims and evidence. Eventually, we obtain unified claim and evidence embeddings for multi-modal verification.\n\n\nThird, how to leverage multi-modal data for explanation generation? Most explainable models leverage textual claims and evidence to generate explanation, e.g., JustiLM Zeng and Gao (2024) and Mocheg Yao et al. (2023). We design a multi-modal Fusion-in-Decoder module, which generates explanation by capturing both multi-modal and multiple pieces of evidence. Besides, to make the explanation consistent with the verification result, we further design a consistency regularizer.\n\n\nBesides, since most multi-modal datasets are in general domain, we create a scientific dataset in AI domain to complement research community. Our dataset, AIChartClaim in Fig. 1, contains scientific discoveries as claims, chart images with textual captions as evidence, and explanations. It is necessary to have such scientific dataset, since understanding increases and decreases in quantities in charts with scientific language is a crucial reasoning ability for language models. Though ChartCheck Akhtar et al. (2024) and ChartFC Akhtar et al. (2023) are chart datasets, their content is still in general domain and does not discuss scientific concepts. See Table 1 for comparison to selected datasets.\n\n\nContributions. First, we propose a novel model, MEVER, consisting of multi-modal evidence retrieval, claim verification, and explanation generation. Specifically, we design a two-layer multi-modal graph for evidence retrieval. Second, to fully exchange multi-modal information between claims and evidence for verification, we design token- and evidence-level fusion. Third, to make the ruling process transparent, we design multi-modal Fusion-in-Decoder and a consistency regularizer for explanation generation. Fourth, we create a scientific dataset to complement research community. Experiments verify the strength of our model.\n\n\n\n\n2 Related Work\n\nClaim verification. Previous works are text-only, GEAR Zhou et al. (2019), KGAT Liu et al. (2020), TransformerXH Zhao et al. (2020), MultiVerS Wadden et al. (2022), HESM Subramanian and Lee (2020), DREAM Zhong et al. (2020), CausalWalk Zhang et al. (2024a), ProgramFC Pan et al. (2023), CareerScape Yamashita et al. (2025), UKE Wu et al. (2024a), and AKEW Wu et al. (2024b). They ignore visual evidence, leading to inaccurate result. Multi-modal methods include Mocheg Yao et al. (2023), ESCNet Zhang et al. (2024b), CCN Abdelnabi et al. (2022), MR2Retrived Hu et al. (2023), CutPaste&amp;Find Nguyen et al. (2025a), etc. These methods, except Mocheg, focus on verification, and ignore evidence retrieval or explainability. Our model consists of evidence retrieval, claim verification, and explanation generation.\n\n\nRetrieval-augmented verification conducts evidence retrieval and claim verification jointly with Retrieval-Augmented Generation Izacard et al. (2023), such as JustiLM Zeng and Gao (2024), RAV Zheng et al. (2024), RAFTS Yue et al. (2024), ARSJoint Zhang et al. (2021), etc. They consider textual evidence only. We construct a two-layer graph for multi-modal evidence retrieval and claim verification.\n\n\nExplainable claim verification produces textual explanation to justify the verification result Atanasova (2024); Kotonya and Toni (2020a); Zhao et al. (2024); Russo et al. (2023); Yao et al. (2023). They are text-only and do not capture images. Our model designs a multi-modal Fusion-in-Decoder to incorporate multi-modal data for explainability.\n\n\nScientific and chart claim verification trains models on scientific and chart claims. SciFact Wadden et al. (2020) is a scientific text-based dataset with no images or explanation. ChartCheck Akhtar et al. (2024) and ChartFC Akhtar et al. (2023) are chart datasets in general domain. Ours is a multi-modal dataset with explanation in scientific domain. Table 1 shows comparison to selected data Thorne et al. (2018); Aly et al. (2021); Kotonya and Toni (2020b); Wu"
  },
  {
    "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
    "url": "https://arxiv.org/abs/2602.10021v1",
    "source": "arxiv",
    "summary": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of ca",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n\n2.1 Prompt Compression\n\nHard Compression (Token Selection).\nSoft Compression (Latent Representation).\n\n\n2.2 Learned Memory\n\n\n\n3 Methodology\n\n3.1 Bucketed Compression: Beyond Fixed-Ratio Compression\n3.2 DRIFT: Decoupled Reasoning with Implicit Fact Tokens\n\n3.3 Task Definition\n\n3.3.1 Latent Fact Reconstruction Pretraining (LFRP)\n\n3.3.2 Query-Aware Fine-Tuning (QAFT) with Single-Context\n\nDynamic Compression Task\nQuestion-answering Task\n\n\n\n\n3.4 Multi-Context Inference without Fine-Tuning\n3.5 Training Data\n\n\n\n4 Experiments\n\n\n4.1 Implement Details\n\nBenchmarks\nMetrics\nBaselines\nResults and Analysis\n\n\n\n\n5 Conclusion\n6 Limitations\n\nA Data Generation Details\n\nA.1 LFRP Data\n\nA.2 QAFT Data\n\nData Sampling Strategy\nQuestion Type Diversification\nData Filtering\n\n\nA.3 Training Strategy: Token-Level Curriculum Learning\nA.4 Construction Prompt\n\n\nB Dataset Statistics\n\nC Training Details\n\nC.1 Hyperparameter Settings\n\nC.2 Training Loss Curves Across Three Stages\n\nC.2.1 Stage 1: Latent Fact Reconstruction Pretraining (LFRP)\nC.2.2 Stage 2: Query-Aware Fine-Tuning (QAFT) with Single-Context Dynamic Compression\nC.2.3 Stage 3: Query-Aware Fine-Tuning (QAFT) with Single-Context Question Answering\n\n\n\n\n\nD Additional Experimental Results\n\nLogical Reasoning\nMath Reasoning\nScientific Knowledge\nInstruction Following\nCode Generation\nFactuality\n\n\n\nE Other Details\n\nE.1 Instructions for DRIFT\nE.2 Text Reconstruction Case\n\n\n\n\n\n\n\nDecoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference\n\n\n\n\nWenxuan Xie1,2,3,\nYujia Wang4,\nXin Tan1,\nChaochao Lu1,\nXia Hu1,\nXuhong Wang1,\n1Shanghai Artificial Intelligence Laboratory, Shanghai, China \n2Fudan University, Shanghai, China \n3Shanghai Innovation Institute, Shanghai, China \n4Tongji University, Shanghai, China \nwxxie25@m.fudan.edu.cn, wangxuhong@pjlab.org.cn\n  Corresponding author.\n\n\nAbstract\nThe integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model’s embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.\n\n\n\nDecoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference\n\n\n\n\n\nWenxuan Xie1,2,3,\nYujia Wang4,\nXin Tan1,\nChaochao Lu1,\nXia Hu1,\nXuhong Wang1,††thanks:   Corresponding author.\n\n1Shanghai Artificial Intelligence Laboratory, Shanghai, China\n\n2Fudan University, Shanghai, China\n\n3Shanghai Innovation Institute, Shanghai, China\n\n4Tongji University, Shanghai, China\n\nwxxie25@m.fudan.edu.cn, wangxuhong@pjlab.org.cn\n\n\n\n\n\n\n1 Introduction\n\nThe applicability of Large Language Models (LLMs) to knowledge-intensive tasks is limited by the static nature of their pre-training data. To address this limitation, prior work has explored two complementary strategies.The first focuses on augmenting the input context, while the second emphasizes knowledge parameter editing.\n\n\nTraditional input augmentation via RAG or long-context prompting is increasingly constrained by the “retriever’s ceiling” and the quadratic computational costs of processing long sequences. Neural compression methods (e.g., COCOM (Rau et al., 2024), C3 (Liu and Qiu, 2025)) attempt to mitigate this by distilling text into latent representations; however, as they primarily focus on static compression, task-critical information relevant to the query is frequently lost. Conversely, internalizing knowledge through direct parametric updates, such as fine-tuning or knowledge editing, often disrupts the inherent coupling between a model’s internal knowledge and its reasoning logic, while also risking catastrophic forgetting. While modular approaches like MLP Memory (Wei et al., 2025) offer a plug-and-play alternative, they remain bound to pre-indexed resources and struggle to handle instantaneous, unseen long-context inputs in real-time.\n\n\nTo address these challenges, we introduce DRIFT, a dual-model architecture that decouples context processing from core reasoning. In this framework, a lightweight knowledge model extracts query-relevant information from document chunks and compresses it into high-density implicit fact tokens within a latent space. These tokens serve as concise, knowledge-rich representations that are projected into a larger reasoning model’s embedding space. The reasoning model can perform sophisticated inference efficiently based on this compact factual context instead of raw text, even in long-context or knowledge-intensive scenarios. By delegating the processing of redundant background knowledge to the knowledge module, this design allows the reasoning model to remain unburdened by raw context, focusing instead on \"clean\" and deep inference through the distilled factual representations.\n\n\nOur main contributions are as follows:\n\n\n•\n\nA Decoupled Inference Paradigm for Large Language Models:\nWe propose a dual-model framework that decouples knowledge extraction from reasoning. A lightweight knowledge model encodes query-relevant information into compact fact tokens, which are consumed by a larger reasoning model for inference. Compared with directly processing long contexts, DRIFT improves performance while substantially reducing inference latency, achieving an average 7× speedup on 256k-token documents.\n\n\n\n•\n\nExpanding Effective Context Window with High-Ratio Compression:\nBy encoding extensive textual knowledge into compact fact tokens, our framework significantly extends the model’s usable context window.\nSpecifically, our DRIFT model (based on Mistral 7B) achieves a 32×\\times compression ratio while improving accuracy from 20.87% to 29.22% on the LongBench v2 benchmark, demonstrating superior reasoning capabilities with substantially reduced time overhead. Furthermore, even under more aggressive compression settings (64×\\times and 128×\\times), DRIFT remains highly competitive, indicating strong robustness to extreme compression ratios.\n\n\n\n•\n\nComprehensive Empirical Analysis and Resource Contribution:\nWe construct a large-scale Document–QA–Evidence dataset with fine-grained supervision, comprising over 300K instances and documents ranging from 1K to 8K tokens. We further conduct extensive ablation studies and quantitative evaluations, rigorously validating the framework and demonstrating its robustness.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Prompt Compression\n\nDirectly feeding new knowledge as context into Large Language Models (LLMs) is constrained by limited context windows and incurs significant overhead in both memory and computation. To mitigate these issues, various prompt compression methods have been proposed, which generally fall into two paradigms: Hard Compression and Soft Compression.\n\n\nHard Compression (Token Selection).\n\nHard compression methods, also known as token pruning or selection, aim to reduce input length by discarding tokens deemed less informative. Early approaches like Selective Context (Li et al., 2023b) utilize self-information or perplexity metrics to filter out redundancy. More advanced frameworks, such as LongLLMLingua (Jiang et al., 2024) and LLMLingua-2 (Pan et al., 2024), perform prompt compression via task-aware coarse-to-fine filtering and distillation-based token selection, respectively.\nDespite these advancements, hard compression approaches inherently limit the model’s reasoning potential. They irreversibly discard information and rely on rigid, locally made retention decisions that often fail to preserve the global semantic structure required for reliable complex reasoning.\n\n\n\nSoft Compression (Latent Representation).\n\nEarly soft compression approaches, such as AutoCompressor (Chevalier et al., 2023), Gist Tokens (Mu et al., 2024), and ICAE (Ge et al., 2024), integrate compression and reasoning within a single language model. While effective for moderate context reduction, this tightly coupled design makes it difficult for a single model to simultaneously excel at both high-fidelity compression and complex reasoning, particularly under extreme compression ratios. To address this limitation, subsequent work explores decoupling compression from reasoning. Methods such as xRAG (Cheng et al., 2024) and COCOM (Rau et al., 2024) build upon the Retrieval-Augmented Generation (RAG) paradigm by compressing retrieved documents into compact latent representations before passing them to the language model. Although effective in reducing input length, these approaches remain fundamentally constrained by the retrieval stage and inherit the upper-bound limitations of RAG systems. Beyond RAG-based compression, E2LLM (Liao et al., 2025) employs a lightweight encoder to compress long inputs into latent representations for downstream LLM reasoning, though the model weights are not publicly released, limiting reproducibility and practical adoption. Context Cascade Compression (C3) (Liu and Qiu, 2025) demonstrates strong compression fidelity but lacks task-specific adaptation for "
  },
  {
    "title": "ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.10019v1",
    "source": "arxiv",
    "summary": "Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation,",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\nCurriculum Learning.\nReinforcement Learning for Reasoning in LLMs and VLMs.\n\n\n\n3 Method\n\n3.1 Preliminaries\n\n3.2 ADORA\n\n\n3.2.1 Criteria for Sample Differentiation\n\nLength Advantage.\nDifficulty Advantage.\n\n\n3.2.2 Adaptive Advantage for Weak and Strong Reasoning Models\n\n\n\n\n\n4 Experiment\n\nSetup.\nEvaluation.\n\n4.1 VLM\n\nBaselines.\nResults.\n\n\n\n4.2 LLM\n\nBaselines.\nResults.\n\n\n\n\n\n5 Analysis\n\n\n5.1 Empirical Study\n\nTraining Comparison.\nThinking Pattern.\n\n\n\n5.2 Ablation Study\n\nHyperparameter Sensitivity.\nAdvantage Criteria.\nRL Algorithms.\n\n\n\n\n6 Conclusion\n\nA Training Details\n\nA.1 Training Hyperparameters\nA.2 Comparison of Dataset Sizes\n\n\n\nB Additional Experiments\n\nB.1 Data Scalability\nB.2 How Adora affects the learning trajectory of RL?\nB.3 PASS@K: ADORA vs. GRPO\nB.4 Thinking Pattern\nB.5 Results of Advantage Criteria Ablation\nB.6 Overthinking\n\n\nC Study Cases\nD Overthinking\n\n\n\n\n\nADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning\n\n\nQingnan Ren1  Shiting Huang1  Zhen Fang1\nZehui Chen1  Lin Chen1  Lijun Li2  Feng Zhao1\n1University of Science and Technology of China  2Shanghai AI Laboratory\n Equal Contribution  Corresponding author\n\n\nAbstract\nReinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning.\nThe optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function.\nHowever, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time.\nThis limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively.\nTo address this problem, we introduce ADORA (Advantage Dynamics via Online Rollout Adaptation), a novel framework for policy optimization.\nADORA dynamically adjusts the advantage function’s weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts.\nThis tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates.\nExtensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.\n\n\n\nADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning\n\n\n\nQingnan Ren1   Shiting Huang1   Zhen Fang††thanks: EqualContribution1{}^{1}\\lx@make@thanks{\\quad EqualContribution}\n\nZehui Chen1  Lin Chen1  Lijun Li2  Feng Zhao††thanks: Correspondingauthor1{}^{1}\\lx@make@thanks{\\quad Correspondingauthor}\n\n1University of Science and Technology of China  2Shanghai AI Laboratory\n\n\n\n\n\n1 Introduction\n\nRecent developments of reasoning models, exemplified by R1 Guo et al. (2025), have expanded the scope of large language models (LLMs) into a reinforcement learning (RL) based paradigm.\nBy introducing long chain-of-thought (CoT) reasoning, these models can achieve effective test-time scaling and generate more sophisticated reasoning patterns, including verification, reflection, and backtracking Guo et al. (2025); Xie et al. (2025).\nThis capability is further internalized within the model through RL, which enhances generalization and enables it to address complex real-world problems, such as math Liu et al. (2025), agent Feng et al. (2025), and visual reasoning Wang et al. (2025a); Huang et al. (2026); Lab et al. (2025).\nDespite these successes, slow convergence and unstable learning remain key challenges restricting the scalability of RL.\n\n\nTo enable scalable RL, it is crucial to efficiently utilize samples to achieve both fast convergence and stable learning.\nHowever, existing methods such as Proximal Policy Optimization (PPO) Schulman et al. (2017) and Group Relative Policy Optimization (GRPO) Zhang and Zuo (2025) assume that the informativeness of each training example remains constant throughout policy optimization, ignoring the dynamic nature of learning. This results in diminished learning gains from individual samples, slower convergence, and a greater demand for training iterations and data to achieve an acceptable performance level, thereby significantly limiting both training efficiency and the ultimate performance potential of reinforcement learning. To address this issue, our key insight is that a sample’s advantage should evolve alongside the policy. Specifically, as the model is trained and the policy improves, the learning signal provided by the same example changes over different training iterations. Some samples may provide significant learning opportunities at certain stages, while others may involve concepts that are either already mastered or beyond the model’s current capacity to learn effectively. Treating all samples with uniform importance or with pre-defined static weights fails to leverage this dynamic utility, potentially leading to suboptimal learning trajectories and inefficient data use, as also noted by observations that current methods lack robust mechanisms for handling samples of varying utility during training Ye et al. (2025).\nTherefore, during the dynamic training process, a simple yet effective method is required to distinguish between high- and low-value samples in real time and to weight them accordingly, thereby enabling efficient sample utilization to promote stable and fast reinforcement learning.\nMotivated by these patterns and our key insight, we propose ADORA (Advantage Dynamics via Online Rollout Adaptation), a novel and unified RL framework designed to dynamically calibrate advantage estimation for both LLMs and\nVLMs. ADORA categorizes training data into Temporarily Advantageous Samples (TAS) and Temporarily Disadvantageous Samples (TDS) based on the model’s rollout performance under a predefined data differentiation strategy. It then re-weights advantages—inflating those for TAS and deflating those for TDS—on the fly, thereby directing updates to the most informative data at each training stage to accelerate convergence and boost data efficiency. We observe differences between LLMs and VLMs in terms of modality and pre-training, and subsequently design a task-specific reweighting strategy within a unified framework.\n\n\nWe conduct extensive controlled experiments on both VLMs for geometry reasoning and LLMs for mathematical reasoning.\nOur experiments cover a wide range of architectures (Dense and MoE) and model families, including Llama-3, Mistral, DeepSeek, and InternVL.\nEmpirically, ADORA significantly improves long chain-of-thought reasoning and task generalization. For instance, on the Qwen-7B-base model, ADORA achieves an average of 3.4 percentage points improvement over vanilla GRPO on math tasks. For VLMs, using fewer than 2,000 samples and no task-specific cold-start, the Qwen2.5-VL-7B model achieves 73.5% accuracy on MathVista with ADORA.\n\n\nOur key contributions and findings include:\n\n\n•\n\nThe ADORA framework: We propose a simple, elegant, and efficient method for dynamically calibrating advantage estimation weights in RL based on live rollout statistics.\n\n\n\n•\n\nTask-specific differentiation strategies: We design and validate distinct strategies for distinguishing TAS and TDS across different reasoning domains, consistently demonstrating improvements over vanilla-GRPO.\n\n\n\n•\n\nComprehensive empirical analysis: Extensive experiments are conducted to statistically evaluate ADORA across multiple dimensions, including training dynamics and thinking patterns, thereby offering insights into its underlying mechanisms. We further provide detailed ablation studies demonstrating that ADORA is robust to hyperparameter variations, effective under different advantage criteria, and applicable to diverse RL algorithms, establishing it as a stable and generalizable framework.\n\n\n\n\n\n\n\n2 Related Works\n\nCurriculum Learning.\n\nThe core idea of Curriculum Learning (CL) Bengio et al. (2009); Elman (1993) is to present training samples in a meaningful order, typically from easy to hard, to enhance learning efficiency and generalization.\nSeveral variants have been proposed. Kumar et al. (2010)dynamically selects easier samples based on the model’s current prediction loss, thereby implementing an easy-to-hard training schedule. Matiisen et al. (2019)introduces a teacher-student framework where the teacher selects sub-tasks demonstrating the fastest learning progress for the student, guided by the student’s learning curve. More recently, Wang et al. (2025b) dynamically adjusts sampling probabilities across different data distributions to achieve an adaptive training schedule. Deng et al. (2025) proposed a three-stage reinforcement learning approach employing a progressive difficulty reward mechanism to optimize RL training. Wen et al. (2025) utilizes a two-stage curriculum-guided training.\nHowever, methods relying on pre-defined difficulty metrics or staged curricula are often costly, complex to implement, and may not be universally applicable across all models. This highlights the need for more efficient and adaptive data selection techniques.\n\n\n\nReinforcement Learning for Reasoning in LLMs and VLMs.\n\nLeveraging GRPO, DeepSeek-R1 Guo et al. (2025) demonstrated significant improvements in reasoning capabilities through rule-based reward reinforcement learning (RL), often accompanied"
  },
  {
    "title": "SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation",
    "url": "https://arxiv.org/abs/2602.10017v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering prima",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Dataset Construction\n\nUser Profile.\nProfessional Context.\nLocation and Hazard Type.\nQuestion Generation.\nAnswer Generation.\n\n\n\n3 Evaluation Framework\n\n3.1 Specificity\n\n3.2 Robustness\n\nParaphrasing:\nPerturbation:\n\n\n\n3.3 Answer Relevance\n\nAnswer Relevance with Masking.\n\n\n3.4 Context-Utilization\n\n\n\n4 Experimental Setup\n\n4.1 Human Evaluation Setup\n\n\n\n5 Results\n\n\n5.1 Evaluation with SCORE Metrics\n\n5.1.1 Specificity\n5.1.2 Robustness\n5.1.3 Answer Relevance\n5.1.4 Context-Utilization\n\n\n\n5.2 Human Evaluation\n\nInter-Annotator Agreement.\nHuman vs. Automated Agreement.\nError Analysis\n\n\n\n\n6 Conclusion\n\nA Dataset Details\n\nA.1 Profession\nA.2 Hazard and Location\n\n\n\nB Evaluation Framework\n\nB.1 Specificity\n\nB.2 Context Utilization: Proxy-Based Scoring\n\nSensitivity Analysis of Proxy Models.\n\n\nB.3 Readability\n\n\nC Human Evaluation\nD Implementation Details\n\nE Prompts\n\nE.1 Answer Generation\nE.2 Specificity\nE.3 Answer Relevance\n\n\n\n\n\n\n\n\nScore: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation\n\n\n\nHomaira Huda Shomee1,2  Rochana Chaturvedi2  Yangxinyu Xie2,3  Tanwi Mallick2\n1University of Illinois Chicago, Chicago, IL, USA \n2Argonne National Laboratory, Lemont, IL, USA \n3University of Pennsylvania, Philadelphia, PA, USA \n{hshome2@uic.edu, rchaturvedi@anl.gov, xinyux@wharton.upenn.edu, tmallick@anl.gov}\n\n\n\nAbstract\nLarge language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question–answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.\n\n\n\nScore: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation\n\n\n\n\n\nHomaira Huda Shomee1,2   Rochana Chaturvedi2   Yangxinyu Xie2,3   Tanwi Mallick2\n\n1University of Illinois Chicago, Chicago, IL, USA\n\n2Argonne National Laboratory, Lemont, IL, USA\n\n3University of Pennsylvania, Philadelphia, PA, USA\n\n{hshome2@uic.edu, rchaturvedi@anl.gov, xinyux@wharton.upenn.edu, tmallick@anl.gov}\n\n\n\n\n\n\n1 Introduction\n\nLLMs have shown remarkable capabilities across a wide range of tasks, specially in answer generation and decision support Brown et al. (2020); Ouyang et al. (2022); Wei et al. (2022). As these models are increasingly deployed in high-stakes domains such as healthcare, legal reasoning, and climate risk assessment Wu et al. (2025); Nagar et al. (2024); Li et al. (2024a); Luo et al. (2025); Li et al. (2024b); the need for robust and reliable evaluation methods has become essential. One such critical domain is natural hazard response, where LLMs are used to recommend mitigation strategies and provide factual information about disasters in specific geographic regions Xie et al. (2025). In such scenarios, even minor inaccuracies can lead to real-world consequences.\n\n\nDespite this growing reliance on LLMs, evaluating their outputs in the above mentioned settings remains challenging. Traditional reference-based evaluation metrics are often insufficient, as they primarily measure surface-level similarity to gold-standard answer rather than verifying whether generated responses contain correct and specific information Celikyilmaz et al. (2020); Kryściński et al. (2020); Chang et al. (2024). This limitation is particularly acute in natural hazard and extreme weather response, where publicly available gold-standard datasets are scarce due to the diversity of hazards, geographic regions, infrastructure systems, and professional contexts. Effective answers in this domain must be geographically precise, and tailored to specific concerns.\n\n\nRecent work has shifted toward reference-free evaluation, including LLM-as-a-judge pipelines, where LLMs themselves act as evaluators. Early frameworks like MT-Bench Bai et al. (2024) and AlpacaEval Li et al. (2023) pioneered the use of high-capacity models like GPT-4 to score multi-turn dialogues or perform pairwise comparisons, with later variants adjusting for verbosity bias. Chatbot Arena Chiang et al. (2024) leverages large-scale human preferences to rank models via Elo scores. While these benchmarks primarily focus on general-purpose conversational ability, they are not designed to capture the level of fine-grained specificity and answer relevance required in high-stakes applications. In parallel, the rise of Retrieval-Augmented Generation (RAG) has motivated evaluation frameworks that focus on factuality, answer relevance, claim-level verification, and context relevance or utilization (Es et al., 2024; Ru et al., 2024; Saad-Falcon et al., 2024; Ni et al., 2024).\n\n\nAlthough effective for general settings, existing RAG evaluation metrics fall short in domain-specific settings. Faithfulness and factuality metrics assess whether answers are broadly supported by retrieved documents, but does not verify whether critical specific details are present and correct Min et al. (2023). Answer relevance metrics mostly rely on semantic similarity or LLM-based scores, which can overestimate quality in cases where responses are generic rather than informative and relevant Liu et al. (2023). Claim-level verification focuses on factual correctness at the level of individual claims, but do not evaluate whether the answer sufficiently addresses the user’s query intent or decision context. Finally, for context utilization, prior work (Ru et al., 2024) measures the claim-level semantic overlap between retrieved knowledge and generated answer. While this captures whether the generated answer is supported by the retrieved information, it does not measure whether the model actually relied on that information during generation as LLMs may produce overlapping claims from their parametric knowledge.\n\n\nTo address these limitations, we propose Score, a multi-perspective evaluation framework for assessing LLM-generated answers in natural hazard analysis and decision support. Our framework evaluates responses along five dimensions: specificity, relevance, robustness, context utilization, and readability. For specificity, we employ multiple LLM-as-a-judge setup to verify whether key details (e.g., hazard type, location, timeline, intensity) are explicitly stated and supported by retrieved evidence, and aggregate the judges’ decisions into a final score. We assess robustness by applying paraphrasing and controlled perturbations to the question. By measuring the consistency of the model’s responses across these variations, we detect over-sensitivity in model behavior. For answer relevance, we adopt a question-regeneration strategy with an additional semantic masking step that removes domain-specific entities. Given the use of retrieval-augmented generation, we evaluate context utilization by measuring whether and how retrieved passages contribute to the generated answer. Our metric measures evidence utilization at a finer granularity through counterfactual dependence by removing individual claims and quantifying the resulting change in answer confidence, enabling us to distinguish necessary evidence from merely consistent, ignored, or distracting context. Finally, we incorporate readability metric using established grade-level metrics (Appendix B.3) to ensure that responses are appropriate for professional users.\n\n\nIn this work, we present a reference-free evaluation framework Score for assessing LLMs outputs in high-stakes hazard response. Our key contributions are as follows: (1) We construct a synthetic dataset of 1412 domain-specific question–answer pairs that cover different hazards, locations, infrastructure types, and concerns to support controlled and scalable evaluation; (2) We design a multi-dimensional, reference-free evaluation framework that assesses specificity, robustness, answer relevance, and context utilization, and validate these metrics through human evaluation.\n\n\nCode &amp; Data: Our code and curated data are available at https://anonymous.4open.science/r/score-8D5F/readme.md\n\n\n\n\n\n2 Dataset Construction\n\nWe construct the largest dataset to date of infrastructure-related questions that simulate real-world hazard scenarios and decision-making contexts. Recent work by Xie et al. (2025) demonstrates the effectiveness of agentic frameworks for addressing critical infrastructure risks by incorporating structured user profiles that encode professional background and user concerns. While their work focuses on a conversational retrieval-augmented generation (RAG) system for wildfire risk mitigation, we extend similar role-aware design principles to develop a comprehensive evaluation benchmark that systematically assesses whether LLMs generate contextually appropriate responses across a broader range of infrastructure sectors, hazard types, and geographic settings.\n\n\nUser Profile. \n\nE"
  },
  {
    "title": "Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design",
    "url": "https://arxiv.org/abs/2602.10016v1",
    "source": "arxiv",
    "summary": "Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Recommendation System Architectures\n2.2 Scaling Laws in Machine Learning\n2.3 Model-Efficiency Co-design\n\n\n\n3 Preliminaries\n\n3.1 Problem Formulation: CTR Prediction with Heterogeneous Features\n3.2 Evaluation Metrics\n3.3 Scaling Laws and Efficiency\n\n\n\n4 Method\n\n4.1 Architecture Overview\n4.2 Feature Preprocessing\n\n4.3 Low-Level Module Optimization\n\n4.3.1 Multi-Head PFFN with Generalized Dot-Product Attention (GDPA)\n4.3.2 Hierarchical Seed Pooling (HSP): Sequence Summarization\n4.3.3 Sliding Window Attention\n\n\n\n4.4 High-Level Computation Reallocation\n\n4.4.1 Computation Skip (CompSkip)\n4.4.2 Event-Level Personalization\n4.4.3 Global Interaction Module with Mixture of Wukong Experts\n\n\n4.5 Multi-Layer Architecture\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results: Architecture Comparison at Different Scales (RQ1)\n5.3 Ablation Study: Component Analysis (RQ2 &amp; RQ3)\n5.4 Scaling Law Analysis\n5.5 Production Deployment Results (RQ4)\n\n\n6 Conclusion\n\nA Evaluation Metrics Details\n\nA.1 Normalized Entropy (NE)\nA.2 Model FLOPs Utilization (MFU)\n\n\n\nB Feature Preprocessing Details\n\nB.1 Non-Sequence Feature Preprocessing\nB.2 Sequence Feature Preprocessing\n\n\n\nC Algorithm Details\n\nC.1 Overview\nC.2 Kunlun Forward Pass\nC.3 Hierarchical Seed Pooling (HSP)\nC.4 Generalized Dot-Product Attention (GDPA)\nC.5 CompSkip Configuration\nC.6 Complexity Analysis\n\n\nD Multi-Layer Architecture and Information Flow Details\nE Architecture Comparison Details\nF GDPA and Fused PFFN Kernel\n\n\n\n\n\n\n1]Meta Platforms, Inc., Menlo Park, CA, USA.\n2]OpenAI, San Francisco, CA, USA.\n\\contribution[*]Equal contribution\n\\contribution[†]Corresponding author\n\\contribution[‡]Work done while at Meta\n\nKunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design\n\n\nBojian Hou\n\n  \nXiaolong Liu\n\n  \nXiaoyi Liu\n\n  \nJiaqi Xu\n\n  \nYasmine Badr\n\n  \nMengyue Hang\n\n  \nSudhanshu Chanpuriya\n\n  \nJunqing Zhou\n\n  \nYuhang Yang\n\n  \nHan Xu\n\n  \nQiuling Suo\n\n  \nLaming Chen\n\n  \nYuxi Hu\n\n  \nJiasheng Zhang\n\n  \nHuaqing Xiong\n\n  \nYuzhen Huang\n\n  \nChao Chen\n\n  \nYue Dong\n\n  \nYi Yang\n\n  \nShuo Chang\n\n  \nXiaorui Gan\n\n  \nWenlin Chen\n\n  \nSantanu Kolay\n\n  \nDarren Liu\n\n  \nJade Nie\n\n  \nChunzhi Yang\n\n  \nJiyan Yang\n\n  \nHuayu Li\n\n[\n\n[\n\n\n\nAbstract\nDeriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs111https://www.nvidia.com/en-us/data-center/b200/ and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.\n\n\nkeywords: Recommendation Systems, Scaling Laws, CTR Prediction, Model Efficiency\n\n\n\\correspondence\n{xiaoyliu, jackiexu0313}@meta.com\n\n\n\n\n\n1 Introduction\n\nClick-through rate (CTR) prediction serves as the cornerstone of modern recommender systems, directly influencing both company revenue and user experience (guo2017deepfm; cheng2016wide; zhou2018din). As these systems grow in scale and complexity, establishing predictable scaling laws—mathematical relationships describing how model performance improves with computational resources—has become increasingly critical for guiding architectural decisions and resource allocation (kaplan2020scaling; hoffmann2022training). While scaling laws are well-established for large language models (kaplan2020scaling; brown2020language) and have been explored for non-sequential recommendation models (zhang2024wukong), they remain an open challenge for systems that jointly model both sequential user behaviors and non-sequential context features—a critical requirement for modern production recommendation systems at massive scale.\n\n\nThe success of scaling laws in language models suggests a natural approach: focus exclusively on sequential user behaviors. However, modern production recommender systems cannot adopt this simplified architecture. Non-sequential context features—including user demographics, ad metadata, and contextual signals—remain essential due to business requirements and deep infrastructure dependencies (han2025mtgr). Any scalable architecture must therefore efficiently handle both feature types simultaneously, yet existing systems that attempt this joint modeling fail to achieve predictable power-law scaling behaviors.\n\n\nWe identify poor scaling efficiency—the rate at which performance improves per unit of computational investment—as the primary barrier preventing predictable scaling laws. Specifically, scaling efficiency requires simultaneously optimizing both algorithmic effectiveness (performance gain per FLOP) and computational efficiency (hardware utilization). When examining state-of-the-art model paradigms that incorporate both sequence and non-sequence context features (zeng2024interformer), we observe limited scaling efficiency due to two critical bottlenecks:\n(1) Inefficient modules: Models achieve only 3-15% Model FLOPs Utilization (MFU), compared to 40-60% for LLMs, due to heterogeneous feature spaces resulting in small embedding dimensions, irregular tensor shapes, and memory-bound operations.\n(2) Inefficient computation resource allocation: Naively scaling all components equally leads to diminishing returns, as different layers and event types benefit from different computational patterns.\n\n\nOur Approach: Model-Efficiency Codesign. We propose Kunlun (named after the Kunlun Mountains, symbolizing a foundational peak that unifies diverse elements), a unified architecture that tackles these challenges through systematic codesign at two levels. As illustrated in Figure 1, Kunlun adopts a multi-layer architecture where each layer processes both sequence and non-sequence features through two main components: (1) a Kunlun Transformer Block for context-aware sequence modeling via GDPA-enhanced personalized feedforward networks and multi-head self-attention, and (2) a Kunlun Interaction Block for bidirectional information exchange through personalized weight generation, hierarchical sequence summarization, and global feature interaction.\n\n\nLow-level module optimization improves computational efficiency of fundamental building blocks. We introduce Generalized Dot-Product Attention (GDPA) for efficient personalized sequence transformation, Hierarchical Seed Pooling (HSP) for effective sequence summarization, and Sliding Window Attention for linear-complexity sequence modeling.\n\n\nHigh-level computation reallocation enables adaptive resource distribution. We propose Computation Skip (CompSkip) for layer-wise component selection and Event-level Personalization for importance-based resource allocation across heterogeneous event types.\n\n\nThrough this codesign approach, Kunlun improves MFU from 17% to 37% on NVIDIA B200 GPUs and achieves 2×\\times scaling efficiency improvement over state-of-the-art approaches, enabling the first predictable scaling laws for joint sequence-nonsequence modeling in recommendation systems.\n\n\nContributions. We summarize our main contributions as:\n\n\n1.\n\nProblem Formulation: We identify and formalize the scaling efficiency challenge in massive-scale recommendation systems, highlighting two key bottlenecks: inefficient modules and inefficient computation resource allocation.\n\n\n\n2.\n\nArchitecture Design: We propose Kunlun, featuring systematic model-efficiency codesign with low-level optimizations (GDPA, HSP, Sliding Window Attention) and high-level innovations (CompSkip, Event-level personalization).\n\n\n\n3.\n\nScaling Laws: We demonstrate predictable scaling behavior, achieving 2×\\times scaling efficiency improvement over state-of-the-art and establishing the first scaling laws for joint sequence-nonsequence modeling at massive scale.\n\n\n\n4.\n\nDeployment Impact: Kunlun has been deployed at major Meta Ads models, achieving 1.2% improvement in topline metrics and significant online impact.\n\n\n\n\n\nFigure 1:  Overview of the Kunlun architecture. The model is composed of multiple stacked layers, and each layer includes two main components: (1) a Kunlun Transformer block, which incorporates GDPA-enhanced PFFN and Multi-Head Self-Attention (MHA) to enable context-aware sequence modeling; and (2) a Kunlun Interaction block, which contains a Weight Generation module (to derive personalized weights for the PFFN from non-sequential features), a HSP module (to efficiently summarize sequential information for subsequent global interaction), and a Global Interaction module that facilitates interactions between sequential and non-sequential inputs, as well as interactions within the non-sequential features themselves.\n\n\n\n\n2 Related Work\n\n\n2.1 Recommendation System Architectures\n\nNon-Sequential Methods. Traditional recommendation models focus on learning interactions among non-sequential features via various mechanisms. Factorization Machines (FM) (rendle2010factorization) model pairwise feature interactions, while deep learning approaches combine FMs with neural networks (lian2018xdeepfm; wang2021dcn) for higher-order interactions. Wukong (zhang2024wukong) demonstrates scaling properties through the stacking of interaction modules; however, sequen"
  },
  {
    "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
    "url": "https://arxiv.org/abs/2602.10015v1",
    "source": "arxiv",
    "summary": "Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RG",
    "full_text": null
  }
]