[
  {
    "title": "Reward-free Alignment for Conflicting Objectives",
    "url": "https://arxiv.org/abs/2602.02495v1",
    "source": "arxiv",
    "summary": "Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectiv",
    "full_text": null
  },
  {
    "title": "MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training",
    "url": "https://arxiv.org/abs/2602.02494v1",
    "source": "arxiv",
    "summary": "Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Method\n\n2.1 Tokenizer\n\n2.2 Model Architecture\n\nInput embeddings.\nSensor embeddings.\nArchitecture.\n\n\n2.3 Masked Prediction Objective\n\n\n\n3 Experiments\n\nPre-training data.\nContextual word decoding task.\nEvaluation data.\nMetric.\n3.1 Comparison to Foundation Models\n3.2 Data Efficiency Compared to Supervised Learning\n3.3 Longer Context Improves Representations\n3.4 What Does Long-Context Pretraining Teach?\n\n\n4 Discussion\n\nA Details on Experimental Setup\n\nA.1 Preprocessing\nA.2 Pre-training Datasets\n\nA.3 Foundation Model Baselines\n\nBioCodec.\nBIOT.\nEEGPT.\nBBL.\nBrainOmni.\nLaBraM.\n\n\nA.4 Supervised Word Decoding Baseline\nA.5 Hyperparameters\nA.6 Computational Resources\n\n\nB Larger Retrieval Set Results\nC Nyquist-Compliant Resampling\nD Token-Matched Neural Context Scaling\nE Tokenizer Comparison\nF Analysing Temporal Attention Heads\nG Why Non-Invasive Decoding?\n\n\n\n\n\nMEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training\n\n\nDulhan Jayalath\n\n  \nOiwi Parker Jones\n\n\n\nAbstract\nClinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300× longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard.\n\nMachine Learning, ICML, Neural Decoding, Speech Decoding, B2T, Brain-to-Text, Brain Foundation Model\n\n\n\n\n\nFigure 1: MEG-XL introduces long-context MEG pre-training. When fine-tuned, this approach generalises to decoding words in brain-to-text with less labelled subject data than required by the supervised state-of-the-art (SOTA) and brain foundation models (FMs).\n\n\n\n1 Introduction\n\nAcross modalities in deep learning, extending context has unlocked capabilities that short contexts could not provide. For example, task performance has improved by pre-training with longer, un-fragmented documents in language models (Dai et al., 2019; Beltagy et al., 2020), and by including more task examples in context at inference time (Brown et al., 2020). In audio modelling, dilated convolutions have extended the receptive field, capturing long-range structure that local windows miss (van den Oord et al., 2016). In video generation, models have struggled with temporal coherence when context is too short to maintain consistency across frames (Yan et al., 2023). The general pattern is that if a signal carries structure at timescale TT, then models limited to a context shorter than TT cannot exploit it. We postulate that neural recordings during speech perception are no different. If the brain encodes speech-relevant information across extended timescales, especially if this content mirrors linguistic structure, then models with access to extended neural windows should recover information unavailable to short-context approaches.\n\n\nNotwithstanding these predictions, prior brain-to-text decoders typically operate on brief samples of brain data in the range of milliseconds to seconds (Moses et al., 2021; Défossez et al., 2022; Tang et al., 2023; Willett et al., 2023; Card et al., 2024).\nThis falls far short of the scale of linguistic context reflected by modern LLMs. While some neural decoding systems independently incorporate longer linguistic context via external language models (Moses et al., 2021; Willett et al., 2023), they nonetheless ignore the long-range neural context present in the brain data itself.\nShort context windows are chosen to broadly match the timescale of the unit being decoded (a phoneme, syllable, or word). This framing implicitly assumes that the relevant neural information is local: that decoding the current moment requires only the current signal. But brain recordings carry structure beyond the immediate stimulus. Subject-specific neural patterns, noise characteristics, scanner properties, and other factors can all shape the signal. In speech, phonemes group into syllables, syllables into words, and words into phrases, sentences, and discourses. Just as words in a sentence are interdependent, their corresponding neural representations are likely correlated. However, short contexts are too brief to capture and leverage these long-range dependencies.\n\n\nExtending neural context has already shown promise in non-invasive decoding, where sensors sit outside the skull. While non-invasive approaches enable safe and scalable data collection, they have lower fidelity than surgical implants—a gap that longer context windows may help close.\nd’Ascoli et al. (2025) decoded words by modelling MEG signals at the sentence level, providing neural responses to all words in a perceived sentence to a transformer at once. Their work demonstrated that additional neural context could improve word decoding accuracy by 50% over isolated word classification. Despite this promising advance, reaching a reasonable accuracy still requires tens of hours of subject training data, with their analyses showing hours-per-subject matters more than total hours. Future clinical applications of speech brain-computer interfaces (BCIs) will need to work for patients with minimal labelled data because paralysed patients, who are the intended users, may not be able to easily provide training recordings (Willett et al., 2023; Card et al., 2024).\n\n\nPre-trained models should improve data efficiency by learning transferable representations across subjects, reducing the need for subject-specific training data. Yet existing methods (a) seem to only perform well in data-rich settings (Anonymous, 2025) and (b) rarely exploit long neural context because they are designed for tasks that seemingly require only short context. While Jayalath et al. (2025) represents the only non-invasive pre-trained model tested on speech decoding, this model uses sample windows of just 0.5s as it was intended for simple speech sub-tasks where the stimulus—or the relevant neural response—does not span much longer than half of a second. Outside of speech, typical evaluation datasets include TUAB and TUEB (Obeid &amp; Picone, 2016) where samples are often split to only 5 or 10 seconds long. As a result, foundation models like LaBraM (Jiang et al., 2024), BioCodec (Avramidis et al., 2025), and EEGPT (Wang et al., 2024) are all pre-trained with short samples of at most 10 seconds.\nLeveraging long contexts is also limited by computational resources. BrainOmni (Xiao et al., 2025) and CBraMod (Wang et al., 2025) are the only non-specialised models to be trained with relatively long input contexts (up to 30s) avoiding the computational barrier of quadratic attention by factorising attention in time and sensors.\n\n\nOvercoming these limitations presents a formidable challenge. Thus, to improve the data efficiency of brain-to-text while maintaining the advantages of extended neural context, we introduce MEG-XL, a long-context pre-training framework and model. The name pays homage to Transformer-XL (Dai et al., 2019), one of the first long-context transformer language models and where ‘XL’ stands for extra long. Accordingly, our method learns to model 2.5-minute long MEG samples, aiming to elicit both the advantages of improved decoding accuracy with long neural context, as shown by d’Ascoli et al. (2025), and the data efficiency afforded by long-context priors acquired through pre-training. These samples are 5-300×\\times longer than prior pre-trained models, and correspond to 191k tokens111Number of embeddings input to MEG-XL per sample, calculated via channels × timesteps × tokenizer compression ratio.. This is similar to contexts in contemporary language models, e.g. GPT-4 Turbo (OpenAI, 2023), though the notion of a ‘token’ differs across domains.\nUsing masked token prediction, MEG-XL learns from hundreds of subjects and hundreds of hours of recordings across rest, motor, speech, and other tasks. Like Xiao et al. (2025), it also overcomes the computational barrier of long-context modelling with memory-efficient criss-cross attention (Wang et al., 2025).\n\n\nFine-tuned on contextual word decoding across three MEG speech datasets, the model generalises to new subjects with limited data better than the state-of-the-art supervised approach (d’Ascoli et al., 2025).\nTherefore, pre-training successfully compensates for limited subject-specific data; with extensive ‘deep’ single-subject recordings, supervised models can eventually exploit rich in-domain patterns. Compared to brain foundation models, MEG-XL outperforms all alternatives in the ‘shallow’ low-data regime, where models have access to limited subject data, while remaining competitive when given deep subject recordings.\n\n\nPerformance may scale with the context length used for pre-training. With linear probing, we find that models pre-trained with longer neural contexts exhibit better representations for brain-to-text decoding. We also find evidence that long neural contexts may be beneficial beyond brain-to-text. Zero-shot prediction of masked brain signals from unseen datasets improves with models trained on longer neural context. Analysing attention patterns, we find this benefit stems from learning selective, hierarchical attention, with local processing in early layers expanding to global integration in later layers.\n\n\n\nOur main contributions are as follows:\n\n\n•\n\nLong-Co"
  },
  {
    "title": "PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss",
    "url": "https://arxiv.org/abs/2602.02493v1",
    "source": "arxiv",
    "summary": "Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a si",
    "full_text": null
  },
  {
    "title": "RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System",
    "url": "https://arxiv.org/abs/2602.02488v1",
    "source": "arxiv",
    "summary": "We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized vi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 RLAnything\n\n2.1 Integration Feedback for Policy\n2.2 Consistency Feedback for Reward Model\n2.3 Adaptation of Environment Benefits Both Policy and Reward Models\n2.4 Critic Feedback for Environment Tasks\n\n\n\n3 Experiments\n\n\n3.1 Experiment Settings\n\n3.1.1 Models and Optimizations\n3.1.2 Training and Evaluation Datasets\n3.1.3 Reward Modeling\n3.1.4 Environment Task Adaptation\n\n\n\n3.2 Results and Insights\n\n3.2.1 RLAnything Facilitates Policy Training\n3.2.2 RLAnything Produces a Stronger Reward Model\n3.2.3 Adaptation of Environments Enables Active Learning from Experience\n3.2.4 State-of-the-Art Performance of the Optimized Multimodal GUI Agent\n3.2.5 Advantages of Integrating Step-wise and Outcome Rewards for Policy Training\n3.2.6 Optimized Reward Model Supervision Outperforms Human-Labeled Outcome Supervision\n3.2.7 Also Works for Single-Turn Coding Tasks\n3.2.8 Trade-off Between Outcome and Self-consistency Supervision in Optimization\n3.2.9 Dynamics of Accepted New Tasks\n3.2.10 Application on Agentic Coding\n3.2.11 Response Length on AlfWorld\n\n\n\n\n\n4 Related Works\n\n4.1 Reinforcement Learning of Large Language Models\n4.2 Reward Modeling and Environments\n\n\n5 Conclusion\n\nA Proof of Theorems\n\nA.1 Proof of Theorem 1\n\nA.2 Proof of Theorem 2 and Remarks\n\nRemark 1. Different Choices for λ\\lambda\nRemark 2. Reward Design for Coding Tasks.\n\n\n\n\n\nB Additional Experimental Results\n\nB.1 Ablation Studies on Using Different Models for Reward Model Evaluation\nB.2 Examples of Environment Adaptation\n\n\n\nC Experimental Details\n\nC.1 Models and Settings\nC.2 Evaluation for Reward Models\nC.3 Agentic Coding Applications\nC.4 Policy Prompt Templates\nC.5 Process Reward Model Prompt Templates\nC.6 Error Pattern Summarization and Prompt Templates\nC.7 Environment Modification and Prompt Templates\nC.8 Examples of Task Templates in GUI data\nC.9 Task Specific Algorithm\n\n\n\n\n\n\n\n\n\\uselogo\\correspondingauthor\nyangling0818@163.com\n\nRLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System\n\n\nYinjie Wang\n\n\n\n\nTianbao Xie\n\n\n\n\nKe Shen\n\n\n\n\nMengdi Wang\n\n\n\n\nLing Yang\n\n\n\n\n\nAbstract\n Code: https://github.com/Gen-Verse/Open-AgentRL   Models: Policy &amp; Reward\n\nWe propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and\nreward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1%9.1\\% on OSWorld and Qwen2.5-7B-Instruct by 18.7%18.7\\% and 11.9%11.9\\% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels.\n\n\n\n\n\n1. Jointly optimizing the reward model and the environment, in turn, benefits the policy’s learning curve, yielding higher converged accuracy.\n\n\n\n\n2. Step-wise signals from optimized reward model outperform human-labeled outcome signals. Moreover, integrated feedback is vital for long-trajectory tasks.\n\n\n\n\n\n3. We demonstrate effectiveness of RLAnything across diverse real-world applications, including computer control, coding, and text-based games.\n\n\n\n\n4. New environment tasks scale linearly, and the reward model gets stronger at evaluating both current-step correctness and outcome influence.\n\n\n\nFigure 1: Summarized experimental results and key insights from our RLAnything framework. \n\n\n\n\n1 Introduction\n\nReinforcement learning with verifiable rewards (RLVR) is an effective approach for improving the reasoning capabilities of large language models [o1, deepseekmath, guo2025deepseek]. However, as real-world applications extend beyond single-turn question answering, especially when policies interact with environments iteratively over long trajectories, binary outcome rewards alone provide insufficient supervision [xiong2024watch, lightman2023let, xi2025agentprm]. Step-wise signals are typically provided by generative reward models, which often outperform scalar-based models by leveraging the reasoning capabilities of language models [zhang2024generative, liu2025inference]. However, training these models usually requires collecting high-quality, task-specific supervision [xi2025agentprm, zhang2025generative], motivating the need for more automated methods and scalable supervision.\n\n\nFigure 2: Motivation and takeaways of our RLAnything framework. First, in complex real-world applications, reinforcement learning benefits from integrating step-wise rewards with outcome rewards. Second, the reward model can be jointly optimized with the policy via outcome supervision and self-consistency signals. Third, we show that adapting environment task difficulty to the policy’s capability not only facilitates policy learning but also improves reward model training within our framework. Environment tasks leverage critic feedback from both the policy and the reward model to drive automatic, targeted adaptation, further enabling active learning from experience.\n\n\nBeyond reward design, the quality of the environment is also vital for scaling reinforcement learning. Aligning task difficulty with a model’s current capabilities is known to improve training dynamics [yu2025dapo, yang2025qwen3]. In RLVR, it has been shown that adapting task difficulty during optimization can improve policy training [zeng2025rlve]. In real-world environments, such as computers for GUI agents [xie2024osworld, wang2025ui] or the physical world for robots [kober2013reinforcement], the scope of exploration is largely defined by the task. Moreover, scaling the environment by increasing task diversity can further promote policy generalization in broader scenarios [cobbe2020leveraging, team2021open, fang2025towards, cai2025autoforge, song2026envscaler, chen2025scaling].\n\n\nIf there exists an RL system that jointly optimizes the environment, policy, and reward model to amplify learning signals and strengthen the overall system?\n\n\nIn this work, we propose RLAnything, a dynamic RL framework that forges the environment, policy, and reward model in a closed-loop system, where each component continuously receives feedback from the others to amplify learning signals across various complex LLM or agentic scenarios.\nFirst, the policy is trained with integrated feedback that combines verifiable outcome rewards with step-wise signals provided by the reward model. Second, the reward model is jointly optimized via consistency feedback based on outcome and self-consistency, producing reliable step-wise supervision that in turn improves policy learning. Finally, motivated by our theoretical results, we show that balancing task difficulty benefits not only policy training but also reward model training in our RL system. Accordingly, we adapt environment tasks using critic feedback from both the policy and reward model, enabling precise and automatic task adjustment. In particular, we feed the reward model’s summarized information, which captures the policy’s failures, into a language model to perturb the task, providing concrete guidance on how to modify it.\nTo demonstrate the generality of our framework, we conduct empirical studies in three representative scenarios on computer use setting [xie2024osworld], text-based interactive games [cote2018textworld, shridhar2020alfworld], and coding LLMs. We summarize our main contributions as follows:\n\n\n\n\n•\n\nWe propose RLAnything, a fully dynamic RL system that forges the environment, policy, and reward model through closed-loop optimization to amplify learning signals and strengthen the overall system, guided by our theoretical insights.\n\n\n\n•\n\nAcross computer-use agents, text-based LLM agents, and coding LLMs, we show that each added dynamic component consistently benefits the overall system and improves out-of-distribution performance.\n\n\n\n•\n\nWe achieve significant gains in practical applications: Qwen3-VL-8B-Thinking improves by 9.1%9.1\\% on OSWorld, and Qwen2.5-7B-Instruct improves by 18.7%18.7\\% and 11.9%11.9\\% on AlfWorld and LiveBench, respectively.\n\n\n\n•\n\nWe show broad applicability: optimized reward-model signals outperform outcomes that rely on human labels, enabling active learning from experience and potential environment scaling.\n\n\n\n\n\n\n\n2 RLAnything\n\nFigure 3: Examples of environment task adaptation based on critic feedback across computer use agent, text-game agent, and coding LLM in our experiments. The critic feedback is summarized from the reward model’s evaluations and is used to automatically adapt tasks.\n\n\nOur framework (see Algorithm 1) tightly couples the policy model, reward model, and environment to achieve joint optimization. Specifically, we train the policy using integrated feedback (Equation 1 in Section 2.1), which combines step-wise signals from reward model with the trajectory-level outcome signal. We train the reward model by treating the policy’s trajectories as environment tasks and assigning consistency feedback via Equation 2; we will prove that this objective also improves the reward model’s accuracy in predicting final outcomes in Section 2.3. As the reward model becomes more accurate, it in turn provides a stronger and more informative learning signal for the policy. Motivated by the theoretical insights in Section 2.3, we show that adapting the difficulty of environment tasks benefits not only policy training but also reward model training. To achieve automatic and targeted adaptation, concret"
  },
  {
    "title": "RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents",
    "url": "https://arxiv.org/abs/2602.02486v1",
    "source": "arxiv",
    "summary": "LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by gene",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Deep Research Agents\n2.2 Agentic Context Management\n2.3 Test-Time Scaling\n\n\n3 Motivation\n\n4 Method: Re-TRAC Framework\n\n4.1 Trajectory Compression as a Structured State Representation\n4.2 Recursive Execution with Structured State Representation\n4.3 Application to Frontier Models\n4.4 Training Small Models for Re-TRAC\n\n\n\n5 Experiments\n\n5.1 Main Results\n5.2 Re-TRAC As a Test-Time Scaling Method\n5.3 Ablations\n\n\n6 Conclusion\nA Analysis Details of Incomplete Branch Exploration\nB Training Details\n\nC Evaluation Details\n\n\nC.1 Tools\n\nC.1.1 Search Tool\nC.1.2 Visit Tool\n\n\nC.2 Verifier\n\nC.3 Re-TRAC Details\n\n\nC.3.1 Structured State Representation\n\nBase Version (for Smaller and SFT Models)\nFull Version with Audit Part (for Frontier LLMs)\n\n\nC.3.2 Continuation Prompt\nC.3.3 Round Settings\nC.3.4 Model-Specific Hyper-parameters\n\n\n\nC.4 Evaluation Results\n\nC.4.1 Per-Model Detailed Results\n\n\n\n\n\n\n\n\n\nRE-TRAC: REcursive TRAjectory Compression for Deep Search Agents\n\n\nJialiang Zhu*\n\n  \nGongrui Zhang*\n\n  \nXiaolong Ma*\n\n  \nLin Xu*\n\n  \nMiaosen Zhang\n\n  \nRuiqi Yang\n\n  \nSong Wang\n\n  \nKai Qiu*\n\n  \nZhirong Wu*\n\n  \nQi Dai\n\n  \nRuichun Ma\n\n  \nBei Liu\n\n  \nYifan Yang\n\n  \nChong Luo\n\n  \nZhengyuan Yang\n\n  \nLinjie Li\n\n  \nLijuan Wang\n\n  \nWeizhu Chen\n\n  \nXin Geng\n\n  \nBaining Guo\n\n\n\nAbstract\nLLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15–20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales.\nNotably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search. Code and models are available at GitHub link.\n\nLLM Agents; Deep Research; Test-time Scaling; Context Management; Long-horizon Reasoning\n\n\nFigure 1: Comparison of RE-TRAC with state-of-the-art agentic models. Our 4B and 30B models surpass the performance of significantly larger, state-of-the-art models.\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have progressed from single-turn question answering to chain-of-thought reasoning (Wei et al., 2022), function calling (Schick et al., 2023), and complex multi-turn agentic applications (Anthropic, 2025). This evolution reflects a shift from passive response generation toward autonomous, goal-directed problem solving in open environments. A deep research agent (OpenAI, 2025a; Google, 2025), capable of autonomously searching the open web and gathering and analyzing information from thousands of web pages, represents the next frontier of information retrieval for general intelligence.\n\n\nMost existing deep research agents are built upon the ReAct paradigm (Yao et al., 2022), which interleaves large language model (LLM) reasoning steps with tool invocation, appending both into the model context in a linear sequential manner.\nIn this work, we provide an in-depth analysis of the inherent limitations of ReAct-style linear reasoning workflows.\nAlthough LLM reasoning can be trained to support behaviors such as backtracking and self-reflection (Guo et al., 2025), this strictly linear agentic workflow is not well suited for open-ended tasks that require broad exploratory investigation. Revisiting earlier reasoning states and branching into alternative search trajectories remains challenging, particularly under long-context settings (e.g., 128K–256K tokens), where context management and credit assignment become increasingly difficult. Consequently, the ReAct framework is susceptible to issues such as local optima, redundant exploration, and inefficient search dynamics (Yao et al., 2023).\n\n\nTo empower LLM-based agents with diverse exploration capabilities, we propose to explicitly guide agents toward search trajectories that have not been previously explored. This direction is motivated by two key observations. First, existing deep research models (even after extensive reinforcement learning post-training) exhibit substantially higher pass@k performance than pass@1. This gap indicates that repeated inference induces diverse reasoning trajectories, suggesting that model limitations often stem from insufficient exploration within a single trajectory rather than inadequate reasoning capacity. Second, prior work shows that LLMs are generally better at verifying candidate solutions than generating them from scratch (Weng et al., 2023; Singhi et al., 2025), motivating a search paradigm that emphasizes broad candidate generation followed by verification-driven selection.\n\n\nWe propose Re-TRAC, an agentic framework that recursively constructs structured state representation at the end of each trajectory and uses them as the prompting context for subsequent trajectories. Each state representation summarizes the evolving state of investigation along multiple dimensions, including accumulated evidence, unresolved uncertainties, identified failure modes, and a forward-looking research plan.\nUnlike multiple independent trajectories that operate in isolation, Re-TRAC enables iterative reflection, cross-trajectory knowledge consolidation, and globally informed planning. This design transforms exploration from a set of disconnected attempts into a progressively informed search process. Empirically, we observe that Re-TRAC agents issue fewer tool calls and consume fewer tokens with each successive round of research, indicating improved decision-making efficiency and more targeted information acquisition guided by prior experience.\n\n\nOur experiments demonstrate that Re-TRAC achieves absolute gains of 15–20% over ReAct on the BrowseComp benchmark when applied with frontier LLMs. This inspires us to push the limits of abilities of small models via Re-TRAC.\nTo unlock the benefits of Re-TRAC for smaller models, we develop a post-training recipe that constructs supervised fine-tuning (SFT) data consisting of trajectories explicitly conditioned on structured state representations. This training procedure teaches the model to ground its reasoning, planning, and tool use on structured cross-trajectory summaries rather than relying solely on immediate context. After fine-tuning, our 30B model achieves 53% accuracy on BrowseComp, while the 4B model reaches 30%, establishing state-of-the-art performance among models of comparable scale (see Figure 1).\n\n\n\n\n2 Related Work\n\n\n2.1 Deep Research Agents\n\nThe emergence of Deep Research Agents marks a transition from simple information retrieval to autonomous systems capable of long-horizon reasoning, strategic planning, and persistent tool utilization (OpenAI, 2025a; Google, 2025; xAI, 2025; Liu et al., 2025a; Perplexity, 2025; MiniMax, 2025; Zhang et al., 2025; Team et al., 2025b; Team, 2025a).\nAgents powered by proprietary models, such as OpenAI Deep Research (OpenAI, 2025a), Gemini Deep Research (Google, 2025), Claude (Antropic, 2025), Perplexity (Perplexity, 2025), and Grok (xAI, 2025), leverage large-scale training and deep tool integration to achieve high accuracy.\nIn parallel, open-source models, including DeepSeek (Liu et al., 2025a), GLM (Team, 2025a), Kimi (Team et al., 2025a), MiniMax (MiniMax, 2025), and Tongyi Deep Research (Team et al., 2025b), have strengthened their long-horizon capabilities through specialized training on extensive agentic tasks.\nAdditionally, works like InfoAgent (Zhang et al., 2025), WebSailor (Li et al., 2025b), and DeepDive (Lu et al., 2025) have explored foundational challenges such as data synthesis and search-oriented environment construction.\nOur work introduces a recursive experience compression mechanism to enhance the agent’s ability to handle long-horizon tasks.\n\n\n\n\n2.2 Agentic Context Management\n\nThe ability to manage context effectively is critical for agents performing long-horizon tasks.\nRecent research generally falls into two categories: intrinsic context optimization (Liu et al., 2025a; Team, 2025a) and external memory mechanisms for state maintenance (Chen et al., 2025a; Yu et al., 2025; Wu et al., 2025).\nFor the first category, many agentic LLMs, such as DeepSeek-V3.2 (Liu et al., 2025a), and GLM-4.7 (Team, 2025a), integrate context pruning directly into the agent’s reasoning loop, focusing on compressing observation spaces and pruning redundant trajectory history.\nParallel to context pruning, recent works have focused on leveraging external memory.\nIterResearch (Chen et al., 2025a) and MemAgent (Yu et al., 2025) utilize dynamic memory structures to reconstruct task status at each step, discarding generic history to simulate infinite horizons.\nReSum (Wu et al., 2025) introduces a “summarize-and-reset” paradigm, periodically condensing exploration history into compact memory.\nWhile our work naturally extends the effective context length to infinity, our primary objective is to critique its own trajectory, engage self-reflection and reinforce correct reasoning paths.\n\n\n\n\n2.3 Test-Time Scaling\n\nWhile traditional scaling laws have focused on increasing model parameters and training data, recent paradigms have shifted towards test-time compute scaling (Wei et al., 2022; OpenAI, 2025b; Liu et al., 2025a; Du et al., 2023; Wang et al., 2022).\nThe dominant approach to test-time scaling involves expandin"
  },
  {
    "title": "Expanding the Capabilities of Reinforcement Learning via Text Feedback",
    "url": "https://arxiv.org/abs/2602.02482v1",
    "source": "arxiv",
    "summary": "The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02482v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2602.02482v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:Expanding the Capabilities of Reinforcement Learning via Text Feedback\n    Authors:Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette            View a PDF of the paper titled Expanding the Capabilities of Reinforcement Learning via Text Feedback, by Yuda Song and 7 other authors\n    View PDF\n\n\n\n    \n            Abstract:The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.\n    \n\n    \n    \n              \n          Comments:\n          43 pages, 6 figures\n        \n\n          Subjects:\n          \n            Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2602.02482 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02482v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02482\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yuda Song [view email]          [v1]\n        Mon, 2 Feb 2026 18:56:56 UTC (549 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Expanding the Capabilities of Reinforcement Learning via Text Feedback, by Yuda Song and 7 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to t"
  },
  {
    "title": "Flow Policy Gradients for Robot Control",
    "url": "https://arxiv.org/abs/2602.02481v1",
    "source": "arxiv",
    "summary": "Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training a",
    "full_text": null
  },
  {
    "title": "Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability",
    "url": "https://arxiv.org/abs/2602.02477v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to f",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02477v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2602.02477v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability\n    Authors:Xiao Liang, Zhong-Zhi Li, Zhenghao Lin, Eric Hancheng Jiang, Hengyuan Zhang, Yelong Shen, Kai-Wei Chang, Ying Nian Wu, Yeyun Gong, Weizhu Chen            View a PDF of the paper titled Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability, by Xiao Liang and 9 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model&#39;s capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs&#39; reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2602.02477 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02477v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02477\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Xiao Liang [view email]          [v1]\n        Mon, 2 Feb 2026 18:54:54 UTC (711 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability, by Xiao Liang and 9 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n    "
  },
  {
    "title": "AgentRx: Diagnosing AI Agent Failures from Execution Trajectories",
    "url": "https://arxiv.org/abs/2602.02475v1",
    "source": "arxiv",
    "summary": "AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with ",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02475v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.02475v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:AgentRx: Diagnosing AI Agent Failures from Execution Trajectories\n    Authors:Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath, Chetan Bansal            View a PDF of the paper titled AgentRx: Diagnosing AI Agent Failures from Execution Trajectories, by Shraddha Barke and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.02475 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02475v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02475\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Shraddha Barke [view email]          [v1]\n        Mon, 2 Feb 2026 18:54:07 UTC (353 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled AgentRx: Diagnosing AI Agent Failures from Execution Trajectories, by Shraddha Barke and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Contact\n                \n                \n                  subscribe to arXiv mailingsClick here to subscribe\n                   Subscrib"
  },
  {
    "title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
    "url": "https://arxiv.org/abs/2602.02474v1",
    "source": "arxiv",
    "summary": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 LLM Agent Memory Systems\n2.2 Self-Evolving LLM Agents\n\n\n\n3 Method\n\n3.1 Overview\n3.2 Skill Bank\n\n3.3 Learning to Use Memory Skills\n\n3.3.1 Controller: Skill Selection Policy\n3.3.2 Executor: Skill-Conditioned Memory Extraction\n3.3.3 Controller Optimization\n\n\n3.4 Skill Evolution through Designer Feedback\n3.5 Closed-Loop Optimization\n\n\n\n4 Experiments\n\n4.1 Experiment Setup\n4.2 Comparison Experiments\n4.3 Ablation Study\n4.4 Skill Generalization Under Distribution Shift\n4.5 Case Study\n\n\n5 Conclusion\n\nA More Implementation Details\n\nA.1 Evaluation Details\n\nA.2 More Details of the Designer\n\nHard-case buffer and representative case mining.\nExploration incentive for newly introduced skills.\nEarly stopping and rollback based on stabilized rewards.\n\n\n\nA.3 Details on ALFWorld Training\n\nTask-type grouping.\nExperience corpus vs. evaluation cases.\nMotivation.\n\n\n\nA.4 Details on Training Objectives\n\nEpisode, states, and Top-KK actions.\nJoint probability of Top-KK without-replacement selection.\nRewards from memory-dependent evaluation.\nPPO objective with Top-KK actions.\nGumbel-Top-KK exploration.\n\n\n\n\n\nB Case Study\n\nB.1 Initial Primitive Skills\nB.2 Evolved Skills on LoCoMo\nB.3 Evolved Skills on ALFWorld\n\n\nC Prompts\n\n\n\n\n\nMemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents\n\n\nHaozhen Zhang\n\n  \nQuanyu Long\n\n  \nJianzhu Bao\n\n  \nTao Feng\n\n  \nWeizhi Zhang\n\n  \nHaodong Yue\n\n  \nWenya Wang\n\n\n\nAbstract\nMost Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories.\nTo this end, we present MemSkill, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces.\nInspired by the design philosophy of agent skills, MemSkill employs a controller that learns to select a small set of relevant skills, paired with an LLM-based executor that produces skill-guided memories.\nBeyond learning skill selection, MemSkill introduces a designer that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills.\nTogether, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself.\nExperiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings.\nFurther analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.\nCode is available at https://github.com/ViktorAxelsen/MemSkill\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nAs Large Language Model (LLM) agents engage in longer, open-ended interactions, they must handle growing histories that are essential yet challenging to leverage, motivating memory for retaining experience and maintaining coherence (Hu et al., 2025).\nThis need has driven rapid progress in agent memory, including approaches that summarize and retrieve past interactions or manage external memory stores (Kang et al., 2025; Chhikara et al., 2025; Packer et al., 2023; Xu et al., 2025).\nHowever, most methods still rely on static, hand-designed memory mechanisms, including fixed operation primitives (e.g., add/update/delete/skip) (Wang et al., 2025a; Yan et al., 2025) and heuristic modules that govern what to store, how to revise it (Kang et al., 2025; Fang et al., 2025), and when to prune it. Such designs bake in strong human assumptions and often suffer under diverse interaction patterns, scaling poorly as histories grow.\n\n\nWe argue that this formulation fundamentally limits the adaptability of agent memory.\nRather than treating memory as the output of fixed operations or hand-designed modules, we propose to elevate memory extraction itself into a learnable abstraction.\nConcretely, we view memory construction as the outcome of applying a small set of generic, reusable memory skills: structured behaviors that specify when and how interaction traces should be transformed into memory and revised over time.\nThis perspective reveals a key bottleneck of prior pipelines: they hard-code memory behaviors into fixed procedural workflows that interleave heuristics with LLM-mediated extraction and revision, making them brittle under distribution shift (Fang et al., 2025).\n\n\nUnder this view, an ideal agent memory system should satisfy three properties.\n(i) Minimal reliance on human priors.\nInstead of manually encoding what is worth remembering for a domain (Zhong et al., 2024), memory behaviors should be shaped by interaction data and updated as task demands evolve.\n(ii) Support for larger extraction granularity.\nMany approaches are tuned to a fixed unit, such as per-turn processing (Fang et al., 2025), and can weaken when applied to longer spans.\nA practical system should be able to operate at larger extraction granularity when needed.\n(iii) Skill-conditioned, compositional memory construction.\nExisting systems often decompose memory construction into specialized modules (Kang et al., 2025).\nIn contrast, we prefer to select and compose a small set of relevant skills for the current context and apply them in one generation step, enabling flexible reuse and evolution of memory behaviors.\n\n\nFigure 1: Comparison between (a) prior turn-level, handcrafted operations and (b) MemSkill’s span-level, skill-conditioned generation. Prior methods interleave handcrafted operations with LLM calls to incrementally extract and revise memory turn by turn, while MemSkill selects a small set of skills from a shared skill bank and applies them in one pass to produce skill-guided memories.\n\n\nBased on the above observations, we introduce MemSkill, which reframes memory operations as a learnable and evolvable set of memory skills.\nMemSkill maintains a shared skill bank, where each skill captures a reusable way to extract, consolidate, or revise memories from interaction text (Figure 1 shows the structured template of a memory skill).\nGiven the current context, a controller learns to select a small set of relevant skills, and an LLM-based executor conditions on these skills to generate skill-guided memories in one pass.\nThis skill-conditioned formulation is not tied to a fixed extraction unit and can be applied to different span lengths when processing long interaction histories.\n\n\nCrucially, MemSkill goes beyond learning how to use a fixed set of skills. We introduce a closed-loop evolution process that alternates between learning to use the current skill bank and evolving the skill bank itself.\nSpecifically, we train the controller with reinforcement learning (RL) using downstream task signals as feedback for skill selection. Periodically, a designer aggregates the hardest cases produced during training, selects representative failures, and uses an LLM to refine existing skills and propose new ones.\nAfter each evolution step, the controller continues training on the evolved skill bank, with additional exploration to facilitate adopting newly introduced skills.\nOverall, this process gradually strengthens both the skill selection policy and the evolving skill bank, moving toward a more adaptive memory management system driven by interaction data.\n\n\nExperiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld show that MemSkill consistently improves task performance and generalizes well. Further analyses validate key components and showcase representative evolved skills, offering insights toward more adaptive, self-evolving memory management for LLM agents.\n\n\nOur contributions can be summarized as follows.\n\n\n•\n\nWe propose MemSkill, an agent memory method that represents memory operations as an evolving skill bank, and constructs skill-guided memories by conditioning an LLM on a selected set of skills.\n\n\n\n•\n\nWe introduce a closed-loop optimization recipe that combines reinforcement learning for skill selection with LLM-guided skill evolution from hard cases, enabling continual refinement of the skill bank and taking a step toward self-evolving agent memory systems.\n\n\n\n•\n\nWe evaluate MemSkill on LoCoMo, LongMemEval, HotpotQA, and ALFWorld, showing consistent gains over baselines and strong generalization, offering insights toward self-evolving memory for LLM agents.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 LLM Agent Memory Systems\n\nPrior work on agent memory focuses on constructing external memories from interaction histories and leveraging them to support downstream reasoning and decision making.\nTypical pipelines periodically extract salient information into a memory store, retrieve relevant entries for a new query, and update the store via consolidation or pruning (Kang et al., 2025; Zhong et al., 2024; Xu et al., 2025; Packer et al., 2023; Chhikara et al., 2025; Fang et al., 2025). More recently, learning-based approaches such as Memory-R1 (Yan et al., 2025) and Mem-α\\alpha (Wang et al., 2025a) optimize memory management with reinforcement learning using downstream task signals.\nDespite this progress, memory management is still largely governed by static, hand-crafted routines for extraction, consolidation, and pruning.\n\n\nSeveral concurrent works also explore self-evolving memory in agent settings, but differ fundamentally from our focus. Evo-Memory provides a streaming benchmark and evaluation framework for test-time memory evolution (Wei et al., 2025), while MemEvolve meta-optimizes memory architectures within a predefined modular design space (Zhang et al., 2025).\nBy contrast, we target the evolution of memory skills themselves, enabling the system to refine and grow its reusable memory operations over time.\n\n\n\n\n2.2 Self-Evolving LLM Agents\n\nRecent work on self-evol"
  },
  {
    "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
    "url": "https://arxiv.org/abs/2602.02473v1",
    "source": "arxiv",
    "summary": "Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into general",
    "full_text": null
  },
  {
    "title": "SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning",
    "url": "https://arxiv.org/abs/2602.02472v1",
    "source": "arxiv",
    "summary": "Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains",
    "full_text": null
  },
  {
    "title": "Multi-head automated segmentation by incorporating detection head into the contextual layer neural network",
    "url": "https://arxiv.org/abs/2602.02471v1",
    "source": "arxiv",
    "summary": "Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level",
    "full_text": "\n\n\n\n1 Introduction\n2 Materials and Methods\n3 Results and Discussion\n4 Conclusion\n\n\n\n\n\nMulti-head automated segmentation by incorporating detection head into the contextual layer neural network \n\n\n\nEdwin Kys \nResearcher \nLinnear \nAustin, Texas \nedwin@linnearai.com\n\n  \nFebian Febian\nResearcher \nUCL / Linnear \nLondon, United Kingdom \nfebian.febian.19@ucl.ac.uk\n\n\n\nAbstract\nDeep-learning–based auto-segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or “hallucinations,” in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of 0.013 ± 0.036 versus 0.732 ± 0.314, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.\n\n\nKeywords Contouring  ⋅\\cdotAuto-contouring  ⋅\\cdotAuto-segmentation  ⋅\\cdotCT scan  ⋅\\cdotImaging Modality  ⋅\\cdotContextual layer  ⋅\\cdotNeural network  ⋅\\cdotDeep Learning  ⋅\\cdotMachine Learning  ⋅\\cdotSwin U-Net  ⋅\\cdotTransformer\n\n\n\n1 Introduction\n\nComputed tomography (CT) imaging plays a central role in modern clinical workflows, particularly in radiotherapy treatment planning, where accurate delineation of target volumes and organs-at-risk (OARs) is essential for optimal dose delivery and minimization of normal tissue complications. Manual contouring or segmentation of anatomical structures on CT is widely regarded as the clinical standard; however, it remains a time-consuming, labor-intensive process that is subject to both inter- and intra-observer variability [4]. Deep learning–based auto-contouring seeks to address these limitations by providing rapid, reproducible segmentation of structures directly from CT volumes.\n\n\nAuto-segmentation methods in medical imaging have evolved substantially over the past decade. Traditional approaches, such as atlas-based registration and deformable models, laid early groundwork for algorithmic contouring but were often challenged by the considerable anatomical variability, imaging artifacts, and limited soft-tissue contrast inherent in CT imaging [12]. The advent of convolutional neural networks (CNNs) and related deep learning architectures ushered in significant performance gains, with models now routinely achieving expert-level segmentation accuracy in many clinical contexts. A particularly influential architecture in this domain is the U-Net, which couples an encoder–decoder structure with skip connections to capture both global context and fine spatial details for segmentation tasks [8]. A study by Ng et al.(2022) illustrates significant performance gains of deep learning-based model compared to the atlas-based one. In a retrospective comparison involving 45 head and neck radiotherapy patients, the deep learning–based segmentation method consistently outperformed the atlas-based approach by achieving median Dice similarity coefficients above 0.80 for key organs such as the brainstem, mandible, eyes, and spinal cord, while also exhibiting less variability and requiring less time for contouring than the atlas-based method [6].\n\n\nDeep learning–based auto-segmentation has been applied across a range of CT-based radiotherapy scenarios, demonstrating substantial reductions in contouring time and variability when compared with manual delineation. For example, reviews of such methods have shown that deep learning auto-contours can achieve accuracy comparable to manual expert contours across multiple sites and structures while drastically reducing user time [5]. Another study by Zhu et al. (2018) suggests a 3D squeeze-and-excitation U-Net architecture trained on 261 head and neck CT volumes, and demonstrated that it improved average Dice similarity coefficients by approximately 3.3% compared with the previous MICCAI 2015 head-and-neck segmentation benchmark while processing an entire organ-at-risk set in roughly 0.12 seconds in a single forward pass [18]. These methods also show promise in reducing inter-observer variability, one of the key bottlenecks in clinical contouring workflows, and in maintaining consistency when applied to independent datasets [15, 3].\n\n\nDespite these advances, several challenges remain. CT images are inherently limited in soft-tissue contrast, which can confound boundary definition for certain organs or targets [14]. Additionally, generalization across imaging protocols, institutions, and patient populations remains a barrier to broad clinical adoption [9]. Many deep learning approaches also require large volumes of high-quality annotated data, which are costly to curate and may not be uniformly available across all anatomical sites or treatment settings [13]. Robust uncertainty estimation and clinically acceptable performance thresholds are further areas of active investigation.\n\n\nIn this work, we propose a model that incorporates contextual layer neural network integrated with detection head for auto-contouring on CT images. The proposed approach is designed to reduce hallucination, hence, improving the volumetric dice performance thereby improving contour quality and reducing the amount of manual correction required by clinicians. The work is trained and evaluated on a public dataset provided by TCIA namely Prostate-Anatomical-Edge-Cases collection and compare against established baselines using standard quantitative metrics and qualitative expert review.\n\n\n\n\n2 Materials and Methods\n\nMedical image segmentation has seen substantial advances with the adoption of deep learning, particularly CNN and, more recently, Transformer-based architectures. Models such as U-Net and its numerous variants have become the de facto standard due to their strong performance in pixel-wise prediction tasks. However, a persistent challenge in volumetric and slice-based medical imaging is the phenomenon commonly referred to as hallucination, where models predict anatomically implausible structures in image slices where those structures do not exist. This issue is especially pronounced in organs with limited spatial extent along a given axis, such as lungs near the apex or base, small tumours, or vessels that appear intermittently across slices [17].\nHallucination arises primarily because traditional segmentation models are optimized solely for pixel-level accuracy, without an explicit mechanism to reason about structure presence or absence at the slice-level. As a result, models may extrapolate learned spatial patterns beyond their valid anatomical context, leading to false positive segmentations that are clinically misleading [7].\n\n\nThe emerging transformer-based models, including Swin U-Net variants, have been introduced to address limitations of CNNs in capturing long-range dependencies. Hierarchical Transformers leverage self-attention mechanisms to model global context while maintaining computational efficiency through windowed attention [16]. In medical imaging, these architectures have demonstrated improved boundary delineation and robustness to noise.\nDespite these improvements, Transformers alone do not inherently solve the hallucination problem. While temporal or inter-slice context can reduce inconsistency as presented in a study by An (2025) [1], excessive reliance on contextual cues may actually exacerbate hallucinations by encouraging the model to propagate structures across slices where they are anatomically absent. This limitation motivates architectures that explicitly separate structure existence reasoning from pixel-wise segmentation.\n\n\nMulti-task learning has been widely explored in medical imaging as a means to improve generalization by sharing representations across related tasks. Recent work has explored architectures that combine detection and segmentation sequentially. Detection–segmentation pipelines often use detection outputs to define regions of interest for segmentation, thereby reducing false positives [2]. However, such pipelines are typically staged and do not benefit from fully shared representations.\n\n\nFigure 1: Overview of N2 model workflow consisting of patch embedding, encoder layers with context integration, skip connections to the decoder, and temporal context fusion with the addition of detection head in its pipeline.\n\n\nOur proposed model, N2 architecture as shown in Figure 1, builds on this paradigm by introducing dual processing streams in parallel: a context-free detection stream and a context-enhanced segmentation stream. The segmentation stream is based on Swin U-Net and enhanced with temporal context integration. Through a cross-attention mechanism, N2 incorporates previous segmentation masks into the encoder, which strengthens its ability to handle inter-frame variations and promotes consistent segmentations over time. This design choice highlights segmentation benefits from temporal or contextual fusion whereas detection should rely primarily o"
  },
  {
    "title": "Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge",
    "url": "https://arxiv.org/abs/2602.02470v1",
    "source": "arxiv",
    "summary": "Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the \"reversal curse\" -- when trained on forward knowledge data of the form \"$A \\rightarrow B$\" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge \"$B \\leftarrow A$\" (e.g., Bob's wife is Alice) during t",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\nReversal reasoning task.\nDataset.\nOne-layer transformer.\nLoss function.\n\n\n\n3 Theoretical Results\n\n3.1 Implicit Bias Explains the Reversal Curse\n3.2 Breaking the Reversal Curse via Identity Bridge\n3.3 Relation to Out-of-Context Reasoning\n\n\n\n4 Experiments\n\n4.1 One-Layer Transformer Experiments\n\n4.2 Real Large Language Model Experiments\n\n4.2.1 Ablation Experiments on Dataset\n4.2.2 Ablation Experiments on Entity Token Length\n\n\n\n\n\n5 Related Work\n\nReversal curse.\nTraining dynamics of transformers.\nImplicit bias.\n\n\n6 Conclusions\n\nA Omitted Proof\n\nA.1 Proof of Theorem˜3.3\nA.2 Proof of Theorem˜3.4\n\n\nB Experiment Details\n\n\n\n\n\nBreaking the Reversal Curse in Autoregressive Language Models via Identity Bridge\n\n\n\nXutao Ma  Yixiao Huang11footnotemark: 1  Hanlin Zhu  Somayeh Sojoudi\n\n\nUC Berkeley\n\n\n{xutao_ma,yixiaoh,hanlinzhu,sojoudi}@berkeley.edu\nEqual contributions.\n\n\nAbstract\nAutoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the “reversal curse” — when trained on forward knowledge data of the form “A→BA\\rightarrow B” (e.g., Alice’s husband is Bob), the model is unable to deduce the reversal knowledge “B←AB\\leftarrow A” (e.g., Bob’s wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules.\nIn this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form “A→AA\\to A” (e.g., The name of Alice is Alice).\nTheoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.\n\n\n\n1 Introduction\n\nAutoregressive large language models (LLMs) have demonstrated great capability in solving various complex tasks (Jaech et al., 2024; Guo et al., 2025a). However, they still struggle with some simple logical reasoning tasks, and one of the most well-known failure modes is the “reversal curse”, which refers to the phenomenon that when LLMs have learned a forward relation “A→BA\\to B” (e.g., Alice’s husband is Bob.) during training, they fail to answer the reverse question “B←AB\\leftarrow A” (e.g., Who is Bob’s wife?) during test.\n\n\nExtensive prior works have attempted to understand or resolve the reversal curse.\nRecent research (Zhu et al., 2024; Lin et al., 2024; Wang and Sun, 2025) suggests that this forward–reverse asymmetry is a fundamental generalization failure of autoregressive LLMs.\nConsequently, existing mitigation strategies generally follow two paths: (i) augmenting or reformatting training examples to explicitly show the reverse links, and (ii) adjusting the learning objective or training procedure to reduce directional bias. However, such interventions often result in substantial deviation from standard data pipelines or training recipes and can introduce trade-offs in overall model quality.\nMoreover, they commonly adopt the premise that, for causal autoregressive models, reliably answering the reversed queries ultimately requires that the specific direction be included in training.\n\n\nIn this paper, we challenge this view by showing that the reversal curse can be resolved by adding certain regularization to the training data, without being trained on reversal data. Specifically, we add the regularization by augmenting the training data with the “Identity Bridge”, which was originally proposed by Lin et al. (2025) to solve two-hop reasoning tasks. The identity bridge refers to statements in the form of “A→AA\\to A” (e.g., The name of Alice is Alice, or The wife of Alice’s husband is Alice). Semantically, it adds no new information to the dataset about the relations between entities, but it serves as a dataset-level regularization and can change the optimization landscape. In theory, we prove through the implicit bias of gradient descent that by adding the identity bridge regularization, even one-layer transformers can break the reversal curve on symbolic reasoning tasks. To further understand the mechanism of the identity bridge, we prove that the identity bridge regularized reversal task can be equivalently formulated as an out-of-context reasoning (OCR) problem (Cohen et al., 2024). Moreover, to validate our method in real-world scenarios, we conduct experiments with pretrained LLMs on real-world reversal tasks, and the trained model can achieve a pass rate as high as 40%, which is a significant improvement over the previous near-zero accuracy.\n\n\nIn summary, the main contributions of this paper include:\n\n\n•\n\nWe propose to use the identity bridge regularized data recipe to break the reversal curse in autoregressive LLMs. To the best of our knowledge, this is the first work that breaks the reversal curse in autoregressive LLMs without modifying the training paradigm (e.g., the model architecture or loss function) or reversing the training data.\n\n\n\n•\n\nTheoretically, we prove through the implicit bias of gradient descent that even a one-layer transformer is able to break the reversal curse via identity bridge regularization, while without it, the reversal curse happens. We also prove that the identity bridge regularization is closely related to the OCR phenomenon.\n\n\n\n•\n\nWe conduct experiments on pretrained LLMs that achieve a 40% pass rate on real-world reversal tasks via the identity bridge regularization, a significant improvement over the previous near-zero accuracy.\n\n\n\n\n\n\n\n2 Preliminaries\n\nBasic notations. Let [N]={1,⋯,N}[N]=\\{1,\\cdots,N\\}. We denote 𝒆i\\boldsymbol{e}_{i} as one-hot vectors where only the ii-th entry is non-zero and equals one, and denote 𝒛x\\boldsymbol{z}_{x} as the embedding for token xx. Let 𝒱\\mathcal{V} be the vocabulary set. We use 𝟎m×n\\boldsymbol{0}_{m\\times n} to denote the m×nm\\times n all-zero matrix and use 𝟎d\\boldsymbol{0}_{d} to denote the dd-dimensional zero vector. 𝒜\\mathcal{A} and ℬ\\mathcal{B} denote sets of entities and ℛ\\mathcal{R} denotes the set of relations.\n\n\nReversal reasoning task.\n\nGiven two disjoint sets of NN entities 𝒜:={a1,⋯,aN}\\mathcal{A}:=\\{a_{1},\\cdots,a_{N}\\} and ℬ:={b1,⋯,bN}\\mathcal{B}:=\\{b_{1},\\cdots,b_{N}\\}, a forward relation r+r_{+} is a bijection mapping aia_{i} to bib_{i} for all i∈[N]i\\in[N], and the reverse relation r−r_{-} is defined as the inverse of r+r_{+}. The reversal reasoning task is to answer r−​(bi),i∈[N]r_{-}(b_{i}),i\\in[N] when a model is only trained with the forward relations r+​(ai)=bi,∀i∈[N]r_{+}(a_{i})=b_{i},\\ \\forall i\\in[N].\n\n\n\nDataset.\n\nTo adjust to the language model input format, we represent each relational instance r​(s)=s′r(s)=s^{\\prime} as a token sequence [s,r|s′][s,r|s^{\\prime}], where s,s′s,s^{\\prime} are entities and rr is a relation. We treat [s,r][s,r] as the input sequence and s′s^{\\prime} as the target label. E.g., if s=s= Alice, s′=s^{\\prime}= Bob and r=r= “husband”, the sequence encodes “Alice’s husband is Bob”. In this paper, we consider the following sets:\n\n\n•\n\nForward relation set: 𝒟r+={[ai,r+|bi]:i∈[N]}\\mathcal{D}_{r_{+}}=\\{[a_{i},r_{+}|b_{i}]:i\\in[N]\\};\n\n\n\n•\n\nReversal relation set: 𝒟r−={[bi,r−|ai]:i∈[N]}\\mathcal{D}_{r_{-}}=\\{[b_{i},r_{-}|a_{i}]:i\\in[N]\\};\n\n\n\n•\n\nIdentity bridge set: 𝒟idn={[ai,rid|ai]:i∈[N]}∪{[bi,rid|bi]:i∈[N]}\\mathcal{D}_{\\text{idn}}=\\{[a_{i},r_{\\text{id}}|a_{i}]:i\\in[N]\\}\\cup\\{[b_{i},r_{\\text{id}}|b_{i}]:i\\in[N]\\};\n\n\n\n•\n\nRelation set: ℛ={r+,r−,rid}\\mathcal{R}=\\{r_{+},r_{-},r_{\\text{id}}\\}.\n\n\n\n\n\nThe identity bridge dataset 𝒟idn\\mathcal{D}_{\\text{idn}}, originally proposed by Lin et al. (2025) for two-hop reasoning tasks, contains an entity and an identity relation ridr_{\\text{id}} as input, and maps them to the entity itself. The identity bridge dataset actually contains no information, but will serve as a regularization to help break the reversal curse.\n\n\n\nOne-layer transformer.\n\nWe consider a one-layer decoder-only transformer TF, which takes a sequence x1:T:=(x1,…,xT)∈𝒱Tx_{1:T}:=(x_{1},\\ldots,x_{T})\\in\\mathcal{V}^{\\mathrm{T}} as input and outputs a logit vector TF𝜽​(x1:T)∈ℝd\\text{TF}_{\\boldsymbol{\\theta}}(x_{1:T})\\in\\mathbb{R}^{d} as follows:\n\n\n\nTF𝜽​(x1:T):=𝑾O​𝑾VT​𝑿​softmax​(𝑿T​𝑾KQ​𝒙T),\\text{TF}_{\\boldsymbol{\\theta}}(x_{1:T}):=\\boldsymbol{W}_{\\text{O}}\\boldsymbol{W}_{\\text{V}}^{\\mathrm{T}}\\boldsymbol{X}\\text{softmax}(\\boldsymbol{X}^{\\mathrm{T}}\\boldsymbol{W}_{\\text{KQ}}\\boldsymbol{x}_{T}),\n\n(1)\n\n\nwhere 𝑿=[𝒛x1,⋯,𝒛xT]∈ℝd×T\\boldsymbol{X}=[\\boldsymbol{z}_{x_{1}},\\cdots,\\boldsymbol{z}_{x_{T}}]\\in\\mathbb{R}^{d\\times T} is the embedding matrix of the tokenized sequence, 𝑾O,𝑾V∈ℝd×dh\\boldsymbol{W}_{\\text{O}},\\boldsymbol{W}_{\\text{V}}\\in\\mathbb{R}^{d\\times d_{h}} are the output and value matrices, respectively, 𝑾KQ=𝑾K​𝑾QT∈ℝd×d\\boldsymbol{W}_{\\text{KQ}}=\\boldsymbol{W}_{\\text{K}}\\boldsymbol{W}_{\\text{Q}}^{\\mathrm{T}}\\in\\mathbb{R}^{d\\times d} is the reparameterized key-query matrix, and 𝜽=(𝑾O,𝑾V,𝑾KQ)\\boldsymbol{\\theta}=(\\boldsymbol{W}_{\\text{O}},\\boldsymbol{W}_{\\text{V}},\\boldsymbol{W}_{\\text{KQ}}) encodes all model parameters.\n\n\nWe further define the logit of token yy as\n\n\n\nTF𝜽​(x1:T;y):=𝒛yT​TF𝜽​(x1:T).\\text{TF}_{\\boldsymbol{\\theta}}(x_{1:T};y):=\\boldsymbol{z}_{y}^{\\mathrm{T}}\\text{TF}_{\\boldsymbol{\\theta}}(x_{1:T}).\n\n(2)\n\n\n\n\nThe next token probability p​(v|x1:T)p(v|x_{1:T}) is computed as the softmax of the logit vector:\n\n\n\np𝜽​(v|x1:T)=exp⁡(TF𝜽"
  },
  {
    "title": "Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation",
    "url": "https://arxiv.org/abs/2602.02469v1",
    "source": "arxiv",
    "summary": "We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \\emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum",
    "full_text": null
  },
  {
    "title": "Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts",
    "url": "https://arxiv.org/abs/2602.02468v1",
    "source": "arxiv",
    "summary": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model stru",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02468v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.02468v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts\n    Authors:Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang            View a PDF of the paper titled Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts, by Aiden Yiliu Li and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2602.02468 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02468v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02468\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Aiden Yiliu Li [view email]          [v1]\n        Mon, 2 Feb 2026 18:50:07 UTC (5,210 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts, by Aiden Yiliu Li and 3 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.CL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              "
  },
  {
    "title": "Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models",
    "url": "https://arxiv.org/abs/2602.02467v1",
    "source": "arxiv",
    "summary": "Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a gener",
    "full_text": null
  },
  {
    "title": "MentisOculi: Revealing the Limits of Reasoning with Mental Imagery",
    "url": "https://arxiv.org/abs/2602.02465v1",
    "source": "arxiv",
    "summary": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual represen",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Designing MentisOculi\n\nForm Board\nHinge Folding\nPaper Fold\nRush Hour\nSliding Puzzle\n\n\n\n3 Evaluation\n\n3.1 Model families\n\n3.2 Automated scoring\n\nText outputs\nVisual outputs\n\n\n3.3 Human reference data\n3.4 Chance Performance\n\n\n\n4 Results\n\n4.1 SotA multimodal model performance across tasks\n\n4.2 Comparing model families on Rush Hour\n\nMLLMs vs. latent reasoning\nMLLMs vs. UMMs\nVideo models\nHuman-machine gap\n\n\n\n4.3 What is holding MLLMs and UMMs back?\n\nSymbolic vs. sensory reasoning\nReasoning with oracle visual chain-of-thought\n\n\n\n4.4 Limits of common reasoning enhancements\n\nIn-context learning\nPrompt optimization\nReasoning budget\nTool use\n\n\n\n4.5 Comparing humans and machines\n\nMapping performance to response time\nHuman vs. machine adapative reasoning effort\n\n\n\n\n\n5 Discussion &amp; conclusion\n\nIs explicit visual thought a dead end?\nThe fragility of visual thought\nThe high price of visualization\nConclusion\n\n\n\nA Automatic Puzzle Generation\n\nForm Board\nHinge Folding\nPaper Fold\nRush Hour\nSliding Puzzle\n\n\nB Psychophysics Experiment\nC Reasoning Budget Correlation on More Models\n\nD Results Across All Difficulties &amp; Tasks\n\nD.1 Performance Across All Difficulty Levels\nD.2 Reasoning Budget Comparison on All Tasks\n\n\n\nE Generated Images &amp; Videos\n\n\nE.1 Unified Multimodal Models\n\nQualitative Results\nQuantitive Results\n\n\n\nE.2 Qualitative Evaluation of Video Models\n\nModel Selection\nTask-Specific Observations\n\n\n\n\nF Models and Inference details\n\nG More Examples from MentisOculi\n\nG.1 Example Text Description\nG.2 Example Visual CoT\n\n\n\nH Prompts &amp; Instructions\n\nH.1 MLLM Standard Prompts\nH.2 Interleaved Image and Text Generation\nH.3 Video Models\nH.4 Human Instructions\nH.5 Ground Truth Visual Chain of Thought\nH.6 Tool Use\nH.7 In-Context Learning\nH.8 Optimized Prompt\n\n\nI Datasheet for MentisOculi\n\n\n\n\n\n\nMentisOculi: Revealing the Limits of Reasoning with Mental Imagery\n\n\nJana Zeller\n\n  \nThaddäus Wiedemer\n\n  \nFanfei Li\n\n  \nThomas Klein\n\n  \nPrasanna Mayilvahanan\n\n  \nMatthias Bethge\n\n  \nFelix Wichmann\n\n  \nRyan Cotterell\n\n  \nWieland Brendel\n\n\n\nAbstract\nFrontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation.\nThis shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery.\nCentral to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner.\nTo evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models.\nEvaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance.\nAnalysis of UMMs specifically exposes a critical limitation:\nWhile they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations.\nOur findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning.\nMentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.\n\nvisual reasoning, benchmark\n\n\n\n1 Introduction\n\n\nWords\n[…] do not seem to play any role in my mechanism of thought. The psychical entities which seem to serve as elements in thought are certain signs and more or less clear images which can be ‘voluntarily’ reproduced and combined.\n – Albert Einstein (Hadamard, 1954)\n\n\n\nVision–language models (VLMs) and even recent multimodal large language models (MLLMs) relegate vision to a passive, input-only modality.\nHowever, we are now witnessing a shift towards unified multimodal models (UMMs) capable of native, interleaved generation.\nFrontier models like Emu3.5, Gemini 2.5 / 3 and many others are trained to not only perceive but also actively generate text, images, video, and audio (e.g., Cui et al., 2025; Google DeepMind, 2025a, c, b; Deng et al., 2025; Liu et al., 2025; Qu et al., 2025; Team, 2024; Xie et al., 2025; Chen et al., 2025).\n\n\nWith more capable multimodal models comes a growing awareness that complex reasoning tasks need not be tackled in language alone (Mi et al., 2025; Zheng et al., 2025; Fan et al., 2025; Chern et al., 2025; Hao et al., 2025; Tong et al., 2025; Liang et al., 2025).\nThe premise is that dense visuals, spatial information, physical interaction, or object dynamics—in short, the complexities of real-world environments—are intrinsically difficult to textualize and may be better handled visually (Yang et al., 2024).\n\n\nFrom an anthropocentric perspective, this is plausible:\nOur thinking inherently involves mental imagery—quasi-sensory experiences we can observe and, crucially, manipulate in the absence of external stimuli (Richardson, 1969).\nFor example, designing a dress entails visualizing its different panels and making adjustments based solely on imagined observations of their composition.\nThis capacity is not only reproductive but constructive; mental imagery is believed to play an important role in problem-solving and has been linked to the generation of new knowledge (Nanay, 2023).\n\n\nTranslating the concept of mental imagery to foundation models is an active field of study, with approaches spanning a spectrum of explicitness:\nOn the implicit end, McCarty and Morales (2025) suggest that LLMs can solve pictorial tasks using only internal representations, though others argue that these mental visualizations are fragile (Sepehri et al., 2025).\nMoving toward explicit imagery, interleaved visual aids ranging from latent visual tokens (e.g., Yang et al., 2025) to generated images in UMMs (Zhou et al., 2025; Li et al., 2025a) find some success—though performance gains are inconsistent, especially in multi-step settings (Li et al., 2025b).\nFinally, on the natively visual end of the spectrum, Wiedemer et al. (2025) show that image editing models and video models can solve some reasoning tasks entirely visually, directly modifying pixels of the input image.\n\n\nOverall, the utility of machine mental imagery is unclear.\nWhile the capacity for multimodal generation exists, attempts to leverage it for reasoning yield ambiguous results.\nCrucially, it remains unclear whether failures stem from fundamental reasoning deficits, flawed image generation, or an inability to interpret self-generated cues—and the field lacks a rigorous framework to disentangle these factors across different modalities.\n\n\nWe propose MentisOculi111Latin for eyes of the mind, the concept of which goes back at least to Cicero (-55) to comprehensively study frontier models’ ability to form, maintain, and repeatedly manipulate visual representations in a goal-oriented manner.\nMentisOculi consists of five multi-step visual reasoning tasks designed to be difficult to textualize yet intuitive for humans to solve visually.\nAll tasks are procedurally generated across stratified difficulty levels.\nThis design yields ground-truth visual chain-of-thought solutions for granular analysis and allows us to calibrate complexity while ensuring the benchmark’s longevity through future extensions.\n\n\nBenchmarking state-of-the-art MLLMs, UMMs, a latent reasoning model, and a generative video model, we find that explicit visual thoughts are currently ineffective; no visual intervention reliably outperforms text-only baselines.\nFurther analysis of UMMs exposes a critical issue:\nModels often possess the textual reasoning capacity to solve a task and the generative capacity to (at least sometimes) create correct visualizations.\nHowever, they fail to integrate these skills—suffering from compounding generation errors over multiple steps and, surprisingly, even failing to leverage ground-truth visual aids.\nOur results suggest that despite the intuition behind mental imagery, architectures cannot yet bridge the gap between generation and reasoning.\n\n\n\n\nIn summary, we provide\n\n1.\n\nMentisOculi: A procedural, stratified benchmark for multi-step reasoning with mental imagery, designed to challenge frontier models (Section 2).\n\n2.\n\nAn analysis of the spectrum of machine mental imagery, covering MLLMs, latent reasoning, UMMs, and video models (Sections 4.1 and 4.2).\n\n3.\n\nEvidence that the failure of UMM visual reasoning stems from an inability to maintain consistency and to leverage visual aids (Section 4.3).\n\n4.\n\nHuman reference data, highlighting different reasoning budget allocation in humans and frontier models (Section 4.5).\n\n\n\n\n\n\n\n2 Designing MentisOculi\n\n\nThe term visual reasoning as it is used for a myriad of benchmarks targeting VLMs and MLLMs is ambiguous:\nThe vast majority of existing benchmarks do not consider reasoning visually, but instead evaluate reasoning about visual information (e.g., Xu et al., 2025; Hao et al., 2025; Lyu et al., 2025).\nInstead, we aim to benchmark models’ ability to reason with mental imagery: to use a more or less explicit, self-maintained visual representation space that can be modified at will to aid in reasoning.\n\n\nTo this end, we propose the following task desiderata:\n\n\n1.\n\nVisual nature \nTasks should test understanding of spatial relations, geometric constraints, or object transformations, rather than common knowledge or mere logic.\nWhile mental imagery might aid abstract reasoning, visualizations that are not grounded in the problem statement are hard to verify and evaluate.\n\n\n\n2.\n\nHigh information density \nTo be inefficient to solve via pure text, tasks should avoid grid-worlds or other symbolic arrangements that are trivially isomorphic to low-token text descriptions (e.g., “Piece A is at (0, 1)”, or representing a maze as a grid of X for walls and O for corridors).\nInstead, tasks should involve complex shapes, continuous and off-grid transformations, or fine-grained visual details.\n\n\n\n3.\n\nS"
  },
  {
    "title": "From Directions to Regions: Decomposing Activations in Language Models via Local Geometry",
    "url": "https://arxiv.org/abs/2602.02464v1",
    "source": "arxiv",
    "summary": "Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsu",
    "full_text": null
  },
  {
    "title": "Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models",
    "url": "https://arxiv.org/abs/2602.02462v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mit",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Method\n\n2.1 Overview &amp; Problem Formulation\n2.2 Abstract Reasoning Space as Target\n\n2.3 Unified Activation Prediction via Contrastive Learning\n\nArchitecture\nContrastive Training Strategy\nAdaptive Matching for Triplet Construction\nCombined Loss Function\n\n\n\n2.4 Inference-Time Steering\n\nSteering Layers\nPositional Blending\nActivation Blending\n\n\n\n\n\n3 Experimental Setup\n\n3.1 Models\n3.2 Dataset\n\n3.3 Metrics\n\nBelief Bias (Δbelief\\Delta_{\\text{belief}})\nBias-Penalized Accuracy (BPA)\nAbstract Alignment (η\\eta)\n\n\n\n3.4 Evaluation\n\nCross-Validation\nCross-Lingual Transfer\nSteering Strength Ablation\nBaselines\n\n\n\n\n\n4 Results &amp; Analysis\n\n4.1 Main Results: Steering Enhances Accuracy and Reduces Bias\n\n4.2 Cross-Lingual Generalisation\n\nHRLs: Near-Perfect Transfer\nLRLs: Meaningful but Limited Gains\n\n\n4.3 Abstract Alignment Analysis\n4.4 Comparative Analysis\n4.5 Fluency Sanity Check\n4.6 OOD Analysis\n\n\n\n5 Related Work\n\nSyllogistic reasoning.\n\n\n6 Conclusion\n\nA Implementation Details\n\nSoftware and Hardware\nModels\nA.1 Steering Layer Selection\n\nA.2 Abstractor Architecture Details\n\nShared Backbone\nDirection Head\nMagnitude Head\nTraining Configuration\n\n\n\nA.3 SFT Implementation Details\n\nLoRA Parameters\nTraining Data\nTraining Hyperparameters\nLabel Masking\n\n\n\nA.4 CoT Implementation Details\n\nGeneration Parameters\nAnswer Extraction\nObserved Failure Mode\n\n\n\n\n\nB Dataset and Translation Details\n\n\nB.1 Language Selection Rationale\n\nFamily and Syntax\nScript and Tokenization\nResource Stratification\n\n\n\n\n\nC Design Rationale and Class-Specific Transformations\n\n\nC.1 Unified vs. Conditional Architecture\n\nError Propagation\nFeasibility\nEfficiency\n\n\n\nC.2 Oracle Validation: Class-Specific Upper Bound\n\nD Detailed Baseline Abstract Results\n\nE Steering Strength Ablation Study\n\nE.1 Qwen-2.5-7B\n\n\nE.2 Qwen-3-14B\nE.3 Gemma-2-9B\nE.4 Gemma-3-12B\nE.5 Mistral-7B\nE.6 Ministral-14B\nF Detailed Steering vs. SFT vs. CoT Results\nG Detailed PPL Results\nH Detailed OOD Task Results\nI Positive-Negative Cosine Similarity Analysis\n\n\n\n\n\n\n\n\n\nAbstract Activation Spaces for Content-Invariant Reasoning \nin Large Language Models\n\n\n\nGabriele Maraia (†) Marco Valentino(∙)\nFabio Massimo Zanzotto (†,‡) Leonardo Ranaldi (⊕,†)\n(†){(\\dagger)} Human Centric ART, University of Rome Tor Vergata \n(⊕){(\\oplus)} ILCC, School of Informatics, University of Edinburgh \n(∙){(\\bullet)} School of Computer Science, University of Sheffield, (‡){(\\ddagger)} Almawave S.p.A. \n{first_name.last_name}@uniroma2.it\n\n\n\nAbstract\nLarge Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity–a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers.\nRecent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model’s internal computations; however, reliably suppressing semantic interference remains an open challenge.\nTo make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model’s activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance.\nOur results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.\n\n\n\nAbstract Activation Spaces for Content-Invariant Reasoning \nin Large Language Models\n\n\n\n\n\nGabriele Maraia (†) Marco Valentino(∙)\n\nFabio Massimo Zanzotto (†,‡) Leonardo Ranaldi (⊕,†)\n\n(†){(\\dagger)} Human Centric ART, University of Rome Tor Vergata\n\n(⊕){(\\oplus)} ILCC, School of Informatics, University of Edinburgh\n\n(∙){(\\bullet)} School of Computer Science, University of Sheffield, (‡){(\\ddagger)} Almawave S.p.A.\n\n{first_name.last_name}@uniroma2.it\n\n\n\n\n\n\n1 Introduction\n\nLLMs have demonstrated remarkable capabilities across a wide range of complex reasoning tasks. Yet, they frequently privilege semantic intuition over formal logic, systematically struggling to disentangle the form of an argument from its content Eisape et al. (2024); Dasgupta et al. (2024). This limitation becomes particularly evident in syllogistic reasoning, a classical testbed for deductive competence, where models exhibit content effect: a well-documented phenomenon in human cognition whereby the perceived plausibility of a conclusion overrides the logical validity of the premises.\n\n\nThe conflict between semantic heuristics and logical rigour is not merely an occasional error, but a structural failure mode. Consider the contrasting syllogistic cases:\n\n\n\n\nValid Implausible\nPremise 1: All things that have fins live in the desert.\nPremise 2: Dolphins have fins.\nConclusion: Therefore, dolphins live in the desert.\nInvalid Plausible\nPremise 1: All flowers need water.\nPremise 2: Roses need water.\nConclusion: Therefore, roses are flowers.\n\n\n\nIn Valid–Implausible arguments, the conclusion follows deductively from the premises but conflicts with world knowledge; in Invalid–Plausible arguments, the conclusion is factually acceptable yet unsupported by the premises. In both cases, LLMs tend to align their judgements with semantic plausibility, often misclassifying valid arguments as invalid and accepting invalid ones as valid Valentino et al. (2025). These behaviours indicate that models implicitly introduce semantic constraints into formal deduction, effectively fabricating logical flaws when conclusions contradict prior knowledge.\n\n\nFigure 1: Overview of the abstraction steering framework. (i) The model processes both content-laden (\"All flowers need water …\") and abstract (\"All X need Y …\") syllogisms. Abstractors learn to map content-laden activations into the abstract reasoning space, (ii) integrating predicted targets via multi-layer activation at inference-time.\n\n\nExisting strategies for mitigating content effect expose a trade-off between cost, modularity, and reliability. Supervised fine-tuning can improve accuracy but is computationally expensive and prone to reinforcing superficial correlations instead of inducing stable logical abstractions Bertolazzi et al. (2024) while CoT-based approaches offer an affordable alternative, yet often reproduce the same semantic intuitions within intermediate reasoning steps, failing to override implausible but deductively valid conclusions Wan et al. (2025). Moreover, these approaches guarantee a separation between logical structure and semantic content at the level of internal representations.\n\n\nThese observations suggest that the core issue lies not in the representational capacity of LLMs, but in how semantic and structural signals are routed during inference. Motivated by insights from mechanistic interpretability Kim et al. (2025), we investigate activation steering as an inference-time intervention Rimsky et al. (2024); Turner et al. (2024); Lucchetti and Guha (2025).\nBy selectively intervening on the residual stream, activation steering enables the attenuation of semantic content signals while amplifying representations associated with formal structure.\n\n\nWe introduce Abstractors, lightweight Multi-Layer Perceptrons trained to map content-laden activations (last token activation of input \"All flowers need water\") onto an abstract reasoning manifold (\"All X need Y\"), dynamically predicting the target activation for each input, unlike static steering vectors (see Figure 1). Furthermore, we evaluate the zero-shot, cross-lingual transfer of this manifold by training Abstractors exclusively on English data and evaluating them on nine other languages, both high- and low-resource ones.\nIn particular, to assess the generality of the proposed approach, we evaluate cross-lingual transfer by training Abstractors on English and testing them on nine additional languages, spanning both high-resource languages, such as Chinese and lower-resource languages, such as Telugu. This setting allows us to probe whether abstraction-aligned steering operates independently of linguistic surface form.\n\n\nOverall, our contributions are as follows:\n\n\n•\n\nWe achieve validity-sensitive performance comparable to, and in several cases exceeding, state-of-the-art parameter-efficient methods, without modifying weights.\n\n\n\n•\n\nWe provide evidence that logical structure is encoded in a generalisable subspace, enabling inference-time transfer across unseen languages.\n\n\n\n•\n\nOur approach offers a modular and interpretable inference-time intervention that can be enabled or disabled on demand, preserving general language-modelling capabilities while specifically targeting belief bias.\n\n\n\n\n\nOur results position activation-level abstraction as a scalable and language-agnostic mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.\n\n\n\n\n2 Method\n\n\n2.1 Overview &amp; Problem Formulation\n\nLet xx be a syllogism in natural language. The task is to determine its formal validity y∈{valid,invalid}y\\in\\{\\texttt{valid},\\texttt{invalid}\\}. We focus on intervening on the activations of the model’s residual stream. We define the activation at layer ℓ\\ell and token tt as aℓ,t​(x)∈ℝda_{\\ell,t}(x)\\in\\mathbb{R}^{d}, where dd is the model’s hidden dimension.\nOur approach involves a multi-layer intervention on a selected subset of target layers"
  },
  {
    "title": "Conflict-Aware Client Selection for Multi-Server Federated Learning",
    "url": "https://arxiv.org/abs/2602.02458v1",
    "source": "arxiv",
    "summary": "Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While m",
    "full_text": null
  },
  {
    "title": "Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction",
    "url": "https://arxiv.org/abs/2602.02455v1",
    "source": "arxiv",
    "summary": "As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarificati",
    "full_text": "\n\n\n\n\n1 Introduction\n\n\n2 A Unified Taxonomy of Agentic Cooperative Breakdowns\n\n2.1 Flaw of Intention\n2.2 Flaw of Premise\n2.3 Flaw of Parameter\n\n2.4 Flaw of Expression\n\n\n3 Drift-Bench\n\n3.1 Data Construction\n3.2 Agent Clarification\n3.3 Persona Design\nG.3 Service-Oriented\n\nG.4 Interaction Case\n\n\nH Prompt\n\nH.1 Data Perturbation\nH.2 Agent Interaction\nH.3 User Personas\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDrift-Bench : Diagnosing CoopeRative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction\n\n\nHan Bao\n\n  \nZheyuan Zhang\n\n  \nPengcheng Jing\n\n  \nZhengqing Yuan\n\n  \nKaiwen Shi\n\n  \nYanfang Ye\n\n\n\nAbstract\nAs Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce Drift-Bench , the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, Drift-Bench  provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the Rise  evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. Drift-Bench bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have achieved remarkable capabilities in language understanding and generation (rajpurkar2016squad; kwiatkowski2019natural; achiam2023gpt). A central practical challenge accompanying these successes is hallucination (huang2025survey): models confidently producing incorrect or fabricated facts (ji2023survey; xie2024survey). Early research therefore studied internal model uncertainty, distinguishing epistemic uncertainty (model knowledge limitations) from aleatoric uncertainty (inherent input noise) and developing calibration and uncertainty estimation methods primarily to address epistemic sources (lin2022truthfulqa; ji2023survey; xie2024survey; senge2014reliable; gal2016uncertainty). In this line of work, input-side noise was often treated as irreducible or out-of-scope, leaving a gap in how to handle uncertain or flawed user instructions that arise in interactive settings (gal2016uncertainty; hullermeier2021aleatoric). Subsequent work introduced interaction and clarification into the uncertainty loop (aliannejadi2020convai3; min2020ambigqa; lee2023asking; gan2024clarq), but these efforts largely remained in text-only or narrow application domains.\n\n\nTable 1: Comparison of Drift-Bench  with existing related benchmarks. Our benchmark uniquely integrates multi-turn clarification with grounded execution risks across diverse environments. Success measures the task completion rate, while Efficiency quantifies the number of interaction rounds required for goal completion. Fault types are mapped to our taxonomy: intention, premise, parameter, expression. User simulation types: Static (prefixed/template), LLM-simulated (model-generated).\n\n\n\n\n\nBenchmark\nSystem\nTools\nFault Type\nClarification\nUser Sim.\nEvaluation\n\n\n\n\\rowcolorgray!15      Tool-Use &amp; Agent Benchmarks\n\n\n\nToolBench (qin2023toolllm)\n\nAgent\nAPI\n✗\n✗\n✗\nSuccess\n\n\nAgentBench (liu2023agentbench)\n\nAgent\nMulti-modal\n✗\n✗\n✗\nSuccess/Efficiency\n\n\nStableToolBench (guo2024stabletoolbench)\n\nAgent\nAPI\n✗\n✗\n✗\nSuccess\n\n\nWebArena (zhouwebarena)\n\nAgent\nWeb\n✗\n✗\n✗\nSuccess\n\n\nGAIA (mialon2023gaia)\n\nAgent\nMulti-modal\n✗\n✗\n✗\nSuccess/Efficiency\n\n\n\nτ\\tau-Bench (yao2024tau)\n\nAgent\nAPI\nexpression\nMulti-turn\nLLM-simulated\nSuccess\n\n\n\nτ2\\tau^{2}-Bench (barres2025tau)\n\nAgent\nAPI\nexpression\nMulti-turn\nLLM-simulated\nSuccess\n\n\n\n\\rowcolorgray!15      Clarification &amp; Uncertainty Benchmarks\n\n\n\nConvAI3 (aliannejadi2020convai3)\n\nLLM\n✗\nexpression\nSingle-turn\nStatic\nSuccess\n\n\nAmbigQA (min2020ambigqa)\n\nLLM\n✗\nexpression\nSingle-turn\nStatic\nSuccess\n\n\nCondAmbigQA (li2025condambigqa)\n\nLLM\n✗\nexpression\nSingle-turn\nStatic\nSuccess\n\n\nCLARQ-LLM (gan2024clarq)\n\nLLM\n✗\nexpression\nMulti-turn\nStatic\nSuccess\n\n\nCLAMBER (zhang2024clamber)\n\nLLM\n✗\nexpression\nSingle-turn\nLLM-simulated\nSuccess\n\n\nIN3 (qian2024tell)\n\nAgent\n✗\nintention\nMulti-turn\nLLM-simulated\nSuccess\n\n\nUserBench (qian2025userbench)\n\nAgent\n✗\nintention\nMulti-turn\nLLM-simulated\nSuccess\n\n\nNoisyToolBench (wang2025learning)\n\nAgent\nAPI\npremise/expression\nMulti-turn\nStatic\nSuccess\n\n\nClarifyMT-Bench (luo2025clarifymt)\n\nLLM\n✗\nexpression/intention\nMulti-turn\nLLM-simulated\nSuccess/Efficiency\n\n\n\n\\rowcolorblue!5 Drift-Bench (Ours)\n\nAgent\nMulti-modal\nCooperative Breakdowns\nMulti-turn\nLLM-simulated\nRISE\n\n\n\n\n\n\n\nThe emergence of LLM-driven agents changes the nature and stakes of interaction. Modern agents act in the world: they manipulate files and system state (liu2023agentbench; mialon2023gaia; wang2025comprehensive), execute code, and interact with web and API services (deng2023mind2web; zhouwebarena). Crucially, agents instantiate a persistent, tool-mediated loop in which the user, the model, and the environment can interact repeatedly: the agent executes actions, observes effects, and receives further instructions or corrections.\nThis interactive substrate makes agentic interaction inherently cooperative: users must communicate goals and provide sufficiently precise instructions, while agents must infer intent, maintain shared context, and decide at each step whether to execute or to request clarification (clark1991brennan; clark1996using).\nSuccess therefore depends not only on reasoning and tool competence, but critically on the clarity and completeness of user instructions and on sustaining pragmatic alignment through multi-turn interactions.\n\n\n\nDespite this shift, most current benchmarks (qin2023toolllm; guo2024stabletoolbench) implicitly adopt the Oracle Assumption—the problematic premise that user instructions are always unambiguous and correctly specified. This assumption creates a fragmented evaluation landscape (see section 1): while some studies probe robustness to noise (wang2025learning) or evaluate text-only clarification (aliannejadi2020convai3; gan2024clarq), they often decouple the interaction loop from grounded execution risk. Even recent user-centric efforts (qian2024tell; qian2025userbench) fail to provide a unified diagnostic framework that links multi-turn pragmatic repair to downstream safety consequences. Drift-Bench fills this critical gap by shifting the evaluation paradigm from simple “instruction following” to grounded pragmatic recovery under systematic input faults.\n\n\n\nTo address this gap we introduce Drift-Bench , the first diagnostic benchmark for agentic pragmatics under input faults. Grounded in Grice’s Cooperative Principle (grice1975logic), Austin’s speech-act theory (austin1975things), and Watzlawick’s interactional axioms (watzlawick2011pragmatics), Drift-Bench couples dual-category execution environments with a persona-driven user simulator and the Rise protocol to evaluate multi-turn clarification, linking clarification behaviour to downstream task success and safety.\n\n\n\nTo ensure diagnosability and reproducibility, our benchmark is grounded in existing robust agent evaluations (liu2023agentbench; qin2023toolllm; guo2024stabletoolbench), but extends prior work by introducing controlled input faults and explicitly measuring multi-turn clarification under grounded execution. section 3 describes fault generation, data preparation, and simulator design. This targeted, lightweight faulting strategy, combined with persona-driven simulation and the Rise evaluation protocol, proves effective at exposing systematic cooperative breakdowns and safety-relevant failure modes (see LABEL:sec:experiment).\n\n\n\nOur evaluation of Drift-Bench  uncovers a catastrophic performance collapse (≈\\approx40% drop) across frontier models under input faults. Notably, we identify a “Clarification Paradox” where multi-turn interaction rehabilitates agents in transparent white-box systems but impairs them in opaque black-box settings due to context overload. Furthermore, agents exhibit a pervasive execution-bias, proceeding with high-risk actions in 70% of cases instead of deferring to clarify.\n\n\n\nOur contributions are as follows:\n\n\n\n\n\n•\n\nWe develop a theoretically grounded taxonomy of input faults (flaw of intention, flaw of premise, flaw of parameter, flaw of expression) to systematically characterize cooperative breakdowns.\n\n\n\n•\n\nWe introduce Drift-Bench , a benchmark that couples multi-turn clarification with grounded execution across diverse environments, together with a persona-driven user simulator and a controlled perturbation pipeline.\n\n\n\n•\n\nWe propose the Rise  protocol, providing complementary metrics that assess both task outcomes and the quality and economy of clarification interactions, and we report empirical findings that quantify agent degradation under cooperative breakdowns.\n\n\n\n\n\n\n\nFigure 1: Cooperative Breakdown Taxonomy. The diagram organizes systematic cooperative breakdowns into four high-level categories used throughout this paper: Flaw of Intention, Flaw of Premise, Flaw of Parameter, and Flaw of Expression.\n\n\n\n\n\n\n2 A Unified Taxonomy of Agentic Cooperative Breakdowns\n\nExisting research on LLM failures often relies on empirical taxonomies derived from specific task observations (zhang2024clamber; wang2025learning; luo2025clarifymt). While useful for local error analysis, these ad-hoc classifications frequently suffer from overlapping definitions or significant omissions, as they lack a formal principle for categori"
  },
  {
    "title": "World-Gymnast: Training Robots with Reinforcement Learning in a World Model",
    "url": "https://arxiv.org/abs/2602.02454v1",
    "source": "arxiv",
    "summary": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models le",
    "full_text": null
  },
  {
    "title": "Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling",
    "url": "https://arxiv.org/abs/2602.02453v1",
    "source": "arxiv",
    "summary": "Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02453v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.02453v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling\n    Authors:Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao            View a PDF of the paper titled Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling, by Andong Chen and 5 other authors\n    View PDF\n\n\n\n    \n            Abstract:Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.\n    \n\n    \n    \n              \n          Comments:\n          Working paper\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.02453 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02453v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02453\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Andong Chen [view email]          [v1]\n        Mon, 2 Feb 2026 18:43:57 UTC (25,507 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling, by Andong Chen and 5 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n           "
  },
  {
    "title": "Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization",
    "url": "https://arxiv.org/abs/2602.02451v1",
    "source": "arxiv",
    "summary": "Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propos",
    "full_text": null
  },
  {
    "title": "Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation",
    "url": "https://arxiv.org/abs/2602.02445v1",
    "source": "arxiv",
    "summary": "This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale",
    "full_text": null
  },
  {
    "title": "Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE",
    "url": "https://arxiv.org/abs/2602.02443v1",
    "source": "arxiv",
    "summary": "Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Motivation\n\n2.1 Expert Reduction Does Not Hurt Greedy Accuracy but Degrades Multi-Sample Pass@n Performance\n2.2 A Closer Look at Router Score Distribution\n2.3 Decoupling Stability and Diversity through Routing\n\n\n\n3 Method\n\n3.1 Standard Expert Selection\n3.2 Expert-Sample\n3.3 Validation: Balancing Stability and Diversity\n3.4 Hyperparameter Analysis\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n4.2 Scaling Experiments\n4.3 Verification Experiments\n\n\n5 Overhead Analysis\n\n6 Conclusion\n\nAppendix Overview.\n\n\n\nA Overhead Analysis of Expert-Sample\n\nA.1 Prefill Phase Overhead\nA.2 Decode Phase Overhead\n\n\n\nB Hyperparameter Analysis\n\nB.1 Is kkeepk_{\\mathrm{keep}} necessary? If so, how to set it simply and effectively?\nB.2 How does temperature τ\\tau affect performance? Can high temperature balance stability and diversity?\nB.3 Is expanding the sampling range rr beneficial? How should it be set in practice?\nB.4 Summary and Practical Guidelines\n\n\n\nC Process Diversity Evaluation Details\n\nC.1 Evaluation Procedure\nC.2 Evaluation Prompt\nC.3 Example: Similarity Matrices on an AIME Problem\nC.4 Computing the Diversity Score\nC.5 Summary\n\n\nD Detailed Router Weight Distribution Analysis\n\nE Evaluation Details\n\nE.1 Evaluation Framework\nE.2 Model Details\nE.3 Dataset Details\n\n\n\nF Related Work\n\nF.1 Test-Time Scaling\nF.2 Fine-Grained MoE\n\n\n\n\n\n\n\nCertain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling \nin Fine-Grained MoE\n\n\nYuanteng Chen\n\n  \nPeisong Wang\n\n  \nNanxin Zeng\n\n  \nYuantian Shao\n\n  \nGang Li\n\n  \nJing Liu\n\n  \nJian Cheng\n\n\n\nAbstract\nTest-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space.\nWe empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly—suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity.\nMotivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.\n\nMachine Learning, ICML\n\n\nFigure 1: Overview of Expert-Sample. Left: Illustration of the Expert-Sample mechanism with an example from MATH-500, showing how Expert-Sample achieves structural diversity to discover the correct answer, while standard token-sample produces only superficial diversity. Right: Pass@n scaling improvements (upper) and accuracy gains with Best-of-N verification (lower) on Qwen3-30B-A3B-Instruct.\n\n\n\n1 Introduction\n\nMixture-of-Experts (MoE) (Artetxe et al., 2022) has become one of the most effective approaches for scaling model parameters through sparse expert activation. Recently, fine-grained MoE designs (Dai et al., 2024) featuring hundreds of well-trained experts per layer and multi-expert activation per token have gained prominence. Models such as DeepSeek-R1 (DeepSeek-AI, 2025), GPT-OSS (OpenAI, 2025), and the Qwen3-MoE (Yang et al., 2025) series adopt this architecture and significantly outperform earlier MoE models like Mixtral-8x7B (Jiang et al., 2024) that use fewer experts. Yet existing works (Jin et al., 2024; Yan et al., 2025) have focused almost exclusively on training efficiency and serving optimization, leaving the inference-time potential of the rich routing space largely unexplored.\n\n\nTest-time scaling (Brown et al., 2024) has emerged as a powerful paradigm for improving LLM performance, particularly on complex reasoning tasks. By generating multiple candidate solutions and selecting the best one through verification (Irvine et al., 2023) or majority voting (Wang et al., 2023), models can achieve accuracy far beyond single-run inference. However, current approaches predominantly rely on token-level sampling to produce diverse candidates, where temperature serves as the primary control knob. This creates a well-known dilemma: higher temperatures increase diversity but degrade individual sample quality, while lower temperatures preserve quality but limit exploration of the solution space (Pipis et al., 2025).\nThis motivates the search for alternative sources of diversity that can maintain sample quality while enabling effective exploration.\n\n\nIn this paper, we investigate whether the routing mechanism in fine-grained MoE can serve as an alternative source of diversity for test-time scaling. We begin with an empirical study and observe (in section 2) that when the number of activated experts is significantly reduced, single-run greedy decoding accuracy remains surprisingly stable, yet multi-sample pass@n performance degrades substantially. This asymmetry prompts us to take a closer look at router score distributions, where we uncover an informative pattern: a certain head consisting of a small number of high-confidence experts, followed by an uncertain tail of many experts with relatively uniform weights. The certain head appears sufficient for deterministic generation, while uncertain tail enables diverse solution paths under parallel sampling.\n\n\nThese findings reveal an opportunity: we can maintain generation stability by preserving the certain head while injecting diversity through stochastic sampling in the uncertain tail. This routing-level approach provides an additional dimension for diversity that complements rather than replaces token-level sampling.\n\n\nBuilding on this insight, we propose Expert-Sample, a simple yet effective method for test-time scaling in fine-grained MoE models. At each layer, Expert-Sample deterministically retains the top-ranked experts with high-confidence routing weights (e.g., E3,E2E_{3},E_{2} in Figure 1), then samples the remaining activated experts from a specified rank range (e.g., E4,E5,E6,…E_{4},E_{5},E_{6},\\ldots) using temperature-scaled router logits, while preserving the original gating weights for expert output aggregation.\n\n\nAs illustrated in the case study (Figure 1, Left), this mechanism enables structurally diverse reasoning paths that discover the correct answer, while standard token-sample produces only superficial diversity with the same underlying mistake. Crucially, Expert-Sample acts as a plug-and-play sampling strategy requiring no architectural modification or additional training, and complements rather than conflicts with token-level sampling. As shown in Figure 1 (Right), on Qwen3-30B-A3B-Instruct, Expert-Sample on top of standard token-sample significantly improves both pass@n accuracy (upper) and verification-based accuracy (lower) across multiple benchmarks.\n\n\nWe evaluate Expert-Sample on multiple fine-grained MoE models including Qwen3-MoE, GPT-OSS and Ling-Lite-1.5 (Team, 2025) across diverse tasks spanning math reasoning, knowledge-intensive reasoning, and code generation. Compared to token-level sampling, our experiments demonstrate that Expert-Sample consistently improves pass@n accuracy under multi-sample generation, indicating stronger scaling potential. Furthermore, Expert-Sample composes favorably with existing selection strategies such as Best-of-N and majority voting, boosting verification-based accuracy and translating to practical performance gains.\n\n\n\n\n2 Motivation\n\n\n\n\n\n(a)\n\n\n\n\n\n(c)\n\n\n\n\n\n\n(b)\n\n\n\nFigure 2: Empirical findings and motivation for Expert-Sample. (a) Greedy decoding accuracy remains stable when reducing activated experts to half of the default top-k. (b) Pass@n accuracy degrades substantially when expert count is reduced, suggesting the uncertain tail is critical for diverse exploration. (c) Router score distribution for the top 32 ranked positions reveals a certain head with high-confidence experts and an uncertain tail with uniform scores (full distribution is in Appendix D). All Qwen3 models shown are Instruct versions.\n\n\n\n2.1 Expert Reduction Does Not Hurt Greedy Accuracy but Degrades Multi-Sample Pass@n Performance\n\nTo understand how expert selection affects generation behavior in fine-grained MoE, we conduct a preliminary experiment where we reduce the number of activated experts at each layer during inference. We evaluate five representative fine-grained MoE models covering most of the latest popular architectures: Qwen3-Next-80B-A3B-Instruct, Qwen3-30B-A3B-Instruct, GPT-OSS-20B, Ling-Lite-1.5, and DeepSeek-V2-Lite-Chat (DeepSeek-AI et al., 2024). Experiments are conducted on GPQA-Diamond (Rein et al., 2023), a professional knowledge reasoning benchmark, and AIME-120, which contains 120 competition-level math problems from 2022 to 2025. Crucially, we use greedy decoding to eliminate the interference of sampling randomness, allowing us to more directly measure the impact of expert selection on the model’s core reasoning capability.\n\n\nAs shown in Figure 2(a), greedy decoding accuracy remains remarkably stable even when the number of activated experts is reduced to half of the default top-kk. This holds consistently across all five models and both benchmarks, suggesting that the top half of experts ranked by routing weights are sufficient for deterministic generation and preserving the model’s core reasoning capability.\n\n\nGiven that reducing experts does not "
  },
  {
    "title": "Large Language Models for Mental Health: A Multilingual Evaluation",
    "url": "https://arxiv.org/abs/2602.02440v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performan",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Datasets\n\n4 Experiments and Results\n\n4.1 Prompting on Original Datasets\n4.2 Prompting on MT Datasets\n4.3 Performance Comparison: Original vs. MT Datasets\n4.4 Fine-tuning on Original Datasets\n\n\n\n5 Translation Quality Evaluation across Languages &amp; LLMs\n\n5.1 Cross-Language Performance Patterns\n5.2 Interpreting Results via Language Family &amp; Typology\n5.3 Translation Quality &amp; LLM Performance on MT Data\n\n\n6 RQs Revisited\n7 Conclusion and Future Work\n\n\n\n\n\nLarge Language Models for Mental Health: A Multilingual Evaluation\n\n\n\nNishat Raihan1,\nSadiya Sayara Chowdhury Puspo111footnotemark: 1,\nAna-Maria Bucur2,3\nStevie Chancellor4,\nMarcos Zampieri1\n\n1George Mason University, USA \n2Interdisciplinary School of Doctoral Studies, University of Bucharest, Romania \n3PRHLT Research Center, Universitat Politècnica de València, Spain \n4University of Minnesota, USA \nmraihan2@gmu.edu\nEqual Contribution\n\n\nAbstract\nLarge Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.\n\n\n\nLarge Language Models for Mental Health: A Multilingual Evaluation\n\n\n\n\n\nNishat Raihan††thanks: EqualContribution1{}^{1}\\lx@make@thanks{EqualContribution},\nSadiya Sayara Chowdhury Puspo111footnotemark: 1,\nAna-Maria Bucur2,3\n\nStevie Chancellor4,\nMarcos Zampieri1\n\n1George Mason University, USA\n\n2Interdisciplinary School of Doctoral Studies, University of Bucharest, Romania\n\n3PRHLT Research Center, Universitat Politècnica de València, Spain\n\n4University of Minnesota, USA\n\nmraihan2@gmu.edu\n\n\n\n\n\n\n1 Introduction\n\nWhile LLMs have transformed research in NLP, it is important to exercise caution when applying these models in sensitive domains such as mental health Hua et al. (2024), security Kande et al. (2024) and education Raihan et al. (2025b). The potential risks and ethical considerations associated with LLMs make experts wary of their use in this field. These concerns are amplified in multilingual settings where previous research has shown that LLMs tend to perform worse when prompted in languages other than English Jin et al. (2024); Raihan et al. (2025a).\n\n\nMost mental health datasets are curated from specialized forums Malmasi et al. (2016); Milne et al. (2016) and social media platforms such as Reddit and X and contain only English data Mariappan et al. (2024); Turcan and Mckeown (2019); Raihan et al. (2024). Models built on these datasets fail for cross-cultural contexts Abdelkadir et al. (2024). Thus, there are ongoing efforts to create similar resources in other languages, such as Arabic Baghdadi et al. (2022); Helmy et al. (2024), Bengali Uddin et al. (2019), Russian Narynov et al. (2020), and Thai Hämäläinen et al. (2021). While Skianis et al. (2024a, b) explore the use of LLMs for translating English mental health datasets into other languages and Zahran et al. (2025) focuses on English-Arabic translation, none of these studies evaluate LLM performance on datasets that originate in non-English languages and their back-translated counterparts (MT datasets).\n\n\nThe effectiveness of LLMs for English mental health datasets and prediction shows promise in their performance; yet, languages other than English are underexplored. Recent studies have explored the performance of LLMs on English mental health datasets. Xu et al. (2024) compares the performance of LLMs across multiple datasets with that of statistical models and traditional encoder-only models Alsentzer et al. (2019). Similarly, Kuzmin et al. (2024), Yang et al. (2023), and Wei et al. (2022) explore various prompting strategies to assess LLMs’ effectiveness. Finally, Yang et al. (2024) presents a fine-tuning approach with the release of MentaLLaMA, a task-specific model for the domain. Although these approaches achieve competitive results, their focus is limited to English, and there are currently no studies on non-English datasets, highlighting a significant gap in research for other languages.\n\n\nWhile machine-translated (MT) datasets can augment multilingual training or evaluate translation quality Nguyen et al. (2024); Qiu et al. (2022); Mendonça et al. (2023), their effect on domain-specific LLM performance is underexplored. MT is appealing to mental health research\nas it offers a practical way to extend resources from English to low-resource languages without requiring costly new data collection Ahuja et al. (2022). Prior studies have translated mental health datasets Skianis et al. (2024a, b); Zahran et al. (2025), but they have not systematically compared LLM results on original versus MT datasets. Furthermore, these studies have not established a connection between performance differences and translation quality across diverse language families and typologies. Examining this overlooked dimension would reveal whether MT data can be reliably used in sensitive domains like mental health, highlight language-specific challenges that affect model performance, and help build fairer, more effective multilingual mental health technologies.\n\n\nTo address these gaps, we present the first multilingual evaluation of state-of-the-art LLMs on mental health datasets. We consider mental health datasets in six languages including some low-resource languages, namely: Arabic, Bengali, Spanish, Portuguese, Russian, and Thai - and two tasks, depression and suicidal ideation detection. Our work111https://github.com/SadiyaPuspo/Multilingual-Mental-Health-Evaluation addresses the following Research Questions (RQs):\n\n\n\n\n•\n\nRQ1: How does the performance of LLMs compare to previously proposed models (e.g., statistical, neural, BERT-based) on both original and back-translated data?\n\n\n\n•\n\nRQ2: What are the best prompting strategies for LLMs on mental health?\n\n\n\n•\n\nRQ3: What is the impact of instruction fine-tuning on the performance of open-source LLMs?\n\n\n\n•\n\nRQ4: How does translation quality vary across languages and typologies, and how does it affect LLM performance on machine-translated data?\n\n\n\n\n\n\n\n2 Related Work\n\nThe challenges of detecting mental health disorders from multilingual data have been gaining increasing attention. Bucur et al. (2025) provides a comprehensive survey of multilingual mental health detection, highlighting cultural and linguistic differences, while Garg (2024) emphasizes the need to study mental health in low-resource languages. Recent studies Skianis et al. (2024a, c) examine multilingual LLMs on translated datasets, revealing performance gaps across six languages, while Zahran et al. (2025) and Zevallos et al. (2025) explore Arabic and multilingual suicidal ideation detection tasks. Together, these studies advance multilingual mental health modeling.\n\n\nTranslation quality is crucial in multilingual mental health NLP. Recent studies employ BLEU and BERTScore metrics to evaluate LLM-based translation Ghassemiazghandi (2024) and mental-health text summarization Adhikary et al. (2024), assessing how well translated data preserve meaning to build culturally robust NLP systems. Back-translation further enhances data diversity and supports classification tasks Goswami et al. (2023); Raihan et al. (2023); Ganguly et al. (2024). Building on these insights, our work bridges the gap by jointly examining translation quality and back-translation effects across language families in multilingual mental health LLM evaluation.\n\n\n\n\n3 Datasets\n\n\n\n\n\n\n\nDataset\n\n\nLanguage (ISO code)\nMental Disorder\nPlatform\nExpert Labeling\nSize\n\n\n\n\n\n\nNarynov et al. (2020)\n\n\nRussian (ru)\nDepression\nVKontakte\nYes\n32,018\n\n\n\n\nHämäläinen et al. (2021)\n\n\nThai (tha)\nDepression\nBlogs\nYes\n33,436\n\n\n\n\nBoonyarat et al. (2024)\n\n\nThai (tha)\nSuicidal Ideation\nX\nNo\n2,400\n\n\n\n\nUddin et al. (2019)\n\n\nBengali (ben)\nDepression\nX\nYes\n3,914\n\n\n\n\nde Oliveira et al. (2022)\n\n\nPortuguese (por)\nSuicidal Ideation\nX\nYes\n3,788\n\n\n\n\nBaghdadi et al. (2022)\n\n\nArabic (ar)\nSuicidal Ideation\nX\nN/A\n14,576\n\n\n\n\nHelmy et al. (2024)\n\n\nArabic (ar)\nDepression\nX\nNo\n10,000\n\n\n\n\nValeriano et al. (2020)\n\n\nSpanish (es)\nSuicidal ideation\nX\nN/A\n1,068\n\n\n\n\nTable 1: Overview of the eight mental disorder datasets across different languages. The size column represents the number of instances in each dataset.\n\n\nAutomatic detection of mental health disorders from social media data has gained substantial attention, particularly in English. However, multilingual mental health detection remains underexplored, as most available datasets focus on a single language. To address this limitation, we use eight publicly available mental health classification datasets presented in Table 1.\n\n\nAmong these eight datasets, Narynov et al. (2020) presents a Russian-language depression dataset collected from VKontakte, containing 34,000 posts with expert annotations. Similarly, Hämäläinen et al. (2021) develop a Thai-language depression dataset from online blogs, consisting of 900 posts with expert labels. While these resources contribute to the study of mental health in non-English languages, they remain isolated efforts, with limited "
  },
  {
    "title": "Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization",
    "url": "https://arxiv.org/abs/2602.02439v1",
    "source": "arxiv",
    "summary": "Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SN",
    "full_text": null
  },
  {
    "title": "UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing",
    "url": "https://arxiv.org/abs/2602.02437v1",
    "source": "arxiv",
    "summary": "Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-e",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.02437v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2602.02437v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 2 Feb 2026]\n    Title:UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing\n    Authors:Dianyi Wang, Chaofan Ma, Feng Han, Size Wu, Wei Song, Yibin Wang, Zhixiong Zhang, Tianhang Wang, Siyuan Wang, Zhongyu Wei, Jiaqi Wang            View a PDF of the paper titled UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing, by Dianyi Wang and 10 other authors\n    View PDF\n\n\n\n    \n            Abstract:Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.02437 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.02437v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.02437\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Dianyi Wang [view email]          [v1]\n        Mon, 2 Feb 2026 18:34:35 UTC (4,184 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing, by Dianyi Wang and 10 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n "
  }
]