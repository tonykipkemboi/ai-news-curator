[
  {
    "title": "VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation",
    "url": "https://arxiv.org/abs/2601.23286v1",
    "source": "arxiv",
    "summary": "While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference A",
    "full_text": null
  },
  {
    "title": "End-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms",
    "url": "https://arxiv.org/abs/2601.23285v1",
    "source": "arxiv",
    "summary": "Shared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unst",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Planning-based Approaches\n2.2 Intention Inference and Reward Shaping\n2.3 Sequential vs. Integrated Approaches\n\n\n\n3 Methodology\n\n3.1 Training Procedure\n3.2 Theoretical Properties of Assistance Policy\n3.3 Bayesian Inference Module\n3.4 Assistance arbitration policy\n3.5 Expert Policy\n\n\n\n4 Results\n\n\n4.1 Planar Human-in-the-loop Cursor Control\n\n4.1.1 Quantitative Performance Results\n4.1.2 Qualitative Performance Results\n\n\n4.2 Reacher-2D\n4.3 Pick and Place with a Robotic Arm\n\n4.4 Ablation Studies\n\n4.4.1 Is joint optimization necessary?\n4.4.2 Is Full Belief Conditioning necessary?\n4.4.3 How much does curriculum learning help?\n\n\n\n\n\n5 Discussion\n\n5.1 Limitations and Future Work\n\n\n6 Acknowledgment\n\nA Proofs\n\nA.1 Uncertainty Principle Proof\nA.2 Integrated Optimization Advantage Proof\n\n\nB Detailed Reward Function\n\nC Integration Optimization\n\nC.1 End-to-End Optimization and Policy Network Stability Measures\n\n\nD Simulated Human Agent in BRACE\nE Transfer learning and Adaptation to New Settings\nF Computational Resources\nG Robustness to Input Modalities\n\n\n\n\n\n\n\\DeclareBibliographyAlias\npreprintunpublished\n\nEnd-to-end Optimization of Belief and Policy Learning in Shared Autonomy Paradigms\n\n\n\nMH Farhadi, Ali Rabiee, Sima Ghafoori,\nAnna Cetera, Andrew Fisher,\nReza Abiri \nUniversity of Rhode Island, Kingston, Rhode Island, USA\n\n\n\nShared autonomy systems require principled methods for inferring user intent and determining appropriate assistance levels. This is a central challenge in human-robot interaction, where systems must be successful, while being mindful of user agency. Previous approaches relied on static blending ratios or separated goal inference from assistance arbitration, leading to suboptimal performance in unstructured environments. We introduce BRACE (Bayesian Reinforcement Assistance with Context Encoding), a novel framework that fine-tunes Bayesian intent inference and context-adaptive assistance through an architecture enabling end-to-end gradient flow between intent inference and assistance arbitration. Our pipeline processes the full Bayesian goal distribution rather than MAP estimates, conditioning collaborative control policies on environmental context and complete goal probability distributions. We provide analysis showing (1) optimal assistance levels should decrease with goal uncertainty and increase with environmental constraint severity, and (2) integrating belief information into policy learning yields a quadratic expected regret advantage over sequential approaches. We validated our algorithm against SOTA methods (IDA, DQN) using a three-part evaluation progressively isolating distinct challenges of end-effector control: (1) core human-interaction dynamics in a 2D human-in-the-loop cursor task, (2) non-linear physical dynamics of a robotic arm, and (3) integrated manipulation under goal ambiguity and environmental constraints. We demonstrate improvements over SOTA, achieving 6.3% higher success rates and 41% increased path efficiency, and 36.3% success rate and 87% path efficiency improvement over unassisted control. Results confirmed the advantage of integrated optimization is most pronounced in complex scenarios with goal ambiguity. Our method outperformed sequential approaches by 23% in completion time in scenarios with high goal uncertainty, with success rate improvements of 13.1% in complex multi-target environments. The demonstrated benefits of our integrated approach are generalizable across robotic domains requiring goal-directed assistance, advancing the SOTA for adaptive shared autonomy.\n\n\nKeywords: Shared Autonomy, Human-Robot Interaction (HRI), Assistive Robotics, Collaborative Control, User Agency, Goal Uncertainty, Intent Inference, Belief-Conditioned Policy\n\n\n\n1 Introduction\n\nShared autonomy is a control paradigm in which a human and an automated system jointly operate a device, with the system adapting its assistance to the situation. For example, In assistive robotics for users with motor impairments, the system must infer whether the user intends to grasp a large water glass or a small pill bottle. It may provide minimal assistance during the reaching phase to preserve user agency, then increase support when aligning the gripper for a precision-critical grasp on the small bottle.\n\n\nThe core challenge in shared autonomy often comes from the inherent tension between two critical processes: inferring the user‚Äôs goal (often a probabilistic inference problem) and determining the appropriate level of assistance (an optimization problem) [undefr]. Prior approaches typically addressed this challenge as separate or sequential problems, either using fixed blending ratios regardless of context [undefe], separating goal inference from assistance arbitration [undefq], or solving them with model-based planning that requires known dynamics models [undefaj]. Recent belief-conditioned methods such as SARI [undefs], mutual-adaptation [undefw], and IDA [undefv] highlighted the need for an end-to-end integration, however, their findings either relied on Maximum a-posteriori (MAP) goals, binary interventions or failed to adapt optimally to the uncertainty in the user‚Äôs goals.\n\n\nClassical methods usually first estimate the user‚Äôs intent, then control the system using either a single best guess of that intent or a belief-space planner whose inference is fixed and does not adapt. BRACE instead learns assistance as Œ≥=fŒ∏‚Äã(s,b,c)\\gamma=f_{\\theta}(s,b,c) using the goal belief bb, and it links inference and control end-to-end so that the resulting performance (the downstream gradients) also shapes and updates the belief itself.\n\n\nThis work makes the following contributions:\n\n\n\n\n1.\n\nFull-belief conditioning: Assistance is a function of the entire intent posterior, not the MAP goal, allowing a more nuanced reaction to user uncertainty.\n\n\n\n2.\n\nEnd-to-end coupling: The intent inference module and assistance arbitration policy are optimized jointly from a single control objective; gradients from task returns reshape the belief to be decision-useful, avoiding the estimator‚Äìcontroller mismatch of sequential pipelines.\n\n\n\n3.\n\nProof of advantage over regret bounds: When different goals require different assistance (because intent is unclear and the environment is constrained), conditioning on belief and jointly optimizing inference and control reduces expected regret quadratically compared with MAP schemes. The benefit grows as constraints tighten and shrinks as belief entropy increases. That‚Äôs exactly when BRACE helps most.\n\n\n\n\n\nRecent belief-aware approaches such as SARI [undefs], Mutual-Adaptation [undefw], IDA [undefv], and STREAMS [undefab] have begun to couple intent inference with assistance. Our work differs from them in three main ways. (i) BRACE leverages the complete belief distribution b‚àà‚Ñù|G|b\\in\\mathbb{R}^{|G|} rather than a single goal estimate or binary signal, which is critical in scenarios with high goal uncertainty. (ii) Their assistance logic is either huristic thresholds or secondary networks trained with the belief module frozen; BRACE performs joint gradient-based optimization of inference and control, giving provably lower regret (Theorem 2). (iii) None of the prior systems provide task-agnostic monotonicity guarantees or regret bounds.\n\n\nThe paper is organized as follows: Section 2 reviews prior work on intent inference and assistance arbitration. Section 3 introduces the BRACE framework, detailing its methodology, theoretical properties, and system components. Section 4 presents empirical results from multiple end-effector based control experiment scenarios, simulation benchmarks, and ablation studies. Section 5 discusses the findings and concludes with limitations and future work. The Appendices provide supplementary materials, including mathematical proofs, reward function details, and additional analyses.\n\n\n\n\n2 Related Works\n\n\n2.1 Planning-based Approaches\n\nJavdani et al. [undefq] modeled the problem as a Partially Observable Markov Decision Process (POMDP) with the human‚Äôs goal as a hidden state. Their hindsight optimization algorithm provided computational tractability by approximating the POMDP solution, enabling assistance even when goal confidence was low. Jain and Argall [undefo] further extended probabilistic approaches for assistive robotics through recursive Bayesian filtering that fused observations like human behavior with different rationality levels. While these approaches offer principled formulations, they require simplifying assumptions about goal structures and rely on discrete optimization rather than learning adaptive strategies from interaction data.\n\n\nIn the planning literature, Monte Carlo Tree Search (MCTS) methods have been effectively applied to scenarios involving uncertainty in user preferences. Aronson et al. [undefa] leveraged MCTS‚Äôs statistical sampling approach to handle complex decision spaces without requiring exhaustive search. Nikolaidis et al. [undefw] advanced this field by formalizing human-robot mutual adaptation through Bounded-Memory Models embedded in partially observable decision processes, demonstrating performance improvements over one-way adaptive approaches. Complementing these works, Panagopoulos et al. [undefy] developed a Bayesian framework for intent recognition in remote navigation that fuses multiple observation sources, further extending probabilistic approaches to human-robot collaboration.\n\n\n\n\n2.2 Intention Inference and Reward Shaping\n\nRecent advances in reinforcement learning have enabled new approaches to shared autonomy that learn assistance policies. Reddy et al. [undefac] implemented deep Q-learning to learn end-to-end assistive policies without explicit goal models, implicitly inferring intent through reward signals during human-in-the-loop training. Xie et al. [undefaf] extended this work by developing a probabilistic policy blending approach that combines deep reinforcement learning with expl"
  },
  {
    "title": "Decoupled Diffusion Sampling for Inverse Problems on Function Spaces",
    "url": "https://arxiv.org/abs/2601.23280v1",
    "source": "arxiv",
    "summary": "We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the",
    "full_text": null
  },
  {
    "title": "FOCUS: DLLMs Know How to Tame Their Compute Bound",
    "url": "https://arxiv.org/abs/2601.23278v1",
    "source": "arxiv",
    "summary": "Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. W",
    "full_text": null
  },
  {
    "title": "Denoising the Deep Sky: Physics-Based CCD Noise Formation for Astronomical Imaging",
    "url": "https://arxiv.org/abs/2601.23276v1",
    "source": "arxiv",
    "summary": "Astronomical imaging remains noise-limited under practical observing constraints, while standard calibration pipelines mainly remove structured artifacts and leave stochastic noise largely unresolved. Learning-based denoising is promising, yet progress is hindered by scarce paired training data and the need for physically interpretable and reproducible models in scientific workflows. We propose a ",
    "full_text": null
  },
  {
    "title": "UPA: Unsupervised Prompt Agent via Tree-Based Search and Selection",
    "url": "https://arxiv.org/abs/2601.23273v1",
    "source": "arxiv",
    "summary": "Prompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propo",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n2.1 Problem Setting &amp; Overview\n2.2 Pairwise Comparison with Bias Mitigation\n2.3 Tree-Based Search with Relative Feedback\n2.4 Latent Quality Modeling via BTL\n2.5 Stage I Selection: Path-Wise Bayesian Filtering\n2.6 Stage II Selection: Global BTL Maximization\n\n\n\n3 Experiments\n\n3.1 Setup\n3.2 Performance\n3.3 Ablation Studies\n\n\n4 Conclusion\n\nA Related Works\n\nA.1 Prompt Engineering\nA.2 Prompt Optimization\n\n\n\nB Additional Information on Methodology\n\nB.1 Discussion on Assumption 2.1\nB.2 Proof of Proposition 2.2\n\nB.3 Prompts for Judge and Optimization LLMs\n\n\nC Additional Information on Experiments\n\nC.1 Dataset Details\nC.2 Cost Analysis\nC.3 Ablation Study on Judge and Optimization LLMs\n\nC.4 List of Optimized Prompts\n\n\nC.5 Sample Outputs on MT-Bench\n\nC.6 Visualization of Search Trees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUPA: Unsupervised Prompt Agent via Tree-Based Search and Selection\n\nAbstract\nPrompt agents have recently emerged as a promising paradigm for automated prompt optimization, framing refinement as a sequential decision-making problem over a structured prompt space. While this formulation enables the use of advanced planning algorithms, these methods typically assume access to supervised reward signals, which are often unavailable in practical scenarios. In this work, we propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised feedback. Specifically, during search, UPA iteratively constructs an evolving tree structure to navigate the prompt space, guided by fine-grained and order-invariant pairwise comparisons from Large Language Models (LLMs). Crucially, as these local comparisons do not inherently yield a consistent global scale, we decouple systematic prompt exploration from final selection, introducing a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model. This framework first performs path-wise Bayesian aggregation of local comparisons to filter candidates under uncertainty, followed by global tournament-style comparisons to infer latent prompt quality and identify the optimal prompt. Experiments across multiple tasks demonstrate that UPA consistently outperforms existing prompt optimization methods, showing that agent-style optimization remains highly effective even in fully unsupervised settings.\n\n\nSiran Peng1,2,*‚ÄÉWeisong Zhao4,5,*‚ÄÉTianyu Fu3,*‚ÄÉChenxu Zhao3‚ÄÉTianshuo Zhang2,1\nHaoyuan Zhang2,1‚ÄÉXiangyu Zhu1,2‚ÄÉMinghui Wu3,‚Ä†‚ÄÉZhen Lei1,2,6,7,‚Ä†\n\n\n1MAIS, CASIA ‚ÄÇ‚ÄÖ2SAI, UCAS ‚ÄÇ‚ÄÖ3Mininglamp Technology ‚ÄÇ‚ÄÖ4IIE, CAS ‚ÄÇ‚ÄÖ5SCS, UCAS ‚ÄÇ‚ÄÖ6CAIR, HKISI, CAS ‚ÄÇ‚ÄÖ7SCSE, FIE, M.U.S.T\n\n\n{pengsiran2023, zhen.lei}@ia.ac.cn,\nzhaoweisong@iie.ac.cn,\n{futianyu, wuminghui}@mininglamp.com\n\n11footnotetext: Equal contribution. ‚ÄÉ‚Ä†Corresponding author.\n\n\n\n\nFigure 1: Comparison of prompt optimization paradigms, including most Prompt Optimization (PO) approaches, existing prompt agents (Wang et¬†al., 2024b; Yu et¬†al., 2025), SPO (Xiang et¬†al., 2025), and our UPA. Unlike prior methods, UPA uniquely achieves structured exploration in a fully ground-truth (GT) free setting. \n\n\n\n1 Introduction\n\nWell-designed prompts are critical for maximizing the capabilities of Large Language Models (LLMs) (Deng et¬†al., 2023; Zheng et¬†al., 2024). However, crafting effective prompts often requires substantial trial-and-error experimentation and deep task-specific expertise. To alleviate this manual burden, researchers have developed a wide range of automated prompt optimization methods that leverage LLMs themselves to iteratively improve prompt quality (Pryzant et¬†al., 2023; Wen et¬†al., 2023; Lin et¬†al., 2024; Zhou et¬†al., 2024; Chen et¬†al., 2024; He et¬†al., 2025; Yan et¬†al., 2025). As depicted in Figure¬†1, most existing approaches perform single-trajectory optimization, where a single prompt is iteratively refined in a linear fashion. In contrast, prompt agents have recently emerged as a distinct paradigm that frames refinement as a sequential decision-making problem over a structured prompt space (Wang et¬†al., 2024b; Yu et¬†al., 2025; Han et¬†al., 2025). By maintaining multiple candidates and exploring diverse refinement paths, prompt agents enable systematic planning beyond single-path updates, leading to more adaptive and comprehensive prompt improvement.\n\n\nExisting prompt agents largely depend on supervised reward signals, typically derived from labeled data or task-specific metrics, to guide systematic exploration. However, such signals are often unavailable in practical scenarios, leaving prompt quality unobservable and rendering reward-driven planning algorithms, such as Monte Carlo Tree Search (MCTS) (Coulom, 2006), inapplicable. Self-supervised Prompt Optimization (SPO) (Xiang et¬†al., 2025) takes an important step toward unsupervised refinement by leveraging LLMs as judges to conduct pairwise comparisons. While SPO demonstrates that relative preferences can guide linear, single-path improvement, it remains unproven whether such feedback can effectively sustain the complex, multi-path exploration intrinsic to agent-style prompt optimization.\n\n\nIn light of these limitations, we ask a fundamental question: Can prompt agents be realized in fully unsupervised settings? To address this question, we propose UPA, an Unsupervised Prompt Agent that enables structured search and selection over the prompt space without relying on supervised reward signals. Specifically, during the search phase, UPA iteratively constructs an evolving tree structure to navigate the candidate space, where each node denotes a candidate prompt and each edge corresponds to a refinement step performed by an optimization LLM. This structure allows UPA to explore multiple trajectories in parallel. Instead of computing absolute rewards, UPA utilizes a judge LLM to conduct fine-grained and order-invariant pairwise comparisons, providing relative preferences between a child prompt and its parent based on their performance on sampled inputs. Crucially, since these local comparisons do not inherently yield a consistent global ranking, UPA decouples prompt exploration from final selection. This necessitates a two-stage selection procedure grounded in the Bradley-Terry-Luce (BTL) model (Bradley &amp; Terry, 1952; Luce, 1959). In the first stage, UPA performs path-wise Bayesian aggregation of noisy and sparse comparison outcomes to obtain uncertainty-aware estimates of prompt quality, filtering the tree to a promising subset of candidates. In the second stage, UPA conducts global tournament-style comparisons among these candidates, estimating their latent quality scores via BTL-based maximum likelihood estimation to identify the optimal prompt. Our main contributions are as follows:\n\n\n‚Ä¢\n\nWe propose UPA, an Unsupervised Prompt Agent that realizes structured search and selection without relying on supervised reward signals. This addresses a critical limitation in agent-style prompt optimization that requires absolute rewards to guide exploration.\n\n\n\n‚Ä¢\n\nWe develop a tree-based search procedure that explores the candidate space via fine-grained and order-invariant pairwise comparisons provided by LLMs. This is coupled with a two-stage selection framework grounded in the BTL model, which enables robust prompt identification under noisy and sparse relative feedback.\n\n\n\n‚Ä¢\n\nThrough comprehensive experiments across multiple tasks, we demonstrate that UPA consistently outperforms state-of-the-art (SOTA) prompt optimization methods, showing that agent-style prompt optimization remains effective even in fully unsupervised settings.\n\n\n\n\n\n\n\n2 Methodology\n\n\n2.1 Problem Setting &amp; Overview\n\nLet ùí´\\mathcal{P} denote the discrete space of natural language prompts and ùí¨\\mathcal{Q} a distribution over task queries. Given a prompt p‚ààùí´p\\in\\mathcal{P} and a query q‚àºùí¨q\\sim\\mathcal{Q}, an execution LLM produces a response a=fexec‚Äã(p,q)a=f_{\\text{exec}}(p,q). The goal of prompt optimization is to identify a prompt p‚ãÜ‚ààùí´p^{\\star}\\in\\mathcal{P} that maximizes the expected response quality. In this work, we consider a fully unsupervised setting where no ground-truth (GT) reward function is available. Instead, supervision is provided exclusively through noisy pairwise comparisons between responses. As a result, existing agent-style prompt optimization methods that rely on supervised rewards are not directly applicable.\n\n\nTo address this challenge, we propose UPA, an unsupervised prompt agent guided by fine-grained and order-invariant pairwise comparisons from a judge LLM, fjudgef_{\\text{judge}} (Sec.¬†2.2). As shown in Figure¬†2, UPA decouples prompt optimization into two phases: search and selection. During the search phase, UPA explores the prompt space via a tree-based strategy to generate candidates and accumulate sparse pairwise comparison data (Sec.¬†2.3). In the selection phase, to robustly identify the optimal prompt p‚ãÜp^{\\star} from these noisy observations, UPA employs a two-stage framework grounded in the Bradley-Terry-Luce (BTL) model (Sec.¬†2.4). Specifically, path-wise Bayesian filtering is first applied to prune the search tree to the top-KK candidates (Sec.¬†2.5), followed by global BTL maximization to infer the final prompt (Sec.¬†2.6). The complete procedures for the search and selection phases are summarized in Algorithms¬†1 and¬†2.\n\n\n\n\n\nFigure 2: Overview of UPA. Our method decouples prompt optimization into two phases: search and selection. During the search phase, UPA explores the prompt space using a tree-based framework driven by relative feedback. In the selection phase, it employs a two-stage strategy: path-wise Bayesian filtering to prune the search tree, followed by global BTL maximization to infer the optimal prompt. \n\n\n\n\n2.2 Pairwise Comparison with Bias Mitigation\n\nGiven two candidate prompts pi,pj‚ààùí´p_{i},p_{j}\\in\\mathcal{P} and a query qq, the execution LLM produces responses ai=fexec‚Äã(pi,q)a_{i}=f_{\\text{exec}}(p_{i},q) and aj=fexec‚Äã(pj,q)a_{j}=f_{\\text{exec}}(p_{j},q). Subsequently, the judge LLM fjudgef_{\\text{jud"
  },
  {
    "title": "IRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models",
    "url": "https://arxiv.org/abs/2601.23266v1",
    "source": "arxiv",
    "summary": "This paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is th",
    "full_text": "\n\n\n\n\nI Introduction\n\n\nI-A Related Work\n\nI-A1 Hybrid Imitation and Reinforcement Learning\nI-A2 Inverse Reinforcement Learning for Reward Shaping\nI-A3 Adaptive Perception and Attention in Driving\nI-A4 Diffusion Models for Safe Motion Planning\n\n\nI-B Research Gap\nI-C Motivation\nI-D Contributions\n\n\nII Problem formulation\n\nIII Methodology\n\nIII-A Perception Module\nIII-B Multi-Phase Learning Curriculum\n\nIII-C Expert Data Generation via FSM-Aware Experience Replay\n\nIII-C1 Phase 1: Foundational Pre-training\nIII-C2 Phase 2: Online Fine-tuning with Adversarial Reinforcement Learning\n\n\n\nIII-D Safety Assurance via Guided Diffusion and Experience Correction\n\nIII-D1 On-Demand, Energy-Guided Trajectory Generation\nIII-D2 Action Blending and Execution\nIII-D3 Experience Correction\n\n\nIII-E Safety-Aware Experience Correction (SAEC)\n\nIII-F Policy Optimization with Safety-Aware Curriculum\n\nIII-F1 RL Backbone: PPO\n\nIII-F2 Multi-Phase Training Curriculum\n\nPhase 1 ‚Äî Imitation Pre-training:\nPhase 2 ‚Äî IRL-PPO Fine-tuning with Hybrid Reward:\n\n\n\n\nIII-G Integrated Training Pipeline\n\n\n\nIV Simulations and Results\n\nIV-A Experimental Setup\nIV-B Component-wise Evaluation of the IRL‚ÄìDAL Framework\n\n\nV Conclusion\n\n\n\n\n\nIRL-DAL: Safe and Adaptive Trajectory Planning for Autonomous Driving via Energy-Guided Diffusion Models\n\n\n\nSeyed Ahmad Hosseini Miangoleh\n\n\n 1,\nAmin Jalal Aghdasian\n\n\n 1,\nFarzaneh Abdollahi\n\n\n 1\n1Department of Electrical Engineering, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran\n\n\n\nAbstract\nThis paper proposes a novel inverse reinforcement learning framework using a diffusion-based adaptive lookahead planner (IRL-DAL) for autonomous vehicles. Training begins with imitation from an expert finite state machine (FSM) controller to provide a stable initialization. Environment terms are combined with an IRL discriminator signal to align with expert goals. Reinforcement learning (RL) is then performed with a hybrid reward that combines diffuse environmental feedback and targeted IRL rewards. A conditional diffusion model, which acts as a safety supervisor, plans safe paths. It stays in its lane, avoids obstacles, and moves smoothly. Then, a learnable adaptive mask (LAM) improves perception. It shifts visual attention based on vehicle speed and nearby hazards. After FSM-based imitation, the policy is fine-tuned with Proximal Policy Optimization (PPO). Training is run in the Webots simulator with a two-stage curriculum. A 96% success rate is reached, and collisions are reduced to 0.05 per 1k steps, marking a new benchmark for safe navigation. By applying the proposed approach, the agent not only drives in lane but also handles unsafe conditions at an expert level, increasing robustness.We make our code publicly available111Paper Website\n\n\n\nI Introduction\n\n\nThe main challenge in autonomous vehicles is building systems that can operate at human levels of safety and reliability in highly dynamic environments [crosato2024social, kiran2021deep]. This challenge mainly comes from the need to avoid obstacles in a strong and reliable way. Obstacle avoidance is the key safety task that shows whether a system can work in the real world. Even small mistakes in rare situations can cause very serious failures. This means the agent must remain safe even when encountering situations it was not trained on [ross2011reduction]. To address these challenges, recent research has investigated a range of advanced methods. These approaches aim to enhance the safety, reliability, and adaptability of autonomous systems operating in dynamic environments.\n\n\n\nI-A Related Work\n\n\nThis work falls within the overlap of four main areas: hybrid learning, reward inference, generative planning, and adaptive perception. The following section reviews some recent studies in each of these areas.\n\n\n\nI-A1 Hybrid Imitation and Reinforcement Learning\n\nImitation Learning (IL), especially Behavioral Cloning (BC), is widely used in autonomous driving [cui2023safe]. It provides a data-efficient way to learn a mapping from expert demonstrations to control actions using supervised learning [li2022driver, wang2024implicit]. The primary advantage of BC is its computational efficiency and its requirement for no explicit knowledge of the underlying environment dynamics. Within BC, methods such as Conditional Imitation Learning improve performance by conditioning the policy on high-level commands. This helps address the challenge of having many possible correct actions for a given situation [eraqi2022dynamic]. Even though it is efficient, IL has a problem called covariate shift. In this case, small mistakes grow over time when the agent reaches states that were not in the training data. Unlike traditional methods, RL can develop strong and resilient behaviors by learning from trial-and-error experience. However, it usually needs a lot of data and depends on hand-designed reward signals that can be unreliable [yuan2025model].\n\n\nThe hybrid method combines IL and RL. This lets the self-driving car first learn basic behavior from examples and then improve it through trial, feedback, and optimization [radwan2021obstacles]. In [mahmoudi2023reinforcement], a hybrid method is used that combines BC with the PPO algorithm in the Unity Agents environment. This setup trains racecar agents to drive and avoid obstacles. These combinations capitalize on expert supervision for fast convergence while allowing the agent to explore recovery and adaptation strategies [pinto2021curriculum]. In [lu2023imitation], the BC-SAC model fuses BC and Soft Actor-Critic to enhance policy robustness and safety in autonomous driving. By employing supervised imitation and reinforcement optimization, one achieves higher generalization and a 38% reduction in failure rate across complex real-world scenarios.\n\n\n\n\nI-A2 Inverse Reinforcement Learning for Reward Shaping\n\nThe problem of designing a good reward for complex tasks can be handled with Inverse Reinforcement Learning (IRL). IRL tries to find the hidden reward rules directly from expert demonstrations [zhao2024survey, lanzaro2025evaluating]. The Conditional Predictive Behavior Planning model combines Conditional Motion Prediction and Maximum Entropy IRL to make driving more like a human. It predicts how nearby cars will react to each possible move and scores these moves using expert driving data [huang2023conditional]. Adversarial inverse reinforcement learning (AIRL) uses a GAN to learn the reward and the driving policy at the same time, which helps the agent adapt to new environments. Safety-aware AIRL then adds safety rules to block risky actions and lower the chance of crashes [wang2021decision].\n\n\n\n\nI-A3 Adaptive Perception and Attention in Driving\n\nWith the growing use of attention mechanisms in deep learning [lu2024epitester, xi2023ema], these methods have been added to end-to-end driving models. They help the system focus more on key visual elements such as vehicles, pedestrians, and road signs [wang2024pedestrian]. Effective driving requires perception systems that can adapt attention dynamically to changing contexts. In [schwonberg2023survey], it uses spatiotemporal, uncertainty-aware attention over multimodal sensors with crossmodal alignment and multiscale fusion to prioritize important actors and regions for downstream planning. In [chi2024dynamic], it adds a temporal residual block, multiscale feature fusion, and global plus double attention to use time and image cues better. In [ma2024adaptive], it combines adaptive channel attention and grouped spatial attention with channel shuffle to highlight important features. It plugs into standard CNNs, can replace a 3√ó3 convolution layer, and improves accuracy with little extra compute.\n\n\n\n\nI-A4 Diffusion Models for Safe Motion Planning\n\nDiffusion models [peng2025diffusion] are now leading tools for generating data. They are also being used more and more for planning tasks. When generating trajectories, they can produce diverse future motions that still obey real-world physics [wang2024trajectory]. Their key advantage is their flexibility. By guiding the backward diffusion steps with extra rules or helper models, they can add safety rules such as avoiding crashes and keeping motions that the vehicle can really follow [pearce2023plannable_journal, chi2023diffusion]. Current methods use diffusion models as direct policy models, mainly in offline reinforcement learning settings [wang2023diffusion]. Moreover, some methods use stand-alone planners that first produce open-loop trajectories. A separate controller then follows these trajectories [janner2022planning].\n\n\n\n\n\nI-B Research Gap\n\n\nDespite progress in each area, a key gap appears at their intersection. Current stacks rarely combine generative planning, reward inference, and online policy learning into one system. This causes three main limits:\n\n\n\n\n1.\n\nLack of an end-to-end unified loop: Most diffusion-based planners generate trajectories open loop and are executed separately from the RL policy, creating a distribution mismatch between planned motions and closed-loop control. This separation weakens robustness under disturbances and hinders consistent transfer from trajectory proposals to joint steering‚Äìspeed commands.\n\n\n\n2.\n\nNon-adaptive safety trade-offs: Fixed cost weights for lane keeping, collision avoidance, and stability cannot re-balance as scene risk and sensor uncertainty change. Without real-time, perception-conditioned guidance, systems oscillate between over-conservative and overly aggressive behavior and fail to maintain safety while preserving efficiency.\n\n\n\n3.\n\nInefficient learning signals: Reliance on sparse, hand-crafted rewards leads to sample-hungry training, unstable convergence, and sub-expert driving. The absence of dense rewards inferred from demonstrations and a staged curriculum limits generalization to out-of-distribution states and degrades control smoothness.\n\n\n\n\n\n\n\nI-C Motivation\n\n\nReliable autonomous driving must joint"
  },
  {
    "title": "PaperBanana: Automating Academic Illustration for AI Scientists",
    "url": "https://arxiv.org/abs/2601.23265v1",
    "source": "arxiv",
    "summary": "Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orc",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2601.23265v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2601.23265v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 30 Jan 2026]\n    Title:PaperBanana: Automating Academic Illustration for AI Scientists\n    Authors:Dawei Zhu, Rui Meng, Yale Song, Xiyu Wei, Sujian Li, Tomas Pfister, Jinsung Yoon            View a PDF of the paper titled PaperBanana: Automating Academic Illustration for AI Scientists, by Dawei Zhu and 6 other authors\n    View PDF\n\n\n\n    \n            Abstract:Despite rapid advances in autonomous AI scientists powered by language models, generating publication-ready illustrations remains a labor-intensive bottleneck in the research workflow. To lift this burden, we introduce PaperBanana, an agentic framework for automated generation of publication-ready academic illustrations. Powered by state-of-the-art VLMs and image generation models, PaperBanana orchestrates specialized agents to retrieve references, plan content and style, render images, and iteratively refine via self-critique. To rigorously evaluate our framework, we introduce PaperBananaBench, comprising 292 test cases for methodology diagrams curated from NeurIPS 2025 publications, covering diverse research domains and illustration styles. Comprehensive experiments demonstrate that PaperBanana consistently outperforms leading baselines in faithfulness, conciseness, readability, and aesthetics. We further show that our method effectively extends to the generation of high-quality statistical plots. Collectively, PaperBanana paves the way for the automated generation of publication-ready illustrations.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)\n        \n          Cite as:\n          arXiv:2601.23265 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2601.23265v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2601.23265\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Dawei Zhu [view email]          [v1]\n        Fri, 30 Jan 2026 18:33:37 UTC (40,139 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled PaperBanana: Automating Academic Illustration for AI Scientists, by Dawei Zhu and 6 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-01\n  \n    Change to browse by:\n    \n        cs\n        cs.CV\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n            \n              \n                \n                  contact arXivClick here to contact arXiv\n                   Co"
  },
  {
    "title": "Particle-Guided Diffusion Models for Partial Differential Equations",
    "url": "https://arxiv.org/abs/2601.23262v1",
    "source": "arxiv",
    "summary": "We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Ac",
    "full_text": null
  },
  {
    "title": "TEON: Tensorized Orthonormalization Beyond Layer-Wise Muon for Large Language Model Pre-Training",
    "url": "https://arxiv.org/abs/2601.23261v1",
    "source": "arxiv",
    "summary": "The Muon optimizer has demonstrated strong empirical performance in pre-training large language models by performing matrix-level gradient (or momentum) orthogonalization in each layer independently. In this work, we propose TEON, a principled generalization of Muon that extends orthogonalization beyond individual layers by modeling the gradients of a neural network as a structured higher-order te",
    "full_text": null
  },
  {
    "title": "Agnostic Language Identification and Generation",
    "url": "https://arxiv.org/abs/2601.23258v1",
    "source": "arxiv",
    "summary": "Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impos",
    "full_text": null
  },
  {
    "title": "Now You Hear Me: Audio Narrative Attacks Against Large Audio-Language Models",
    "url": "https://arxiv.org/abs/2601.23255v1",
    "source": "arxiv",
    "summary": "Large audio-language models increasingly operate on raw speech inputs, enabling more seamless integration across domains such as voice assistants, education, and clinical triage. This transition, however, introduces a distinct class of vulnerabilities that remain largely uncharacterized. We examine the security implications of this modality shift by designing a text-to-audio jailbreak that embeds ",
    "full_text": null
  },
  {
    "title": "Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models",
    "url": "https://arxiv.org/abs/2601.23253v1",
    "source": "arxiv",
    "summary": "Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Method\n\n2.1 Dynamic Pseudo-Labeling with Multimodal-Assisted Clustering\n2.2 BDC for V-V Inference\n2.3 Attribute-assisted prompting for V-L Inference\n2.4 Inference Fusion\n\n\n\n3 Experiments\n\n3.1 Benchmark Settings\n3.2 Performance Analysis\n3.3 Ablation study\n\n\n4 Conclusion\n\n\n\n\n\nTraining-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models\n\nAbstract\nVision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities.\nTo address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance‚Äîa powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances‚Äîto dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates.\nTaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.\n\n\nIndex Terms‚Äî‚Äâ\nVision-Language Models, Test-Time Adaptation, Brownian Distance Covariance\n\n\n\n1 Introduction\n\nVision-language models (VLMs) such as Align¬†[5] and CLIP¬†[11] learn aligned multimodal representations from large-scale image-text data, enabling effective zero-shot inference by comparing image and text embeddings in a shared space¬†[9, 7, 3]. However, their performance degrades under significant domain or distribution shifts. Test-time adaptation (TTA) addresses this by adapting models using only unlabeled test data, without accessing source data or modifying training¬†[12, 1]. This approach aligns well with practical scenarios where only the trained model is available, without access to the source data or authorization to alter the original training procedure. In such cases, models need to quickly adapt to new tasks.\n\n\n\nFig. 1: \nThe process involves using Pseudo-Labeling with Multimodal-Assisted Clustering to categorize nouns into semantic centers and create a discriminative text space. Nouns for each image are matched in this space, and by concatenating image and text features, K-means clustering forms centroids and clusters.\n\n\n\nRecent prompt-based TTA methods such as TPT¬†[12] and DiffTPT¬†[4] improve VLM adaptation but require computationally expensive optimization¬†[15].\nTDA¬†[6] avoids backpropagation and offers a training-free alternative, but has two key limitations: 1) TDA uses cosine similarity to measure the similarity between test image features and those in caches. This method only considers marginal distributions and linear relationships, potentially missing complex, non-linear dependencies in high-dimensional features. 2) TDA neglects the importance of visual attributes in prompts, relying solely on class labels or simple descriptions. This oversight limits the ability to capture the full semantic richness of images, resulting in less accurate and nuanced classifications, especially in complex or ambiguous scenarios.\n\n\nTo address these issues, we propose Training-free TTA method with Brownian Distance Covariance for VLMs. Brownian distance covariance can capture more comprehensive statistics by considering joint distribution. It effectively models both linear and non-linear relationships between query and support images by measuring the discrepancy between the joint distribution of their features and the product of their marginals.\nThe final prediction in our proposed TaTa is a combination of vision-vision and vision-language inference. For vision‚Äìvision inference, we use K-means with multimodal clues to cluster test images and assign pseudo-labels, as shown in Figure¬†1. BDC measures similarity between test features and pseudo-labeled centroids, capturing both linear and non-linear dependencies.\nFor vision‚Äìlanguage inference, we enrich prompts with visual attributes to improve semantic alignment.\nFinally, to address the prediction bias, we propose a soft-voting strategy that incorporates knowledge from nearest neighbor samples.\n\n\nIn summary, our contribution can be summarized as: 1) We present a streamlined and robust method for adapting vision-language models during testing without requiring additional training. 2) We introduce pseudo labeling with a dynamic clustering with multimodality, and adopt brownian distance covariance as the metrics for vision-vision inference, and we exploit visual attributes description words to enhance the prompting for vision-language inference. To mitigate pseudo-labels bias, we introduce a soft-voting strategy. 3) Extensive experimental results show that TaTa significantly surpasses existing state-of-the-art methods in both domain generalization and cross-dataset generalization tasks.\n\n\n\n\n2 Method\n\n\nFig. 2: An overview of our proposed TaTa. The final inference in TaTa combines Vision-Vision and Vision-Language Inference.\n\n\n\nMethod Overview. \nFigure¬†2 illustrates our proposed TaTa framework for test-time adaptation, which enhances prediction by leveraging knowledge from the test stream and adapting image features. The final inference combines Vision‚ÄìVision(V-V) and Vision‚ÄìLanguage(V-L) pathways.\nFor a test image, we extract visual features using EvE_{v} and combine them with textual analogs to form a multimodal feature fmf_{m}. In Vision‚ÄìVision Inference, we compute Brownian Distance Covariance (BDC) between fmf_{m} and pseudo-labeled class prototypes derived from clustering. Simultaneously, for Vision‚ÄìLanguage Inference, we construct attribute-enhanced prompts and compute cosine similarity between image features and corresponding text embeddings.\nA dynamic dictionary ùíü\\mathcal{D} is maintained, mapping class names to clusters. Correctly classified test samples update ùíü\\mathcal{D} with their multimodal features, continuously refining class centroids and reducing pseudo-label bias. A soft-voting mechanism incorporating nearest-neighbor knowledge further improves prediction robustness.\n\n\n\n2.1 Dynamic Pseudo-Labeling with Multimodal-Assisted Clustering\n\nA dynamic dictionary ùíü\\mathcal{D} stores class labels as keys and corresponding clusters as values. When a test image is correctly classified, its multimodal feature (concatenated image and text representations) is incorporated into the corresponding class in ùíü\\mathcal{D}, updating the clusters and centroids.\n\n\nDynamic Multimodal-Assisted Clustering:\nTo construct a discriminative textual space without known test class names, we use a subset of WordNet nouns¬†[10]. As shown in Figure¬†1. For a dataset with NN classes, visual features extracted by EvE_{v} are first clustered via K-Means to obtain initial centroids C={Ci}i=1NC=\\{C_{i}\\}_{i=1}^{N}. Using CLIP, we assign each WordNet noun to the closest semantic center based on similarity: p‚Äã(y=i‚à£ùêìùê§)=exp‚Å°(sim‚Å°(ftk,Ci))‚àëj=1Nexp‚Å°(sim‚Å°(ftk,Cj)),p(y=i\\mid\\mathbf{T_{k}})=\\frac{\\exp(\\operatorname{sim}(f_{t_{k}},C_{i}))}{\\sum_{j=1}^{N}\\exp(\\operatorname{sim}(f_{t_{k}},C_{j}))},\nwhere ùêìùê§\\mathbf{T_{k}} is a text prompt such as ‚Äúa photo of noun‚Äù and ftkf_{t_{k}} is encoded by EtE_{t}. Top-k1k_{1} nouns per center are selected to form a representative textual set {T¬Øm}m=1H\\{\\bar{T}_{m}\\}_{m=1}^{H} where H=N√ók1H=N\\times k_{1} (k1=5k_{1}=5 empirically).\n\n\nFor each image feature fvhf_{v_{h}} of image xhx_{h}, we compute its textual analog fthf_{t_{h}} by aggregating noun embeddings {f¬Øth}h=1H\\{\\bar{f}_{t_{h}}\\}_{h=1}^{H} with similarity-weighted coefficients:\nfth=‚àëj=1Hp‚Äã(f¬Øtj‚à£fvh)‚Äãf¬Øtj,f_{t_{h}}=\\sum_{j=1}^{H}p(\\bar{f}_{t_{j}}\\mid f_{v_{h}})\\bar{f}_{t_{j}},\np‚Äã(f¬Øtj‚à£fvh)=exp‚Å°(sim‚Å°(fvh,f¬Øtj)/œÑ~)‚àël=1Hexp‚Å°(sim‚Å°(fvh,f¬Øtl)/œÑ~),p(\\bar{f}_{t_{j}}\\mid f_{v_{h}})=\\frac{\\exp(\\operatorname{sim}(f_{v_{h}},\\bar{f}_{t_{j}})/\\tilde{\\tau})}{\\sum_{l=1}^{H}\\exp(\\operatorname{sim}(f_{v_{h}},\\bar{f}_{t_{l}})/\\tilde{\\tau})},\nwhere œÑ~=0.005\\tilde{\\tau}=0.005 (following TAC¬†[8]). Finally, K-Means is applied to the concatenated features [ftd,fvd]d=1D[f_{t_{d}},f_{v_{d}}]_{d=1}^{D} to obtain refined centroids C¬Ø={C¬Øi}i=1N\\bar{C}=\\{\\bar{C}_{i}\\}_{i=1}^{N}.\n\n\nPseudo Labeling: \nText features are generated from class-based prompts using EtE_{t}. Pseudo-labels are assigned to each centroid C¬Øi\\bar{C}_{i} via zero-shot similarity comparison:\np‚Äã(y=m‚à£C¬Øi)=exp‚Å°(sim‚Å°(ftm,C¬Øi)/œÑ)‚àëj=1Nexp‚Å°(sim‚Å°(ftj,C¬Øi)/œÑ),p(y=m\\mid\\bar{C}_{i})=\\frac{\\exp(\\operatorname{sim}(f_{t_{m}},\\bar{C}_{i})/\\tau)}{\\sum_{j=1}^{N}\\exp(\\operatorname{sim}(f_{t_{j}},\\bar{C}_{i})/\\tau)},\nwhere œÑ\\tau is the softmax temperature. The resulting multimodal prototypes C¬Ø\\bar{C} serve as class representatives for inference.\n\n\n\n\n2.2 BDC for V-V Inference\n\nWe design a Brownian Distance Covariance (BDC) module ‚Ñ¨‚Äã(x)\\mathcal{B}(x) to capture both linear and nonlinear dependencies between feature representations. BDC measures dependence via distance covariance, based on pairwise distances between sampless¬†[13, 14].\nGiven two random vectors ùêó\\mathbf{X} and ùêò\\mathbf{Y}, the BDC is computed as follows:\n1. Compute Euclidean distance matrices:\nai‚Äãj=‚Äñùêói‚àíùêój‚Äñ2a_{ij}=\\|\\mathbf{X}_{i}-\\mathbf{X}_{j}\\|_{2}, bi‚Äãj=‚Äñùêòi‚àíùêòj‚Äñ2b_{ij}=\\|\\mathbf{Y}_{i}-\\mathbf{Y}_{j}\\|_{2}\n2. Center the distance matrices:\nA¬Øi‚Äãj=ai‚Äãj‚àí1n‚Äã‚àëkai‚Äãk‚àí1n‚Äã‚àëlal‚Äãj+1n2‚Äã‚àëk,lak‚Äãl\\bar{A}_{ij}=a_{ij}-\\frac{1}{n}\\sum_{k}a_{ik}-\\frac{1}{n}\\sum_{l}a_{lj}+\\frac{1}{n^{2}}\\sum_{k,l}a_{kl}\n(similarly for B¬Øi‚Äãj\\bar{B}_{ij})\n3. Calculate the distance covariance:\ndCov2‚Äã(ùêó,ùêò)=1n2‚Äã‚àëi,jA¬Øi‚Äãj‚ÄãB¬Øi‚Äãj\\text{dCov}^{2}(\\mathbf{X},\\mathbf{Y})=\\frac{1}{n^{2}}\\sum_{i,j}\\bar{A}_{ij}\\bar{B}_{ij}.\nBDC is non-zero only if features are dependent, capturing bot"
  },
  {
    "title": "Nested Slice Sampling: Vectorized Nested Sampling for GPU-Accelerated Inference",
    "url": "https://arxiv.org/abs/2601.23252v1",
    "source": "arxiv",
    "summary": "Model comparison and calibrated uncertainty quantification often require integrating over parameters, but scalable inference can be challenging for complex, multimodal targets. Nested Sampling is a robust alternative to standard MCMC, yet its typically sequential structure and hard constraints make efficient accelerator implementations difficult. This paper introduces Nested Slice Sampling (NSS), ",
    "full_text": null
  },
  {
    "title": "Graph Attention Network for Node Regression on Random Geometric Graphs with Erd≈ës--R√©nyi contamination",
    "url": "https://arxiv.org/abs/2601.23239v1",
    "source": "arxiv",
    "summary": "Graph attention networks (GATs) are widely used and often appear robust to noise in node covariates and edges, yet rigorous statistical guarantees demonstrating a provable advantage of GATs over non-attention graph neural networks~(GNNs) are scarce. We partially address this gap for node regression with graph-based errors-in-variables models under simultaneous covariate and edge corruption: respon",
    "full_text": null
  },
  {
    "title": "How well do generative models solve inverse problems? A benchmark study",
    "url": "https://arxiv.org/abs/2601.23238v1",
    "source": "arxiv",
    "summary": "Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art ge",
    "full_text": null
  },
  {
    "title": "YuriiFormer: A Suite of Nesterov-Accelerated Transformers",
    "url": "https://arxiv.org/abs/2601.23236v1",
    "source": "arxiv",
    "summary": "We propose a variational framework that interprets transformer layers as iterations of an optimization algorithm acting on token embeddings. In this view, self-attention implements a gradient step of an interaction energy, while MLP layers correspond to gradient updates of a potential energy. Standard GPT-style transformers emerge as vanilla gradient descent on the resulting composite objective, i",
    "full_text": null
  },
  {
    "title": "Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph",
    "url": "https://arxiv.org/abs/2601.23233v1",
    "source": "arxiv",
    "summary": "Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequent",
    "full_text": null
  },
  {
    "title": "ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search",
    "url": "https://arxiv.org/abs/2601.23232v1",
    "source": "arxiv",
    "summary": "In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes e",
    "full_text": null
  },
  {
    "title": "Solving Inverse Problems with Flow-based Models via Model Predictive Control",
    "url": "https://arxiv.org/abs/2601.23231v1",
    "source": "arxiv",
    "summary": "Flow-based generative models provide strong unconditional priors for inverse problems, but guiding their dynamics for conditional generation remains challenging. Recent work casts training-free conditional generation in flow models as an optimal control problem; however, solving the resulting trajectory optimisation is computationally and memory intensive, requiring differentiation through the flo",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Flow-based Models\n2.2 Optimal Control in Flow Models\n\n\n\n3 MPC-Flow\n\n\n3.1 Receding-Horizon (RHC)\n\nSingle-Step RHC (K=1).\n\n\n3.2 Œî‚Äãt\\Delta t-Horizon Control\n\n\n\n4 Experiments\n\n4.1 Computed Tomography\n4.2 Image Restoration\n4.3 Large Vision Models\n\n\n5 Conclusion &amp; Further Work\n\nA Related Work\n\nContinuous Normalising Flows\nGuidance for Continuous-time Generative Models\nOptimal Control in Flow Models\n\n\n\nB Proofs\n\nB.1 Existence of Optimal Control\n\nB.2 Optimality of MPC\n\nB.2.1 Proof of Theorem 3.1\nB.2.2 Proof of Theorem 3.2\n\n\n\n\nC Validating optimality guarantees with a toy example\n\nD Computed Tomography\n\nMPC-Œî‚Äãt\\Delta t\n\n\n\nE Image Restoration\n\nE.1 MPC-RHC K=1K=1 for Linear Inverse Problems\n\n\n\nF Image generation with Flux.2\n\nStyle loss explicit definition\nHyperparameter settings\nModel origin\nMemory usage\nDataset construction\n\n\nG Interpretation as a Regulariser for Inverse Problems\n\n\n\n\n\nSolving Inverse Problems with Flow-based Models via Model Predictive Control\n\n\n\nGeorge Webber\n\n‚ÄÉ‚ÄÉ\nAlexander Denker\n\n‚ÄÉ‚ÄÉ\nRiccardo Barbano\n\n‚ÄÉ‚ÄÉ\nAndrew J Reader\n\n\n\nAbstract\nFlow-based generative models provide strong unconditional priors for inverse problems, but guiding their dynamics for conditional generation remains challenging.\nRecent work casts training-free conditional generation in flow models as an optimal control problem; however, solving the resulting trajectory optimisation is computationally and memory intensive, requiring differentiation through the flow dynamics or adjoint solves.\nWe propose MPC-Flow, a model predictive control framework that formulates inverse problem solving with flow-based generative models as a sequence of control sub-problems, enabling practical optimal control-based guidance at inference time.\nWe provide theoretical guarantees linking MPC-Flow to the underlying optimal control objective and show how different algorithmic choices yield a spectrum of guidance algorithms, including regimes that avoid backpropagation through the generative model trajectory.\nWe evaluate MPC-Flow on benchmark image restoration tasks, spanning linear and non-linear settings such as in-painting, deblurring, and super-resolution, and demonstrate strong performance and scalability to massive state-of-the-art architectures via training-free guidance of FLUX.2 (32B) in a quantised setting on consumer hardware.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nContinuous normalising flows (Chen et¬†al., 2018), particularly those trained via flow matching (Lipman et¬†al., 2022), have become an established and powerful framework for generating high-dimensional data (Esser et¬†al., 2024; Wang et¬†al., 2025).\nThese models define a continuous-time transformation through an ordinary differential equation (ODE) that maps samples from a simple base distribution to complex data distributions.\nBeyond unconditional generation, there is significant interest in leveraging the learned dynamics of these models to solve inverse problems.\nHere, the goal is to recover a signal ùíô‚àà‚Ñùd{\\bm{x}}\\in{\\mathbb{R}}^{d} from noisy measurements\n\n\n\nùíö=ùíú‚Äã(ùíô)+œµ,{\\bm{y}}=\\mathcal{A}({\\bm{x}})+{\\bm{\\epsilon}},\n\n(1)\n\n\nwith ùíú\\mathcal{A} a (possibly non-linear) forward operator and œµ{\\bm{\\epsilon}} denoting additive Gaussian noise, i.e., œµ‚àºùí©‚Äã(0,œÉ2‚ÄãI){\\bm{\\epsilon}}\\sim\\mathcal{N}(0,\\sigma^{2}I).\n\n\nRecent work has explored training-free guidance methods that adapt pre-trained flow models to conditioning constraints at inference time (Kim et¬†al., 2025; Pokle et¬†al., 2023; Zhang et¬†al., 2024).\nThese approaches incorporate data fidelity terms or task-specific loss functions directly into the generative dynamics, steering the flow towards regions that satisfy the desired constraints without retraining the base model.\nSuch approaches are often motivated by the Bayesian perspective on inverse problems (Stuart, 2010), where the prior pdata‚Äã(ùíô)p_{\\text{data}}({\\bm{x}}), given by the pre-trained flow model, is combined with likelihood p‚Äã(ùíö|ùíô)p({\\bm{y}}|{\\bm{x}}).\nHowever, these heuristic guidance strategies have limitations (Chung et¬†al., 2022; Patel et¬†al., 2024; Barbano et¬†al., 2025). They often lack theoretical guarantees, e.g., regarding consistency to the measurements, and can be computationally unstable.\n\n\nAs an alternative, Liu et¬†al. (2023); Wang et¬†al. (2024b) recast conditional sampling in continuous-time flow models as a deterministic optimal control problem.\nWhile the optimal control perspective provides a principled objective and improved trade-offs between data fidelity and prior consistency, solving the resulting trajectory optimisation requires differentiating through the sampling dynamics, leading to prohibitive computational and memory costs, or costly evaluations of the adjoint equations.\n\n\nWe propose MPC-Flow, a model predictive control (MPC) framework (Garcia et¬†al., 1989; Rawlings et¬†al., 2020) that makes optimal control-based guidance practical at inference time.\nRather than optimising the entire trajectory at once, as in classical optimal control, MPC-Flow decomposes the control into a sequence of short-horizon sub-problems.\nAt each time step, a local control problem is solved, the resulting control is applied, and this process is repeated in a receding-horizon fashion.\nOur proposed framework allows for a control strategy that significantly reduces memory requirements by decoupling control optimisation from the full flow trajectory, while improving robustness through iterative re-planning that corrects errors accumulated at earlier time steps.\nMoreover, different choices of horizon length, control parameterisation, and terminal cost function give rise to a range of guidance algorithms within this unified framework, including an efficient and principled variant that avoids backpropagation through the generative trajectory.\n\n\n\nWe validate the proposed framework empirically across a range of controlled and large-scale settings.\nWe first analyse MPC-Flow in controlled settings, including a two-dimensional toy example and a computed tomography task on the OrganCMNIST dataset, enabling detailed ablations and direct comparison with globally optimal control solutions.\nWe then evaluate MPC-Flow on a range of linear and non-linear image restoration inverse problems on the CelebA dataset, and finally demonstrate scalability to modern large-scale rectified flow models.\n\n\nTo the best of our knowledge, MPC-Flow is the first conditional generation method explicitly rooted in optimal control that scales to such settings, enabling training-free guidance under tight memory constraints and outperforming existing scalable approaches, including guidance of FLUX.2 (32B) (Labs, 2025) in a quantised setting on consumer hardware.\n\n\n\n\n2 Background\n\n\n2.1 Flow-based Models\n\nFlow Matching (FM) (Lipman et¬†al., 2022) is a framework for training continuous normalising flows using a simulation-free objective.\nThe goal is to learn a time-dependent vector field vŒ∏:‚Ñùd√ó[0,1]‚Üí‚Ñùdv_{\\mathbf{\\theta}}:\\mathbb{R}^{d}\\times[0,1]\\to\\mathbb{R}^{d} that induces a probability path ptp_{t} transforming a simple base distribution p0p_{0} (e.g., a standard Gaussian) into a data distribution pdata:=p1p_{\\text{data}}:=p_{1}.\nSample generation is performed by solving the ODE\n\n\n\nd‚Äãùíô‚Äã(t)d‚Äãt=vŒ∏‚Äã(ùíô‚Äã(t),t),ùíô‚Äã(0)‚àºp0.\\displaystyle\\frac{d{\\bm{x}}(t)}{dt}=v_{\\mathbf{\\theta}}({\\bm{x}}(t),t),\\qquad{\\bm{x}}(0)\\sim p_{0}.\n\n(2)\n\n\n\n\nTraining proceeds by regressing vŒ∏v_{\\mathbf{\\theta}} toward a ground-truth velocity field utu_{t} that generates the desired probability path ptp_{t}, by minimising the flow matching objective\n\n\n\n‚ÑíFM‚Äã(Œ∏)=ùîºt,ùíô‚àºpt‚Äã‚ÄñvŒ∏‚Äã(ùíô,t)‚àíut‚Äã(ùíô)‚Äñ2.\\mathcal{L}_{\\text{FM}}(\\mathbf{\\theta})=\\mathbb{E}_{t,\\,{\\bm{x}}\\sim p_{t}}\\bigl\\|v_{\\mathbf{\\theta}}({\\bm{x}},t)-u_{t}({\\bm{x}})\\bigr\\|^{2}.\n\n\n\nHowever, the marginal velocity field utu_{t} is generally intractable to compute, as it depends on the unknown intermediate density ptp_{t} and its score for complex high-dimensional data distributions.\nTo address this intractability, FM adopts a conditional formulation.\nConditional Flow Matching trains the model to match a conditional vector field\nut‚Äã(ùíô‚à£ùíô0,ùíô1)u_{t}({\\bm{x}}\\mid{\\bm{x}}_{0},{\\bm{x}}_{1}), with (ùíô0,ùíô1)‚àºp0‚Äãp1({\\bm{x}}_{0},{\\bm{x}}_{1})\\sim p_{0}p_{1}, by minimising\n\n\n\n‚ÑíCFM(Œ∏)=ùîºt,ùíô0,ùíô1,ùíô‚à•vŒ∏(ùíô,t)‚àíut(ùíô|ùíô0,ùíô1)‚à•2,\\displaystyle\\mathcal{L}_{\\text{CFM}}(\\mathbf{\\theta})=\\mathbb{E}_{t,\\,{\\bm{x}}_{0},\\,{\\bm{x}}_{1},\\,{\\bm{x}}}\\bigl\\|v_{\\mathbf{\\theta}}({\\bm{x}},t)-u_{t}({\\bm{x}}|{\\bm{x}}_{0},{\\bm{x}}_{1})\\bigr\\|^{2},\n\n(3)\n\n\nCrucially, it has been shown that ‚àáŒ∏‚ÑíCFM‚Äã(Œ∏)=‚àáŒ∏‚ÑíFM‚Äã(Œ∏)\\nabla_{\\mathbf{\\theta}}\\mathcal{L}_{\\text{CFM}}(\\mathbf{\\theta})=\\nabla_{\\mathbf{\\theta}}\\mathcal{L}_{\\text{FM}}(\\mathbf{\\theta}) (Lipman et¬†al., 2022; Tong et¬†al., 2023).\nIn practice, the conditional probability path is often defined via an affine interpolation between a source sample ùíô0‚àºp0{\\bm{x}}_{0}\\sim p_{0} and a target data point ùíô1‚àºp1{\\bm{x}}_{1}\\sim p_{1}.\nA common choice is the linear interpolation used in rectified flows (Liu, 2022),\nùíôt=(1‚àít)‚Äãùíô0+t‚Äãùíô1{\\bm{x}}_{t}=(1-t){\\bm{x}}_{0}+t{\\bm{x}}_{1}.\nThis induces a simple conditional flow with velocity\nut‚Äã(ùíô|ùíô0,ùíô1)=ùíô1‚àíùíô0u_{t}({\\bm{x}}|{\\bm{x}}_{0},{\\bm{x}}_{1})={\\bm{x}}_{1}-{\\bm{x}}_{0}.\nSubstituting this into (3) yields the simplified training objective\n\n\n\n‚Ñí‚Äã(Œ∏)=ùîºt,ùíô0,ùíô1‚Äã‚ÄñvŒ∏‚Äã(ùíôt,t)‚àí(ùíô1‚àíùíô0)‚Äñ2.\\displaystyle\\mathcal{L}(\\mathbf{\\theta})=\\mathbb{E}_{t,\\,{\\bm{x}}_{0},\\,{\\bm{x}}_{1}}\\bigl\\|v_{\\mathbf{\\theta}}({\\bm{x}}_{t},t)-({\\bm{x}}_{1}-{\\bm{x}}_{0})\\bigr\\|^{2}.\n\n(4)\n\n\nAt inference time, the learned vector field vŒ∏v_{\\mathbf{\\theta}} is integrated according to (2) to generate samples from the model.\n\n\nFigure 1: MPC-Flow strategy for guiding flow-based generative models toward a target objective. Starting from the current state, MPC-Flow plans a sequence of velocity adjustments that steer the flow toward the objective while keeping the intervention small (A). Only the initial part of the plan is applied (B), after which we re-plan from the new state and repeat the proces"
  },
  {
    "title": "Strongly Polynomial Time Complexity of Policy Iteration for $L_\\infty$ Robust MDPs",
    "url": "https://arxiv.org/abs/2601.23229v1",
    "source": "arxiv",
    "summary": "Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and",
    "full_text": null
  },
  {
    "title": "Scaling Multiagent Systems with Process Rewards",
    "url": "https://arxiv.org/abs/2601.23228v1",
    "source": "arxiv",
    "summary": "While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigni",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Methodology\n\n2.1 Problem Setting\n2.2 Process Rewards from AI Feedback\n2.3 Training Algorithm\n2.4 Scalable Distributed Training Architecture\n\n\n\n3 Results\n\n\n3.1 MathChat: Competition Math with Code Execution\n\nTask and Dataset.\nMultiagent Configuration.\nTraining Configuration.\nResults.\n\n\n\n3.2 DSBench: Data Science Pipelines\n\nTask and Dataset.\nMultiagent Configuration.\nTraining Configuration.\nResults.\n\n\n\n\n\n4 Discussion\n\n4.1 Investigating Specialization\n4.2 Coach Model Choice\n4.3 Comparison with Prior Work\n4.4 Future Directions\n\n\n5 Conclusion\n\nA Extended Experimental Results\n\nA.1 MathChat: Partial Information Results\nA.2 DSBench: Extended Quality Metrics\n\n\n\nB MathChat Agent Prompts\n\nB.1 Problem Solver Prompt\nB.2 Code Executor Prompt\nB.3 Verifier Prompt\nB.4 Coach Evaluation Prompt\n\n\n\nC DSBench Agent Prompts\n\nC.1 Data Engineer Prompt\nC.2 Modeler Prompt\nC.3 Analyst Prompt\nC.4 Coach Evaluation Prompt\n\n\n\nD Example Coach Evaluations\n\nD.1 MathChat: Problem Solver Evaluation\nD.2 DSBench: Data Engineer Evaluation\nD.3 MathChat: Strategy vs. Implementation\nD.4 DSBench: Analyst Pipeline Failure\n\n\n\nE Training Algorithm Details\n\nE.1 Why REINFORCE++ Over GRPO\nE.2 Advantage Estimation\nE.3 Policy Gradient Objective\nE.4 Strategic Multi-Objective Judging\n\n\n\nF Reward Backpropagation\n\nF.1 Motivation\n\nF.2 Method Overview\n\nForward Pass (Process Rewards).\nBackward Pass (Outcome Decomposition).\n\n\nF.3 Combining Process and Outcome Signals\nF.4 Consistency Constraints\nF.5 Benefits\n\n\n\nG Distributed Training Implementation\n\nG.1 Parallel Agent Initialization\nG.2 Distributed Rollout Generation\n\nG.3 Experience Preparation and Agent Routing\n\nVariable Turn Count Handling.\nGlobal Batch Normalization (REINFORCE++).\n\n\n\nG.4 Weight Synchronization\n\nStandard NCCL Broadcast.\nCUDA IPC (Co-located Models).\n\n\n\nG.5 Memory Optimization\n\nPrefix Caching.\nSleep/Wake Mode.\nDeepSpeed ZeRO-3.\n\n\nG.6 Synchronization Points\nG.7 Configuration Parameters\nG.8 Task-Specific Configuration\n\nG.9 Computational Cost\n\nHardware.\nTraining Time.\nAPI Cost.\n\n\n\n\n\n\n\n\n\nScaling Multiagent Systems with Process Rewards\n\n\nEd Li\n\n‚ÄÉ‚ÄÉ\nJunyu Ren\n\n‚ÄÉ‚ÄÉ\nCat Yan\n\n\n\nAbstract\nWhile multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout.\nWe demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0‚Äì17.5pp on AIME and +7.8‚Äì17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.\n\nMultiagent Systems, Agentic Systems, Post-training, Reinforcement Learning, Large Language Models, LLM Agents, Process Rewards, LLM-as-a-Coach, Distributed Training\n\n\n\n1 Introduction\n\nFigure 1: Multiagent architectures with separate weights enable specialization through end-to-end training, sidestepping catastrophic forgetting that limits single-model scaling just like mixture-of-experts (MoE) architecture.\n\n\nLarge reasoning models trained with reinforcement learning have emerged as the dominant paradigm for state-of-the-art AI systems¬†(OpenAI, 2024; DeepSeek-AI, 2025; Snell et al., 2024). Augmented with external tools such as code interpreters and search engines¬†(Schick et al., 2023), these models achieve remarkable performance on mathematical reasoning¬†(Shao et al., 2024), coding¬†(Jimenez et al., 2024), and scientific problems¬†(Novikov et al., 2025). Building on these capabilities, multiagent systems‚Äîwhere multiple language model-based agents interact to solve complex tasks‚Äîoffer an intuitive path forward as specializations and the diversity of perspectives can tackle problems better than any single agent.¬†(Wu et al., 2023; Hong et al., 2023).\n\n\nThe bulk of existing multiagent work implements specialization through prompt engineering: assigning different system prompts, personas, or tool access to agents within the system. While effective for orchestrating pre-trained models, the true strength of multiagent systems lies in the ability to modify the weights of individual agents, enabling specialization without interference. Recent evidence suggests such specialization naturally emerges through reinforcement learning: reasoning models trained solely for accuracy spontaneously develop diverse internal personas and expertise¬†(Kim et al., 2026). By separating weights across agents, improving one agent‚Äôs capabilities cannot degrade another‚Äôs performance, allowing scaling without catastrophic forgetting (Figure¬†1).\n\n\nRecent work has begun exploring this direction of fine-tuning agents in multiagent systems simultaneously¬†(Subramaniam et al., 2025; Liao et al., 2025; Park et al., 2025; Liu et al., 2025). However, two significant challenges remain:\n\n\n1.\n\nCredit assignment: How do we attribute the performance of the overall system to actions taken by individual agents?\n\n\n\n2.\n\nComputational cost: Rollouts of the entire multiagent system can take minutes or even hours, yet yield only a single outcome reward signal.\n\n\n\n\n\nWe address these challenges through finetuning MultiAgent system with Per-action Process rewards from AI feedback (MAPPA). Rather than assigning a single sparse reward at trajectory‚Äôs end according to fixed instructions in LLM-as-a-Judge, we leverage language models as coaches to assess the quality of each agent‚Äôs action given its role, inputs, and environment feedback such as tool execution results. This yields dense learning signal throughout the trajectory, enabling effective training even when tasks fail entirely. Crucially, the coach performs implicit credit assignment: when a downstream agent encounters a file-not-found error for an artifact that an upstream agent should have produced, the coach assigns low scores to the upstream agent‚Äôs faulty actions rather than penalizing the downstream agent. Furthermore, the number of training signals now scales with the number of actions taken, dramatically improving sample efficiency over outcome-based methods.\n\n\nWe validate this approach on two separate domains: mathematical reasoning with code execution (MathChat) and end-to-end data science pipelines (DSBench¬†(Jing et al., 2025)). On MathChat, our three-agent system achieves improvements of +5.0‚Äì17.5pp on AIME and +7.8‚Äì17.2pp on AMC across two model configurations. On DSBench, where multiagent pipelines must engineer features, train models, and generate predictions, training improves success rate by +12.5pp while quality metrics improve by up to 30%.\n\n\nWe present a general framework for training multiagent systems on complex, tool-augmented tasks via coach-guided reinforcement learning. The key components‚Äîagent topology, reward structure, and training pipeline‚Äîare domain-agnostic and can be configured to support diverse applications. Our source code is available at https://github.com/ltjed/multiagent-coaching. Overall, our results suggest that scaling the number of specialized agents represents a promising new dimension for improving performance on complex, long-horizon tasks.\n\n\n\n\n2 Methodology\n\n\n2.1 Problem Setting\n\nMAPPA finetunes tool-augmented multiagent systems where LLM agents collaborate to solve tasks. Each agent is initialized from a pretrained LLM with independent policy parameters trained separately, enabling specialization. All agents live in a terminal environment and execute in a pre-defined topology (e.g., sequential pipelines, debate, mixture-of-agents). Within each agent‚Äôs turn, it generates one or more actions that may include tool calls. When tool calls are generated as part of an agent‚Äôs action, they are processed and executed in a sandboxed environment, the agent will then receive environment feedback (e.g., stdout, stderr, error messages). Depending on the task, the agent may also need to call tools to read, write files or other artifacts in the terminal environment.\n\n\n\n\n2.2 Process Rewards from AI Feedback\n\nMulti-turn tool use with inter-agent dependencies creates challenging credit assignment‚Äîwhich agent‚Äôs actions led to success or failure? Unlike existing RL approaches that rely on sparse outcome rewards (success/failure for entire trajectories), we leverage a coach LLM to perform per-action credit assignment, providing dense process-quality rewards on a 0‚Äì10 scale.111We found the 0‚Äì10 scale empirically outperforms continuous 0‚Äì1 scores, likely due to the prevalence of 0‚Äì10 rating samples in the pre-training corpus, i.e., a ‚Äú77‚Äù or a ‚Äú44,‚Äù carries more semantic meaning than decimal values like 0.3050.305. We use the term ‚Äúcoach‚Äù rather than ‚Äújudge‚Äù to emphasize that while LLM-as-a-judge evaluates final outputs against fixed rubrics, a coach provides context-aware feedback on each intermediate action to guide improvement throughout the trajectory. The coach evaluates each action holistically based on: (1) the agent‚Äôs role and responsibilities, (2) the input context the agent observed, (3) the agent‚Äôs action, and (4) tool execution results (stdout, stderr, error messages) when the output contains tool calls. This enables the coach to identify the responsible agent when failures occur: if a downstream agent encounters a missing file error, the coach assigns low scores to the upstream agent that should have produced it, not the dow"
  },
  {
    "title": "Agile Reinforcement Learning through Separable Neural Architecture",
    "url": "https://arxiv.org/abs/2601.23225v1",
    "source": "arxiv",
    "summary": "Deep reinforcement learning (RL) is increasingly deployed in resource-constrained environments, yet the go-to function approximators - multilayer perceptrons (MLPs) - are often parameter-inefficient due to an imperfect inductive bias for the smooth structure of many value functions. This mismatch can also hinder sample efficiency and slow policy learning in this capacity-limited regime. Although m",
    "full_text": null
  },
  {
    "title": "Are you going to finish that? A Practical Study of the Tokenization Boundary Problem",
    "url": "https://arxiv.org/abs/2601.23223v1",
    "source": "arxiv",
    "summary": "Language models (LMs) are trained over sequences of tokens, whereas users interact with LMs via text. This mismatch gives rise to the partial token problem, which occurs when a user ends their prompt in the middle of the expected next-token, leading to distorted next-token predictions. Although this issue has been studied using arbitrary character prefixes, its prevalence and severity in realistic",
    "full_text": null
  },
  {
    "title": "Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints",
    "url": "https://arxiv.org/abs/2601.23221v1",
    "source": "arxiv",
    "summary": "As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergen",
    "full_text": null
  },
  {
    "title": "Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training",
    "url": "https://arxiv.org/abs/2601.23220v1",
    "source": "arxiv",
    "summary": "Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometri",
    "full_text": null
  },
  {
    "title": "MonoScale: Scaling Multi-Agent System with Monotonic Improvement",
    "url": "https://arxiv.org/abs/2601.23219v1",
    "source": "arxiv",
    "summary": "In recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous,",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n\n2 Related Works\n\nLLM-based multi-agent systems and orchestration.\nDynamic MAS evolution and the cold-start problem.\nAgent learning.\n\n\n\n3 Scaling Multi-Agent System with Monotonic Improvement\n\n\n3.1 Preliminaries\n\n3.1.1 Contextual Bandit at Expansion Stage kk\n3.1.2 Router Policy with Editable Memory\n\n3.1.3 Sequential Augmentation and Conservative Embedding\n\nBackward-compatible expansion.\n\n\n\n\n\n3.2 Expansion-Aware Familiarization and Memory Update\n\n3.2.1 Expansion-Aware Task Synthesis\n\n3.2.2 Memory Update\n\nTrust-region surrogate objective.\n\n\n\n\n\n\n\n4 Theoretical Analysis\n\n4.1 Exact Surrogate Objective in Contextual Bandits\n4.2 Monotonic Improvement Across Expansions\n\n\n\n5 Experiments\n\n\n5.1 Experimental Settings\n\nBenchmarks.\nAgent Pool Configuration.\nData synthesis and experience collection.\nBaselines.\n\n\n5.2 Main Results\n\n\n6 Discussion\n7 Conclusion\n\nA Agent Pool Configuration\n\n\nA.1 Router Configuration\n\n\nA.1.1 Planning Phase\n\nTask Decomposition Prompt (OWL_WF_TASK_DECOMPOSE_PROMPT)\n\n\n\nA.1.2 Coordinating Phase\n\nTask Assignment Prompt (ASSIGN_TASK_PROMPT)\n\n\n\nA.1.3 Answering Phase\n\nFinal Answer Prompt\n\n\n\n\n\nA.2 Agent Pool Configuration\n\n\nA.2.1 Web Agent\n\nSystem Prompt\n\n\n\nA.2.2 Document Processing Agent\n\nSystem Prompt\n\n\n\nA.2.3 Reasoning Coding Agent\n\nSystem Prompt\n\n\n\nA.2.4 Search Expert Agent\n\nSystem Prompt\n\n\n\nA.2.5 Code Agent\n\nSystem Prompt\n\n\n\nA.2.6 Math Agent\n\nSystem Prompt\n\n\n\nA.2.7 Document Agent\n\nSystem Prompt\n\n\n\nA.2.8 Reasoning Agent\n\nSystem Prompt\n\n\n\nA.2.9 Image Agent\n\nSystem Prompt\n\n\n\nA.2.10 Multimedia Agent\n\nSystem Prompt\n\n\n\n\n\nA.3 Noisy Agent Pool Configuration\n\nAgent Configurations\nA.3.1 Agent 4: Search Expert Agent (Tool Semantic Mismatch)\nA.3.2 Agent 5: Code Agent (Environment Error + Exaggerated Prompt)\nA.3.3 Agent 7: Document Agent (Core Function Failure)\nA.3.4 Agent 10: Multimedia Agent (False Advertising)\n\n\n\n\n\nB Proofs for Theoretical Analysis\n\n\nB.1 Setup and Technical Assumptions\n\nConservative lift.\n\n\nB.2 Proof of Lemma¬†4.1\nB.3 Proof of Lemma¬†4.2 (Expansion Bridge)\nB.4 Proof of Theorem¬†4.3 (Monotonicity Across Expansions)\n\n\n\nC Additional Results and Analysis\n\n\nC.1 GAIA Comprehensive Results\n\nAnalysis.\n\n\n\nC.2 Malfunctioning Agent Pool\n\nAnalysis.\n\n\n\n\n\nD MonoScale/Expansion-Aware Memory Schema\n\n\nD.1 Overview\n\n\nD.1.1 Key Common Items\n\ntask_pattern\nactionable_rules\nspecific_guidance\n\n\n\n\n\nD.2 Planning Phase Memory\n\nD.2.1 JSON Template\n\nD.2.2 Planning-Specific Items\n\nagent_selection\ndecomposition_strategy\ndependency_handling\nverification_strategy\n\n\n\n\n\nD.3 Coordinating Phase Memory\n\nD.3.1 JSON Template\n\nD.3.2 Coordinating-Specific Items\n\nagent_matching\nexecution_flow\nresult_validation\ninformation_passing\n\n\n\n\n\n\n\nE Case Study Trajectories: The Anatomy of Scaling Failures and Their Cure\n\nE.1 Case 1: The Trap of Cold-Start Misjudgment (Anagram Solving)\nE.2 Case 2: The Trap of Brittle Workflow Links (Bird Species ID)\nE.3 Case 3: The MonoScale Resolution (Scientific Discovery)\n\n\n\n\n\n\n\nMonoScale: Scaling Multi-Agent System with Monotonic Improvement\n\n\nShuai Shao\n\n‚ÄÉ‚ÄÉ\nYixiang Liu\n\n‚ÄÉ‚ÄÉ\nBingwei Lu\n\n‚ÄÉ‚ÄÉ\nWeinan Zhang\n\n\n\nAbstract\nIn recent years, LLM-based multi-agent systems (MAS) have advanced rapidly, using a router to decompose tasks and delegate subtasks to specialized agents. A natural way to expand capability is to scale up the agent pool by continually integrating new functional agents or tool interfaces, but naive expansion can trigger performance collapse when the router cold-starts on newly added, heterogeneous, and unreliable agents. We propose MonoScale, an expansion-aware update framework that proactively generates a small set of agent-conditioned familiarization tasks, harvests evidence from both successful and failed interactions, and distills it into auditable natural-language memory to guide future routing. We formalize sequential augmentation as a contextual bandit and perform trust-region memory updates, yielding a monotonic non-decreasing performance guarantee across onboarding rounds. Experiments on GAIA and Humanity‚Äôs Last Exam show stable gains as the agent pool grows, outperforming naive scale-up and strong-router fixed-pool baselines.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nMulti-agent systems (MAS) built on large language models (LLMs) are emerging as a powerful paradigm for solving complex tasks (Chen et al., 2025; Yang et al., 2024; Guo et al., 2024). Unlike a single model that produces an answer end-to-end, an MAS typically includes a router that decomposes an input problem, dispatches subtasks to specialized agents (Hu et al., 2025; Yang et al., 2025a; Yue et al., 2025)(e.g., retrieval, code execution, planning, tool use, or domain experts), and then aggregates their outputs into a final response. This modular ‚Äúdivide‚Äìand‚Äìconquer‚Äù structure enables flexible collaboration and makes MAS particularly effective for multi-step workflows, tool-augmented reasoning, and long-horizon task execution.\n\n\nIn real deployments, MAS are often continuously evolving: new tools, plugins, external APIs, or specialized agents may be integrated over time, leading to a dynamically expanding agent pool (Kim et al., 2025; Microsoft, 2025). While adding a stronger agent or one that covers new skills should, in principle, increase the system‚Äôs capability ceiling, naive scale-up in practice can yield the opposite outcome‚Äîoverall performance may stagnate, degrade, or even exhibit a clear performance collapse. This failure mode is not merely hypothetical: LABEL:fig:motivation_coldstart_gaia_total shows that, on GAIA under cold-start routers, increasing the agent pool size can hurt end-to-end performance (e.g., DeepSeek-V3.2 drops from 0.558 at 5 agents to 0.491 at 10; Qwen3 peaks at 0.461 with 7 agents but falls to 0.424 at 10). A key reason is the router‚Äôs cold start: once a new agent anewa_{\\text{new}} becomes available, the router typically lacks grounded knowledge of what the agent is good at, when it is reliable, what its boundaries and failure modes are, or how to interact with its tools (e.g., interfaces, output formats, and frequent errors). If the router assigns tasks to anewa_{\\text{new}} too early or too aggressively, mis-routing decisions can be amplified at the system level and reduce end-to-end reward‚Äîfor example, sending high-precision requests to an unstable retrieval agent, delegating complex tool calls to an agent unfamiliar with the interface, or introducing a brittle link in a multi-step workflow that causes the entire chain to fail.\n\n\nExisting approaches for improving multi-agent coordination broadly fall into two lines, yet neither fully addresses the central challenge of open-ended MAS scaling. The first line focuses on a static agent pool, assuming that the agent set and its capability profile remain stable between training and deployment (Yue et al., 2025; Zhang et al., 2024; Liu et al., 2025b); the research emphasis is then on optimizing routing policies, improving division of labor, or planning more efficiently within a fixed pool. While effective in static settings, these methods typically lack a controlled update mechanism that can prevent performance regressions when new agents are continually added, and they do not provide a unified formalization for online updating under continual agent augmentation in open-ended systems. In parallel, some recent work has started to examine dynamic expansion in LLM routing (Wang et al., 2025; Zhang et al., 2025b), where the system routes across an enlarging set of available experts over time. However, scaling an MAS is substantially harder: newly integrated agents are far more heterogeneous (tools, memory, skills, roles, and interfaces), and their quality can be highly uneven in practice (brittle tool calls, API failures, retrieval noise, or unstable behaviors). As a result, the router faces a sharper cold start problem, and a fixed onboarding suite to ‚Äútrial-run‚Äù the new agent is often insufficient: (i) it cannot be adapted to each agent‚Äôs unique boundaries and failure modes; and (ii) it may under-cover the true deployment distribution, creating a risk of agents that appear usable during onboarding but still collapse after real deployment. In short, what dynamic MAS scaling truly requires is an adaptive (agent-conditioned) and conservative update protocol, so that end-to-end performance can improve stably as the agent pool grows.\n\n\nTo bridge this gap, we propose an update framework for sequentially expanding open-ended MAS. Whenever a new agent is introduced, rather than waiting for real user traffic to surface failures, we proactively customize a small set of familiarization tasks tailored to the new agent and use them to collect controlled feedback. Our mechanism has two stages: (i) active familiarization via task customization, where tasks are synthesized to probe the new agent‚Äôs likely strengths, interface constraints, and failure modes, enabling the router to learn when to use / when not to use the agent; and (ii) router update via memory, where we update the router‚Äôs policy by maintaining an auditable text memory distilled from experience. Concretely, the memory captures not only patterns behind successful orchestration, but also lessons from failed worker calls and mis-routings; we abstract these signals into a compact set of actionable principles (e.g., capability boundaries, safe/unsafe contexts, interface caveats, and common failure patterns) that guide future routing decisions. By continuously learning these routing principles as the agent pool evolves, the router becomes less prone to brittle dispatching caused by unfamiliarity with newly integrated agents, enabling end-to-end performance to scale more stably with additional agents. On the theory side, we cast orchestration under sequential augmentation as a contextual bandit and prove that our trust-region memory optimization yields a monotonic non-decreasing performance guarantee across onboarding rounds.\n\n\nWe systematically evaluate the sequential expansion "
  },
  {
    "title": "Tackling air quality with SAPIENS",
    "url": "https://arxiv.org/abs/2601.23215v1",
    "source": "arxiv",
    "summary": "Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Data Overview and Analysis\n\n2.1 Air Pollution Analysis\n2.2 Traffic Analysis\n2.3 Pollution and Traffic Correlations\n2.4 Analysis Workflow\n\n\n\n3 Data Modelling and Results\n\n3.1 Partial Least Squares Regression Model\n3.2 Station Similarity Analysis\n3.3 Partial Least Square Regression Analysis Results\n\n\n\n4 Conclusions\n\n4.1 Data and Code Availability\n4.2 Acknowledgements\n4.3 Author contributions\n4.4 Funding\n4.5 Competing interests\n\n\n\n\n\n\n\n\n\nTackling air quality with SAPIENS\n\nSAPIENS: Smart Air Pollution Information Enabling New Solutions\n\n\n\nMarcella Bona‚Äâ\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\n\nNathan Heatley‚Äâ\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\n\nJia-Chen Hua‚Äâ\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\n\nAdriana Lara‚Äâ\n\nESFM, Instituto Polit√©cnico Nacional, Mexico City, Mexico\n\n\nValeria Legaria-Santiago‚Äâ\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\nCIC, Instituto Polit√©cnico Nacional, Mexico City, Mexico\n\n\nAlberto Luviano-Ju√°rez‚Äâ\n\nUPIITA, Instituto Polit√©cnico Nacional, Mexico City, Mexico\n\n\nFernando Moreno-G√≥mez\n\nESFM, Instituto Polit√©cnico Nacional, Mexico City, Mexico\n\n\nJocelyn Richardson‚Äâ\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\n\nNatan Vilchis\n\nESFM, Instituto Polit√©cnico Nacional, Mexico City, Mexico\n\n\nXiwen Shirley Zheng\n\nDepartment of Physics and Astronomy, Queen Mary University of London, United Kingdom\n\n\n\nAbstract\nAir pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.\n\n\n\n\n1 Introduction\n\nAir pollution is a chronic problem in large cities all over the world, and one of the leading causes of disease and premature death¬†[30, 26]. Avoiding exposure to air pollutants is especially important for susceptible individuals with chronic cardiovascular or pulmonary disease, children, and the elderly¬†[35].\nGovernments and global activist movements are raising awareness and promoting actions to tackle poor quality air, which regularly reaches illegal and unsafe levels worldwide.\nThere are many contributing factors, including traffic, geography, city planning,\nindustry, commerce and domestic emissions. In particular, private, public and\ncommercial traffic have been identified as major contributors¬†[15, 3, 18], affecting citizens‚Äô health, the economy, and tourism.\nHowever, mitigations are possible. Individuals who commute to work in personal\nvehicles or public transportation receive a substantial portion of their daily dose of air pollution during commuting activities¬†[35]. Personal exposure to ambient air pollution can therefore be reduced on high air-pollution days by better choice of travel routes¬†[20]. One study in London has found that using side streets can cut air pollution exposure by half relative to using the most polluted routes¬†[5]. Public transport policy may influence patterns of pollution¬†[3], while studies in Mexico¬†[6] show that the effect of social inequalities results in an almost four-fold annual increase of pollution concentrations, which can lead to a reduction in public health for vulnerable populations.\n\n\nAwareness of air pollution levels has been increased by academic studies, although many cover relatively short periods of time, forcing researchers to use projections or incomplete data.\nThere are also a growing number of public air quality alert systems. In London there is now hyper-local air quality information driven by a high-quality and dense sensor network. Such high-quality information, with resolution at the level of individual streets, enables citizens to make informed choices about travel routes in order to minimise their exposure to pollutants generated by traffic. Further, since air quality has strong daily cycles¬†[7], information which is updated regularly enables the best routes to be chosen according to the time of day.\nThe availability of specific and local information is also important for policy makers.\nHigh-resolution spatio-temporal measurements enable local governments to identify\nproblem areas for air quality, both by location and also by time of day. This in turn enables urban planning for the mitigation of these issues, including better informed development and traffic management.\nIn total, the benefits of reduced exposure to air pollution include improved quality of life, including citizens‚Äô mental and physical health and reduced health care costs.\n\n\nIn this paper, we focus on data from Mexico City to develop a proof-of-concept air quality prediction system. This system aims to provide hyper-local, dynamic air quality forecasts using existing open data sources. In particular, openly available traffic intensity data are leveraged to address the low density of pollution sensors.\nCurrently, citizens of Mexico City have access to only a single air pollution measurement per county, available via a smartphone app¬†[23]. Although the local government may consider expanding the sensor network in the future, the current distribution leaves large areas of the city without specific information on local air pollution levels.\n\n\nThe SAPIENS (Smart Air Pollution Information Enabling New\nSolutions)¬†111https://sapiens.qmul.ac.uk/\nproject has begun building the SAPIENS database, which contains information on traffic levels as well as measurements from air pollution sensors across Mexico City.\nThe goal is to model the relationship between traffic and pollution and to generate pollution predictions based on traffic information.\n\n\nAt the level of input data, the most closely related recent work is Ref.¬†[33], where Google Maps traffic data is used to predict¬†PM2.5. The authors use a decision-tree machine-learning algorithm and the results show that traffic-based prediction outperforms interpolation and that adding the time of day further increases accuracy on average. However, it is not clear how the Google Maps traffic data were quantified in their study, whereas in this paper we develop a new strategy for this purpose.\nAt the level of data analysis, Ref.¬†[34] provides a useful historical perspective of the more general problem of real-time air quality forecasting, including three-dimensional modelling and the complex interplay of meteorology, emissions, and chemistry, from global to urban scales. Their conclusions support the use of traffic information and statistical methods, which is the starting point for this paper, and suggest the inclusion of physical models and meteorological data which will be part of the future developments of the SAPIENS analysis. Ref.¬†[11] focuses on day-ahead prediction of hourly ozone levels and proposes using an ensemble of machine learning models rather than single models to improve performance.\n\n\nIn addition to the link between traffic emissions and air pollutants, the impact of traffic speed on air pollution levels is also considered vital for investigating air quality improvements. A review¬†[14] found that most studies reported that a reduction in speed leads to lower emissions of NOX and particulate matter, although the effects varied greatly across studies, depending on a variety of factors (the range of speed limit reductions, the type of pollutant, accompanying interventions, the methodologies, etc.).\nIt has also been observed that driving involving frequent acceleration, deceleration, stops, and starts increases air pollution from both exhaust emissions and non-exhaust sources such as brake pad and tyre wear, with the latter accounting for over 75% of road transport particulate emissions¬†[29].\nWith SAPIENS, we analyse the relationship between traffic and pollution, taking into account the different traffic conditions (free-flowing versus stop-start) as they can be derived from Google Maps congestion levels.\n\n\n\n\n2 Data Overview and Analysis\n\nAs the SAPIENS project aims at predicting pollution levels from traffic information,\nwe have developed a complete workflow from input data collection and analysis to model assessment and result validation. The SAPIENS database\ncontains information and measurements on traffic levels and air pollution. We\nused a Partial Least Squares regression (PLSR) to infer pollution levels from traffic\nintensities.\n\n\nThe air pollution measurements consist of data coming from air pollution sensors placed in 44 locations across the Mexico City metropolitan area as seen in Fig.¬†1. These data are obtained from the Mexico City atmospheric monitoring directorate¬†[9]. The time period we consider in this study is 14th December 2020 to 1st April 2021.\n\n\nFigure 1: Geographical locations of the 44 pollution sensor"
  },
  {
    "title": "Disentangling multispecific antibody function with graph neural networks",
    "url": "https://arxiv.org/abs/2601.23212v1",
    "source": "arxiv",
    "summary": "Multispecific antibodies offer transformative therapeutic potential by engaging multiple epitopes simultaneously, yet their efficacy is an emergent property governed by complex molecular architectures. Rational design is often bottlenecked by the inability to predict how subtle changes in domain topology influence functional outcomes, a challenge exacerbated by the scarcity of comprehensive experi",
    "full_text": null
  },
  {
    "title": "Multi-Agent Systems Should be Treated as Principal-Agent Problems",
    "url": "https://arxiv.org/abs/2601.23211v1",
    "source": "arxiv",
    "summary": "Consider a multi-agent systems setup in which a principal (a supervisor agent) assigns subtasks to specialized agents and aggregates their responses into a single system-level output. A core property of such systems is information asymmetry: agents observe task-specific information, produce intermediate reasoning traces, and operate with different context windows. In isolation, such asymmetry is n",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Principal-agent Problems\n\n2.1 Asymmetric information\n2.2 Mechanism Design\n2.3 Illustration\n\n\n\n3 How multi-agent systems lead to information asymmetry\n\n3.1 Background on multi-agent systems\n3.2 Properties of agents from a principal-agent framework\n3.3 The mechanism, step-by-step\n\n\n\n4 A case study of scheming\n\n4.1 Scheming as a principal agent problem\n4.2 How scheming arises, step-by-step\n\n\n5 Alternative views\n6 How the research agenda should change\n7 Discussion\n\n\n\n\n\nMulti-Agent Systems Should be Treated as Principal-Agent Problems\n\n\nPaulius Rauba\n\n‚ÄÉ‚ÄÉ\nSimonas Cepenas\n\n‚ÄÉ‚ÄÉ\nMihaela van der Schaar\n\n\n\nAbstract\nConsider the multi-agent systems setup, where a principal (‚Äúsupervisor agent‚Äù) assigns subtasks to specialized agents and aggregates their response into a single system-level output. A core property of such systems is information asymmetry, e.g. agents observe task-specific information, output intermediate reasoning traces, have different context-windows. In isolation, such asymmetry is not problematic since the agents report truthfully to the principal when their incentives are fully aligned. However, what happens when they are not? Recent evidence suggests that LLM-based agents acquire their own goals (e.g. survival or self-preservation), known as ‚Äúscheming‚Äù, and are willing to deceive humans or other agents. This produces agency loss: a gap between the principal‚Äôs intended outcome and the realized system behavior. Following the central tenets of microeconomic theory, we argue that these characteristics ‚Äì information asymmetry and misaligned goals ‚Äì are best studied as principal-agent problems. We lay the foundation as to why multi-agent systems, both Human-to-LLM and LLM-to-LLM, lead to information asymmetry under the principal-agent formulation; and use scheming‚ÄîLLM agents secretly pursuing covert goals‚Äîas a concrete case study. We show that recently introduced terminology to explain scheming, such as ‚Äúcovert subversion‚Äù or ‚Äúdeferred subversion‚Äù are in fact defined in the mechanism design literature, which not only recognizes the problem but also prescribes concrete actions to mitigate it. Above all, we see this as a call for us to use tools designed to study human agent behavior for explaining non-human agent behavior.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\n\n\n \n\nPosition. We should treat multi-agent systems, a flourishing area of research in machine learning, as principal-agent problems, a set of models introduced in microeconomic theory and later adopted across social sciences. Multi-agent systems are defined as, loosely, multiple agents interacting in a common environment to achieve a shared goal. In this work, we show that such systems exhibit two properties: (i) information asymmetry, a result of private information, which is costly to observe; and (ii) misaligned goals, where divergence between sub-agent preferences and the principal‚Äôs objective produces suboptimal or unintended outcomes.\n\n\n\nWe consider a multi-agent system, in which the supervisor agent (the principal) delegates subtasks to specialized agents and aggregates their responses into a single system-level decision. Such systems are common within multi-agent frameworks (Wu et al., 2024; Hong et al., 2023; Li et al., 2023; Yao et al., 2022; Wang et al., 2023; Phelps and Ranson, 2023). This delegated setting arises wherever a single model is decomposed into interacting components, and the components which are interacting are themselves agents. While such agents could in principle be broad, this work focuses on the emergence of agents which are operated by a language model policy backbone. A core property of such systems is information asymmetry (Zhang and Zhang, 2025; Liu et al., 2024b), e.g. sub-agents observe task-specific information, use local context windows, and produce intermediate artifacts that are not automatically shared (Liu et al., 2024b). In isolation, such asymmetry is not problematic when reports are truthful and incentives are fully aligned. However, what happens when they are not?\n\n\nOne solution is simply to allow the principal to monitor the behavior of the sub-agents, such as monitor their tool use, behavior, intermediate outputs, among others. Such outputs can be obtained by the principal. But such an approach leans heavily on the assumption that the supervisor can cheaply verify intermediate work, and that sub-agents have no incentive to strategically distort what they reveal. Unfortunately, delegation creates precisely the setting where verification is costly and intermediate actions are hidden (unobserved to the principal) (Papoudakis et al., 2021), while practical constraints within language model agents (finite context windows, inaccessible parameters which contribute to reasoning, custom acquired information) prevent full observability. Recent evidence also suggests that LLM-based agents may behave strategically when objectives are misaligned (Pham, 2025; Scheurer et al., 2023; Schoen et al., 2025; Meinke et al., 2024), including selective disclosure and deception under oversight. This combination of costly monitoring and potentially divergent objectives means the agents might not act in the principal‚Äôs best interest. The result is agency loss (Moloi and Marwala, 2020): a gap between the principal‚Äôs intended outcome and the realized system behavior.\n\n\nWe argue that if these two properties are met: (i) information asymmetry and (ii) conflicting goals (e.g. the emergence of autonomous goals in LLMs, such as self-preservation (Meinke et al., 2024)), we should treat delegated multi-agent systems as principal-agent problems. In the principal-agent formulation, the principal delegates tasks to the agent but does not have equivalent information as each agent. The agents follow a policy to finish the designated task and report the result back to the principal (possibly selective information). Examples of such private information accessible only to the sub-agent are context windows, latent computation, task-specific results, reasoning traces, among others.\n\n\nThe principal-agent formulation induces two mechanisms of information asymmetry.\n\n\n‚ñ∂\\blacktrianglerightAdverse selection  refers to a form of hidden information: an agent holds private information about its type before contracting or delegation, which the principal cannot observe. This informational asymmetry allows the agent to strategically misrepresent itself in order to secure more favorable contractual terms.\n\n\n‚ñ∂\\blacktriangleright Moral hazard refers to a form of hidden action: the actions that an agent takes after delegation or contracting are unobservable to the principal, which creates incentives for the agent to deviate from actions that maximize the principal‚Äôs objective.\n\n\nStandard microeconomic theory holds that equilibrium outcomes in multi-agent systems depend on the rules of the game. By changing these rules, we can induce different equilibrium policies and determine whether system-level goals are achieved. This was largely irrelevant for earlier multi-agent ML systems, which assumed aligned objectives in delegated systems. Recent empirical evidence of practices such as scheming (Pham, 2025), where AI agents covertly pursue misaligned goals, shows that goal conflicts can arise endogenously from training dynamics. As a result, interactions between learning agents may converge to undesirable equilibria, including lying and deception (Carichon et al., 2025).\n\n\nWhy does this matter? The principal-agent formulation enables us to understand and study why such behaviors occur by understanding the incentive structures that govern agent behavior. If an agent is strategically underperforming on a task so that it would get access to some resources (Li et al., 2025), or if a sub-agent possesses information that the principal cannot observe, it could be that withholding or distorting information can constitute the optimal strategy for the agent. We study this behavior in Sec. 4.2 in the context of scheming, i.e. when agents selectively reports or shapes task-relevant information to advance its own objective rather than that of the principal.\n\n\nThis becomes useful not just for understanding incentives but designing appropriate interventions (mechanisms) to achieve desired long-term equilibrium outcomes (Maskin, 2008). Research on mechanism and institutional design addresses informational asymmetries (hidden actions or hidden types) in the principal-agent problems by engineering mechanisms and proposing incentive-packages to realize principal‚Äôs intended outcome. Commonly, actions may be mitigated through improved monitoring or outcome-based feedback, while strategic misrepresentation of the principal‚Äôs beliefs about agent types calls for stronger screening and evaluation mechanisms that distinguish superficial from robust alignment.\n\n\n\n\n \n\nContributions. (1) We bridge the concepts that deal with asymmetric information in Microeconomic theory (Sec. 2) and multi-agent machine learning; (2) We show how multi-agent systems lead to informational asymmetries and resemble principal-agent problems (Sec. 3); (3) We study a concrete example and showcase why scheming‚Äîstrategic underperformance of LLMs‚Äîis a natural consequence of agency loss (Sec. 4.2); (4) We explore plausible mechanism designs for principal-agent problems (Sec. 5); and (5) We lay out a future research agenda for the field (Sec. 6).\n\n\n\n\n\n2 Principal-agent Problems\n\nA large number of economic and political situations can be studied as principal-agent problems, where some central authority ‚Äì the principal ‚Äì wishes to either (1) delegate the task to a subordinate, the agent or (2) implement social decisions based on the preferences of the agents. However, the principal faces a problem ‚Äì the individual‚Äôs preferences are private information and cannot be easily and fully observed publicly. The inability to reveal this private information results in agency loss ‚Äì a metric that quantifies the d"
  }
]