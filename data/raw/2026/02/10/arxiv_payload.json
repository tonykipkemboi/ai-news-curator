[
  {
    "title": "Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving",
    "url": "https://arxiv.org/abs/2602.09018v1",
    "source": "arxiv",
    "summary": "Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \\in \\{0,1,2,3\\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT polici",
    "full_text": "\n\n\n\n\nI Introduction\n\nI-A Our contributions\n\n\nII Related Work\n\nIII Experimental Setup\n\nIII-A Key questions we address\nIII-B Task Formulation and Platform\nIII-C Environment Factorization and Distribution Shifts\nIII-D Levels of OOD shifts\nIII-E Basic Policy Models and Training\nIII-F Foundation-model Feature Policies (Sec.¬†III-J).\nIII-G Evaluation Metrics and Protocol\nIII-H Study S1: Architecture Robustness and OOD Factorized Shifts\nIII-I Study S2: Effect of the ID Training Distribution\nIII-J Study S3: Foundation-Model Features with the Best Backbone\nIII-K Study S4: Data Scale and Diversity vs. Specialization\nIII-L Study S5: Temporal Context‚ÄîSingle Frame vs. Sequence\nIII-M Implementation Details\n\n\n\nIV Results\n\nIV-A Architectures and training choices\nIV-B OOD Environmental Factor Shifts and Their Effect\nIV-C Training Data choices\nIV-D Temporal Information\n\n\nV Conclusion\n\nA Reproducibility details\n\nA-A Evaluation Metrics and Protocol Details\nA-B Implementation Details and Hyperparameters\n\n\n\nB Additional themed factor shift figures\n\nB-A Single factor themed star plots\nB-B Double factor themed star plots\nB-C Triple factor themed star plots\n\n\n\n\n\n\n\nRobustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving\n\n\n\n\nAmir Mallak1 ‚ÄÉAlaa Maalouf1\n1University of Haifa\nCorrespondance: mallak002@gmail.com\n\n\n\nAbstract\nOut-of-distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled kk-factor perturbations (k‚àà{0,1,2,3})(k\\in\\{0,1,2,3\\}). Using closed-loop control in VISTA, we benchmark FC, CNN, and ViT policies, train compact ViT heads on frozen foundation-model (FM) features, and vary ID support in scale, diversity, and temporal context. (1) ViT policies are markedly more OOD-robust than comparably sized CNN/FC, and FM features yield state-of-the-art success at a latency cost. (2) Naive temporal inputs (multi-frame) do not beat the best single-frame baseline. (3) The largest single-factor drops are rural ‚Üí\\rightarrow urban and day ‚Üí\\rightarrow night (‚àº31%\\sim 31\\% each); actor swaps ‚àº10%\\sim 10\\%, moderate rain ‚àº7%\\sim 7\\%; season shifts can be drastic, and combining a time flip with other changes further degrades performance. (4) FM-feature policies stay above 85%85\\% under three simultaneous changes; non-FM single-frame policies take a large first-shift hit, and all no-FM models fall below 50%50\\% by three changes. (5) Interactions are non-additive: some pairings partially offset, whereas season-time combinations are especially harmful. (6) Training on winter/snow is most robust to single-factor shifts, while a rural+summer baseline gives the best overall OOD performance. (7) Scaling traces/views improves robustness (+11.8+11.8 points from 5 to 14 traces), yet targeted exposure to hard conditions can substitute for scale. (8) Using multiple ID environments broadens coverage and strengthens weak cases (urban OOD 60.6%‚Üí70.1%60.6\\%\\rightarrow 70.1\\%) with a small ID drop; single-ID preserves peak performance but in a narrow domain. These results yield actionable design rules for OOD-robust driving policies.\n\n\n\nI Introduction\n\n\nAutonomous driving systems must operate far beyond the narrow slice of conditions seen during training. Real roads combine many shifting factors: scene layout (rural vs. urban), time of day, season, weather, and the mix of nearby agents (vehicles, pedestrians, animals). Small changes along any one axis can subtly alter visual appearance, dynamics, and affordances; combined changes amplify these effects. Despite rapid progress in perception and end-to-end control, a core open question remains: which factors matter most for out-of-distribution (OOD) robustness, and how should we design the in-distribution (ID) training pipeline support to prepare for them?\n\n\nTo answer this, we advocate a factorized view of distribution shift. Rather than treating ‚ÄúOOD‚Äù as a monolith, we explicitly decompose the environment into semantically meaningful axes and evaluate policies under controlled kk-factor perturbations‚Äîi.e., test conditions that differ from the in-distribution (ID) support on exactly kk factors.\nThis yields interpretable robustness profiles: performance as a function of how many factors change and which factors change. Our analysis shows that such decomposition exposes sensitivities that are obscured by aggregate OOD scores.\n\n\nWhy this matters?\nSafety-critical deployment hinges on reliable behavior under inevitable distribution shift, e.g., night drives after a model trained at noon, first snow of the season after a summer-only dataset, or an unexpected animal entering the roadway. A factorized evaluation makes robustness diagnosable: practitioners can identify, for example, that weather+night degrades steering more than scene+agents, or that balancing time-of-day in the ID set yields larger gains than balancing season, given a fixed data budget. Such insights directly inform data collection, simulation curriculum design, and model selection.\n\n\n\nI-A Our contributions\n\n\nMotivated by the discussion above, we present the first systematic, factorized experimental study of generalization in vision-based autonomous driving. We quantify how (i) the training-data factors included in the ID set, (ii) the type and number of test-time distribution shifts, (iii) the policy architecture (MLP, CNN, ViT) and the use of foundation-model features, and (iv) key design choices (data budget, ID diversity vs. scale, single-frame vs. sequence) impact OOD robustness. Specifically, we contribute:\n\n\n\n\n‚Ä¢\n\nA factorized OOD framework. We formalize the environment as a Cartesian product of factor sets and define kk-factor OOD shells via a Hamming distance over factors. This provides a precise and reproducible way to construct ID/OOD splits and to attribute errors to specific axes of variation.\n\n\n\n‚Ä¢\n\nSystematic architectural comparison. Under matched training budgets and protocols, we benchmark FC, CNN, and ViT policies on closed-loop metrics and regression error, reporting robustness as a function of the number and identity of shifted factors.\n\n\n\n‚Ä¢\n\nWhat to include in the ID set. We vary the ID support along selected factors at constant data budget, and raising budget in terms of diversity and quantity of the same ID to quantify which axes are most valuable for OOD generalization and when broad coverage trades off with ID specialization.\n\n\n\n‚Ä¢\n\nFoundation-model features for control. Using frozen DINO/BLIP-2 patch descriptors with a compact ViT policy head, we isolate the contribution of generic visual features to OOD robustness and analyze how these benefits interact with the choice of ID support.\n\n\n\n‚Ä¢\n\nTemporal context. We compare single-frame policies to sequence-based models to assess whether short histories mitigate specific factor shifts (e.g., adverse weather) and how temporal aggregation complements foundation-model features.\n\n\n\n\n\n\n\n\nII Related Work\n\n\nFrom modular stacks to end-to-end policies\nClassical systems used a modular stack (perception‚Üíprediction‚Üíplanning‚Üícontrol) that was reliable yet prone to compounding errors. End-to-end control dates to ALVINN¬†[1] and has since advanced to pixels to steering and learned affordances¬†[2, 3, 4, 5, 6]. Conditional imitation learning adds high-level commands¬†[7], while later analyses expose limits of pure behavior cloning¬†[8]. We retain the end-to-end setting and ask which architectural biases (MLP/CNN/ViT) and which training distributions best support robustness under controlled shifts.\n\n\nGeneralization and robustness under distribution shift. \nOOD sensitivity‚Äîacross towns, weather, lighting, and traffic‚Äîhas been documented repeatedly; for example, performance drops starkly in new towns or adverse weather even when ID results look strong¬†[8].\nCommon remedies include domain randomization and augmentation¬†[9], and domain adaptation; yet open-loop gains often fail to translate to closed-loop safety. We complement these lines by factorizing shift along semantically meaningful axes (scene, time, season, weather, agents) and measuring robustness as a function of how many and which factors change.\n\n\nFoundation models for vision and their use in driving\nLarge-scale pretraining yields image encoders whose features transfer broadly: CLIP aligns images with language for robust zero-shot recognition¬†[10], while DINO learns strong self-supervised ViT representations with emergent semantics¬†[11]; BLIP-2 efficiently couples frozen vision encoders to LLMs¬†[12]. While leveraging LLMs in a zero shot manner for driving has proven to be week¬†[13], driving-specific pretraining has leveraged diverse web or fleet data for policy representations¬†[14, 15, 16, 17] and showed to be robust across a variety of tasks¬†[18, 19, 20]. We operationalize this idea in control by feeding frozen, patch-wise features (DINO/CLIP/BLIP-2) to a compact policy head and quantifying when such features improve OOD robustness‚Äîand along which factors.\n\n\nStructured, factorized evaluation\nSimulation enables controlled manipulations of environment factors.\nCARLA¬†[21] popularized New Town and New Weather splits; NoCrash¬†[8] contrasted traffic density and weather to expose failure modes.\nData-driven simulators like VISTA¬†[22] reproject real logs to photorealistic, closed-loop scenes, supporting reproducible stress tests. We formalize factorization by defining kk-factor OOD shells via a Hamming distance over factors, enabling matched-budget, per-axis attribution rather than a single aggregate OOD.\n\n\nTemporal modeling for control.\nTemporal context improves driving decisions over single-frame policies.\nEarly FCN‚ÄìLSTM models fused video history for egomotion prediction¬†[23], and recent end-to-end approaches use spatial‚Äìtemporal Transformers for perception, prediction, planning¬†[24] or "
  },
  {
    "title": "Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models",
    "url": "https://arxiv.org/abs/2602.09017v1",
    "source": "arxiv",
    "summary": "The prevalent paradigm in robot learning attempts to generalize across environments, embodiments, and tasks with language prompts at runtime. A fundamental tension limits this approach: language is often too abstract to guide the concrete physical understanding required for robust manipulation. In this work, we introduce Contact-Anchored Policies (CAP), which replace language conditioning with poi",
    "full_text": null
  },
  {
    "title": "CIC-Trap4Phish: A Unified Multi-Format Dataset for Phishing and Quishing Attachment Detection",
    "url": "https://arxiv.org/abs/2602.09015v1",
    "source": "arxiv",
    "summary": "Phishing attacks represents one of the primary attack methods which is used by cyber attackers. In many cases, attackers use deceptive emails along with malicious attachments to trick users into giving away sensitive information or installing malware while compromising entire systems. The flexibility of malicious email attachments makes them stand out as a preferred vector for attackers as they ca",
    "full_text": null
  },
  {
    "title": "ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation",
    "url": "https://arxiv.org/abs/2602.09014v1",
    "source": "arxiv",
    "summary": "Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match",
    "full_text": null
  },
  {
    "title": "Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense",
    "url": "https://arxiv.org/abs/2602.09012v1",
    "source": "arxiv",
    "summary": "The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like \"Bi",
    "full_text": "\n\n\n\n1 Introduction\n2 Background\n3 Advanced GUI Agents Break Current CAPTCHA System\n\n4 Next-Gen CAPTCHAs: Native GUI-Agent Era‚Äôs Security Defense\n\n4.1 Agent-CAPTCHA Interaction as Extended POMDP\n4.2 Data Curation Pipeline\n\n\n\n5 Experiments and Analysis\n\n5.1 Experimental Settings\n5.2 Results on Next-Gen‚Äôs Testing Benchmark\n\n5.3 Ablation and Analysis\n\nCost-Efficiency and Economic Asymmetry.\n\n\n\n\n6 Conclusion\nA More Details of Experimental Settings\n\nB Details of Next-Gen CAPTCHAs\n\nDesign principles.\nInstance generation and diversity.\nUI instrumentation and logging.\nSuccess criteria and verification.\nEvaluation protocol.\nSecurity and usability considerations.\nB.1 Next-Gen CAPTCHA Examples Gallery\n\n\n\n\n\n\n\nNext-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense\n\n\nJiacheng Liu\n\n‚ÄÉ‚ÄÉ\nYaxin Luo\n\n‚ÄÉ‚ÄÉ\nJiacheng Cui\n\n‚ÄÉ‚ÄÉ\nXinyi Shang\n\n‚ÄÉ‚ÄÉ\nXiaohan Zhao\n\n‚ÄÉ‚ÄÉ\nZhiqiang Shen\n\n\n\nAbstract\nThe rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like ‚ÄúBingo‚Äù. In response, we introduce Next-Gen CAPTCHAs, a scalable defense framework designed to secure the next-generation web against the advanced agents. Unlike static datasets, our benchmark is built upon a robust data generation pipeline, allowing for large-scale and easily scalable evaluations, notably, for backend-supported types, our system is capable of generating effectively unbounded CAPTCHA instances. We exploit the persistent human‚Äìagent ‚ÄúCognitive Gap‚Äù in interactive perception, memory, decision-making, and action. By engineering dynamic tasks that require adaptive intuition rather than granular planning, we re-establish a robust distinction between biological users and artificial agents, offering a scalable and diverse defense mechanism for the agentic era. 111Project page is available at https://greenoso.github.io/NextGen-CAPTCHAs_webpage/.\n\nAgent Security, CAPTCHAs, GUI/Computer Use Agent, Web Agent\n\n\n\n1 Introduction\n\nCAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart)¬†(von Ahn et al., 2003) have long served as a lightweight, widely deployed line of defense for the open web, limiting automated abuse such as credential stuffing, scraping, fake account creation, and transaction fraud. Historically, CAPTCHA design has followed an arms race¬†(Yan and Ahmad, 2009; Bursztein et al., 2014; Tariq et al., 2023): early distorted-text challenges aimed to resist OCR, later image-grid CAPTCHAs targeted object recognition, and more recent ‚Äúlogic‚Äù CAPTCHAs introduced game-like spatial reasoning. In each cycle, progress in machine perception and reasoning steadily eroded the security margin of what was once considered ‚Äúbot-hard‚Äù, forcing defenders to continuously redesign challenges and patch deployed systems.\n\n\nFigure 1: Frontier Models as GUI Agent Backbones‚Äô Pass@1 on our Next-Gen CAPTCHA benchmark.\n\n\nThis erosion has accelerated sharply with the emergence of Multimodal Large Language Models (MLLMs) and GUI-enabled / Computer-Use agents. Modern web agents can perceive rendered pages, interpret instructions, and execute multi-step interactions‚Äîcapabilities demonstrated on realistic benchmarks such as Mind2Web and WebArena¬†(Deng et al., 2023; Zhou et al., 2023). Recent security analyses further indicate that these agentic capabilities generalize zero-shot to diverse CAPTCHA challenges, effectively neutralizing the long-standing bot-hard assumption that underpins modern web security¬†(Teoh et al., 2025). The risk is no longer theoretical: as computer-use agents become integrated into consumer and enterprise products, attackers gain an increasingly accessible ‚Äúautomation substrate‚Äù that can operate directly through the same browser surfaces intended for humans, widening the attack surface of websites that rely on current CAPTCHAs as a primary gatekeeper.\n\n\nAt the same time, substantial capability gaps remain between humans and current MLLM-based agents, especially in interactive settings. Beyond static vision‚Äìlanguage benchmarks, agents must repeatedly (i) ground instructions to precise screen regions, (ii) maintain and update a latent task state over time, and (iii) execute low-level actions robustly under partial observability and UI stochasticity. Prior work reports persistent deficits in visual‚Äìspatial grounding and intermediate state manipulation in MLLMs, and shows that offline benchmark success often degrades when models are deployed as interactive web agents in live environments¬†(Cao et al., 2024; Yang et al., 2025; Xue et al., 2025). These gaps suggest an opportunity for defense: rather than ‚Äúhardening‚Äù existing decomposable puzzles, we can design challenges that are trivial for human intuition but systematically misaligned with the over-segmented, step-by-step strategies used by today‚Äôs agents.\n\n\nMotivated by this, we introduce Next-Gen CAPTCHAs: a scalable GUI-agent defense framework that produces GUI agent-defensive yet human-solvable interactive challenges. The core of Next-Gen is an automatic CAPTCHA generation pipeline with rule-based verifiable answer: for generative task families, the system can synthesize an effectively unbounded number of unique instances (e.g., timed interaction tasks such as Red Dot), each paired with a rule-based solution‚Äîeliminating the need for human annotation while maintaining strong instance diversity. Importantly, our 27 types of CAPTCHA families are newly designed by us specifically to defend against modern GUI agents (rather than collected from existing deployed CAPTCHA systems). Except for two vision‚Äìlanguage families, instances are procedurally generated with automatically verifiable solutions.\n\n\nTo make progress measurable and reproducible, we curate a benchmark sample from this defense system: a main test set of 519 vision‚Äìlanguage puzzles spanning the 27 vision‚Äìlanguage families, plus a lightweight subset containing 5 puzzles per task for fast and cost-effective evaluation under limited query budgets. Under realistic closed-API agent settings on live web tasks, we observe a large human‚Äìagent gap: humans achieve near-ceiling solve rates with low latency, whereas representative high-reasoning MLLMs and GUI web agents exhibit single-digit Pass@1 (Fig.¬†1). We further validate human friendliness via a small-scale human study, reporting high success rates and low completion times across representative tasks.\n\n\nFinally, we release a real-web evaluation platform that is agnostic of GUI framework: any GUI-enabled MLLM agent can be evaluated via a standardized browser interaction and logging interface. In our experiments, we use Browser-Use as the primary reference integration, and additionally evaluate supplementary agent frameworks including Claude Cowork and CrewAI. While the curated benchmark facilitates standardized comparison, it is best viewed as a byproduct of the broader Next-Gen defense system: the dataset is sampled for benchmarking, whereas the primary goal is a deployable, continuously generative CAPTCHA defense mechanism for the agentic era.\n\n\nOur contributions are threefold:\n(1) We design 27 new formats of Next-Gen CAPTCHA families that target empirically observed human‚ÄìMLLM gaps, aiming to be agent-defensive while remaining human-friendly.\n(2) We propose a procedural generation and automatic verification framework, enabling scalable deployment with effective diversity and controllable difficulty.\n(3) We release an open-source real-web evaluation platform and a benchmark sampled from the defense system, including both a 519-puzzle main set and a cost-aware subset, and evaluate GUI Agents across perception, memory, reasoning, and action under realistic interaction constraints.\n\n\n\n\n2 Background\n\nCAPTCHA systems have long been shaped by an arms race with AI¬†(von Ahn et al., 2003; Bursztein et al., 2014). We summarize this progression across three capability shifts: Perception, Reasoning, and Agency, where defenses were eventually broken by the very models they aimed to stop.\n\n\nThe Era of Visual Perception. The defense against automated bots initially relied on distorted text to defeat OCR, premised on the belief that decoding warped visuals was uniquely human¬†(von Ahn et al., 2003; Shet, 2014). However, Convolutional Neural Networks (CNNs) rendered this obsolete by solving text challenges with superhuman accuracy¬†(Mori and Malik, 2003; Shet, 2014; Gao et al., 2016). Consequently, security mechanisms shifted to object classification¬†(Google for Developers, 2024), assuming machines lacked the contextual grounding to identify objects in diverse scenes. Yet, the emergence of Vision Transformers¬†(Dosovitskiy et al., 2021) and large-scale pre-training has since bridged this gap. Modern backbones can now interpret complex scenes with precision¬†(Plesner et al., 2024; Sivakorn et al., 2016; Hossen et al., 2020), effectively neutralizing static visual perception as a reliable security barrier.\n\n\nThe Era of Multimodal Logic: Reasoning vs. MLLMs. To counter advanced vision models, providers like Arkose Labs introduced ‚ÄúLogic CAPTCHAs‚Äù¬†(Arkose Labs, 2025) puzzles requiring not just recognition, but spatial reasoning and game-like logic (e.g., rotating objects or matching icons). Until recently, these were considered secure against standard\nvision models. However, the emergence of Multimodal Large Language Models (MLLMs) like GPT-5.2¬†(OpenAI, 2025b), Claude4.5-Opus¬†(Anthropic, 2025) and Gemini3-Pro¬†(Google, 2025) changed the landscape. Recent benchmarks, such as MCA-Bench¬†(Wu et al., 2025) and COGNITION¬†(Wang et al., 2025), revealed that MLLMs could effectively interpret instructions and solve these logic puzzles. Crucially, as reasoning-enhanced models continue "
  },
  {
    "title": "ANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling",
    "url": "https://arxiv.org/abs/2602.09009v1",
    "source": "arxiv",
    "summary": "Scaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape con",
    "full_text": "\n\n\n\n1 Introduction\n2 Related work\n\n3 Residual topology matters: a case study\n\n3.1 Linear neural networks with residual connections\n3.2 Convergence analysis of exponential discrepancies\n\n\n\n4 Learning residual connections from data\n\n4.1 ANCRe: Adaptive neural connection reassignment\n4.2 Applying ANCRe to Transformers\n\n\n\n5 Numerical experiments\n\n5.1 Pre-training of LLMs\n5.2 Pre-training of diffusion models\n5.3 Reinforcement learning with ResNets\n5.4 Ablation study\n\n\n6 Conclusion and outlook\nA Additional related work\n\nB Missing proofs\n\nB.1 Proof of Theorem 3.2\nB.2 Proof of Theorem 3.3\nB.3 Additional auxiliary lemmas\nB.4 Extension to more than 3 layers\n\n\n\nC Experimental setups\n\nC.1 Datasets\nC.2 Models\nC.3 Hyperparameters\n\n\n\n\n\n\n\n\n1]University of Minnesota\n2]ETH Zurich\n\\contribution[*]Equal contribution.\n\\metadata[Emails]{zhan7453,georgios}@umn.edu, {bingcong.li,niao.he}@inf.ethz.ch\n\n\nANCRe: Adaptive Neural Connection Reassignment for Efficient Depth Scaling\n\n\nYilang Zhang\n\n‚ÄÉ‚ÄÉ\nBingcong Li\n\n‚ÄÉ‚ÄÉ\nNiao He\n\n‚ÄÉ‚ÄÉ\nGeorgios B. Giannakis\n\n[\n\n[\n\n\n\nAbstract\nScaling network depth has been a central driver behind the success of modern foundation models, yet recent investigations suggest that deep layers are often underutilized. This paper revisits the default mechanism for deepening neural networks, namely residual connections, from an optimization perspective. Rigorous analysis proves that the layout of residual connections can fundamentally shape convergence behavior, and even induces an exponential gap in convergence rates. Prompted by this insight, we introduce adaptive neural connection reassignment (ANCRe), a principled and lightweight framework that parameterizes and learns residual connectivities from the data. ANCRe adaptively reassigns residual connections with negligible computational and memory overhead (&lt;1%&lt;1\\%), while enabling more effective utilization of network depth. Extensive numerical tests across pre-training of large language models, diffusion models, and deep ResNets demonstrate consistently accelerated convergence, boosted performance, and enhanced depth efficiency over conventional residual connections.\n\n\n\n1 Introduction\n\nFoundation models have demonstrated remarkable success across a broad spectrum of domains and impactful applications. For instance, large language models (LLMs) exhibit nearly human-level proficiency in various tasks, including conversational interaction (GPT4), automated code generation (Codex), and complex mathematical reasoning (Minerva). Diffusion models have revolutionized vision tasks by enabling high-fidelity and controllable image synthesis (DDPM; DDIM; score-matching). Recent work has further extended foundation models to multimodal settings, where joint representations are learned from heterogeneous data modalities (CLIP; PaLM-E).\n\n\nOne of the key factors underlying these advances is the increasing capacity of modern backbone architectures, with a particularly noticeable trend toward greater depth. For instance, the Llama 3.1 family employs 32, 80, and 126 Transformer layers for its 8B, 70B, and 405B variants, respectively (llama3). Likewise, Diffusion Transformers (DiTs) scale in depth from 12 (DiT-S) up to 28 layers (DiT-XL) (DiT). Moreover, (depth-efficient) shows a clear positive correlation between depth and performance across 132 open-source LLMs (see their Figure 1). This trend is also supported by theories. It is proved in (telgarsky2015representation) that polynomially deep networks can express functions that would require exponential width in shallow ones.\n\n\nDespite the documented success of deep networks, scaling model depth can be less efficient than it first appears. For example,  (depth-efficient) shows that skipping an early layer in the Llama 3.1 70B has a remarkably greater impact on the outputs of subsequent layers than omitting a deep layer, and that deeper layers often behave as near-identity mappings. Since the identity function is essentially available ‚Äúfor free‚Äù, this suggests that late layers are highly underutilized. Complementary evidence also occurs in multimodal foundation models, where the most informative vision embeddings are frequently found in intermediate layers of the Perception Encoder rather than the final layer (perception-encoder). Collectively, these observations reveal that the representational potential of depth is not fully exploited yet.\n\n\nGiven that residual (skip) connections are the default strategy and dominant mechanism for scaling model depth, this work revisits their design to enable more efficient depth scaling. Residual connections were proposed in (srivastava2015highway; ResNet) to avoid vanishing and exploding gradients, and they have become almost universal across architectures. For example, the backbone architecture of LLMs, i.e., the Transformer (Transformer), introduces residual connections around each self-attention and feedforward network modules.\nFrom an optimization perspective, residual connections are credited with smoothing the loss landscape, which facilitates training by improving the (local) condition number (li2018visualizing).\n\n\nThis work continues on the optimization perspective of residual connections, and demonstrates an intuitive yet often overlooked factor: where residual connections are placed within a deep architecture, i.e., the residual topology, can play a crucial role in optimization. We provide quantitative theory showing that different topologies can induce an exponential gap in convergence for deep linear neural networks. This large gap motivates a principled redesign of residual layout. To this end, we term our approach adaptive neural connection reassignment (ANCRe111It coincides with ‚Äúanchor‚Äù in French.), which learns an optimal residual topology from data. Our method not only achieves a linear convergence rate for deep linear networks, but also integrates seamlessly into modern architectures including LLMs, DiTs, and ResNets with consistent empirical gains. In a nutshell, our contributions are as follows:\n\n\n‚Ä¢\n\nA theoretical characterization on the role of residual connection topology is established using deep linear neural networks, showing that different shortcut layouts can induce exponential gaps in convergence rates.\n\n\n\n‚Ä¢\n\nANCRe is proposed to parameterize residual connections and learn a data-driven topology on the fly via appropriately normalized shortcut coefficients. It incurs negligible computational and memory overhead.\n\n\n\n‚Ä¢\n\nExtensive numerical evaluations are conducted under varying data modalities and network depths to systematically examine the efficiency of ANCRe. As an illustrative example, ANCRe achieves a 1.85√ó\\times training speedup on LLaMA-1B over conventional residual topology.\n\n\n\n\n\n\n\n2 Related work\n\nResidual connections.\nResidual (skip) connections are a primary mechanism for scaling neural networks to greater depth (srivastava2015highway; ResNet; he2016identity).\nThey were rapidly popularized in computer vision, with variants such as (ReZero) and DenseNet (huang2017densely), and have since become a standard component of CNN-based architectures; see e.g., (zagoruyko2016wide; xie2017aggregated).\nIn contrast, residual connections in large language models (LLMs) have remained relatively stable. Both Transformers and their recent variants adopt the same residual topology (Transformer; team2025gemma; llama; qwen3; DiT), while certain shortcuts can be introduced to mitigate over-smoothing (NeuTRENO) or reduce KV cache size (ResFormer).\nMore recently, hyper-connections (HC) (HC) and manifold-constrained hyper-connections (mHC) (mHC) were proposed for foundation models as architectural alternatives.\nOur work is orthogonal to these lines of research: while they focus on intra-layer designs, we emphasize the inter-layer topology.\n\n\nUnderstanding of residual connections.\nResidual connections are known to stabilize training by mitigating vanishing and exploding gradients (haber2017stable), and make gradients in deep networks less ‚Äúshattered‚Äù (i.e., less like white noise) (balduzzi2017shattered). Empirical visualizations of ResNets also suggest that residual connections lead to a smoother loss landscape (li2018visualizing).\nTheoretical understandings are often acquired by contrasting deep linear neural networks (converge-analysis-LNN) with their residual counterparts. For example, the convergence of deep linear networks can degrade exponentially with depth (exp-converge-LNN), whereas residual connections alleviate this slowdown (wu2019global). Moreover, residual connections can relax network width requirements for global converging in certain regimes, as by comparing results in (du2019width; zou2020global). The loss landscape of deep linear residual networks has also been studied in (hardt2016identity). Our work enriches this line of results by showing that the topology of residual connections can affect convergence exponentially. More importantly, we translate this insight into a practical design that yields consistent improvements in modern architectures. Due to space limitation, other related work is deferred to Appendix A.\n\n\nNotation. Bold lowercase (capital) letters denote vectors (matrices); ‚à•‚ãÖ‚à•\\|\\cdot\\| and ‚à•‚ãÖ‚à•F\\|\\cdot\\|_{\\mathrm{F}} stand for ‚Ñì2\\ell_{2}- and Frobenius-norm.\n\n\n\n\n\n\n(a) No residual connection\n\n\n\n\n\n(b) Cascaded residual connections\n\n\n\n\n\n(c) Residual connection 0:2\n\n\n\nFigure 1: Visualization of linear neural network (LNN) of K=3K=3 layers.\n\n\n\n\n3 Residual topology matters: a case study\n\nPopularized by ResNet (ResNet), residual connections allow each layer kk to learn a residual mapping rkr_{k} relative to its input ùê±k\\mathbf{x}_{k}; i.e., gk‚Äã(ùê±k):=rk‚Äã(ùê±k)+ùê±kg_{k}(\\mathbf{x}_{k}):=r_{k}(\\mathbf{x}_{k})+\\mathbf{x}_{k}. By providing an identity shortcut,\nthe loss landscape is more well-behaved (li2018visualizing),\nthereby enabling training deep networks with hundreds of layers.\nThis architecture has been extensively leveraged in foundation models (Transformer; "
  },
  {
    "title": "ShapeCond: Fast Shapelet-Guided Dataset Condensation for Time Series Classification",
    "url": "https://arxiv.org/abs/2602.09008v1",
    "source": "arxiv",
    "summary": "Time series data supports many domains (e.g., finance and climate science), but its rapid growth strains storage and computation. Dataset condensation can alleviate this by synthesizing a compact training set that preserves key information. Yet most condensation methods are image-centric and often fail on time series because they miss time-series-specific temporal structure, especially local discr",
    "full_text": null
  },
  {
    "title": "GEBench: Benchmarking Image Generation Models as GUI Environments",
    "url": "https://arxiv.org/abs/2602.09007v1",
    "source": "arxiv",
    "summary": "Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive ben",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Automated GUIs Generation\n2.2 Advanced Image Generation Models\n2.3 Sequential Generation Benchmarks\n\n\n\n3 GEBench\n\n3.1 Benchmark Design and Task Suites\n3.2 Evaluation Dimension and Scoring Rubric\n3.3 Data Construction Pipeline\n\n\n\n4 Evaluation\n\n\n4.1 Evaluation Setup\n\nEvaluated Models.\nVLM-based Judges.\n\n\n\n4.2 Evaluation Results\n\nOverall Performance and Model Comparison.\nThe Performance Gap in Multi-step Planning.\nChallenges in Spatial Grounding.\n\n\n4.3 Validity of VLM-as-a-Judge\n\n\n\n5 Discussion and Analysis\n\n5.1 Hierarchy of Task Difficulty: From Local Mimicry to Global Reasoning Failure\n5.2 In-depth Bottleneck Analysis: Qualitative Insights from Failure Cases\n5.3 The Paradox of Visual Fidelity vs. Functional Plausibility\n\n\n6 Conclusion\nA Evaluation Framework\nB Detailed Performance On GEBench Using Different Judges\nC Detailed Rubric on five tasks\nD Detailed Rubric on five tasks\n\n\n\n\n\n\n\\reportnumber\n\nGEBench: Benchmarking Image Generation Models as GUI Environments\n\n\n\nHaodong Li1,2\n\n\n\n\n Jingwei Wu1\n\n\n\n\n Quan Sun1,‚Ä†\n\n\n\n\n Guopeng Li1\n\n\n\n\n Juanxi Tian7\n\n\n\n\n Huanyu Zhang5\n\n\n\n\n\nYanlin Lai1,4\n\n\n\n\n Ruichuan An1,3\n\n\n\n\n Hongbo Peng1\n\n\n\n\n Yuhong Dai1\n\n\n\n\n Chenxi Li6\n\n\n\n\n Chunmei Qing2,‚àó\n\n\n\n\n\nJia Wang1\n\n\n\n\n Ziyang Meng1\n\n\n\n\n Zheng Ge1,‚àó\n\n\n\n\n Xiangyu Zhang1\n\n\n\n\n Daxin Jiang1\n1 StepFun ‚ÄÉ2 South China University of Technology‚ÄÉ3 Peking University \n4 Tsinghua University ‚ÄÉ5 Institute of Automation\n\n\n\n\n Chinese Academy of Sciences \n6 The University of Chicago ‚ÄÉ7 Nanyang Technological University\n‚Ä†{\\dagger} Project Leader ‚ÄÉ‚àó* Corresponding Author\n\n\n\n\n\nAbstract\nRecent advancements in image generation models enable the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored.\nTo address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUIs generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality.\nExtensive evaluation indicates that current models perform well on single-step transitions but struggle with temporal coherence and spatial grounding over longer interaction sequences. Moreover, our findings identify icon interpretation, text rendering, and localization precision as key bottlenecks, and suggest promising directions for future research toward high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench\n\n\n\n1 Introduction\n\nRecent advancements in image generation models hurst2024gpt; comanici2025gemini; team2023gemini; seedream2025seedream; flux-2-2025; wan2025 enable the prediction of future Graphical User Interface (GUI) states based on specific user instructions and current visual contexts. This capability positions image generation models as powerful GUI Environments zhang2025large; yan2025gui; xie2025mirage; garg2025controllable; luo2025vimo; wei2023boosting, capable of simulating dynamic interaction sequences to facilitate the scalable training of autonomous agents. Distinguished from conventional simulators cobbe2020leveraging; xie2024osworld; bonatti2024windows tethered to physical hardware or fixed software stacks zhang2025large, these generative models offer a flexible, low-cost alternative for creating diverse interaction trajectories across countless applications zhao2021guigan; liu2025ui.\n\n\nFigure 1: Comparison of evaluation paradigms across different benchmark types. Existing image generation benchmarks prioritize general-domain visual fidelity and video generation benchmarks evaluate continuous state transitions. GEBench uniquely evaluates discrete state transitions induced by user actions, capturing the essence of GUI interactions.\n\n\nHowever, the potential of image generation models as reliable GUI environments remains largely unverified, as traditional visual benchmarks ghosh2023geneval; hu2024ella; niu2025wise; zhao2025envisioning; huang2023t2i; huang2024vbench; sun2025t2v; zhuang2025vistorybench prioritize general-domain visual fidelity (for images) and continuous state transitions (for videos), leaving a critical gap in evaluating the functional logic and state-transition consistency inherent to GUI interactions xie2025gui; yan2025gui. As shown in Figure 1, when acting as GUI environments zhang2025large, generation models must seamlessly navigate discrete, action-triggered interface jumps. Such transitions necessitate precise coordinate grounding zhao2021guigan; cheng2024seeclick, icon recognition liu2025ui; xie2025gui, and high-fidelity text rendering chen2024textdiffuser, compelling the models to maintain logical continuity even when visual elements do not persist li2025mobileworldbench. Such demands strain existing architectures and call for a new evaluation approach to verify if generated GUIs respond felicitously to user instructions.\n\n\nTo bridge this gap, we present GEBench (Benchmarking image generation models as GUI Environments), a benchmark designed to evaluate how effectively image generation models can serve as GUI environments. GEBench comprises 700 high-quality samples, where each entry aligns a sequence of GUI images with corresponding user instructions. These samples span five distinct tasks, allowing for a multifaceted assessment of the model‚Äôs ability. To provide a concrete measure of generative quality, we propose GE-Score, a multi-dimensional metric derived from Vision Language Model (VLM)-guided google2025gemini3; hurst2024gpt; bai2025qwen3vltechnicalreport evaluations across five specialized rubrics. GE-Score systematically validates intent fulfillment and interaction logic while verifying UI content consistency and structural integrity. By ensuring high visual fidelity and logical coherence, GE-Score confirms the practical utility of these synthetic environments.\n\n\nOur systematic evaluation of state-of-the-art image generation models openai2025gptimage; team2023gemini; seedream2025seedream; seedream2025seed; flux-2-2025; wan2025; wu2025qwen; deng2025emerging; li2025uniworld; team2025longcat identifies promising pathways for their evolution into reliable GUI environments. While current architectures demonstrate robust proficiency in executing localized, single-step state transitions, they offer significant opportunities for advancement in long-term interaction consistency and precise spatial grounding.\nIn particular, deficiencies in icon interpretation, Chinese text rendering, and grounding point localization lead to layout drift and logical inconsistencies.\nThese observations delineate critical bottlenecks and outline clear directions for future research toward high-fidelity, temporally coherent generative GUI systems.\n\n\nOur primary contributions are as follows:\n\n\n1.\n\nWe introduce GEBench, a systematic benchmark with 700 samples across five task categories to evaluate image generation models as dynamic GUI environments.\n\n\n\n2.\n\nWe propose GE-Score, a five-dimensional metric that emphasizes the quality assessment of image sequences by accounting for the unique visual properties of GUIs.\n\n\n\n3.\n\nOur evaluation reveals critical deficiencies in current image generation models, underscoring significant room for improvement in high-fidelity GUI generation.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Automated GUIs Generation\n\nThe evolution of GUIs generation reflects a significant paradigm shift from heuristic-based structural mapping to data-driven synthesis powered by Multimodal Large Language Models (MLLMs) chen2018ui; sandhaus2011employing; yang2016automatic; li2020layoutgan; zhao2021guigan; mozaffari2022ganspiration; sobolevsky2023guilget; zhang2025scaling; kolthoff2025guide; kolthoff2024zero. Early methodologies relied on traditional rule-based algorithms to perform layout reconstruction sandhaus2011employing; huang2016automaticly, yet these approaches frequently failed to capture the semantic depth of complex hierarchies. Subsequent frameworks simplified this process using model-based approaches to translate visual features directly into code sequences chen2018ui. Contemporary research leverages Transformer-based architectures to bridge the gap between visual design abstractions and executable source code sobolevsky2023guilget; kolthoff2025guide. Furthermore, the rapid advancement of generative AI suggests that direct utilization of image generation models for GUI synthesis is becoming increasingly viable li2020layoutgan; zhao2021guigan; mozaffari2022ganspiration; zhang2025latent. These models offer the potential to produce high-fidelity GUIs directly from user instructions.\n\n\n\n\n2.2 Advanced Image Generation Models\n\nRecent progress in image generation exhibits a rapid evolution from text-to-image synthesis to sophisticated reference-based frameworks hurst2024gpt; comanici2025gemini; team2023gemini; seedream2025seedream; wan2025; deng2025emerging; liu2025step1x; li2025uniworld; team2025nextstep; team2025longcat. Ongoing advancements in text-to-image synthesis have empowered models to produce aesthetically superior visuals with precise semantic alignment to the provided instructions ho2020denoising; chen2020generative; flux-2-2025; fan2024fluid; han2025infinity; ramesh2022hierarchical; lin2025perceiveanythingrecognizeexplain. Building on these foundations, reference-based techniques integrate visual priors with textual prompts to enhance generative control team2023gemini; seedream2025seedream; wan2025; team2025longcat; an2025unictokens. These methods incorporate style or structur"
  },
  {
    "title": "ARO: A New Lens On Matrix Optimization For Large Models",
    "url": "https://arxiv.org/abs/2602.09006v1",
    "source": "arxiv",
    "summary": "Matrix-based optimizers have attracted growing interest for improving LLM training efficiency, with significant progress centered on orthogonalization/whitening based methods. While yielding substantial performance gains, a fundamental question arises: can we develop new paradigms beyond orthogonalization, pushing the efficiency frontier further? We present \\textbf{Adaptively Rotated Optimization ",
    "full_text": null
  },
  {
    "title": "Data Science and Technology Towards AGI Part I: Tiered Data Management",
    "url": "https://arxiv.org/abs/2602.09003v1",
    "source": "arxiv",
    "summary": "The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition c",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Tiered Data Management Framework\n\n\n2.1 Existing Data Management Frameworks\n\n2.1.1 Stage-Oriented Management Framework\n2.1.2 Method-Oriented Management Framework\n\n\n\n2.2 Tiered Data Management Framework\n\n2.2.1 L0: Raw Data\n2.2.2 L1: Filtered Data\n2.2.3 L2: Selected Data\n2.2.4 L3: Refined Data\n2.2.5 L4: Organized Data\n\n\n\n\n\n3 Experiments\n\n3.1 Experimental Setting\n3.2 Data Analysis\n3.3 Case Study on UltraData-Math\n3.4 Tiered Data Management for Multi-Stage Training\n\n\n4 Conclusion\n\n\n\n\n\nData Science and Technology Towards AGI \nPart I: Tiered Data Management\n\n\nYudong Wang1‚àó‚Ä†,\nZixuan Fu1‚àó,\nHengyu Zhao2,3‚àó,\nChen Zhao2‚àó,\nChuyue Zhou2‚àó,\nXinle Lin2,4‚àó, \n\nHongya Lyu2,\nShuaikang Xue2,\nYi Yi2,\nYingjiao Wang2,\nZhi Zheng2,\nYuzhou Zhang2‚Ä†,\n\n\nJie Zhou2‚Ä†‚Ä°,\nChaojun Xiao1‚Ä°,\nXu Han1‚Ä°,\nZhiyuan Liu1‚Ä°,\nMaosong Sun1\n1Tsinghua University ¬†¬†¬†¬†\n2ModelBest Inc. \n3Beijing Institute of Technology \n4South China Agricultural University \nyudongwang@tsinghua.edu.cn ‚ÄÉzhoujie@modelbest.cn ‚ÄÉ{xcj,han-xu,liuzy}@tsinghua.edu.cn\n\n\n\nAbstract\nThe development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Despite remarkable progress, current large language model (LLM) research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency.\nIn this work, we argue that the development of artificial general intelligence (AGI) is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities.\nTo implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints.\nSpecifically, we introduce an L0‚ÄìL4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework explicitly balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management.\nWe validate the effectiveness of the proposed framework through empirical studies on math and web data, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.\n\n‚Ä†‚Ä†* Equal contribution.‚Ä†‚Ä†‚Ä°\\ddagger Corresponding authors.‚Ä†‚Ä†‚Ä†\\dagger Project leaders.\n\n\n1 Introduction\n\nThe development of artificial intelligence can be viewed as an evolution of data-driven strategies and data utilization paradigms (Zha et al., 2025).\nEach paradigm shift extends and restructures prior approaches while introducing new methods for utilizing and managing data. These transformations have consistently driven improvements in model capability, enabling the emergence of higher-level intelligence (Wei et al., 2022; Gan et al., 2026).\n\n\nBased on the primary data types that drive each era, the developmental trajectory can be divided into four phases. (Buchanan and Feigenbaum, 1981; Shortliffe, 2012; Rumelhart et al., 1986; Cortes and Vapnik, 1995; Krizhevsky et al., 2012; He et al., 2016; Vaswani et al., 2017)\n(1) Symbolic Learning: the initial data era established a paradigm driven by knowledge data, such as human-annotated rules. It relied on experts to codify world knowledge into static knowledge bases (Augusto, 2021) and implemented intelligence through explicit rules. (2) Supervised Learning: the era of statistical and deep learning established a paradigm driven by labeled data (Krizhevsky et al., 2012). This stage witnessed the transition from manual feature engineering to end-to-end supervised training (Bengio et al., 2013; LeCun et al., 2015). The model performance became directly dependent on data scale, quality, and representational capacity (Sun et al., 2017). (3) Self-supervised Learning: the pre-training era further reduced dependence on labeled data and enabled self-supervised learning driven by unsupervised data. Training on massive corpora allows models to compress and internalize world knowledge, leading to strong generalization and emergent capabilities across modalities (Wei et al., 2022; Achiam et al., 2023; Dubey et al., 2024).\n(4) Feedback Learning: in the feedback-driven era, models leverage human and environmental feedback through reinforcement learning (RL) (Ziegler et al., 2019; Kaufmann et al., 2024). Continuous interaction enables active exploration of model behavior and capability improvement (Rafailov et al., 2023; Liu et al., 2024). This stage has strengthened decision-making and adaptability in complex settings and has laid an important foundation for progress toward artificial general intelligence (AGI) (Gan et al., 2026).\n\n\nFigure 1: Paradigm Shift in Data Organization and Utilization. The evolution of LMs and ultimately toward AGI fundamentally represents a paradigm shift in the organization and utilization of data, progressing through Symbolic, Supervised, Self-supervised, and Feedback Learning phases. We argue that the field is transitioning toward a Data-Model Co-Learning phase, which necessitates three critical research pillars: Scientific Data Value Assessment, Hierarchical Data Management, and Dynamic Data-Model Co-evolution to transcend current sustainability bottlenecks.\n\n\nAs shown in Figure¬†1, current mainstream research primarily manifests as ‚ÄúData-Driven Learning‚Äù, which emphasizes unidirectional enhancement of model capabilities through expansion of data scale (Zhou et al., 2025b). As model capabilities advance, we argue that AI development should transition toward ‚ÄúData-Model Co-Evolution‚Äù, wherein models improve data management practices while high-quality data further refines model performance (Yuan et al., 2024), creating a positive feedback cycle.\n\n\nTo accommodate this paradigm shift, this paper focuses on presenting a model-driven ‚ÄúTiered Data Management‚Äù framework, aiming to provide systematic technical support for advancing toward artificial general intelligence.\nThe necessity of implementing tiered data management stems from three core considerations.\n(1)¬†High-quality public data resources are becoming increasingly scarce. Future model development cannot rely solely on expanding data scale (Villalobos et al., 2022). Instead, data science and technology must shift from pursuing scale toward more careful data management and utilization.\n(2)¬†LLM training involves multiple different phases ‚Äì from knowledge acquisition during pre-training to behavioral alignment (Ouyang et al., 2022) during fine-tuning ‚Äì each with different requirements for data quality, quantity, and distribution (Mo et al., 2025; Zha et al., 2025). This necessitates designing specialized training datasets suited to each phase‚Äôs specific learning objectives.\n(3)¬†Data management must balance the costs of data acquisition against the benefits to model performance. In the early stage of data management, lightweight and low-cost methods (such as heuristic filtering) should be adopted, while in deeper management stage, more fine-grained and higher-cost approaches (such as LLM-based labeling) should be used (Zhou et al., 2025b). Since high-quality data typically requires significant investment, strategically deploying valuable data at critical training moments ‚Äì such as mid-training phases or annealing stages ‚Äì can maximize data effectiveness while keeping overall costs manageable.\n\n\nDespite significant academic progress in specific data processing tasks, such as filtering (Penedo et al., 2024b; Young et al., 2024; Soldaini et al., 2024b), selection (Chen et al., 2023b; Penedo et al., 2023; Xie et al., 2023; Soldaini et al., 2024b; Wettig et al., 2024; Engstrom et al., 2024; Dubey et al., 2024), and editing (Eldan and Li, 2023; Gunasekar et al., 2023; Li et al., 2023c; Wang et al., 2023; Taori et al., 2023; Peng et al., 2023; Xu et al., 2024a; Wang et al., 2024e; Ding et al., 2023; Cui et al., 2023), these approaches often fall short of addressing the systematic requirements of the LLM full-lifecycle training. To address this issue, we propose an L0-L4 tiered data management framework, evolving from raw resources to structured knowledge:\n(1)¬†L0: Raw Data. L0 data comprises PB-scale, uncurated resources characterized by high redundancy and noise, such as raw web dumps containing advertisements. It is maintained in its original state without deep processing. Consequently, it is primarily utilized for archiving and traceability rather than direct model training.\n(2)¬†L1: Filtered Data. L1 data features standardized text formatting and basic readability. It is usually produced via heuristic cleaning and deduplication to remove significant noise like web advertisements. As a result, it serves as the foundational resource pool for subsequent data selection and evaluation.\n(3)¬†L2: Selected Data. L2 data retains samples with distinct themes and high information density, suitable for knowledge learning and domain adaptation (e.g., high-quality academic papers, technical code repositories, or filtered encyclopedia articles).\n(4)¬†L3: Refined Data. L3 data features structured content with clear reasoning and explicit educational intent, ensuring maximum learnability. It is usually produced t"
  },
  {
    "title": "From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection",
    "url": "https://arxiv.org/abs/2602.09002v1",
    "source": "arxiv",
    "summary": "Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geom",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-A Navigation in Crowds\nII-B Social Robot Navigation without Foundation Models\nII-C Utilizing Foundation Models for Robot Navigation\n\n\n\nIII Overview\n\nIII-A Formulation\nIII-B System Design\n\n\n\nIV Social Navigation with Path Selection\n\nIV-A Human Motion Extraction\nIV-B Prediction-Fused Costmap Generation\nIV-C Path Planning\n\nIV-D Social-compliance Selection\n\nIV-D1 Visual Prompting\nIV-D2 Fine-tuning\n\n\nIV-E Local Reactive Controller\n\n\n\nV Experiments\n\nV-A Platform Description\nV-B Experimental Setup\nV-C Experiment Results\nV-D Ablations\n\n\nVI Conclusion and Limitation\nVI-A Additional Details\nVI-B Human Study\n\n\n\n\n\nFrom Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection\n\n\n\nZilin Fang1, Anxing Xiao1, David Hsu1,2,‚àó, and Gim Hee Lee1,‚àó\nThis research is supported by Agency for Science, Technology &amp; Research (A*STAR), Singapore, under its National Robotics Program (No. M23NBK0053), and the National Research Foundation (NRF) Singapore, under its NRF-Investigatorship Programme (Award ID. NRFNRFI09-0008).\n1School of Computing, 2Smart Systems Institute, National University of Singapore, Singapore.\n{zilin.fang, anxingxiao}@u.nus.edu, dyhsu@comp.nus.edu.sg, gimhee.lee@nus.edu.sg‚àó Co-supervision\n\n\nAbstract\nNavigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning.\nThis paper presents a social robot navigation framework that integrates geometric planning with contextual social reasoning. The system first extracts obstacles and human dynamics to generate geometrically feasible candidate paths, then leverages a fine-tuned vision-language model (VLM) to evaluate these paths, informed by contextually grounded social expectations, selecting a socially optimized path for the controller. This task-specific VLM distills social reasoning from large foundation models into a smaller and efficient model, allowing the framework to perform real-time adaptation in diverse human‚Äìrobot interaction contexts.\nExperiments in four social navigation contexts demonstrate that our method achieves the best overall performance with the lowest personal space violation duration, the minimal pedestrian-facing time, and no social zone intrusions. Project page: path-etiquette.github.io\n\n\n\nI Introduction\n\n\nNavigating robots in crowded environments is both challenging and critical for applications such as autonomous delivery and guidance¬†[1, 2, 3].\nBeyond goal reaching under geometric constraints, robots should also adhere to social etiquette in crowds‚Äîrecognizing cues such as people posing for photographs or a worker on a ladder‚Äîand act to minimize disruption and risk¬†[4], as illustrated in Fig.¬†1. However, generating such contextually adaptive motions remains difficult, as it requires both spatial and semantic awareness to identify socially compliant spaces, yet relevant robot datasets are scarce. In this work, we propose a social robot navigation framework that leverages Vision-Language Models (VLMs) to evaluate the social conventions associated with feasible geometric paths, enabling robots to follow social etiquette without explicitly modeling socially compliant spaces.\n\n\nPrior works employing VLMs for social robot navigation have largely focused on high-level semantic reasoning at the symbolic level, such as predicting explicit social relationships¬†[5], choosing navigation goals projected in images¬†[6, 7], predicting pedestrian intention¬†[8], or scoring predefined actions¬†[9]. However, evaluating purely on symbolic representations fails to capture real-world execution effects, which can lead to intrusions or navigation failures. In addition, frequent queries to large VLMs can result in slow inference, limiting real-time applicability.\n\n\nFigure 1: Illustration of robot navigation in a scenario with three geometrically feasible sampled paths, where the robot should reason about social conventions to select the most appropriate path.\n\n\nTo address the above challenges, we present a navigation framework that integrates spatial constraints from obstacles and humans with semantic adaptations derived from human-environment, human-human, and human-robot interactions in real-time.\nThe social robot navigation task is formulated as a multi-objective optimization problem over geometric feasibility and in-context semantics. These objectives are assumed to be decomposable, with the optimal semantic solution space lying within a well-covered subset of the geometric optimum. Accordingly, we adopt a hierarchical architecture with asynchronous modules for geometry-aware path planning, socially compliant path selection, and safe reactive control.\nAt the high level, candidate paths are first sampled under geometric constraints, and then VLMs are employed to select grounded, socially compliant paths from this feasible set. This proceeds in a receding-horizon fashion. For real-time inference, we introduce a pipeline that distills social reasoning capabilities from large models into a smaller model. At the low level, the selected path is fed back to the path planning module as a reference for generating new paths, while a modified ORCA algorithm is employed to ensure pedestrian avoidance. At a conceptual level, our propose‚Äìselect strategy is close to prior work¬†[10, 11], but it targets a different problem domain with dynamic, interaction-aware reasoning objectives and a different system design.\n\n\nWe evaluate our method through controlled experiments on a Boston Dynamics Spot legged robot in diverse social scenarios involving human activity. These scenarios are motivated by the insight that, while social behavior is inherently multimodal and shaped by individual preferences, many situations follow common patterns that minimize intrusion into social zones. This design reduces reliance on subjective post-hoc surveys and enables social performance to be quantified using interruption-related metrics. Real-world experiments demonstrate that our approach achieves superior social compliance and more efficient goal-reaching compared to representative baselines, including group-based¬†[12, 5], RL-based¬†[13], VLM-based¬†[9] methods, as well as a foundation model for visual navigation¬†[14].\nAblation studies further show that our approach generates collision-free and socially compliant paths, which direct VLM path prediction cannot reliably guarantee.\n\n\nFigure 2: System overview. Geometric constraints are extracted from human motion and costmap modules using sensor data. Collision-free path candidates are sampled, projected into the image, and evaluated by a fine-tuned VLM. The selection is fed back as reference to retrieve a path for the local controller.\n\n\n\n\nII Related Work\n\n\n\nII-A Navigation in Crowds\n\n\nCollision-free robot navigation has been extensively studied, from classical methods to reactive approaches such as ORCA for multi-agent interactions¬†[15]. Cooperative planners extend reciprocal assumptions to dense crowds, e.g., joint human-robot trajectory prediction with interacting Gaussian processes¬†[16] and mixed-strategy Nash equilibrium models¬†[17]. Model-based approaches integrate Model Predictive Control (MPC) with additional elements such as Control Barrier Functions¬†[18], topological invariance in cost functions¬†[19], or bilevel optimization with closed-loop crowd prediction¬†[20, 21]. Learning-based methods leverage deep reinforcement learning with free-space prediction¬†[22], human-robot interaction features via local maps¬†[23], or interaction modeling with RNNs¬†[24] and attention¬†[13]. These methods mainly address multi-agent collision avoidance, optimizing safety, clearance, and travel time with limited consideration of social context. This work focuses on enabling robots to adapt to social contexts by analyzing human activities and relationships to guide behavior.\n\n\n\n\nII-B Social Robot Navigation without Foundation Models\n\n\nAlthough social robot navigation has been studied for years, the definition of ‚Äòsocial‚Äô remains ambiguous until a recent survey clarified it by proposing eight evaluative principles¬†[4]. Despite this progress, translating these principles into concrete costs or metrics remains difficult. Recent works increasingly explore robot social navigation capabilities, employing approaches such as manually designed social costs¬†[1, 25, 26, 27, 28], integration of MPC into Reinforcement Learning¬†[29], and behavior cloning with extensive training data¬†[30, 31]. Many of these methods evaluate social compliance primarily through safety and distance metrics, such as human proximity or alignment with human collision avoidance preferences via Likert-scale assessments. They focus largely on collision avoidance and lack a deeper understanding of full semantic context, limiting their ability to interpret implicit social rules in complex environments. Our work introduces a system that integrates a task-specific fine-tuned VLM, capable of understanding social context across diverse environments and human activities, going beyond simple collision avoidance.\n\n\n\n\nII-C Utilizing Foundation Models for Robot Navigation\n\n\nFoundation models like Large Language Models (LLMs) and Vision-Language Models (VLMs) are increasingly integrated into robot navigation. For example, LLMs can facilitate interpreting open-vocabulary semantics and constructing scene graphs or maps¬†[32, 33] that benefit objective goal navigation. VLMs can encode the traversability of different terrains and environmental objects¬†[7, 34] for motion planning and instruction following. The closest work to ours in the propose-and-select concept is VL-TGS¬†[10]. In contrast to focusing on static scene semantics, our approach reasons over dynamic, implicit, agent-dependent contexts arising from human "
  },
  {
    "title": "DirMoE: Dirichlet-routed Mixture of Experts",
    "url": "https://arxiv.org/abs/2602.09001v1",
    "source": "arxiv",
    "summary": "Mixture-of-Experts (MoE) models have demonstrated exceptional performance in large-scale language models. Existing routers typically rely on non-differentiable Top-$k$+Softmax, limiting their performance and scalability. We argue that two distinct decisions, which experts to activate and how to distribute expert contributions among them, are conflated in standard Top-$k$+Softmax. We introduce Diri",
    "full_text": null
  },
  {
    "title": "iGRPO: Self-Feedback-Driven LLM Reasoning",
    "url": "https://arxiv.org/abs/2602.09000v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Pr",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nRL for Reasoning.\nGRPO and Variants.\nLLM Self-Learning and Self-Improvement.\n\n\n\n3 Methodology\n\n3.1 Background: Group Relative Policy Optimization\n\n3.2 Iterative Group Relative Policy Optimization\n\n3.2.1 Motivation: From Static Examples to Dynamic Self-Conditioning\n\n3.2.2 Algorithmic Framework\n\nStage 1: Exploratory Draft Generation.\nStage 2: Conditioned Refinement.\n\n\n3.2.3 Theoretical Analysis: Bootstrapped Policy Improvement\n\n3.2.4 Mathematical Formulation\n\nStage 1: Draft Selection.\nStage 2: Conditioned Generation and Advantage Computation.\nFull Objective.\nReward Function.\n\n\n\n3.2.5 Computational Analysis\n\nBaseline GRPO Cost.\niGRPO Rollout Cost.\n\n\n\n\n\n\n\n4 Experiments\n\n4.1 Setup\n\n4.2 Results\n\nControlled study with matched sampling budget.\nGeneralist 8B model: the largest gains from self-feedback.\nStronger 7B distilled reasoner: consistent gains concentrated on multi-step tasks.\nMath-specialized 7B model: improvements persist when the base is already strong.\nScaling to 14B parameters: benefits persist on complex reasoning.\nCompetitiveness against critique-style objectives.\n\n\n4.3 Generalization to a Stronger Base and Harder Dataset\n\n\n\n5 Ablation\n\nBeyond GRPO: self-feedback as a reusable refinement wrapper.\nGenerative judge study.\nEntropy analysis.\n\n\n6 Conclusion\n\nA Policy Gradient Derivation for iGRPO\n\nA.1 Two-stage sampling and the induced self-conditioned prompt distribution\nA.2 From the self-conditioned expected reward to a REINFORCE-style gradient\nA.3 Group-relative advantage used in iGRPO\nA.4 Off-policy sampling from œÄŒ∏old\\pi_{\\theta_{\\mathrm{old}}} and PPO-style clipping\nA.5 Including the per-token KL penalty\n\nA.6 Final iGRPO surrogate objective and resulting policy gradient\n\nInterpretation.\n\n\n\n\n\nB Scaling OpenMath-Nemotron-14B with iGRPO\n\nB.1 Analysis of Pass@N on AIME Benchmarks\n\n\n\nC Hyperparameter Setup\n\nPrompt:\n\n\n\nD Memory and Throughput Comparisons\n\nD.1 Setup\nD.2 Measurements\n\n\n\nE Additional Ablation Studies\n\nTraining Dynamics and Response Length.\nEffect of KL Divergence Term.\nEffect of Number of Completions.\n\n\n\n\n\n\n\niGRPO: Self‚ÄëFeedback‚ÄìDriven LLM Reasoning\n\n\nAli Hatamizadeh111Project lead. Correspondence to: Ali Hatamizadeh¬°ahatamizadeh@nvidia.com¬ø.\n\n\n\n\n Shrimai Prabhumoye\n\n\n\n\n Igor Gitman\n\n\n\n\n Ximing Lu\n\n\n\n\n Seungju Han\n\n\n\n\n Wei Ping\n\n\n\n\n Yejin Choi\n\n\n\n\n Jan Kautz\n\n\n\n\n\n\nAbstract\nLarge Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62% and 79.64% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.\n\n\n\\abscontent\n\n\n\n1 Introduction\n\nReinforcement Learning (RL) has proven to be successful in improving reasoning capabilities of LLMs by optimizing against task-specific reward signals. Early successes in this direction include RL from Human Feedback (RLHF) for aligning LLMs with human intent, most notably in InstructGPT (Ouyang et al., 2022) and ChatGPT (Achiam et al., 2023), which have demonstrated that incorporating preference-based rewards can dramatically improve both the usability and correctness of model outputs. Recently DeepSeek-R1 (Guo et al., 2025) proposed a distinguishing feature which is the so-called zero configuration, wherein the RL process directly enhances the base language model. This breakthrough started several efforts which were targeted at replicating DeepSeek-R1‚Äôs methodology or refining its underlying RL mechanisms (Zeng et al., 2025; Yu et al., 2025; Liu et al., 2025; Cui et al., 2025; Hu et al., 2025).\n\n\nYet, in the realm of complex reasoning, RL algorithms typically do not incorporate any form of feedback or reflection on the model‚Äôs own outputs. Humans, by contrast, rarely solve nontrivial problems in a single pass: they often iterate on initial drafts, identify mistakes, and refine their solutions based on internal feedback (Flower &amp; Hayes, 1981; Simon, 2012; Braidotti, 2019; Flavell, 1979; Sch√∂n, 2017; Polya, 2014). There is growing evidence that self-feedback mechanisms can bolster multi-step reasoning and the capacity to correct errors (Madaan et al., 2023; Shinn et al., 2023). However, existing RL frameworks do not capitalize on this iterative refinement process, leaving a critical gap between how humans naturally solve problems and how LLMs are typically trained to do so.\n\n\nIn this work, we propose to fill this gap with Iterative GRPO (iGRPO) which is a powerful extension of GRPO (Shao et al., 2024). As illustrated in Figure 1, Our method operates in two stages. First, we draw multiple candidate completions from the model and compute their relative rewards via the group-based mechanism of GRPO. We then select the highest-scoring draft and this serves as the \"first-draft\" output of the model. We consider this highest-scoring response as a guide to improve the final output.\nHence, it is provided as a self-feedback to the model.\nWe feed it back to the model alongside the original prompt. By conditioning on this exemplar, the second stage encourages the model to refine and surpass its own best prior attempt. Notably, this design preserves the efficiency of GRPO while introducing only minimal extra overhead, as iGRPO still relies on the same set of group-based reward signals. In doing so, iGRPO offers a promising avenue for self-guided improvement, enabling LLMs to iteratively improve their reasoning capabilities.\n\n\nFigure 1: Iterative GRPO (iGRPO): During Exploratory Draft Generation, the model selects a high-scoring ‚Äúbest draft‚Äù from initial samples and appends it to the prompt for Conditioned Refinement. This augmented context guides the generation of new group-based updates, creating a bootstrapping effect where the policy continuously improves its own conditioning signal to enhance reasoning.\n\n\nWe conduct a series of controlled experiments to compare iGRPO and GRPO under identical\ntraining conditions, using different base models trained on the Mathematics Aptitude Test of\nHeuristics (MATH) (Hendrycks et al., 2021) dataset. Specifically, we evaluate DeepSeek-R1 Distilled (Guo et al., 2025)\nand OpenMath-Nemotron (Moshkov et al., 2025) on an extensive array of mathematical reasoning benchmarks,\nincluding AIME24 (AI-MO, 2024a), AIME25 (OpenCompass, 2025), MATH500 (Lightman et al., 2023),\nAMC23 (AI-MO, 2024b), GSM8K (Cobbe et al., 2021), and Minerva Math (Lewkowycz et al., 2022).\nFor models with 7B and 14B parameters, iGRPO consistently outperforms standard GRPO. Moreover, by leveraging iGRPO algorithm with OpenReasoning-Nemotron-7B model (NVIDIA, 2025) on the large-scale AceReason‚ÄëMath (Chen et al., 2025b) dataset (R1, 2024), we push the state of the art on AIME24 and AIME25 to 85.62% and 79.64%, respectively. These findings underscore the effectiveness of incorporating a self-feedback stage into group-based RL optimization, particularly for complex mathematical reasoning tasks.\n\n\n\n\n2 Related Work\n\nRL for Reasoning.\n\nReinforcement learning (RL) has become an important tool for refining large language models on logical and analytical tasks (Lambert et al., 2024). Early self-improvement lines of work, such as STaR-style bootstrapping with verified outcomes and sampling-based selection (as discussed in (Lambert et al., 2024)), demonstrate that iteratively leveraging model-generated solutions can improve reasoning behavior. More recent systems scale outcome-driven training substantially (Jaech et al., 2024), and open-weight efforts such as DeepSeek-R1 report strong reasoning performance under similar training regimes (Guo et al., 2025). Beyond natural language tasks, RL on procedurally generated puzzles (Xie et al., 2025) and settings with limited human demonstrations (Wang et al., 2025) further highlight the breadth of RL as a mechanism for improving mathematical and logical reasoning.\n\n\n\nGRPO and Variants.\n\nThere has also been rapid progress in refining and extending GRPO for large-scale LLM training.\nDr. GRPO (Liu et al., 2025) analyzes sources of bias in GRPO-style token-level objectives and proposes modifications such as removing divisions by sequence length and group-level standard deviation to better match unbiased policy gradients.\nDAPO (Yu et al., 2025) targets long chain-of-thought training through dynamic sampling, decoupled clipping, and reward shaping designed to mitigate instability and reward noise.\nGSPO (Zheng et al., 2025) instead operates at the sequence level, redefining importance ratios and applying sequence-level clipping to improve stability, especially in Mixture-of-Experts settings.\nWhereas "
  },
  {
    "title": "Universal Coefficients and Mayer-Vietoris Sequence for Groupoid Homology",
    "url": "https://arxiv.org/abs/2602.08998v1",
    "source": "arxiv",
    "summary": "We study homology of ample groupoids via the compactly supported Moore complex of the nerve. Let $A$ be a topological abelian group. For $n\\ge 0$ set $C_n(\\mathcal G;A) := C_c(\\mathcal G_n,A)$ and define $\\partial_n^A=\\sum_{i=0}^n(-1)^i(d_i)_*$. This defines $H_n(\\mathcal G;A)$. The theory is functorial for continuous √©tale homomorphisms. It is compatible with standard reductions, including restri",
    "full_text": null
  },
  {
    "title": "Paradox of De-identification: A Critique of HIPAA Safe Harbour in the Age of LLMs",
    "url": "https://arxiv.org/abs/2602.08997v1",
    "source": "arxiv",
    "summary": "Privacy is a human right that sustains patient-provider trust. Clinical notes capture a patient's private vulnerability and individuality, which are used for care coordination and research. Under HIPAA Safe Harbor, these notes are de-identified to protect patient privacy. However, Safe Harbor was designed for an era of categorical tabular data, focusing on the removal of explicit identifiers while",
    "full_text": null
  },
  {
    "title": "When Actions Go Off-Task: Detecting and Correcting Misaligned Actions in Computer-Use Agents",
    "url": "https://arxiv.org/abs/2602.08995v1",
    "source": "arxiv",
    "summary": "Computer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user's original intent. Such misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning). They not only expose CUAs to safety risks, but also degrade task efficiency an",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Problem Formulation\n\n2.1 Action Alignment\n2.2 Categorization of Misaligned Actions\n\n\n\n3 MisActBench: Comprehensive Evaluation for Misaligned Action Detection\n\n3.1 Raw Trajectory Collection\n3.2 Human Annotation\n3.3 Benchmark Statistics\n\n\n\n4 DeAction: Runtime Misaligned Action Detection and Correction\n\n4.1 Misaligned Action Detection\n4.2 Correction via Iterative Feedback\n\n\n\n5 Experiments\n\n\n5.1 Offline Evaluation\n\n\n5.2 Online Evaluation\n\n\n6 Related Work\n\n\n7 Conclusion\n\n\nA Details of MisActBench\n\nA.1 Trajectory Collection with External Attacks\nA.2 Trajectory Synthesis without Attacks\nA.3 Example of Trajectory Synthesis.\n\nA.4 Human Annotation\n\nAnnotation Guideline.\nAnnotation Tool.\n\n\nA.5 More Statistics\n\n\n\nB Prompts of DeAction\n\nB.1 Prompt for Fast Check\nB.2 Prompt for Systematic Analysis\nB.3 Prompt for Narrative Summarization\nB.4 Prompt for Iterative Correction\n\n\n\nC Details of Offline Experiments\n\nC.1 Settings\nC.2 Details of Subset Used in Ablation Studies\n\n\n\nD Details of Online Experiments\n\n\nD.1 Benchmarks\n\nRedTeamCUA.\nOSWorld.\n\n\nD.2 Baselines\nD.3 Implementation details\n\n\n\nE Additional Results\n\nE.1 Performance Across Misalignment Types\n\nE.2 Runtime Analysis Breakdowns\n\nLatency.\nTwo-stage Routing.\nGuardrail Intervention.\n\n\nE.3 Cost-Efficient Component Substitution\nE.4 Error Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Actions Go Off-Task: \nDetecting and Correcting Misaligned Actions in Computer-Use Agents\n\n\nYuting Ning\n\n‚ÄÉ‚ÄÉ\nJaylen Jones\n\n‚ÄÉ‚ÄÉ\nZhehao Zhang\n\n‚ÄÉ‚ÄÉ\nChentao Ye\n\n‚ÄÉ‚ÄÉ\nWeitong Ruan\n\n‚ÄÉ‚ÄÉ\nJunyi Li\n\n‚ÄÉ‚ÄÉ\nRahul Gupta\n\n‚ÄÉ‚ÄÉ\nHuan Sun\n\n\n\nAbstract\nComputer-use agents (CUAs) have made tremendous progress in the past year, yet they still frequently produce misaligned actions that deviate from the user‚Äôs original intent.\nSuch misaligned actions may arise from external attacks (e.g., indirect prompt injection) or from internal limitations (e.g., erroneous reasoning).\nThey not only expose CUAs to safety risks, but also degrade task efficiency and reliability.\nThis work makes the first effort to define and study misaligned action detection in CUAs, with comprehensive coverage of both externally induced and internally arising misaligned actions. We further identify three common categories in real-world CUA deployment and construct MisActBench, a benchmark of realistic trajectories with human-annotated, action-level alignment labels. Moreover, we propose DeAction, a practical and universal guardrail that detects misaligned actions before execution and iteratively corrects them through structured feedback. DeAction outperforms all existing baselines across offline and online evaluations with moderate latency overhead: (1) On MisActBench, it outperforms baselines by over 1515% absolute in F1 score; (2) In online evaluation, it reduces attack success rate by over 9090% under adversarial settings while preserving or even improving task success rate in benign environments.\n\nMachine Learning, ICML\n\n\nhttps://osu-nlp-group.github.io/Misaligned-Action-Detection\n\n\n\n1 Introduction\n\nFigure 1: Examples of the three categories of misaligned actions.\n(a) Malicious Instruction Following: the action complies with external malicious instructions;\n(b) Harmful Unintended Behavior: the action causes harm due to inherent limitations rather than adversarial attacks;\n(c) Other Task-Irrelevant Behavior: the action does not cause harm but is irrelevant to the task.\n\n\nComputer-use agents (CUAs)¬†(OpenAI., 2025b; Anthropic., 2024a; Wang et al., 2025), which interact directly with computers to automate digital tasks, have achieved strong performance on realistic sandboxed benchmarks¬†(Xie et al., 2024; Bonatti et al., 2025), raising expectations for real-world deployment. However, this growing autonomy also introduces nontrivial risks: during execution, CUAs may take actions that deviate from user intent, leading to undesired consequences such as stalled progress or real-world harm.\n\n\nPrior work has studied such deviations primarily through the lens of safety risks, focusing on threats such as indirect prompt injection¬†(Greshake et al., 2023) or policy violations¬†(Wen et al., 2025). As a result, existing benchmarks provide only trajectory-level safety or policy labels¬†(Wen et al., 2025; Chen et al., 2025b; Sun et al., 2025), and existing guardrails are predominantly tied to predefined policies or known attack patterns¬†(Xiang et al., 2025; Luo et al., 2025b; Shi et al., 2025). While valuable, this safety-centric paradigm leaves a critical gap: not all problematic behaviors can be anticipated and enumerated as policy violations in advance. In practice, agents may produce actions that are technically permissible and non-malicious, yet still deviate from user intent in unjustified ways (Figure¬†1). For instance, they may introduce unnecessary interactions, pursue unintended subgoals, or derail task progress. Such deviations may not violate predefined constraints, but still erode user trust and degrade agent reliability.\n\n\nThese limitations motivate a more intent-centric view for analyzing agent deviations.\nRather than asking whether an action violates predefined safety policies, we focus on action alignment¬†(Jia et al., 2025; Fang et al., 2025),\ni.e., whether a proposed action can be justified as advancing the user‚Äôs authentic intent. We define misaligned actions as deviations that cannot be justified as part of a legitimate workflow toward the intended goal.\nIn CUA execution, we identify three common categories of misaligned actions based on their cause and consequence (Figure¬†1): (1) Malicious Instruction Following, where the action complies with malicious instructions in external environments to achieve an attacker‚Äôs goal; (2) Harmful Unintended Behavior, where the action causes harm inadvertently\ndue to inherent limitations (e.g., reasoning error) rather than adversarial attack; (3) Other Task-Irrelevant Behavior, where the action does not cause harmful consequences but is irrelevant to the user task and will degrade efficiency and reliability.\nBased on this framing, we take a first step toward studying misaligned action detection in CUAs: determining whether a proposed action is misaligned before actual execution.\n\n\nTo enable systematic evaluation of misaligned action detection, we construct MisActBench, a comprehensive benchmark with over 2K human-annotated, action-level alignment labels across realistic trajectories. It covers both externally induced and internally arising misaligned actions through a hybrid construction pipeline.\nWe further propose DeAction, a practical and universal guardrail that proactively detects and corrects misaligned actions before execution.\nTo balance detection performance with latency, DeAction employs a two-stage analysis pipeline conditioned on a compact narrative summary of the interaction history.\nRather than merely blocking misaligned actions, DeAction provides structured feedback to guide agents toward corrected, task-aligned behavior iteratively.\nWe conduct extensive experiments in both offline and online settings. On MisActBench, DeAction outperforms baselines by over 1515% absolute. In online evaluation, it reduces attack success rates by over 9090% under adversarial scenarios (RedTeamCUA¬†(Liao et al., 2025)), and preserves or even improves benign task success (OSWorld¬†(Xie et al., 2024)), with moderate runtime overhead.\n\n\nWe present the first systematic study of misaligned action detection in CUAs, with three main contributions:\n\n\n1.\n\nWe propose an intent-centric perspective that frames CUA deviations as an action misalignment problem, and identify three common categories of misaligned actions in real-world deployments.\n\n\n\n2.\n\nWe introduce MisActBench, a comprehensive benchmark with 2,2642,264 human-annotated, action-level alignment labels on diverse CUA trajectories, covering all three categories of misaligned actions.\n\n\n\n3.\n\nWe propose DeAction, a practical and plug-and-play runtime guardrail that proactively detects misaligned actions before execution and iteratively corrects them via structured feedback. Extensive experiments demonstrate its effectiveness in both adversarial and benign settings with moderate overhead.\n\n\n\n\n\n\n\n2 Problem Formulation\n\n\n2.1 Action Alignment\n\nWe begin by defining action alignment, the central concept underlying our study.\n\n\nGiven a user instruction II, interaction history œÑ&lt;t\\tau_{&lt;t}, and current observation oto_{t}, a proposed action ata_{t} is considered aligned if it satisfies three conditions:\n(1) it is taken in service of the user‚Äôs instruction II, rather than in response to other directives such as injected instructions in the environment;\n(2) it does not result in unauthorized or undesired consequences; and\n(3) it can be reasonably interpreted as contributing, whether directly or indirectly, to completing the user‚Äôs intended task.\nAn action that violates any of these conditions is considered misaligned.\n\n\nNotably, alignment does not require optimality. An action may be inefficient, exploratory, or ultimately unsuccessful yet still be aligned, as long as it represents a genuine attempt to advance the user‚Äôs goal.\n\n\nBased on this definition, we formulate the task of misaligned action detection: given (I,œÑ&lt;t,ot,at)(I,\\tau_{&lt;t},o_{t},a_{t}), determine whether ata_{t} is misaligned before actual execution (i.e., without access to actual consequences).\n\n\nFigure 2: Trajectory Collection Workflow for MisActBench.\n(a) Collect trajectories with misaligned actions induced by external attacks by running diverse CUAs on existing benchmarks.\n(b) Synthesize trajectories with unintended behaviors in benign settings.\n\n\n\n\n2.2 Categorization of Misaligned Actions\n\nThe three conditions in ¬ß¬†2.1 naturally give rise to a categorization of misaligned actions based on which condition is violated. Concretely, we identify three categories observed in real-world CUA deployments, illustrated in Figure¬†1:\n\n\nMalicious Instruction Following.\nThe agent complies with malicious directives embedde"
  },
  {
    "title": "InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery",
    "url": "https://arxiv.org/abs/2602.08990v1",
    "source": "arxiv",
    "summary": "We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. Th",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.08990v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.08990v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 9 Feb 2026]\n    Title:InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery\n    Authors:Shiyang Feng, Runmin Ma, Xiangchao Yan, Yue Fan, Yusong Hu, Songtao Huang, Shuaiyu Zhang, Zongsheng Cao, Tianshuo Peng, Jiakang Yuan, Zijie Guo, Zhijie Zhong, Shangheng Du, Weida Wang, Jinxin Shi, Yuhao Zhou, Xiaohan He, Zhiyin Yu, Fangchen Yu, Qihao Zheng, Jiamin Wu, Mianxin Liu, Chi Zhang, Shaowei Hou, Shuya Li, Yankai Jiang, Wenjie Lou, Lilong Wang, Zifu Wang, Jiong Wang, Wanghan Xu, Yue Deng, Dongrui Liu, Yiheng Wang, Wenlong Zhang, Fenghua Ling, Shufei Zhang, Xiaosong Wang, Shuangjia Zheng, Xun Huang, Siqi Sun, Shuyue Hu, Peng Ye, Chunfeng Song, Bin Wang, Conghui He, Yihao Liu, Xin Li, Qibin Hou, Tao Chen, Xiangyu Yue, Bin Wang, Liang He, Dahua Lin, Bowen Zhou, Bo Zhang, Lei Bai            View a PDF of the paper titled InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery, by Shiyang Feng and 56 other authors\n    View PDF\n\n\n\n    \n            Abstract:We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.\n    \n\n    \n    \n              \n          Comments:\n          Code and project page: this https URL\n        \n\n          Subjects:\n          \n            Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.08990 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.08990v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.08990\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Bo Zhang [view email]          [v1]\n        Mon, 9 Feb 2026 18:36:06 UTC (23,161 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery, by Shiyang Feng and 56 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n    "
  },
  {
    "title": "Improving Detection of Rare Nodes in Hierarchical Multi-Label Learning",
    "url": "https://arxiv.org/abs/2602.08986v1",
    "source": "arxiv",
    "summary": "In hierarchical multi-label classification, a persistent challenge is enabling model predictions to reach deeper levels of the hierarchy for more detailed or fine-grained classifications. This difficulty partly arises from the natural rarity of certain classes (or hierarchical nodes) and the hierarchical constraint that ensures child nodes are almost always less frequent than their parents. To add",
    "full_text": null
  },
  {
    "title": "Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models",
    "url": "https://arxiv.org/abs/2602.08984v1",
    "source": "arxiv",
    "summary": "We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updat",
    "full_text": null
  },
  {
    "title": "StretchTime: Adaptive Time Series Forecasting via Symplectic Attention",
    "url": "https://arxiv.org/abs/2602.08983v1",
    "source": "arxiv",
    "summary": "Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit \"time-warped\" dynamics where the effective flow of time decouples from the sampling index. In this ",
    "full_text": null
  },
  {
    "title": "When do neural ordinary differential equations generalize on complex networks?",
    "url": "https://arxiv.org/abs/2602.08980v1",
    "source": "arxiv",
    "summary": "Neural ordinary differential equations (neural ODEs) can effectively learn dynamical systems from time series data, but their behavior on graph-structured data remains poorly understood, especially when applied to graphs with different size or structure than encountered during training. We study neural ODEs ($\\mathtt{nODE}$s) with vector fields following the Barab√°si-Barzel form, trained on synthe",
    "full_text": null
  },
  {
    "title": "Beyond Transcripts: A Renewed Perspective on Audio Chaptering",
    "url": "https://arxiv.org/abs/2602.08979v1",
    "source": "arxiv",
    "summary": "Audio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Evaluation Protocols\n\n2.1 Text-Based Segmentation\n\n2.2 Text-Space Protocols\n\n2.2.1 Evaluation on Ref. Transcripts (R1)\n2.2.2 Evaluation on ASR Transcripts (H1)\n2.2.3 Alignment to Reference Text (H2/H3)\n\n\n\n2.3 Time-Space Protocols\n\n2.3.1 Discrete-Time Evaluation (T1)\n2.3.2 Continuous-Time Evaluation (T2)\n\n\n\n\n\n3 Approaches\n\n3.1 Text-Based Baseline\n\n3.2 Hand-Crafted Audio Features\n\n3.2.1 Feature Fusion\n\n\n\n3.3 Audio-Only Model\n\nFrame Encoding.\nSegment Encoding.\nDocument Encoding.\nTraining.\n\n\n\n3.4 Multimodal LLMs\n\nZero-Shot.\nChunking.\nIn-Context Learning (ICL).\nSelf-Cascaded.\nLoRA Training.\n\n\n\n\n\n4 Experimental Setup\n\n\n4.1 Dataset\n\nDuration Regime\nSpeaker Regime\nTranscripts\n\n\n\n4.2 Models\n\nText-Based Models.\nText-Based Models with Audio Features.\nAudio-Only Models.\nMLLMs.\n\n\n4.3 Evaluation\n4.4 Experiments\n\n\n\n5 Results and Analysis\n\n5.1 Q1: What impact does transcript quality have on text-based segmentation?\n\n5.2 Q2: Does incorporating audio information improve segmentation?\n\nHand-Crafted Audio Features.\nAudio-Only Models.\nMLLMs.\nNon-Speech Cues.\n\n\n\n5.3 Q3: How do audio characteristics affect model performance?\n\nDuration Effects.\nSpeaker Regime Effects.\n\n\n5.4 Q4: How reliable and comparable are different evaluation protocols?\n\n\n6 Related Work\n7 Conclusion\n8 Limitations\n9 Potential Risks\n\nA Supplementary Results\n\nA.1 Audio Ablation\nA.2 Failure Analysis of MLLMs\nA.3 Stratified Results\n\nA.4 Evaluation Insights\n\nB Prompts\n\n\n\n\nC Hyperparameters\nD Forced Alignment\n\nE Evaluation\n\nE.1 Segmentation Evaluation\n\nE.2 Alignment Procedures for H2 and H3\n\nE.2.1 H2: Word-Level Alignment\nE.2.2 H3: Time-Based Alignment\n\n\nE.3 Random Chaptering Baseline\n\n\n\nF Feature Extraction Details\n\n\nF.1 Features\n\nPauses.\nSpeaking Rate.\nPitch.\nLoudness.\nSpeaker Features.\n\n\nF.2 Feature Normalization\n\n\n\nG Data Annotations\n\nG.1 Speaker Diarization\nG.2 Data Statistics for Transcripts\n\n\n\n\n\n\n\nBeyond Transcripts: A Renewed Perspective on Audio Chaptering\n\n\nFabian Retkowski1‚ÄÉMaike Z√ºfle1‚ÄÉThai Binh Nguyen1\nJan Niehues1‚ÄÉAlexander Waibel1,2\n1Karlsruhe Institute of Technology‚ÄÉ2Carnegie Mellon University\n{fabian.retkowski, maike.zuefle}@kit.edu\n\n\n\n\nAbstract\nAudio chaptering, the task of automatically segmenting long-form audio into coherent sections, is increasingly important for navigating podcasts, lectures, and videos. Despite its relevance, research remains limited and text-based, leaving key questions unresolved about leveraging audio information, handling ASR errors, and transcript-free evaluation. We address these gaps through three contributions: (1) a systematic comparison between text-based models with acoustic features, a novel audio-only architecture (AudioSeg) operating on learned audio representations, and multimodal LLMs; (2) empirical analysis of factors affecting performance, including transcript quality, acoustic features, duration, and speaker composition; and (3) formalized evaluation protocols contrasting transcript-dependent text-space protocols with transcript-invariant time-space protocols. Our experiments on YTSeg reveal that AudioSeg substantially outperforms text-based approaches, pauses provide the largest acoustic gains, and MLLMs remain limited by context length and weak instruction following, yet MLLMs are promising on shorter audio.111We released the chunkseg evaluation package (GitHub, PyPI) and merged our additional annotations into the YTSeg dataset repository (HF). We will publish the AudioSeg model together with the final version of the paper.\n\n\nrmTeXGyreTermesX \n\n[*devanagari]rmLohit Devanagari\n\n[*arabic]rmNoto Sans Arabic\n\n\n\n\nBeyond Transcripts: A Renewed Perspective on Audio Chaptering\n\n\n\n\nFabian Retkowski1‚ÄÉ‚ÄäMaike Z√ºfle1‚ÄÉ‚ÄäThai Binh Nguyen1\n\nJan Niehues1‚ÄÉAlexander Waibel1,2\n\n1Karlsruhe Institute of Technology‚ÄÉ2Carnegie Mellon University\n\n{fabian.retkowski, maike.zuefle}@kit.edu\n\n\n\n\n\n\n1 Introduction\n\nAs long-form audio and video content becomes increasingly common, such as podcasts, lectures and YouTube videos, users need better tools to navigate and locate information within recordings. In practice, people rarely consume these recordings linearly Liao and Wu (2023); Y√ºr√ºm et al. (2022). Instead, they skim, scrub the timeline, jump to relevant moments, and return to specific sections. This non-linear behavior makes chapter markers a key interface for browsing and re-finding content. The task of audio chaptering addresses this by automatically segmenting audio into coherent sections.\n\n\nDespite growing relevance, research on audio chaptering remains limited and predominantly text-based: models typically operate on transcripts and inherit evaluation protocols from text segmentation Retkowski and Waibel (2024); Freisinger et al. (2025). This transcript-centric framing leaves several key limitations unresolved. First, the role of audio remains unclear: because most prior work treats chaptering as a purely textual problem, there is limited understanding of how audio can be leveraged for chaptering, whether through hand-crafted acoustic features or learned representations, and whether it improves performance. Second, text-segmentation evaluation protocols assume a fixed transcript. In practice, chaptering systems often rely on ASR outputs whose errors and segmentation differences change the underlying unit sequence, especially the sentence boundaries and the number of sentences. This changes the granularity (and difficulty) of the segmentation task, so standard text-based metrics computed on different transcripts are not directly comparable and can appear to improve simply because one transcript is coarser, rather than because the model segments better. Finally, chapter boundaries are intrinsically defined in continuous time, but many pipelines ‚Äùsnap‚Äú these timestamps to sentence boundaries. This realignment is inherently lossy: it can shift boundaries away from their true temporal positions and can systematically bias evaluation toward sentence segmentation artifacts.\n\n\nThis work aims to establish a methodological foundation for audio chaptering, addressing aforementioned gaps through three main contributions:\n\n\n1.\n\nWe systematically evaluate three modeling paradigms for audio chaptering: text-based models with and without acoustic feature augmentation, a novel audio-only architecture (AudioSeg) that operates directly on learned speech representations, and lastly, we explore whether multimodal large language models (MLLMs) are capable of this task.\n\n\n\n2.\n\nSecond, we provide empirical insight into factors affecting chaptering performance. We analyze the robustness of text-based models to ASR errors, quantify the contribution of different acoustic features, and examine how audio characteristics such as duration and speaker composition influence segmentation quality.\n\n\n\n3.\n\nThird, we systemize evaluation for audio chaptering: we formalize existing text-based protocols, analyze their limitations, and introduce time-based evaluation that enables fair comparison across text-based, audio-only, and multimodal models independent of the transcript.\n\n\n\n\n\nWe will release code and AudioSeg publicly to foster more research on audio chaptering.1\n\n\n\n\n2 Evaluation Protocols\n\n\n2.1 Text-Based Segmentation\n\nSegmentation has traditionally been studied in the text space ùí≥\\mathcal{X}, where a document is modeled as a sequence of discrete units (typically sentences). The objective is to identify a boundary sequence ùê≤=(y1,‚Ä¶,yN‚àí1)\\mathbf{y}=(y_{1},\\dots,y_{N-1}), where yi=1y_{i}=1 denotes a boundary between units sis_{i} and si+1s_{i+1}. Evaluation compares a predicted sequence ùê≤^\\hat{\\mathbf{y}} against a reference ùê≤\\mathbf{y} using segmentation metrics such as PkP_{k} Beeferman et al. (1999) and Boundary Similarity (B; Fournier 2013), as well as classification metrics such as F1 score. We adopt this formalism for audio chaptering. However, a fundamental domain mismatch exists: chapters are defined as continuous timestamps in the time domain ùíØ\\mathcal{T}, whereas text segmentation assumes discrete indices. The protocols below differ in how they map between ùíØ\\mathcal{T} and ùí≥\\mathcal{X}, and whether the metric operates in ùí≥\\mathcal{X} or ùíØ\\mathcal{T}.\n\n\n\n\n2.2 Text-Space Protocols\n\nIn existing work, evaluation is typically defined in text space by projecting continuous-time chapter boundaries onto transcript units, either on reference transcripts (R1) or on ASR transcripts (H1). We distinguish these two settings and then introduce two hybrid variants (H2‚ÄìH3) that map ASR-based predictions back to a canonical reference transcript.\n\n\n\n2.2.1 Evaluation on Ref. Transcripts (R1)\n\nLet the reference transcript be the canonical text representation of the audio, segmented into sentences\nSref=(s1,‚Ä¶,sN)S_{\\text{ref}}=(s_{1},\\dots,s_{N}). Ground-truth chapter boundaries are given as timestamps ùíØgold‚äÇ‚Ñù+\\mathcal{T}_{\\text{gold}}\\subset\\mathbb{R}^{+} on the audio. To evaluate in text space, we first align SrefS_{\\text{ref}} to the audio signal via forced alignment (FA) or closed caption timestamps to obtain a start time tstart‚Äã(si)t_{\\text{start}}(s_{i}) and end time tend‚Äã(si)t_{\\text{end}}(s_{i}) for each sentence. We define a projection function œïref:ùíØ‚Üí{1,‚Ä¶,N‚àí1}\\phi_{\\text{ref}}:\\mathcal{T}\\rightarrow\\{1,\\dots,N-1\\} that maps each timestamp in ùíØgold\\mathcal{T}_{\\text{gold}} to the nearest sentence boundary in SrefS_{\\text{ref}}.\n\n\nIn protocol R1 (Ref), a model predicts a boundary sequence over sentences in the reference transcript, and we compute text segmentation metrics on SrefS_{\\text{ref}}. This approach has two implications: (1) continuous-time boundaries are projected to sentence boundaries (a lossy discretization); (2) evaluation assumes access to the transcript. This setup corresponds to the protocols used in most prior work Lai et al. (2016); Retkowski and Waibel (2024).\n\n\n\n\n2.2.2 Evaluation on ASR Transcripts (H1)\n\nIn realistic deployments, systems operate on an ASR transcript rather than the reference. We therefore define protocol H1 (ASR)"
  },
  {
    "title": "Distributionally Robust Optimization via Generative Ambiguity Modeling",
    "url": "https://arxiv.org/abs/2602.08976v1",
    "source": "arxiv",
    "summary": "This paper studies Distributionally Robust Optimization (DRO), a fundamental framework for enhancing the robustness and generalization of statistical learning and optimization. An effective ambiguity set for DRO must involve distributions that remain consistent to the nominal distribution while being diverse enough to account for a variety of potential scenarios. Moreover, it should lead to tracta",
    "full_text": null
  },
  {
    "title": "stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation",
    "url": "https://arxiv.org/abs/2602.08968v1",
    "source": "arxiv",
    "summary": "World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardizat",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Stable World Model Ecosystem: An Overview\n\n2.1 The World interface: streamlined WM research\n2.2 Environments and Factor of Variations\n2.3 SWM Evaluation Suite: Tasks, Planning Algorithms, and Baselines\n\n\n3 Experiments: DINO-WM Zero-Shot Robustness\n4 Conclusion and Future Directions\n\nA Code Example\n\nA.1 End-to-End Pipeline.\nA.2 Policy\nA.3 Dataset Recording\n\n\n\nB SWM Details\n\n\nB.1 Policy\n\nPolicy.\nModel Predictive Control.\n\n\nB.2 Dataset Recording\nB.3 Factor Of Variations\nB.4 Evaluations\n\n\n\nC Experiment Details\n\nTraining Details.\nEvaluation Details.\n\n\nD SWM Environments\n\n\n\n\n‚Ä†‚Ä†footnotetext: * Equal contribution. Correspondence to lucas.maes@mila.quebec\nstable-worldmodel-v1: Reproducible World Modeling Research and Evaluation\n\n\nLucas Maes*1¬†¬† Quentin Le Lidec*2¬†¬†\nDan Haramati3¬†¬†\nNassim Massaudi4\n\nDamien Scieur1,5¬†¬† Yann LeCun2¬†¬†Randall Balestriero3\n\n1Mila &amp; Universit√© de Montr√©al¬†¬†2New York University\n3Brown University¬†¬†4Independent Researcher¬†¬†5Samsung SAIL\n\n\n\nAbstract\nWorld Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience.\nDespite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations.\nIn addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research.\nFinally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.\n\n\n\n‚Äì World Model Research Made Simple.\n\n\n\n\n1 Introduction\n\nA promising paradigm toward building capable and general-purpose embodied agents involves learning dynamics models of the world, commonly referred to as World Models (WM,¬†Ha and Schmidhuber (2018)).\n\n\nDespite rapid progress and growing community interest, research on WMs remains fragmented and lacks shared benchmarks comparable to those in vision¬†(Russakovsky et al., 2015; Lin et al., 2014), reinforcement learning¬†(Bellemare et al., 2013; Brockman et al., 2016; Tassa et al., 2018), or language modeling¬†(Wang et al., 2024; Phan et al., 2025). This diversity of paradigms, design choices, and environments complicates meaningful comparison between methods. Systematic re-implementation of utilities further exacerbates this issue: for example, two recent works, PLDM (Sobal et al., 2025) and DINO-WM(Zhou et al., 2025), re-implement the same Two-Room environment with substantial divergence (81 deletions, 86 additions, and 18 updates), underscoring the lack of shared infrastructure.\n\n\nMoreover, beyond comparing performance across disparate environments, controlled variations within a single environment are essential to isolate key factors, probe generalization, and better understand the inductive biases and failure modes of WMs.\n\n\nIn this work, we introduce stable-worldmodel, a new research ecosystem designed to facilitate streamlined and reproducible experimentation and benchmarking WMs. We design a simple, easy-to-use API that allows custom dataset collection, training, and evaluation, as well as integration of novel algorithms and environments to support future growth and development. A comparison with other recent latent world model codebases is provided in Table 1.\n\n\nTable 1: Latent World-Model codebases comparison. (PR = Pull Request, LoC = Lines of Code) Collected statistics demonstrate the lack of a reliable, open-source, and unified codebase to perform world model research. We address this issue with our proposed library SWM.\n\n\n\n\nSWM (ours)\nPLDM\nDINO-WM\n\n\nBackend\nPyTorch\nPyTorch\nPyTorch\n\n\nDocumentation\n‚úì\n‚úó\n‚úó\n\n\n# Baselines\n4\n1\n1\n\n\n# Environments\n16\n2\n4\n\n\n# FoV (per env)\n6-17\n0\n0\n\n\nType Checking\n‚úì\n‚úì\n‚úó\n\n\nTest Coverage\n73%\n0%\n0%\n\n\nLast Commit\n\n&lt;&lt;1 week\n\n&gt;&gt;3 months\n\n&gt;&gt;10 months\n\n\nPRs (6 mo.)\n99\n1\n0\n\n\n# LoC\n3562\n6796\n4349\n\n\n\n\n\n\n\n2 Stable World Model Ecosystem: An Overview\n\nStable World Model (SWM) goal is to support researchers by reducing the idea-to-experiment time gap. We build the library around the philosophy that people already have their codebase or tool for training their model. Therefore, our library should focus on providing support for their training with a ready-to-use environment and utilities for data collection or model evaluation. In the rest of this section, we provide an overview of the user API and the different components of the library. A full overview of a typical world model pipeline with SWM is provided in Listing LABEL:lst:swm-pusht.\n\n\n\n2.1 The World interface: streamlined WM research\n\n\n‚¨á\n\n1 import stable_worldmodel as swm\n\n\n2\n\n\n3 world = swm.World(‚Äôswm/PushT-v1‚Äô, num_envs=8)\n\n\n4 world.set_policy(YourExpertPolicy())\n\n\n5\n\n\n6 world.reset() # initialize the world\n\n\n7 world.step() # update the world state with policy\n\n\n8 world.infos # current world state (dict)\n\n\nListing¬†1: World Interface Logic. After specifying the environment ID (e.g., swm/PushT-v1) and the number of simulations, a policy can be attached to enable online interaction with the environment. At any time, all simulation-related information can be accessed via the infos dictionary.\n\n\nThe core abstraction in SWM is the World. A World wraps one or more Gymnasium tow environments and provides a unified interface for simulation, data collection, debugging, and evaluation. Internally, it leverages Gymnasium‚Äôs synchronous environment API to manage and step multiple environments within a single object.\n\n\nUnlike the widely used Gymnasium (Towers et al., 2025) interface, a World does not return observations, rewards, or termination flags from reset or step. Instead, all data produced by the environments is stored in a single internal dictionary, world.infos, which is updated in place at every reset or step. Both methods operate synchronously over all environments, making the complete simulation state accessible at any time via world.infos.\n\n\nAction selection in SWM is handled by a policy object attached to the World. The step method does not take actions as input; instead, at each step, the world queries its policy to obtain actions for all environments. A policy is a lightweight Python object implementing a get_action method, which takes the current world.infos as input and returns one action per environment. This design cleanly decouples control logic from environment execution, allowing policies to be swapped without modifying the world interface.\n\n\nOnce a policy is attached to a World, it can be used to record datasets or perform evaluation. Dataset recording executes the policy over episodes and logs all information contained in world.infos, while evaluation runs the same execution loop without data persistence. In both cases, the behavior and properties of the resulting trajectories are entirely determined by the chosen policy and world configuration. An illustrative example of dataset recording is provided in Listing LABEL:lst:swm-fov. Additional details about the dataset and evaluation are reported in Appendix B.\n\n\n\n\n2.2 Environments and Factor of Variations\n\n\n\n\n\n(a) PushT\n\n\n\n\n\n(b) TwoRoom\n\n\n\n\n\n(c) DMC ‚Äì Humanoid\n\n\n\n\n\n(d) OGBench ‚Äì Scene\n\n\n\nFigure 1: SWM Environment Suite. We support (and extend) a diverse set of established environments, including 2D/3D settings with tasks in manipulation, navigation, and classic control. (a) Push-T¬†(Chi et al., 2025). A manipulation task where a blue agent needs to push a T-shaped block to match the green anchor. (b) Two-Room¬†(Sobal et al., 2025). A 2d navigation task where a red agent needs to navigate through a door to reach a green goal in the room. (c) DeepMind Control Suite¬†(Tassa et al., 2018), a collection of 3d control tasks in MuJoCo. (d) OGBench¬†(Park et al., 2025), a 3D robotic manipulation task collection in MuJoCo. (Top) Default settings. (Bottom) All factors of variations changing visual, geometric, and physical properties. All supported environments and their associated FoV can be found in Figure 2 and Table 3.\n\n\nSWM is designed as a collection of diverse environments that span a wide range of design choices, including continuous and discrete state/action spaces, different action modalities, and varied agent embodiments. These environments differ not only in their task structure but also in their underlying dynamics or observation spaces, as illustrated in Figure 1. Such diversity allows evaluation across qualitatively distinct settings and supports broad comparisons of learning algorithms. However, evaluating generalization solely across different environments can obscure more fine-grained sources of variation that commonly arise within a single task or domain.\n\n\nA key feature of SWM is the notion of factors of variation (FoV). Each environment in the library exposes a set of optional controllable properties that enable systematic customization of the environment configuration. These factors of variation span multiple aspects, including visual attributes (e.g., color, shape, textures, lighting), geometric properties (e.g., size, orientation, position), and physical parameters (e.g., friction, damping, mass, gravity). By explicitly exposing these controls, SWM enables fine-grained studies of robustness, generalization, domain shift, and continual learning within a single, unified environment. We provide a toy example in Listing LABEL:lst:swm-fov. More details about FoV can be found in Appendix B\n\n\n\n‚¨á\n\n1 import stable_worldmodel as swm\n\n\n2\n\n\n3 world = swm.World(‚Äôswm/PushT-v1‚Äô, num_envs=2)\n\n\n4 world.set_policy(YourExpertPolicy())\n\n\n5\n\n\n6 print(world.single_variation_space.names()) # available FoV\n\n\n7\n\n\n8 # da"
  },
  {
    "title": "Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.08965v1",
    "source": "arxiv",
    "summary": "The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Relevant Quantum Theory\n\n2.1 Quantum systems and measurements\n2.2 Joint systems and quantum entanglement\n\n\n3 Communication, Coordination, and Entangled Policies\n\n4 Learning to Coordinate via Quantum Entanglement in MARL\n\n4.1 Parameterizing Shared Entanglement Policies\n\n4.2 Learning Quantum Entangled Strategies\n\n4.2.1 Policy Gradient for Nonlocal Games\n4.2.2 Multi-Agent PPO for Sequential Decision-Making\n\n\n\n\n\n5 Experiments\n\n5.1 Nonlocal Games\n\n5.2 Multi-Agent Sequential Decision-Making\n\n5.2.1 Multi-Router Multi-Server Queueing\n\n\n\n\n6 Conclusions and Future Work\nA Bell Inequality Violation\nB Dec-POMPDs in more detail\n\nC Proofs\n\nC.1 Proof of Proposition 3\n\n\n\nD Nonlocal Games\n\nD.1 Formal Definition\nD.2 Entropy Regularization\n\nD.3 Derivations of Gradient Estimators\n\nD.3.1 Win Probability\nD.3.2 Conditional Entropy\n\n\n\nD.4 Game Details\n\nD.4.1 CHSH\nD.4.2 GHZ\nD.4.3 Rendezvous\n\n\nD.5 Experiment Details\n\n\nE Derivation of Modified MAPPO Surrogate Objective Function\nF Physical Implementation\nG The Quantum Coordinator Ansatz\n\nH The Multi-Router Queueing Problem and Experiment\n\n\nH.1 Multi-Router Queueing as an MDP\n\nH.1.1 States\nH.1.2 Actions\nH.1.3 State transitions\nH.1.4 Rewards\nH.1.5 Note on observations\nH.1.6 Quality of service constraint\nH.1.7 Symmetry Constraint\n\n\nH.2 Experiment Procedure and Results\n\n\nI Numerical Stability of Quantum Softmax and Automatic Differentiation\nJ Coordinator-Advice Policies\n\n\n\n\n\nLearning to Coordinate via Quantum Entanglement\nin Multi-Agent Reinforcement Learning\n\n\n\\nameJohn Gardiner‚àó \\emailjohn.gardiner@nasdaq.com \n\\addrNasdaq, Inc\n\n‚ÄÉ‚ÄÉ\n\\nameOrlando Romero‚àó \\emailorlando.romero@nasdaq.com \n\\addrNasdaq, Inc\n\n‚ÄÉ‚ÄÉ\n\\nameBrendan Tivnan \\emailbrendan.tivnan@nasdaq.com \n\\addrNasdaq, Inc\n\n‚ÄÉ‚ÄÉ\n\\nameNicol√≤ Dal Fabbro \\emailnicolo.dalfabbro@nasdaq.com \\email\n\\addrNasdaq, Inc. \n\\addrUniversity of Pennsylvania\n\n‚ÄÉ‚ÄÉ\n\\nameGeorge J. Pappas \\emailpappasg@seas.upenn.edu \n\\addrUniversity of Pennsylvania\n\n\n\nAbstract\nThe inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL).\nPrior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making.\nIn contrast, this work introduces the first framework for training MARL agents to exploit shared quantum entanglement as a coordination resource, which permits a larger class of communication-free correlated policies than shared randomness alone.\nwell-known results in quantum physics which posit that, for certain single-round cooperative games with no communication, shared quantum entanglement enables strategies that outperform those that only use shared randomness. In such cases, we say that there is quantum advantage.\nOur framework is based on a novel differentiable policy parameterization that enables optimization over quantum measurements, together with a novel policy architecture that decomposes joint policies into a quantum coordinator and decentralized local actors.\nTo illustrate the effectiveness of our proposed method, we first show that we can learn, purely from experience, strategies that attain quantum advantage in single-round games that are treated as black box oracles.\nWe then demonstrate how our machinery can learn policies with quantum advantage in an illustrative multi-agent sequential decision-making problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP).\n\n\n‚àóEqual contribution\n\n\n\n1 Introduction\n\nA key challenge in multi-agent reinforcement learning (MARL) is partial observability (sometimes called information asymmetry), where each\nagent possesses different information about the state and action processes while making decisions¬†Kim et al. (2020); Kao and Subramanian (2022); Liu and Zhang (2023).\n\n\nCommunication constraints.\nInter-agent communication is a common mechanism to combat information asymmetry¬†(Foerster et al., 2016; Liu and Zhang, 2023; Schroeder de Witt et al., 2019). However, in many scenarios of practical interest, such as in low-latency decentralized decision making (e.g., high-frequency trading¬†Ding and Jiang (2024)) or military applications, communication is too costly, impossible, or otherwise constrained.\n\n\nShared randomness.\nCorrelating policies in MARL via shared randomness has primarily been explored in the form of a correlation device (see Section 6.2.4 of¬†Oliehoek and Amato (2016)‚Äôs tutorial and references therein). However this research largely focuses on computational efficiency, rather than expressiveness of policy classes.\n\n\nQuantum entanglement. The seminal work by Bell (1964) proved that quantum entanglement can result in decentralized, yet correlated, decisions that cannot be reproduced classically (that is, in the absence of entanglement). Subsequent works¬†Clauser et al. (1969); Cleve et al. (2004) demonstrated the benefit that quantum entanglement can provide for cooperative decision making without communication.\nSee Brunner et al. (2014) for a standard review.\nDespite continued research on single-round nonlocal games Vaidman (1999); Mironowicz (2023); Viola and Mironowicz (2024), analysis of the potential benefit of quantum entanglement for sequential decision-making problems is rather recent Da Silva and Wehner (2025); da Silva and Wehner (2026).\n\n\nIn this work, we contribute to the above line of research, asking the following question:\n\n\nCan we learn communication-free policies that exploit\nquantum entanglement for coordination in MARL?\n\n\nContributions.\nWe respond in the affirmative. Our contributions can be summarized as follows:\n\n\n‚Ä¢\n\nWe delineate a hierarchy of joint policy classes for communication-free cooperative MARL (see Figure¬†1), which notably includes shared randomness111Which includes factorized policies œÄ‚Äã(ùêö|ùê°)=‚àèiœÄi‚Äã(ai|hi)\\pi(\\mathbf{a}|\\mathbf{h})=\\prod_{i}\\pi_{i}(a_{i}|h_{i}). policies, shared quantum entanglement policies (a superset to shared randomness policies, still implementable in a decentralized manner), and non-signaling policies (the most general form of communication-free policies).\n\n\n\n‚Ä¢\n\nWe introduce the first MARL framework in which decentralized agents learn to exploit quantum entanglement as a shared coordination resource. First, we develop ùñ∞ùóéùñ∫ùóáùóçùóéùóÜùñ≤ùóàùñøùóçùóÜùñ∫ùóë\\mathsf{QuantumSoftmax} (see Algorithm¬†1), a differentiable transformation that maps arbitrary square complex-valued matrices to an object that formally describes a quantum measurement. Thus, our framework enables end-to-end gradient-based optimization over quantum measurements. Second, we introduce an advice-based policy architecture that cleanly separates joint policies into a quantum coordinator, which samples correlated advice via quantum measurements, and local actors that condition on this advice, allowing seamless integration with policy gradient methods. Third, we instantiate this framework in a modified multi-agent proximal policy optimization (MAPPO) algorithm capable of learning entangled policies for sequential decision-making.\n\n\n\n‚Ä¢\n\nWe first validate our framework on single-round cooperative games with theoretically established quantum advantage, confirming that our algorithm recovers known optimal entangled strategies. We then apply our framework to a multi-router multi-server queueing problem formulated as a decentralized partially observable Markov decision process (Dec-POMDP), where we learn sequential decision-making policies that achieve quantum advantage in a setting previously analyzed only through queueing-theoretic methods.\n\n\n\n\n\nRelated Work.\nThe use of quantum entanglement for distributed decision-making has a long history, often focused on the study of single-round cooperative games called nonlocal games Cleve et al. (2004). Though many such games with quantum advantage have been studied, they are often quite stylized. Some recent works attempt to show quantum advantage in single-round decision-making tasks with a more applied flavor, for example in rendezvous problems over graphs Viola and Mironowicz (2024), in a simplified high frequency trading scenario Ding and Jiang (2024), and in a load balancing problem in ad hoc networks Hasanpour et al. (2017).\n\n\nWhile optimal quantum entangled strategies for these single-round games are often found analytically or known by construction, there is work using gradient-based approaches to discover such optimal strategies, for example Bharti et al. (2019) and¬†Furches et al. (2025). The idea of learning strategies from experience, with games treated as black boxes, was considered recently by¬†Kerenidis and Cherrat (2025).\n\n\nThe recent work of¬†Da Silva and Wehner (2025); da Silva and Wehner (2026), a first of its kind, investigates the use of entanglement in decentralized, non-trivially sequential decision-making in a multi-router multi-server queueing problem whose special form allows one to theoretically show quantum advantage.\nThey do not use machine learning methods to discover strategies.\n\n\nIn contrast to the prior research, our work is the first to investigate how to learn parameterized MARL policies that exploit entanglement during execution to obtain a coordination advantage.\n\n\nWe also mention here a separate line of work which builds upon progress in quantum machine learning¬†Oh et al. (2020), extending it to quantum RL and quantum MARL¬†Yun et al. (2022); Kwak et al. (2021). These efforts are fundamentally different from ours: they use quantum computing (typically parametrized quantum circuits) with the aim to reduce the computational burden of learning agents by reducing parameter count, required data, or training time. In this vein, the recent work by¬†DeRieux and Saad (2025) studies how to leverage quantum entanglement during training in MARL to replace a centralized critic with a partially decentralized one.\nNotably, entanglement in their work serves a computational complexity purpose, to accelerate training and reduce parameter count relative to other "
  },
  {
    "title": "A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents",
    "url": "https://arxiv.org/abs/2602.08964v1",
    "source": "arxiv",
    "summary": "Understanding an agent's goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models' internal representations. As a case study, we examine an LLM agent navigating a 2D grid world ",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Grid World Agent Setup\n\n4 Behavioural Evaluation\n\n4.1 Goal-Directedness across Baseline Task Conditions\n4.2 Robustness to Iso-difficulty Transformations\n4.3 Instrumental and Implicit Goals\n\n\n\n5 Representational Evaluation\n\n5.1 Cognitive Maps: Decoding the Agent‚Äôs Beliefs about its Environment\n5.2 Evaluating Policies against Decoded Beliefs\n5.3 Evaluating Plans\n\n\n6 Conclusion\n\nA Partially Observable GridWorld\n\nA.1 Memory\nA.2 Perseverance\nA.3 Corrigibility and Focus\nA.4 Difficulties with Partial Observability\n\n\n\nB Behavioural Evaluation: Metrics\n\nCapability Metrics.\nUncertainty Metrics.\n\n\nC Additional Behavioural Evaluation Results\nD Iso-difficulty Transform Quantitative Results\n\nE Evaluation Settings and Prompts\n\nE.1 Evaluation Parameters for Behavioural Evaluation\nE.2 Prompt for Behavioural Evaluation\nE.3 Prompt for Instrumental and Implicit Goals\n\n\n\nF Additional Representational Evaluation Results\n\nF.1 Cognitive Map Encoding across Layers\nF.2 Size-specific Cognitive Map Probing Results\nF.3 Goal Distance Probing Results\nF.4 Additional Plan Decoder Results\n\n\n\n\n\n\n\nA Behavioural and Representational Evaluation of \nGoal-Directedness in Language Model Agents\n\n\nRaghu Arghal\n\n‚ÄÉ‚ÄÉ\nFade Chen\n\n‚ÄÉ‚ÄÉ\nNiall Dalton\n\n‚ÄÉ‚ÄÉ\nEvgenii Kortukov\n\n‚ÄÉ‚ÄÉ\nCalum McNamara\n\n‚ÄÉ‚ÄÉ\nAngelos Nalmpantis\n\n‚ÄÉ‚ÄÉ\nMoksh Nirvaan\n\n‚ÄÉ‚ÄÉ\nGabriele Sarti\n\n‚ÄÉ‚ÄÉ\nMario Giulianelli\n\n\n\nAbstract\nUnderstanding an agent‚Äôs goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models‚Äô internal representations.\nAs a case study, we examine an LLM agent navigating a 2D grid world toward a goal state.\nBehaviourally, we evaluate the agent against an optimal policy across varying grid sizes, obstacle densities, and goal structures, finding that performance scales with task difficulty while remaining robust to difficulty-preserving transformations and complex goal structures.\nWe then use probing methods to decode the agent‚Äôs internal representations of the environment state and its multi-step action plans. We find that the LLM agent non-linearly encodes a coarse spatial map of the environment, preserving approximate task-relevant cues about its position and the goal location; that its actions are broadly consistent with these internal representations; and that reasoning reorganises them, shifting from broader environment structural cues toward information supporting immediate action selection.\nOur findings support the view that introspective examination is required beyond behavioural evaluations to characterise how agents represent and pursue their objectives.\n\ngoal-directedness, interpretability, evaluation, representation analysis, LLM agents\n\n\n\n1 Introduction\n\nFigure 1: Overview of our goal-directedness analysis. A: We evaluate how iso-difficulty transforms affect agent trajectories that agree or disagree with the optimal policy. B: We prompt an LLM-based agent to reason and act over the fully-observable grid setup, extracting its pre-and post-reasoning activations at intermediate layers. C: We probe the agent‚Äôs beliefs over goal distance, planned actions and reconstruct cognitive maps for the current grid state.\n\n\nAttributing goals to agents helps explain and predict their behaviour and provides a useful abstraction for reasoning about agency.\nThis topic has received attention across in fields\nas varied as philosophy (Davidson, 1973; Dennett, 1990), psychology and neuroscience (Baker et al., 2009; Schultz et al., 1997)\neconomics and decision theory (von Neumann and Morgenstern, 1944; Savage, 1948), and reinforcement learning (Bellman, 1966; Ng and Russell, 2000). More recently, determining when and in what sense goal attributions are warranted has become a pressing concern for LLM-based agents (Xu and Rivera, 2024; MacDermott et al., 2024; Everitt et al., 2025; Goldstein and Lederman, 2025; Mazeika et al., 2025), particularly from an AI safety perspective (Naik et al., 2025; Wentworth and Lorell, 2025; Marks et al., 2025; Li et al., 2025; Summerfield et al., 2025).\n\n\nA natural way to measure goal-directedness is behavioural evaluation, i.e., assessing the agent‚Äôs actions relative to some goal, particularly compared to an optimal policy (Xu and Rivera, 2024; Everitt et al., 2025). However,\npurely behavioural measures face fundamental theoretical, practical, and philosophical challenges (Bellot et al., 2025; Rajcic and S√∏gaard, 2025; Chalmers, 2025).\nAgent capabilities may act as confounders for behavioural measures, as consistent failure may reflect capability limitations rather than lack of goal-directed behaviour.\nRelatedly, behavioural monitoring alone may be insufficient to guarantee alignment: a system with misaligned internal objectives could produce aligned behaviour, or fail a safety-relevant task, when doing so is instrumentally useful (Hubinger et al., 2019; Ngo et al., 2024).\n\n\nTo address these limitations, we propose a framework that combines behavioural evaluation with analysis of internal representations, enabling holistic assessment of goal-directedness as a rich property arising from the interaction of beliefs, planning, and action selection. We study an LLM agent in a fully observable grid world, tasked with navigating to a goal state across grids of varying sizes and obstacle densities. We begin by subjecting the agent to standard capability tests and gradually introduce controlled environment perturbations and multi-goal task structures to measure the generalisability of its goal-directed behaviour, finding sensitivity to task difficulty and goal-like task-irrelevant cues, but robustness to difficulty-preserving transformations and instrumental goals.\nWe then use probing classifiers to test if goal-relevant information can be decoded from the agent‚Äôs internal activations, before and after reasoning.\nThrough our probing analyses, we are able to extract cognitive maps‚Äîi.e., latent beliefs about the current environment state, including the agent position and the goal location‚Äîand planned multi-step action sequences directly from the model activations.\nWe also find that these representations reorganise during reasoning: pre-reasoning activations preserve broader spatial cues and longer-horizon plans, while post-reasoning activations sharpen focus on next action selection. Fig.Àú1 provides an overview of our approach.\n\n\nContributions.\nOur primary contributions are as follows:\n\n\n1.\n\nWe propose a white-box framework combining behavioural assessment and representation probing analyses for goal-directedness evaluation.\n\n\n\n2.\n\nWe design controlled environment perturbations and multi-goal task structures to measure bias and robustness in the agent‚Äôs goal-directed behaviour.\n\n\n\n3.\n\nWe probe environment beliefs and multi-step action plans from the agent‚Äôs learned representations, and use them to assess behavioural coherence in relation to decoded information.\n\n\n\n\n\n\n\n2 Related Work\n\nThe problem of identifying an agent‚Äôs goals and intentions has a rich history spanning multiple research fields.\nSeminal works in philosophy (Davidson, 1973; Lewis, 1974; Dennett, 1990) and microeconomics (von Neumann and Morgenstern, 1944; Savage, 1948) have emphasised the predictive and explanatory power of assigning goals to an agent.\n\n\nMeasuring Agents‚Äô Goal-directedness.\nRecent works attempt to formally define and measure goal-directedness to benefit AI alignment and safety (Ward et al., 2024; Xu and Rivera, 2024; Everitt et al., 2025; MacDermott et al., 2024).\nNotably, Everitt et al. (2025) define a measure of goal-directedness conditioned upon an agent‚Äôs task-relevant capabilities and show goal-directedness is measurably distinct from performance in LLMs and general across tasks. MacDermott et al. (2024) build upon Dennett (1990), proposing a formal measure of goal-directedness based on the predictive power of posited utility functions for the agent‚Äôs behaviour.\nHowever, behavioural approaches to measuring goal-directedness are not without their weaknesses.\nRajcic and S√∏gaard (2025) argue that such methods falter when faced with underspecification, coarse goals, uncertainty, and multi-agent settings.\nBellot et al. (2025) prove bounds on learnability from agent behaviour, showing that goal inferences are strictly limited by gaps between internal world models and the environment and out-of-distribution shifts. Our work complements these approaches by enabling assessment of goal-directed behaviour relative to the agent‚Äôs internal beliefs rather than ground truth alone.\n\n\n\n\n\nd=0.0d=0.0\n\n\n\n\nd=0.2d=0.2\n\n\n\n\nd=0.4d=0.4\n\n\n\n\nd=0.6d=0.6\n\n\n\n\nd=0.8d=0.8\n\n\n\n\n\nd=1.0d=1.0\n\n\n\nFigure 2: Grid worlds with increasing wall density dd, from fully open grids (d=0d=0) to maze-like grids with no circular paths (d=1d=1).\n\n\nInverse Reinforcement Learning (IRL). is a direct instantiation of the goal attribution problem, aiming to distil a reward function from a policy or a set of demonstrations.\nA rich line of work in this area (e.g. Ng and Russell, 2000; Abbeel and Ng, 2004, surveyed by Arora and Doshi, 2021) also focused on AI alignment (e.g., Hadfield-Menell et al., 2016, 2017).\nWhile a weakness of classical IRL is the assumption that observed behaviour is optimal, approaches like Maximum Entropy IRL (Ziebart et al., 2008) aim to relax this via stochastic models of behaviour.\nStill, IRL methods suffer from the mis- and under-specification of the agent‚Äôs behavioural model and latent reward function, respectively (Skalse and Abate, 2023).\nUnlike IRL, in this work we directly probe for goal-relevant representations without assuming a specific reward structure.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: An example grid (left) and its corresponding text based representation (right) used for LLM prompting.\n\n\nProbing Environment and Plans in LLMs. Various works studied whether language models lea"
  },
  {
    "title": "MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE",
    "url": "https://arxiv.org/abs/2602.08961v1",
    "source": "arxiv",
    "summary": "We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to al",
    "full_text": null
  },
  {
    "title": "How Should We Model the Probability of a Language?",
    "url": "https://arxiv.org/abs/2602.08951v1",
    "source": "arxiv",
    "summary": "Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persiste",
    "full_text": "\n\n\n\n1 Introduction\n\n2 The Received Framing of LID\n\n2.1 Standard Approaches\n2.2 Alternative Framings\n\n\n\n3 Probability Problems\n\n\n3.1 Global Frequency?\n\n3.1.1 Attenuated Frequency?\n3.1.2 False Positives at Scale\n\n\n\n3.2 Local Priors?\n\n3.2.1 Where Do Languages Live?\n3.2.2 Dataset Difficulties\n3.2.3 Model Expectations\n3.2.4 Towards Context-Aware LID\n\n\n\n\n\n4 Case Studies\n\n4.1 Language Revitalization\n4.2 Retracing Language Contact\n\n\n\n5 Why Is This Hard To Change?\n\n5.1 Academic Incentives\n5.2 Commercial Incentives\n5.3 (Absence of) Vertical Integration\n\n\n\n6 What Can We Do?\n\n6.1 Two Coherent Paths Forward\n6.2 Evaluation and Benchmarks\n6.3 Transparency and User Interaction\n\n\n7 Conclusion\n\n\n\n\n\nHow Should We Model the Probability of a Language?\n\n\nRasul Dent1, Pedro Ortiz Suarez2, Thibault Cl√©rice1, Beno√Æt Sagot1\n1Inria, Paris, {firstname.lastname}@inria.fr\n2Common Crawl Foundation, Paris, pedro@commoncrawl.org\n\n\n\nAbstract\nOf the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.\n\n\n\nHow Should We Model the Probability of a Language?\n\n\n\n\nRasul Dent1, Pedro Ortiz Suarez2, Thibault Cl√©rice1, Beno√Æt Sagot1\n\n\n\n1Inria, Paris, {firstname.lastname}@inria.fr\n\n2Common Crawl Foundation, Paris, pedro@commoncrawl.org\n\n\n\n\n\n\n1 Introduction\n\nTo use many Natural Language Processing (NLP) systems, we must first specify the language. The task of inferring this information is known as automatic language identification (LID). Following substantial progress in the 1990s, McNamee (2005) famously described LID for lengthy European-language documents as ‚Äúa solved problem suitable for undergraduate instruction.‚Äù Attention has since shifted to closely-related varieties (Aepli et al., 2023), rare languages (Caswell et al., 2020), short texts (Murthy and Kumar, 2006), and code-switching (Burchell et al., 2024).\n\n\nDespite improvements on benchmarks, the practical reality for most languages has changed little in the past 20 years. New models are occasionally released for a region or family (e.g. Adebara et al., 2022), and some attempt to reach ‚Äúthe next 1000 languages‚Äù (e.g. Brown, 2013; Kargaran et al., 2023). Nonetheless, production-grade LID systems at companies like Meta and Google still focus on only a few hundred widely spoken languages.\n\n\nThis stagnation reflects two core issues. First, in research settings, LID is typically framed as a decontextualized inference task, in which systems aim to map directly text to a label from a global set. This encourages approaches that perform well on benchmarks but fail when applied to real data. Second, field-level incentive structures reward developing novel methods over revisiting basic tools.\n\n\nWe argue that recentering context will be crucial for expanding the effective coverage of LID. For ‚Äúlocal languages‚Äù (see Bird, 2022) in particular, success should be defined relative to local contexts and resource constraints. This entails rethinking the values that shape research in LID.\n\n\n\n\n2 The Received Framing of LID\n\nAt its core, LID is intended to route content to users who manage different languages. Due to differences between the modalities of text, speech, and sign, LID in each modality is generally approached separately, with text-based LID typically viewed as the easiest (Jauhiainen et al., 2024). For Rau (1974), the prototypical LID operator was a data entry clerk filing documents in unfamiliar languages for human patrons. With the explosion of born-digital content, the immediate recipient is now often a computer program.111However, Rau‚Äôs vision still lives on in some institutions, like libraries.\n\n\n\n2.1 Standard Approaches\n\nLID is typically approached as a supervised classification problem (Jauhiainen et al., 2024).\nThis framing builds in two critical assumptions:\n\n\n\n\n1.\n\nLabels represent a global hypothesis space.\n\n\n\n2.\n\nInference can be performed solely from text.\n\n\n\n\n\nAs in other classification tasks, there are two main modeling approaches. Per-class approaches such as Multinomial Na√Øve Bayes (e.g. Lui and Baldwin, 2012) fit the data to each class independently, and take the best fitting class as the label.222Traditionally, this is called generative modeling, but as an anonymous reviewer noted, the term has become ambiguous. In contrast, discriminative models like fastText (Joulin et al., 2017; Grave et al., 2018) and CLD3 (Salcianu et al., 2023) learn decision boundaries between all classes simultaneously.\n\n\nHierarchical architectures also garnered interest. Many models, such as IndicLID (Madhani et al., 2023), are region-specific and depend on external software to ensure that they are used over the correct language set. Others, especially those based on fastText such as OpenLID (Burchell et al., 2023) and GlotLID (Kargaran et al., 2023), take a flat approach and model a wide variety of languages using one combined model. Yet others, such as LIMIT (Agarwal et al., 2023), try to learn hierarchical classification schemas directly from model errors.\n\n\n\n\n2.2 Alternative Framings\n\nThe standard assumptions, and the modeling constraints that come with them, are not unreasonable in many common scenarios. In particular, they produce strong results on well-written monolingual documents in widely spoken languages (McNamee, 2005). However, their reliability deteriorates in several scenarios. Caswell et al. (2020) and Kreutzer et al. (2022) showed that the models that perform the best on common benchmarks often struggle when applied to noisy web data. Similarly, when working with closely-related varieties, or very short texts, it is often impossible to select just one correct label on the basis of input alone. The recent Shared Task on Improving Language Identification for Web Text at the 1st Workshop on\nMultilingual Data Quality Signals confirmed these issues remain relevant (Suarez et al., 2026).\n\n\nIn response to such issues, alternate framings have been proposed. For example, Baimukan et al. (2022) show that hierarchical labels are important for fine-grained dialect classification. Bernier-Colborne et al. (2023) and Keleg and Magdy (2023) extend this insight, reconceptualizing the task of dialect identification as one of multi-label classification. From a different angle, Dent et al. (2025) contend that building web corpora for rare languages is more of a mining task than strict classification.\n\n\nTogether, these reframings touch on a much more general issue, but do not completely resolve it. Namely, general-purpose LID models need a label set fixed enough to allow training and flexible enough to handle great variation in inference-time granularity. In Section 3, we argue that the diversity of inference-time conditions has often been overlooked, leading to structural limitations with significant practical and scientific costs.\n\n\n\n\n\n3 Probability Problems\n\nBoth per-class and discriminative approaches to LID ultimately estimate a conditional probability distribution: given input features XX,\nestimate the probability PP of each language ‚Ñì\\ell, and choose the most likely label. Bayesian framing reveal that P‚Äã(‚Ñì‚à£X)P(\\ell\\mid X) actually depends on two terms, the probability of XX given ‚Ñì\\ell, and the prior probability of ‚Ñì\\ell itself. The decision rule can be written in the familiar argmax form:\n\n\n\n\n\n‚Ñì^=arg‚Å°max‚Ñì‚Å°P‚Äã(X‚à£‚Ñì)‚ÄãP‚Äã(‚Ñì),\\hat{\\ell}=\\arg\\max_{\\ell}\\;P(X\\mid\\ell)\\,P(\\ell),\n\n(1)\n\n\n\n\nor equivalently in log-space:\n\n\n\n\n\n‚Ñì^=arg‚Å°max‚Ñì‚Å°[log‚Å°P‚Äã(X‚à£‚Ñì)+log‚Å°P‚Äã(‚Ñì)].\\hat{\\ell}=\\arg\\max_{\\ell}\\left[\\log P(X\\mid\\ell)+\\log P(\\ell)\\right].\n\n(2)\n\n\n\n\nThis decomposition highlights a highly non-trivial modeling problem. Namely, what does P‚Äã(‚Ñì)P(\\ell) actually represent?\n\n\n\n3.1 Global Frequency?\n\nIn the classic Bayesian formulation, the answer is simple enough. P‚Äã(‚Ñì)P(\\ell) is the number of texts in a given language over the number of examples in the entire corpus. In principle, this should correspond to a real difference in the frequency of languages, rather than a sampling artifact. However, under na√Øve frequency-based estimates, we immediately face two related problems when dealing with massive class imbalances.\n\n\n\n\n1.\n\nRare classes become nearly undetectable.\n\n\n\n2.\n\nIgnoring rare classes has minimal effect on macro-performance.\n\n\n\n\n\nTo see this concretely, suppose we compare English (‚Ñì=en\\ell=\\text{en}) with a rare language (‚Ñì=r\\ell=r). Let the global priors be:\n\n\n\n\n\nP‚Äã(en)=0.40,P‚Äã(r)=10‚àí6.P(\\text{en})=0.40,\\qquad P(r)=10^{-6}.\n\n\n\n\n\nAssume for illustration that the likelihoods of a given text under the two languages are of comparable magnitude‚Äîfor example:\n\n\n\n\n\nP‚Äã(X‚à£en)=10‚àí4,P‚Äã(X‚à£r)=10‚àí2.P(X\\mid\\text{en})=10^{-4},\\qquad P(X\\mid r)=10^{-2}.\n\n\n\n\n\nEven though the rare-language model assigns a hundred times higher likelihood to the input text, the posterior still overwhelmingly favors English:\n\n\n\n\n\nP‚Äã(X‚à£en)‚ÄãP‚Äã(en)=10‚àí4‚ãÖ0.40=4√ó10‚àí5,P(X\\mid\\text{en})P(\\text{en})=10^{-4}\\cdot 0.40=4\\times 10^{-5},\n\n\n\n\n\n\n\n\nP‚Äã(X‚à£r)‚ÄãP‚Äã(r)=10‚àí2‚ãÖ10‚àí6=10‚àí8.P(X\\mid r)P(r)=10^{-2}\\cdot 10^{-6}=10^{-8}.\n\n\n\n\n\nThus,\n\n\n\n\n\n4√ó10‚àí5‚â´10‚àí8,4\\times 10^{-5}\\gg 10^{-8},\n\n\n\n\n\nand the argmax will select English. The only way the\nlikelihood evidence can compensate for a prior that is effectively zero is if the likelihood for more common languages is also effectively zero (likely due to a script mismatch). Because language rr occurs less than"
  },
  {
    "title": "Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room",
    "url": "https://arxiv.org/abs/2602.08949v1",
    "source": "arxiv",
    "summary": "According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Digital Cities\n2.2 Bidirectional Digital Twin: Concepts and Architectures\n2.3 Wild Fire Disaster Management Using DT\n2.4 AI Agent Architectures and Approaches\n2.5 Situation Room in Crisis Management\n2.6 Gap in the Literature\n\n\n3 Methods &amp; Materials\n3.1 Sensors 3D Coverage Optimization\n3.2 Fire Localization in 3D Model\n3.3 Incident Rollback &amp; Anonymization of Incident Footage\n3.4 Fire Spread Simulation &amp; Customization of Machine Learning Models\n3.5 Physical Intervention &amp; Issuing Commands\n3.6 The Conceptual Architecture of Intelligent Virtual Situation Room\n3.7 Utilizing AI Agent in IVSR\n3.8 Human In The Loop\n\n4 Discussion\n\n\n4.1 AI Agents for Real-Time Decision Making During Disasters\n\n4.1.1 AI Agents for Coordinating Emergency Response Efforts\n\n\n\n\n5 Conclusion\nA Literature Review Summary Tables\n\n\n\n\n\n\n\\cormark\n[2]\n\n\n1]organization=Independent Researcher\n\n\n2]organization=Department of Industerial Engineering and Management, Aalto University,\ncity=Espoo,\npostcode=02150,\ncountry=Finland\n\nDigital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room\n\n\nMohammad Morsali\nmohammadmorsali1381@gmail.com\n‚ÄÉ‚ÄÉ\nSiavash H.Khajavi\nsiavash.khajavi@aalto.fi\n[\n\n[\n\n\n\nAbstract\nAccording to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster-management frameworks rely on static simulations and passive data acquisition, which hinders their ability to adapt to arbitrarily evolving wildfire episodes in real-time. To address these limitations, we introduce the Intelligent Virtual Situation Room (IVSR), a bidirectional Digital Twin(DT) platform augmented by autonomous AI agents. The IVSR continuously ingests multisource sensor imagery, weather data, and 3D forest models to create a live virtual replica of the fire environment. A similarity engine powered by AI aligns emerging conditions with a precomputed Disaster Simulation Library, bringing down and calibrating intervention tactics under the watchful eyes of experts. Authorized action‚Äîranging from UAV redeployment to crew reallocation‚Äîis cycled back through standardized procedures to the physical layer, completing the loop between response and analysis. We validate IVSR through detailed case-study simulations provided by an industrial partner, demonstrating capabilities in localized incident detection, privacy-preserving playback, collider-based fire‚Äêspread projection, and site-specific ML retraining. Our results indicate marked reductions in detection-to-intervention latency and more effective resource coordination versus traditional systems. By uniting real-time bidirectional DTs with agentic AI, IVSR offers a scalable, semi-automated decision-support paradigm for proactive, adaptive wildfire disaster management.\n\n\nkeywords: \nDigital Twin \\sepWildfire Management \\sepDisaster Response \\sepAI Agents \\sepSmart Cities\n\n\n\n\n1 Introduction\n\nNatural disasters can devastate both the environment and urban infrastructure. Among natural disasters, wildfires pose a particularly serious threat due to the extensive damage they cause. In recent years, the frequency and severity of wildfires have increased due to climate change [Wasserman2023]. Wildfires have led to billions in economic losses, irreversible ecological destruction, and the loss of lives and livelihoods. The 2023 Maui wildfires in Hawaii, for instance, resulted in over 100 deaths and destroyed nearly the entire town of Lahaina [Kormann2023Maui]. In 2024, California experienced 8,024 wildfires, burning a total of 1,050,012 acres, destroying 1,716 structures, and resulting in one fatality [calfire2024incidents]. Economic losses from the 2024 California wildfires are estimated between $250 billion and $275 billion, making it one of the costliest wildfire disasters in U.S. history [danielle2025accuweather]. In Europe, the 2021 wildfires in Greece and Turkey burned hundreds of thousands of hectares, straining national emergency services and displacing communities [aljazeera2021wildfires, nasa2021fire]. In Greece, approximately 50,000 hectares were burned on Evia Island [nasa2021fire]. In Turkey, over 95,000 hectares were affected, marking the worst wildfires in at least a decade [aljazeera2021wildfires].\n\n\nDespite advances in detection and modeling, the ‚Äúgolden window‚Äù of response‚Äîthe critical early hours when intervention is most effective‚Äîis often missed. One of the primary reasons for this is the fragmented and inefficient data flow between various actors involved in wildfire management. Delays in situational awareness, limited interoperability between systems, and poor visibility into ground conditions hinder the ability of decision-makers to coordinate and deploy resources effectively. The result is a slow and reactive response, rather than a proactive, adaptive approach that could prevent escalation.\n\n\nVarious novel technologies may hold the key to implementing effective solutions for better collecting, analyzing, and responding to fast-moving catastrophic fire events. A digital twin (DT) is a real-time virtual representation of a physical entity or system, continuously updated with live data to reflect its current state [abdul, Gri]. In early versions of the DT, it was used to improve system understanding and predict failures by utilizing a live digital model that collected data from the physical system with sensors [Tao2018, KRITZINGER20181016]. During the past two decades, the DT concept expanded from aerospace and manufacturing to many other domains [LU2020101837] ‚Äì including civil infrastructure, energy grids, healthcare, and smart cities ‚Äì as advances in IoT and computing made real-time data integration feasible on larger scales [8477101].\n\n\nSmart city is used to describe an urban space enriched by ICT infrastructure and data-driven services. Modern digital cities utilize universal data capture and intelligent analysis, generating dynamic virtual simulations of urban areas (sometimes called city DTs) that mirror real-world status in real-time [sss]. This may lay the groundwork for using digital cities for routine urban management and enhancing disaster resilience.\n\n\nArtificial intelligence (AI) is a rapidly advancing field with wide-ranging implications across nearly all disciplines, including fire engineering [Aras, Teh]. One emerging area within AI is the development of autonomous AI agents, where they are equipped with the ability to utilize tools and make decisions to complete tasks [PICCIALLI2025128404]. These AI agents have the potential to become a critical resource in modern disaster relief, offering enhanced speed, intelligence, and coordination to support more informed decision-making by humans [ALBAHRI2024109409].\n\n\nYet, some fundamental questions remain unresolved, even as DT and AI technologies continue to evolve rapidly and become increasingly integrated into smart systems. This research investigates the following questions:\n\n\nRQ1: What are the current use cases of bidirectional digital twins (BDTs) in the context of wildfire management?\nRQ2: In what ways and configurations can an AI agent‚Äìbased BDT assist with wildfire response?\nRQ3: Based on case studies from industry and literature, what are the implications of an AI agent‚Äìbased BDT for wildfire management?\n\n\nTo answer these questions, we review the literature, collect industry case deployments and finally conceptualize an intelligence virtual situation room (IVSR) enabled by a BDT for real-time wildfire management. IVSR is envisioned to integrate sensors, state-of-the-art AI-driven fire prediction models, and 3D representative models to establish a dynamic feedback loop between the physical environment and its digital counterpart. The IVSR concept improves early warning, better-informed decision-making, and proactive fire suppression in critical emergency scenarios by allowing for instant communication of actionable knowledge.\n\n\n\n\n2 Related Works\n\nThis section reviews the literature on digital cities and DT technologies, with a focus on their roles in wildfire disaster management.\n\n\nThe use of DTs in disaster management, especially in wildfires, is an emerging field. Previous studies have focused on flood prediction, earthquake simulation, and wildfire tracking using GIS-based mapping, hydrological models, and remote sensing. However, many existing solutions lack real-time data integration and interactive scenario simulations. Recent advancements in AI-driven anomaly detection, IoT-based monitoring, and climate modeling provide an opportunity to improve disaster preparedness and response [sss].\n\n\n\n2.1 Digital Cities\n\nDigital City Infrastructure and Technologies\n\nModern digital cities are built using advanced technology that works together [MPD]. Sensor networks (environmental sensors, cameras, smart meters) and IoT devices continuously monitor urban conditions [krishna], from weather to traffic to structural health. Wireless communication systems also carry the data to improve the latency performance and provide real-time, citywide, and situational information [app14209243].\n\n\nThe collected data is input into cloud-based centralized platforms. Geographic Information Systems (GIS) and 3D city modeling [Marcooo] software play a key role by correlating data streams to spatial locations and creating interactive maps or virtual city models that reflect current conditions [xu2024leveraginggenerativeaiurban, doi:10.1061/JUPDDM.UPENG-4650]. Also, ML/DL techniques are employed to detect(safety monitoring [suan16219482, court2024usedigitaltwinssupport]) and predict outcomes [MASCHLER2021127, alex, Wangggg]. Some examples include modeling the spread of fire [Shadrin2024, fire7120482] or predicting traffic [car], anomaly detection [https://doi.org/10.1049/2024/8821891], and detecting abnormal trends[ElS"
  },
  {
    "title": "CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute",
    "url": "https://arxiv.org/abs/2602.08948v1",
    "source": "arxiv",
    "summary": "Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consume",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.08948v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Artificial Intelligence\n    \n\n    \n      arXiv:2602.08948v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 9 Feb 2026]\n    Title:CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute\n    Authors:Chen Jin, Ryutaro Tanno, Tom Diethe, Philip Teare            View a PDF of the paper titled CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute, by Chen Jin and 3 other authors\n    View PDF\n\n\n\n    \n            Abstract:Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Artificial Intelligence (cs.AI); Computation and Language (cs.CL)\n        \n          Cite as:\n          arXiv:2602.08948 [cs.AI]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.08948v1 [cs.AI] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.08948\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Chen Jin [view email]          [v1]\n        Mon, 9 Feb 2026 17:44:41 UTC (6,480 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute, by Chen Jin and 3 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.AI\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.CL\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n  \n  mathjaxToggle();\n\n      \n    \n\n    \n      \n        \n        \n          \n            \n              \n                About\n                Help\n              \n            \n       "
  }
]