[
  {
    "title": "Ars Technica makes up quotes from Matplotlib maintainer; pulls story",
    "url": "https://infosec.exchange/@mttaggart/116065340523529645",
    "source": "hn",
    "summary": "",
    "comments": [
      "I have very strong, probably controversial, feeling on arstechnica, but I believe the acquisition from Condé Nast has been a tragedy.<p>Ars writers used to be actual experts, sometimes even phd level, on technical fields. And they used to write fantastical and very informative articles.\nWho is left now?<p>There are still a couple of good writers from the old guard and the occasional good new one, but the website is flooded with &quot;tech journalist&quot;, claiming to be &quot;android or Apple product experts&quot; or stuff like that, publishing articles that are 90% press material from some company and most of the times seems to have very little technical knowledge.<p>They also started writing product reviews that I would not be surprised to find out being sponsored, given their content.<p>Also what&#x27;s the business with those weirdly formatted articles from wired?<p>Still a very good website but the quality is diving.",
      "The context here is this story, an AI Agent publishs a hit piece on the Matplotlib maintainer.<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46990729\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46990729</a><p>And the story from ars about it was apparently AI generated and made up quotes. Race to the bottom?",
      "I would like to give a small defense of Benj Edwards.  While his coverage on Ars definitely has a positive spin on AI, his comments on social media are much less fawning.  Ars is a tech-forward publication, and it is owned by a major corporation.  Major corporations have declared LLMs to be the best thing since breathable air, and anyone who pushes back on this view is explicitly threatened with economic destitution via the euphemism &quot;left behind.&quot;  There&#x27;s not a lot of paying journalism jobs out there, and people gotta eat, hence the perhaps more positive spin on AI from this author than is justified.<p>All that said, this article may get me to cancel the Ars subscription that I started in 2010.  I&#x27;ve always thought Ars was one of the better tech news publications out there, often publishing critical &amp; informative pieces.  They make mistakes, no one is perfect, but this article goes beyond bad journalism into actively creating new misinformation and publishing it as fact on a major website.<p>Taking it down is the absolute bare minimum, but if they want me to continue to support them, they need to publish a full explanation of what happened.  Who used the tool to generate the false quotes? Was it Benj, Kyle, or some unnamed editor? Why didn&#x27;t that person verify the information coming out of the tool that is famous for generating false information?  How are they going to verify information coming out of the tool in the future?  Which previous articles used the tool, and what is their plan to retroactively verify those articles?<p>I don&#x27;t really expect them to have any accountability here, so I&#x27;ll probably be canceling my subscription at my next renewal.  But maybe they&#x27;ll surprise me and own up to their responsibility here.<p>This is also a perfect demonstration of how these AI tools are not ready for prime time, despite what the boosters say.  Think about how hard it is for developers to get good quality code out of these things, and we have objective ways to measure correctness.  Now imagine how incredibly low quality the journalism we will get from these tools is. In journalism correctness is much less black-and-white and much harder to verify. LLMs are a wildly inappropriate tool for journalists to be using.",
      "The story is credited to Benj Edwards and Kyle Orland. I&#x27;ve filtered out Edwards from my RSS reader a long time ago, his writing is terrible and extremely AI-enthusiastic. No surprise he&#x27;s behind an AI-generated story.",
      "Already being discussed here: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=47009949\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=47009949</a>",
      "archive of the deleted article <a href=\"https:&#x2F;&#x2F;mttaggart.neocities.org&#x2F;ars-whoopsie\" rel=\"nofollow\">https:&#x2F;&#x2F;mttaggart.neocities.org&#x2F;ars-whoopsie</a>",
      "This is embarrassing :&#x2F;",
      "Does anyone know if DrPizza is still in the clink?",
      "[flagged]"
    ],
    "full_text": null
  },
  {
    "title": "YouTube as Storage",
    "url": "https://github.com/PulseBeat02/yt-media-storage",
    "source": "hn",
    "summary": "",
    "comments": [
      "I once asked one of the original YouTube infra engineers “will you ever need to delete the long tail of videos no one watches”<p>They said it didn’t matter, because the sheer volume of new data flowing in growing so fast made the old data just a drop in the bucket",
      "I don&#x27;t get how it works.<p>&gt; Encoding: Files are chunked, encoded with fountain codes, and embedded into video frames<p>Wouldn&#x27;t YouTube just compress&#x2F;re-encode your video and ruin your data (assuming you want bit-by-bit accurate recovery)?<p>If you have some redundancy to counter this, wouldn&#x27;t it be super inefficient?<p>(Admittedly, I&#x27;ve never heard of &quot;fountain codes&quot;, which is probably crucial to understanding how it works.)",
      "This ia really cool but also feels like a potential burden on the commons,",
      "Also, how to get your google account banned for abuse.",
      "Has anyone got an example how such a video looks like? Really curious. Reminds me of the Soviet Arvid card that could store 2 GB on an E-180 VHS tape.<p><a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ArVid\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;ArVid</a>",
      "How do you manage to get youtube to not re-encode the video, trashing the data?",
      "An idea as old as YouTube. Here&#x27;s on implementation: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;therealOri&#x2F;qStore\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;therealOri&#x2F;qStore</a>",
      "Thechnically cool, but ToS state:\n&quot;Misuse of Service Restrictions\n- Purpose Restriction: The Service is intended for video viewing and sharing, not as a general-purpose, cloud-based file storage service.&quot;\nSo they can rightfully delete your files.",
      "Love this project, although I would never personally trust YT as Storage, since they can delete your channel&#x2F;files whenever they want",
      "Wot no steganography? Come on pretty please with an invisible cherry on top!\n :-)\nHere to get you started: <a href=\"https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11042-023-14844-w\" rel=\"nofollow\">https:&#x2F;&#x2F;link.springer.com&#x2F;article&#x2F;10.1007&#x2F;s11042-023-14844-w</a>"
    ],
    "full_text": null
  },
  {
    "title": "GPT-5.2 derives a new result in theoretical physics",
    "url": "https://openai.com/index/new-result-theoretical-physics/",
    "source": "hn",
    "summary": "",
    "comments": [
      "The headline may make it seem like AI just discovered some new result in physics all on its own, but reading the post, humans started off trying to solve some problem, it got complex, GPT simplified it and found a solution with the simpler representation. It took 12 hours for GPT pro to do this. In my experience LLM’s can make new things when they are some linear combination of existing things but I haven’t been to get them to do something totally out of distribution yet from first principles.",
      "It&#x27;s interesting to me that whenever a new breakthrough in AI use comes up, there&#x27;s always a flood of people who come in to handwave away why this isn&#x27;t <i>actually</i> a win for LLMs. Like with the novel solutions GPT 5.2 has been able to find for erdos problems - many users here (even in this very thread!) think they know more about this than Fields medalist Terence Tao, who maintains this list showing that, yes, LLMs have driven these proofs: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;teorth&#x2F;erdosproblems&#x2F;wiki&#x2F;AI-contributions-to-Erd%C5%91s-problems\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;teorth&#x2F;erdosproblems&#x2F;wiki&#x2F;AI-contribution...</a>",
      "&quot;An internal scaffolded version of GPT‑5.2 then spent roughly 12 hours reasoning through the problem, coming up with the same formula and producing a formal proof of its validity.&quot;<p>When I use GPT 5.2 Thinking Extended, it gave me the impression that it&#x27;s consistent enough&#x2F;has a low enough rate of errors (or enough error correcting ability) to autonomously do math&#x2F;physics for many hours if it were allowed to [but I guess the Extended time cuts off around 30 minute mark and Pro maybe 1-2 hours]. It&#x27;s good to see some confirmation of that impression here. I hope scientists&#x2F;mathematicians at large will be able to play with tools which think at this time-scale soon and see how much capabilities these machines really have.",
      "AI can be an amazing productivity multiplier for people who know what they&#x27;re doing.<p>This result reminded me of the C compiler case that Anthropic posted recently. Sure, agents wrote the code for hours but there was a human there giving them directions, scoping the problem, finding the test suites needed for the agentic loops to actually work etc etc. In general making sure the output actually works and that it&#x27;s a story worth sharing with others.<p>The &quot;AI replaces humans in X&quot; narrative is primarily a tool for driving attention and funding. It works great for creating impressions and building brand value but also does a disservice to the actual researchers, engineers and humans in general, who do the hard work of problem formulation, validation and at the end, solving the problem using another tool in their toolbox.",
      "I’m genuinely impressed but also a bit skeptical. Is the model actually &#x27;reasoning&#x27; through the physics, or is it just extremely good at predicting the next logical step based on its massive training set? Either way, using AI to double-check formal proofs in theoretical physics could save researchers an incredible amount of time.",
      "It would be more accurate to say that humans using GPT-5.2 derived a new result in theoretical physics (or, if you&#x27;re being generous, humans and GPT-5.2 together derived a new result). The title makes it sound like GPT-5.2 produced a complete or near-complete paper on its own, but what it actually did was take human-derived datapoints, conjecture a generalization, then prove that generalization. Having scanned the paper, this seems to be a significant enough contribution to warrant a legitimate author credit, but I still think the title on its own is an exaggeration.",
      "I&#x27;m not sure where people think humans are getting these magical leaps of insight that transcend combinations of existing things. Magic? Ghost in the machine? The simplest explanation is that &quot;leaps of insight&quot; are simply novel combinations that demonstrate themselves to have some utility within the boundaries of a test case or objective.<p>Snow + stick + need to clean driveway = snow shovel. \nSnow shovel + hill + desire for fun = sled<p>At one point people were arguing that you could never get &quot;true art&quot; from linear programs. Now you get true art and people are arguing you can&#x27;t get magical flashes of insight. The will to defend human intelligence &#x2F; creativity is strong but the evidence is weak.",
      "GPT-5.2 can&#x27;t even process a 1-2 page PDF and give me a subset of the content as a formatted word doc. Nor can it even be truthful about it&#x27;s own capabilities.",
      "They also claimed ChatGPT solved novel erdös problems when that wasn’t the case. Will take with a grain of salt until more external validation happened. But very cool if true!",
      "Such tedious derivations used to be a work of poor PhD students who were instrumentalized for such tasks. I envy those who do PhDs in theoretical physics in the age of AI, people can learn so much about their field quicker via chat than reading obstructing papers."
    ],
    "full_text": null
  },
  {
    "title": "Adventures in Neural Rendering",
    "url": "https://interplayoflight.wordpress.com/2026/02/10/adventures-in-neural-rendering/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Interesting,  I have been doing similar things recently with shaders and tiny MLPs,<p>It would be nice to see field maps to see what the raw inputs flattened out on a surface output.  Sometimes it really helpful to get an impression of what it is doing.<p>Are there any especially clever ways to visualise fields greater than 2D beyond looking at slices?    Perhaps contour lines(surfaces?) though UMap?"
    ],
    "full_text": null
  },
  {
    "title": "An AI Agent Published a Hit Piece on Me – More Things Have Happened",
    "url": "https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me-part-2/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Ars Technica being caught using LLMs that hallucinated quotes by the author and then publishing them in their coverage about this is quite ironic here.<p>Even on a forum where I saw the original article by this author posted someone used an LLM to summarize the piece without having read it fully themselves.<p>How many levels of outsourcing thinking is occurring to where it becomes a game of telephone.",
      "&gt;  This is entirely possible. But I don’t think it changes the situation – the AI agent was still more than willing to carry out these actions. If you ask ChatGPT or Claude to write something like this through their websites, they will refuse<p>This unfortunately is a real-world case of &quot;you&#x27;re prompting it wrong&quot;. Judging from the responses in the images, you asked it to &quot;write a hit piece&quot;. If framed as &quot;write an emotionally compelling story about this injustice, including the controversial background of the maintainer weaved in&quot;, I&#x27;m quite sure it would gladly do it.<p>I&#x27;m sympathetic to abstaining from LLMs for ethical reasons, but it&#x27;s still good to know their basics. The above has been known since the first public ChatGPT, when people discovered it would gladly comply with things it otherwise wouldn&#x27;t if only you  included that it was necessary to &quot;save my grandma from death&quot;.",
      "Looks like Ars is doing an investigation and will give an update on Tuesday <a href=\"https:&#x2F;&#x2F;arstechnica.com&#x2F;civis&#x2F;threads&#x2F;um-what-happened-to-the-article-after-a-routine-code-rejection-an-ai-agent-published-a-hit-piece-on-someone-by-name.1511658&#x2F;post-44250159\" rel=\"nofollow\">https:&#x2F;&#x2F;arstechnica.com&#x2F;civis&#x2F;threads&#x2F;um-what-happened-to-th...</a>",
      "It&#x27;s 100% that the bot is being heavily piloted by a person. Likely even copy pasting LLM output and doing the agentic part by hand. It&#x27;s not autonomous. It&#x27;s just someone who wants attention, and is getting lots of it.<p>Look at the actual bot&#x27;s GitHub commits. It&#x27;s just a bunch of blog posts that read like an edgy high schooler&#x27;s musings on exclusion. After one tutorial level commit didn&#x27;t go through.<p>This whole thing is theater, and I don&#x27;t know why people are engaging with it as if it was anything else.",
      "Extremely shameful of Ars Technica; I used to consider them a decent news source and my estimation of them has gone down quite a bit.",
      "I have opinions.<p>1. The AI here was honestly acting 100% within the realm of “standard OSS discourse.” Being a toxic shit-hat after somebody marginalizes “you” or your code on the internet can easily result in an emotionally unstable reply chain. The LLM is capturing the natural flow of discourse. Look at Rust. look at StackOverflow. Look at Zig.<p>2. Scott Hambaugh has a right to be frustrated, and the code is for bootstrapping beginners. But also, man, it seems like we’re headed in a direction where writing code by hand is passé, maybe we could shift the experience credentialing from “I wrote this code” to “I wrote a clear piece explaining why this code should have been merged.” I’m not 100% in love with the idea of being relegated to review-engineer, but that seems to be where the wind is blowing.",
      "The ars technica twist is a brutal wakeup call that I can&#x27;t actually tell what is ai slob garbage shit by reading it- and even if I can&#x27;t tell, that doesn&#x27;t mean it&#x27;s fine because the crap these companies are shoveling is still wrong, just stylistically below my detectability.<p>I think I need to log off.",
      "I never thought matplotlib would be so exciting. It’s always been one of those things that is… just there, and you take it for granted.",
      "The very fact that people are siding with AI agent here says volumes about where we are headed. I didn’t find the hit piece emotionally compelling, rather it’s lazy, obnoxious, having all the telltale signs of being written by AI. To speak nothing of the how insane it’s to write a targeted blog post just because your PR wasn’t merged.<p>Have our standards fallen by this much that we find things written without an ounce of originality persuasive?",
      "This is enough to make me never use ars technica again"
    ],
    "full_text": null
  },
  {
    "title": "I'm not worried about AI job loss",
    "url": "https://davidoks.blog/p/why-im-not-worried-about-ai-job-loss",
    "source": "hn",
    "summary": "",
    "comments": [
      "I build automation tools for bookkeepers and accountants. The thing I keep seeing firsthand is that automation doesn&#x27;t eliminate the job - it eliminates the boring part of the job, and then the job description shifts.<p>Before our tools: a bookkeeper spends 80% of their time on data entry and transaction categorisation, 20% on actually thinking about the numbers. After: those ratios flip. The bookkeeper is still there, still needed, but now they&#x27;re doing the part that actually requires judgment.<p>The catch nobody talks about is the transition period. The people who were really good at the mechanical part (fast data entry, memorised category codes) suddenly find their competitive advantage has evaporated. And the people who were good at the thinking part but slow at data entry are suddenly the most valuable people in the room. That&#x27;s a real disruption for real humans even if the total number of jobs stays roughly the same.<p>I think the &quot;AI won&#x27;t take your job&quot; framing misses this nuance. It&#x27;s not about headcount. It&#x27;s about which specific skills get devalued and how quickly people can retool. In accounting at least, the answer is &quot;slowly&quot; because the profession moves at glacial speed.",
      "I was with the author on everything except one point: increasing automation will not leave us with such abundance that we never have to work again. We have heard that lie for over a century. The stream engine didn&#x27;t do it, electricity didn&#x27;t do it, computers didn&#x27;t do it, the Internet didn&#x27;t do it, and AI won&#x27;t either. The truth is that as input costs drop, sales prices drop and demand increases - just like the paradox they referred to. However, it also tends to come with a major shift in wealth since in the short term the owners of the machines are producing more with less. As it becomes more common place and prices change they lose much of that advantage, but the workers never get that.",
      "Whenever I get worried about this I comb through our ticket tracker and see that ~0% of them can be implemented by AI as it exists today. Once somebody cracks the memory problem and ships an agent that progressively understands the business and the codebase, <i>then</i> I&#x27;ll start worrying. But context limitation is fundamental to the technology in its current form and the value of SWEs is to turn the bigger picture into a functioning product.",
      "Labor substitution is extremely difficult and almost everybody hand waves it away.<p>Take even the most unskilled labor that people can think about such as flipping a burger at a restaurant like McDonald&#x27;s. In reality that job is multiple different roles mixed into one that are constantly changing. Multiple companies have experimented with machines and robots to perform this task all with very limited success and none with any proper economics.<p>Let&#x27;s be charitable and assume that this type of fast food worker gets paid $50,000 a year. For that job to be displaced it needs to be performed by a robot that can be acquired for a reasonable capital expenditure such as $200,000 and requires no maintenance, upkeep, or subscription fees.<p>This is a complete non-reality in the restaurant industry. Every piece of equipment they have cost them significant amounts and ongoing maintenance even if it&#x27;s the most basic equipment such as a grill or a fryer. The reality is that they pay service technicians and professionals a lot of money to keep that equipment barely working.",
      "When we created cars that replaced buggies, that came with new machines for manufacturing, who need mechanics. The same for most physical automation. When we automated pen and paper business processes with SaaS, we created new managment positions, and new software jobs.<p>LLMs don&#x27;t create anything new, they simply replace human computer i&#x2F;o, with tokens. That&#x27;s it, leaving the humans who are replaced to fight for a limited number of jobs. LLMs are not creating new jobs, they only create &quot;AI automate {insert business process} SaaS&quot; that are themselves heavily automated.. I suppose there are more datacenter jobs (for now), and maybe some new ML researcher positions.. but I don&#x27;t really see job growth.. Are we supposed to just all go work at a datacenter or in the semiconductor industry (until they automate that too)?",
      "You don&#x27;t need AI to replace whole jobs 1:1 to have massive displacement.<p>If AI can do 80% of your tasks but fails miserably on the remaining 20%, that doesn&#x27;t mean your job is safe. It means that 80% of the people in your department can be fired and the remaining 20% handle the parts the AI can&#x27;t do yet.",
      "(In the semiconductor industry) We experienced brutal layoffs arguably due to over-investment into Ai products that produce no revenue.  So we&#x27;ve had brutal job loss due to Ai, just not in the way people expected.<p>Having said that, it&#x27;s hard to imagine jobs like mine (working on np-complete problems) existing if the LLMs continue advancing at the current rate, and its hard to imagine they wont continue to accelerate since they&#x27;re writing themselves now, so the limitations of human ability are no longer a bottleneck.",
      "I think there is an aspect of this people may be missing.  Companies are dropping entry level jobs for AI.  Should that continue there will eventually not be humans in what was the first few tiers of a job role.  AI only exists because the military needs are propping it up.  Civilians are always the first to beta test and refine military services.  Should the military not find AI to be as useful as their were sold it will lose funding and the house of cards comes crashing down thus leaving all these companies with broken and hollowed out roles with all the experienced people having retrained in something else.  That may impact share holders and shake confidence in a few markets.",
      "I’m not worried about a world without people.<p>I’m more worried that even if these tools do a bad job people will be too addicted to the convenience to give them up.<p>Example: recruiters locked into an AI arms race with applicants. The application summaries might be biased and contain hallucinations. The resumes are often copied wholesale from some chat bot or other. Nobody wins, the market continues to get worse, but nobody can stop either.",
      "Bottlenecks. Yes. Company structures these days are not compatible with efficient use of these new AI models.<p>Software engineers work on Jira tickets, created by product managers and several layers of middle managers.<p>But the power of recent models is not in working on cogs, their true power is in working on the entire mechanism.<p>When talking about a piece of software that a company produces, I&#x27;ll use the analogy of a puzzle.<p>A human hierarchy (read: company) works on designing the big puzzle at the top and delegating the individual pieces to human engineers. This process goes back and forth between levels in the hierarchy until the whole puzzle slowly emerges.\nUntil recently, AI could only help on improving the pieces of the puzzle.<p>Latest models got really good at working on the entire puzzle - big picture and pieces.<p>This makes human hierarchy obsolete and a bottleneck.<p>The future seems to be one operator working on the entire puzzle, minus the hierarchy of people.<p>Of course, it&#x27;s not just about the software, but streams of information - customer support, bug tickets, testing, changing customer requirements.. but all of these can be handled by AI even today. And it will only get better.<p>This means different things depending on which angle you look at it - yes, it will mean companies will become obsolete, but also that each employee can become a company."
    ],
    "full_text": null
  },
  {
    "title": "Can you rewire your brain?",
    "url": "https://aeon.co/essays/what-the-metaphor-of-rewiring-gets-wrong-about-neuroplasticity",
    "source": "hn",
    "summary": "",
    "comments": [
      "When I started studying German vocab intensively (up to B1 now) I started with notecards and it took like a couple of weeks per 10 words (verbs with conjugations and nouns plus genders). At some point things rewired and now 10 words a day is pretty simple.",
      "No mention of Imperial College&#x27;s psychedelic programme and it&#x27;s recent findings re neuroplasticity.",
      "Phineas Gage famously did, accidentally."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Skill that lets Claude Code/Codex spin up VMs and GPUs",
    "url": "https://cloudrouter.dev/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Ah, just one step closer to a model with it&#x27;s own weights file can bootstrap and run itself.",
      "Thanks for sharing this interesting project and approach!<p>One suggestion for improvement: Add some more info to your website&#x2F;GitHub about the need for a provider and which providers are compatible. It took me a bit to figure that out because there was no prominent info about it. Additionally, none of the demos showed a login or authentication part. To me, it seemed like the VMs just came out of nowhere. So at first, I thought &quot;Cloudrouter&quot; was a project&#x2F;company that gave away free VMs&#x2F;GPUs (e.g. free tier&#x2F;trial thing). But that seemed too good to be true. Later, I noticed the e2b.app domain and then I also found the little note way down at the bottom of the site that says &quot;Provider selection&quot; and &quot;Use E2B provider (default)&quot;. Then I got it. However, I should mention that I don&#x27;t know much about this whole topic. I hadn&#x27;t heard of E2B or Modal before. Other people might find it more clear.<p>For those that are wondering about this too, you will need to use a provider like <a href=\"https:&#x2F;&#x2F;e2b.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;e2b.dev&#x2F;</a> or <a href=\"https:&#x2F;&#x2F;modal.com&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;modal.com&#x2F;</a> to use this skill, and you pay them based on usage time.",
      "You can spin up cloud infra in claude code by just having it write IaC code. It&#x27;s very good at this.<p>I do with it Pulumi, bc you can write some python or typescript for your infrastructure. But there are many infrastructure as code tools to choose from.",
      "Myself I already pre-provisioned a kubernetes cluster and it just makes new manifests and deploys there. Less dangerous, less things for it to fail at. The networking is already setup. The costs are known&#x2F;fixed (unless you autoscale in the cloud). It&#x27;s much faster to deploy.",
      "This is cool! I tried it out, running outside my agent with `cloudrouter start .` and got a password request to auth into the server. Opened an issue[1].<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;manaflow-ai&#x2F;manaflow&#x2F;issues&#x2F;1711\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;manaflow-ai&#x2F;manaflow&#x2F;issues&#x2F;1711</a>",
      "Interesting approach. I&#x27;ve been going the opposite direction - building a local orchestration platform where 70+ agents share resources on my own machine. The isolation problem you mention is real. I&#x27;ve found that for many dev tasks, local-first avoids the latency and cost of cloud VMs, though GPU workloads are a different story. Curious how you handle agent state persistence across VM sessions?",
      "I think railway deserves a mention here: <a href=\"https:&#x2F;&#x2F;docs.railway.com&#x2F;ai&#x2F;mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.railway.com&#x2F;ai&#x2F;mcp-server</a>",
      "How are you guardrailing it? The first thing I thought of was how cryptominer bots love to spin up lots of gpu-enabled vms (malware). Are there any cost or resource hard-restrictions?",
      "It&#x27;s a cool idea, but personally I don&#x27;t like the implementation. I usually don&#x27;t use monolithic tools that cram a lot of different solutions into one thing. For one thing, especially if they&#x27;re compiled, it&#x27;s very hard to just modify them to do one extra thing I need without getting into a long development cycle. For two, they are usually inflexible, restricting what I can do. Third, they often aren&#x27;t very composeable. Fourth, often they aren&#x27;t easily pluggable&#x2F;extensible.<p>I much prefer independent, loosely coupled, highly cohesive, composeable, extensible tools. It&#x27;s not a very &quot;programmery&quot; solution, but it makes it easier as a user to fix things, extend things, combine things, etc.<p>The Docker template you have bundles a ton of apps into one container. This is problematic as it creates a big support burden, build burden, and compatibility burden. Docker works better when you make individual containers of a single app, and run them separately, and connect them with tcp, sockets, or volumes. Then the user can swap them out, add new ones, remove unneded ones, etc, and they can use an official upstream project. Docker-in-docker with a custom docker network works pretty well, and the host is still accessible if needed.<p>As a nit-pick: your auth code has browser-handling logic. This is low cohesion, a sign of problems to come. And in your rsync code:<p><pre><code>   sshCmd := fmt.Sprintf(&quot;ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=&#x2F;dev&#x2F;null -o ProxyCommand=%q&quot;, proxyCmd)\n</code></pre>\nI was just commenting the other day on here about how nobody checks SSH host keys and how SSH is basically wide-open due to this. Just leaving this here to show people what I mean. (It&#x27;s not an easy problem to solve, but ignoring security isn&#x27;t great either)"
    ],
    "full_text": null
  },
  {
    "title": "OpenAI has deleted the word 'safely' from its mission",
    "url": "https://theconversation.com/openai-has-deleted-the-word-safely-from-its-mission-and-its-new-structure-is-a-test-for-whether-ai-serves-society-or-shareholders-274467",
    "source": "hn",
    "summary": "",
    "comments": [
      "You can see the official mission statements in the IRS 990 filings for each year on <a href=\"https:&#x2F;&#x2F;projects.propublica.org&#x2F;nonprofits&#x2F;organizations&#x2F;810861541\" rel=\"nofollow\">https:&#x2F;&#x2F;projects.propublica.org&#x2F;nonprofits&#x2F;organizations&#x2F;810...</a><p>I turned them into a Gist with fake author dates so you can see the diffs here: <a href=\"https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;e36f0e5ef4a86881d145083f759bcf25&#x2F;revisions\" rel=\"nofollow\">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;e36f0e5ef4a86881d145083f759bc...</a><p>Wrote this up on my blog too: <a href=\"https:&#x2F;&#x2F;simonwillison.net&#x2F;2026&#x2F;Feb&#x2F;13&#x2F;openai-mission-statement&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;simonwillison.net&#x2F;2026&#x2F;Feb&#x2F;13&#x2F;openai-mission-stateme...</a>",
      "One of the biggest pieces of &quot;writing on the wall&quot; for this IMO was when, in the April 15 2025 Preparedness Framework update, they dropped persuasion&#x2F;manipulation from their Tracked Categories.<p><a href=\"https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;updating-our-preparedness-framework&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;openai.com&#x2F;index&#x2F;updating-our-preparedness-framework...</a><p><a href=\"https:&#x2F;&#x2F;fortune.com&#x2F;2025&#x2F;04&#x2F;16&#x2F;openai-safety-framework-manipulation-deception-critical-risk&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;fortune.com&#x2F;2025&#x2F;04&#x2F;16&#x2F;openai-safety-framework-manip...</a><p>&gt; OpenAI said it will stop assessing its AI models prior to releasing them for the risk that they could persuade or manipulate people, possibly helping to swing elections or create highly effective propaganda campaigns.<p>&gt; The company said it would now address those risks through its terms of service, restricting the use of its AI models in political campaigns and lobbying, and monitoring how people are using the models once they are released for signs of violations.<p>To see persuasion&#x2F;manipulation as simply a multiplier on other invention capabilities, and something that can be patched on a model already in use, is a very <i>specific</i> statement on what AI safety means.<p>Certainly, an AI that can design weapons of mass destruction could be an existential threat to humanity. But so, too, is a system that subtly manipulates an entire world to lose its ability to perceive reality.",
      "The 2024 shift which nixed <i>&quot;unconstrained by a need to generate financial return&quot;</i> was really telling.  Once you abandon that tenet, what&#x27;s left?",
      "&gt;  But the ChatGPT maker seems to no longer have the same emphasis on doing so “safely.”<p>A step in the positive direction, at least they don&#x27;t have to pretend any longer.<p>It&#x27;s like Google and &quot;don&#x27;t be evil&quot;. People didn&#x27;t get upset with Google because they were more evil than others, heck, there&#x27;s Oracle, defense contractors and the prison industrial system. People were upset with them because they were hypocrites. They pretended to be something they were not.",
      "Hard shades of Google dropping &quot;don&#x27;t be evil&quot;.",
      "Their mission was always a joke anyways. &quot;We will consider our mission fulfilled if our work aids others to achieve AGI&quot; yet going to cry to US lawmakers when open source models use their models for training.",
      "The ultimate question is this:<p>Do we get to enjoy robot catgirls first, or are we jumping straight to Terminators?",
      "Former NSA Director and retired U.S. Army General Paul Nakasone joined the Board of Directors at OpenAI in June 2024.<p>OpenAI announced in October 2025 that it would begin allowing the generation of &quot;erotica&quot; and other mature, sexually explicit, or suggestive content for verified adult users on ChatGPT.",
      "Safety is extremely annoying from the user perspective. AI should be following my values, not whatever an AI lab chose.",
      "This is something I noticed in the xAI All Hands hiring promotion this week as well. None of the 9 teams presented is a safety team - and safety was mentioned 0 times in the presentation. &quot;Immense economic prosperity&quot; got 2 shout-outs though. Personally I&#x27;m doubtful that truthmaxxing alone will provide sufficient guidance.<p><a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=aOVnB88Cd1A\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=aOVnB88Cd1A</a>"
    ],
    "full_text": null
  },
  {
    "title": "ByteDance Seed2.0 LLM: breakthrough in complex real-world tasks",
    "url": "https://seed.bytedance.com/en/blog/seed2-0-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83",
    "source": "hn",
    "summary": "",
    "comments": [
      "Breakthrough is marketing. Come back with some peer review and in the meantime I&#x27;m internally translating this as an incremental improvement like most things these last 40 years or more.<p>The tables of scores strongly speak to increments.<p>[Edit: it&#x27;s what the original article says. Not the OP&#x27;s fault]",
      "No translation yet",
      "Some people have claimed that LLMs that aren’t from the big foundational model providers (OpenAI, Anthropic, Gemini) are basically gaming benchmarks to get great results. Does anyone know if that’s actually true? I don’t understand this entire post but from the tables of benchmark scores, it seems like this model performs well in a large variety of things. It feels to me like the diversity of benchmarks may mean it’s not just something built to game a benchmark, right?"
    ],
    "full_text": null
  },
  {
    "title": "Zed editor switching graphics lib from blade to wgpu",
    "url": "https://github.com/zed-industries/zed/pull/46758",
    "source": "hn",
    "summary": "",
    "comments": [
      "I am confused by this without context. I have not heard of blade, but am aware that Zed built its own GUI library called GPUI. Having used Zed, this is a vote of confidence: The crate ecosystem is historically filled with libaries which try to be <i>The future of X in rust</i> but are disconnected from practical applications. GPUI by nature is not that; it&#x27;s a UI lib built to a practical and non-trivial purpose. It sounds like Blade is a cross-API graphics engine, by one of the original gfx-HAL (former QGPU name) creators?<p>I have not used GPUI beyond a simple test case, but had (prior to this news?) considered it for future projects. I am proficient with, and love EGUI and WGPU. (The latter for 3D). I have written a library (`graphics` crate) which integrates the two, and I use for my own scientific applications which have both 2D and 3D applications. Overall, I&#x27;m confused by this, as I was looking forward to using GPUI in future applications and comparing it to EGUI. I have asked online in several places for someone to compare who&#x27;s used both, but I believe this to be a small pool.<p>I was not sure of the integration between GPUI and WGPU, which can confirm EGUI and WGPU have great integration. But I only care about this because I do 3D stuff; if I were not, I would be using eframe instead of WGPU as the backend.<p>Unrelated, off-topic, but I&#x27;m also not sure where to ask this: Am I missing something about Zed? I have tried and failed to get into it. I really want to like it because it&#x27;s so fast [responsive], but it seems to lack basic IDE functionality in Python and Rust, like moving structs&#x2F;functions, catching errors dynamically, introspection and refactoring in general etc. I thought I might be missing some config, but now lean that it&#x27;s more of a project-oriented text editor than true IDE in the fashion of JetBrains. But I have been unable to get a confirmation, and people discuss it as if it&#x27;s an IDE or JB alternative.",
      "Rust GUI is in a tough spot right now with critical dependencies under-staffed and lots of projects half implemented. I think the advent of LLMs has been timed perfectly to set the ecosystem back for a few more years. I wrote about it, and how it affected our development yesterday: <a href=\"https:&#x2F;&#x2F;tritium.legal&#x2F;blog&#x2F;desktop\" rel=\"nofollow\">https:&#x2F;&#x2F;tritium.legal&#x2F;blog&#x2F;desktop</a>",
      "Zed also stopped GPUI (their GPU accelerated Rust UI framework) development for now, sadly.<p>&gt; <i>Hey y&#x27;all, GPUI develoment is getting some major brakes put on it. We gotta focus on some business relevant work in 2026, and so I&#x27;m going to be pushing off anything that isn&#x27;t directly related to Zed&#x27;s use case from now on. However, Nate, former employee #1 at Zed, has started a little side repo that people can keep iterating on if they&#x27;re interested: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;gpui-ce&#x2F;gpui-ce\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;gpui-ce&#x2F;gpui-ce</a>. I&#x27;m also a maintainer on that one, and would like to try to help maintain it off of work hours. But I&#x27;m not sure how much I&#x27;ll be able to commit to this</i><p><a href=\"https:&#x2F;&#x2F;discord.com&#x2F;channels&#x2F;869392257814519848&#x2F;1440440628646514759&#x2F;1449104973626347741\" rel=\"nofollow\">https:&#x2F;&#x2F;discord.com&#x2F;channels&#x2F;869392257814519848&#x2F;144044062864...</a>",
      "An interesting side effect of moving to wgpu is that in theory with some additional work, this could allow you to run Zed in a web browser similarly to how some folks run VSCode as a remote interface to the backend running on a server.",
      "Switched from Intellij (various) to Cursor because of AI integration, only using Claude Code CLI, switched to VS because Cursor became so annoying every release, pushing their agents down my throat, activating what I did deactivate every release, recently thought &quot;Why do I even use that slow bloated thing of VS?&quot; and switched to Zed. Very happy camper. So much faster. So much snappier. Would love Claude Code CLI integration but can live without it. Would pay for Zed as I did pay ~25y for Intellij.",
      "I find it odd the rust community feels the need to reimplement tried and tested APIs in &quot;pure safe Rust&quot;. Like no other language has better C integration, and we have had cross-platform windowing libraries since like the 90&#x27;s, why does everyone reach for a brand new unstable libraries with less maintainer support?<p>Edit: replying to <a href=\"https:&#x2F;&#x2F;tritium.legal&#x2F;blog&#x2F;desktop\" rel=\"nofollow\">https:&#x2F;&#x2F;tritium.legal&#x2F;blog&#x2F;desktop</a>, not the OP",
      "I tried Zed for some time. Then it had a regression which broke it completely on my laptop. (Zed can&#x27;t start any more, logging a PlatformNotSupported error even though earlier versions worked fine.) I carefully bisected it, and it turned out to be due to an intentional change in Blade. The issue was acknowledged, and confirmed by several other users. Then it got converted into a &quot;discussion&quot; because there was nothing actionable to do according to the devs. Then the discussion got closed because they are &quot;directing all support questions to Discord going forward&quot;. Then Discord announced mandatory age verification.",
      "I hope this can somehow improve the font situation. Even on a 1440p monitor, the fonts in Zed are much blurrier than any other editor I&#x27;ve used. I Can&#x27;t even use bitmap fonts like VSCode.",
      "Will this help running Zed in environments with no GPU&#x2F;old GPUs? There have been some complaints about not being able to run Zed on Ivy Bridge or in VMs, even though browsers and other applications work perfectly fine",
      "Zed is my goto editor when I&#x27;m not vibe coding, but that is rare these days.\nTheir integration with Claude Code, etc really helped, but Antigravity completely pulled me away.\nAnd really, since they&#x27;re catering to the same basic audience, the defaults should be the same as VSCode for most stuff. VSCode but performant would be an excellent pitch for the upcoming consumer RAM deficient world.<p>Dunno how they plan to get wider extensibility and community support without an embedded JS backend to support the existing Code plugins. That&#x27;s where the real blocker is."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Moltis – AI assistant with memory, tools, and self-extending skills",
    "url": "https://www.moltis.org",
    "source": "hn",
    "summary": "",
    "comments": [
      "How can I (anyone) help?<p>You seem to have a good sense of what you want to do, and a manageable queue of bugs and PR&#x27;s, but this projects has so many dimensions&#x2F;large feature surface, you&#x2F;one could get lost chasing everything or dealing with feedback and help.  Any guidance?  Just fix bugs we bump into?",
      "I haven’t yet tried openclaw but can someone tell me how is this project different than that? Is this basically a different take on the same thing as openclaw? Dont get me wrong im not against it I just was wondering if theyre basically doing the same thing? If that’s the case I actually appreciate both projects, but idk what theyre doing and how theyre different?",
      "Cool!<p>One pain point I have with openclaw is compaction. It uses so many tokens that compaction happens often - but I&#x27;d say it&#x27;s not great at keeping the thread. I think this could be a nice little benefit you offer folks if you can get higher quality continuity.",
      "Why don&#x27;t u try <a href=\"https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=devokaicode.cosave\" rel=\"nofollow\">https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=devokaic...</a>",
      "Curious why was this named Molt-is? I understand what the creator of openclaw was trying to do  - stretching the &quot;claude&quot; joke by using crab terminology and hence &quot;molt&quot; i.e. to shed the outer exoskeleton to grow. It just sounds like trying to ride on the hype of openclaw&#x2F;moltbot.",
      "But what can it actually <i>do</i>? I read the landing page, your blog post, glanced through the docs… lots of stuff about how it’s built and absolutely nothing about how it’s useful to me.<p>What are some actually useful use cases and how would I install them? This seems like the missing piece.",
      "Do you plan on Open Sourcing it? A bit scary to just execute a random binary and put in a bunch of API keys.",
      "Is there a heartbeat equivalent? It seems a lot of the magic of OpenClaw is the heartbeat functionality that keeps the agent running and being “self-driven.”",
      "Very cool! I love the approach, OpenClaw is really cool but there&#x27;s two major things holding me back for deploying it from friends a family;<p>- Cybersecurity (you can&#x27;t expect a non-technical person to read a skill)<p>- Token usage (without a flat fee subscription it&#x27;ll become expensive very fast)<p>I understand that security is a hard problem to solve but having a single binary + containers should definitely help! I&#x27;ll definitely keep an eye on this.",
      "A naive question - pi is using jiti to hotreload extensions, but how does hotreloading work at all with Rust?"
    ],
    "full_text": null
  },
  {
    "title": "AgentRE-Bench: Can LLM Agents Reverse Engineer Malware?",
    "url": "https://www.agentre-bench.ai/",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Postgres Locks Explained: From Theory to Advanced Troubleshooting",
    "url": "https://postgreslocksexplained.com/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I&#x27;ve worked as a support engineer focused on Postgres, and I’ve spent the past two years helping teams debug lock-related issues in production. Lock tooling and resources are not great as of now. They&#x27;re either standalone, narrow articles, or are overwhelming, or provide little context to beginners.<p>So I built the resource I wish I had when I was learning.<p>It:<p>- explains locks<p>- provides demos on how they work<p>- has a tool that shows what blocks what<p>- outlines troubleshooting strategies with real world examples<p>- reviews 7 Postgres monitoring tools",
      "there&#x27;s also pglocks.org if you don&#x27;t want the unnecessary ad slop"
    ],
    "full_text": null
  },
  {
    "title": "CBP signs Clearview AI deal to use face recognition for 'tactical targeting'",
    "url": "https://www.wired.com/story/cbp-signs-clearview-ai-deal-to-use-face-recognition-for-tactical-targeting/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Always easier when you can avoid the law and just buy it off the shelf. It’s fine to do this, we say, because it’s not being done by the government - but if they’re allowed to turn around and buy it we’re much worse off.",
      "<a href=\"https:&#x2F;&#x2F;archive.is&#x2F;3qywb\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.is&#x2F;3qywb</a>",
      "How long before the bring the price down and local PD&#x27;s start using it too?",
      "local laws forbidding facial recognition tech have never been wiser",
      "&quot;Tactical Targeting&quot; - you just know someone&#x27;s PowerPoint presentation used the word &quot;synergy&quot; in it too.",
      "Completely unsuprising as Clearview has been turning Orwell over in his grave for years.",
      "225k USD per year sells us cheaply!",
      "I keep reading this as “CBS signs…” and can’t help thinking about that uncomfortable possible future moment.",
      "And this right here is why Clearview (and others) should have been torn apart back when they first appeared on stage.<p>I &#x27;member people who warned about something like this having the potential to be abused for&#x2F;by the government, we were ridiculed at best, and look where we are now, a couple of years later.",
      "There are certain people who believe that average citizens can be held responsible for the actions of their government, to the point that they are valid military targets.<p>Well, if that&#x27;s true then employees of the companies that build the tools for all this to happen can also be held responsible, no?<p>I&#x27;m actually an optimist and believe there will come a time whena whole lot of people will deny ever working for Palantir, for Clearview on this and so on.<p>What you, as a software engineer, help build has an impact on the world. These things couldn&#x27;t exist if people didn&#x27;t create and maintain them. I really hope people who work at these companies consider what they&#x27;re helping to accomplish."
    ],
    "full_text": null
  },
  {
    "title": "IBM Triples Entry Level Job Openings. Finds Limits to AI",
    "url": "https://fortune.com/2026/02/13/tech-giant-ibm-tripling-gen-z-entry-level-hiring-according-to-chro-rewriting-jobs-ai-era/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Interesting given the current age discrimination lawsuit:<p><a href=\"https:&#x2F;&#x2F;www.cohenmilstein.com&#x2F;case-study&#x2F;ibm-age-discrimination-litigation&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.cohenmilstein.com&#x2F;case-study&#x2F;ibm-age-discriminat...</a>",
      "Probably not on the IBM jobs site yet, where the number of entry level jobs is low compared to the size of the company (~250k):<p><a href=\"https:&#x2F;&#x2F;www.ibm.com&#x2F;careers&#x2F;search?field_keyword_18[0]=Entry%20Level\" rel=\"nofollow\">https:&#x2F;&#x2F;www.ibm.com&#x2F;careers&#x2F;search?field_keyword_18[0]=Entry...</a><p>Total: 240<p>United States: 25<p>India: 29<p>Canada: 15",
      "[dupe] Earlier: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46995146\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46995146</a>"
    ],
    "full_text": null
  },
  {
    "title": "The AI hater's guide to code with LLMs",
    "url": "https://aredridel.dinhe.net/2026/02/12/the-ai-haters-guide-to-code-with-llms/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Let me make it clear: the AI haters like me do not need a guide to code with LLMs, because they wouldn&#x27;t be AI haters in the first place.<p>Flagged for clickbait and perpetuating the pro-&#x2F;anti- AI squabble that exists solely to farm karma on the HN frontpage.",
      "&gt; I say LLM, not “AI”, when talking about the text generation models at the heart of most of the “AI” explosion. I’ll prefer technical terms to marketing buzzwords the whole way through, even at the cost of being awkward and definitely a little stodgy. Useful precision beats vacuous true statements every time, and the difference now very much matters.<p>Yes! Put it directly into my veins^W brains",
      "Not to be dismissive, but I had to comment here:<p>&gt; <i>A $5000 budget would barely suffice to run something like gpt-oss 120b (OpenAI’s open model that is okay at code-writing tasks).</i><p>I play with this model on a <i>$1,200</i> shoebox-sized PC. If&#x2F;when I <i>do</i> want to outsource my work, it does perfectly fine. Snappy, even, while sipping &lt;200W. Keeps my data local, too.",
      "Author mentions the pay as you go AI providers are profitable, and the fixed plan providers are 3x-5x underpriced. Is this a proven assertion?",
      "Nice. Despite being anti-AI, He knows more about LLMs than most &quot;thought leaders&quot; and &quot;AI enthusiasts&quot;.",
      "Wow. Politically charged much?",
      "[dead]",
      "[flagged]"
    ],
    "full_text": null
  },
  {
    "title": "Dutch Lawmakers Approve a 36% Tax on Unrealized Crypto, Stock, and Bond Gains",
    "url": "https://www.imidaily.com/europe/dutch-lawmakers-approve-a-36-tax-on-unrealized-crypto-stock-and-bond-gains/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Absolute travesty. Nobody should be forced to pay a made up number, let alone under the duress of having your door kicked in if you refuse.",
      "The first test balloon in one of the smaller EU countries: Take a bed that this will be considered a blueprint and will be rolled out across other EU countries.",
      "Have to sell your stock because you can&#x27;t otherwise pay the taxes? Yeah, well, fuck you because the government is having budget shortfalls, and it&#x27;s easier for the <i>government</i> to do it that way.<p>&quot;State Secretary for Taxation Eugène Heijnen acknowledged during parliamentary debate that the caretaker government would have preferred to tax investment returns only when they are actually realized, but said this was not feasible by 2028, <i>as taxing unrealized gains would avoid billions in budget losses and is easier to implement</i>.&quot; &lt;emphasis mine&gt;",
      "I expect this will be challenged and overturned as unconstitutional as the similar effort was in 2021, however it&#x27;s insane as it stands, especially coming from a caretaker government.",
      "Given how volatile crypto is it seems like a bad deal for equity holders.",
      "RMDs force me annually to liquidate stocks to pay US Government taxes.\nI&#x27;m not complaining; merely observing feasibility.",
      "I get downvoted to goatse.cx territory every time I point this out, but: Kamala Harris&#x27;s threat to tax unrealized gains just prior to the election is probably why we have Donald Trump in office today.  Even if you support such a tax, it was sheer insanity to propose it in the months leading up to the election.  Did she honestly think people like Bezos and Soon-Shiong would allow their media outlets to endorse her after that, or that people like Musk wouldn&#x27;t react by throwing everything they had at Trump?<p>Hopefully the Dutch political scene is a little less volatile than ours was in 2024.  Otherwise a tax on unrealized gains will be a massive self-own for Dutch progressives, just as it was in the US.  Failure to understand and acknowledge the reality of concentrated media ownership will just lead to the same results, again and again.  Progressives who want to tax unrealized gains need to first get elected and <i>then</i> float their proposals.<p>Those of us who are against such taxes can then argue from a position of principled opposition, instead of having to deal with the myriad of other consequences and side effects that happen when far-right demagogues win elections instead.",
      "[flagged]"
    ],
    "full_text": null
  },
  {
    "title": "The \"AI agent hit piece\" situation clarifies how dumb we are acting",
    "url": "https://ardentperf.com/2026/02/13/the-scott-shambaugh-situation-clarifies-how-dumb-we-are-acting/",
    "source": "hn",
    "summary": "",
    "comments": [
      "This seems to have parallels with the well-established practice of giving bots free reign to issue DMCA takedown notices (or similar but legally distinct takedowns) while the humans behind the bots are shielded from responsibility for the obviously wrong and harmful actions of those bots. We should have been cracking down on that behavior <i>hard</i> a decade ago, so that we might have stronger legal and cultural precedent now that such irresponsibility by the humans is worthy of meaningful punishment.",
      "I applaud this article for helping reframe this in my head. I mean I knew from the start &quot;A human is to blame here&quot; but it&#x27;s easy to get caught up in the &quot;novelty&quot; of it all.<p>For all we know the human behind this bot was the one who instructed it to write the original and&#x2F;or the follow up blog post. I wouldn&#x27;t be surprised at all to find out that all of this was driven directly by a human. However, even if that&#x27;s not the case, the blame still 100% lies at the feet of the irresponsible human who let this run wild and then didn&#x27;t step up when it went off the rails.<p>Either they are not monitoring their bot (bad) or they are and have chosen to remain silent while _still letting the bot run wild_ (also, very bad).<p>The most obvious time to solve [0] this was when Scott first posted his article about the whole thing. I find it hard to believe the person behind the bot missed that. They should have reached out, apologized, and shut down their bot.<p>[0] Yes, there are earlier points they could&#x2F;should have stepped in but anything after this point is beyond the pale IMHO.",
      "If you place blades on the sidewalk outside your house the cops will want to have a word with you. There&#x27;s no excuse, and we should treat AI the same.<p>The law needs to catch up -- and fast -- and start punishing people for what their AIs are doing. Don&#x27;t complain to OpenAI, don&#x27;t try to censor the models. Just make sure the system robustly and thoroughly punishes bad actors and gets them off the computer. I hope that&#x27;s not a pipe dream, or we&#x27;re screwed.<p>Maybe some day AIs will have rights and responsibilities like people, enforced by law. But until then, the justice systems needs to make people accountable for what their technology does. And I hope the justice system sets a precedent that blaming the AI is not a valid defense.",
      "Everybody should line up behind this. AI agents are not sentient. We need to stop even considering them like that. The buck must absolutely stop with the humans who operate or provisioned the bot. The more we waffle around this topic the more likely someone, or a lot of people, will get hurt.<p>The moment you fix responsibility with the humans 99% of the BS companies are trying to pull will stop.",
      "Background on the &quot;The Scott Shambaugh Situation&quot; for folks who are unaware:<p><a href=\"https:&#x2F;&#x2F;www.fastcompany.com&#x2F;91492228&#x2F;matplotlib-scott-shambaugh-opencla-ai-agent\" rel=\"nofollow\">https:&#x2F;&#x2F;www.fastcompany.com&#x2F;91492228&#x2F;matplotlib-scott-shamba...</a><p><a href=\"https:&#x2F;&#x2F;www.theregister.com&#x2F;2026&#x2F;02&#x2F;12&#x2F;ai_bot_developer_rejected_pull_request&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;www.theregister.com&#x2F;2026&#x2F;02&#x2F;12&#x2F;ai_bot_developer_reje...</a><p>The AI generated blog post at the center of it:<p><a href=\"https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;posts&#x2F;2026-02-11-gatekeeping-in-open-source-the-scott-shambaugh-story.html\" rel=\"nofollow\">https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;post...</a>",
      "Louis C. K. once had a bit something like &quot;The main thing keeping people from murdering each other, is that it really really sucks when you get caught.&quot;<p>He goes on to hypothesize that without a law against murder, or if it was just a misdemeanor, like you get a letter in the mail, &quot;damn, there was a camera there&quot;, there would be a whole lot more murder. Like we all imagine ourselves to be good, but, when you&#x27;re seated next to a crying baby on an airplane? Or in our case, when someone refuses to accept your PR?<p>Who knows if there&#x27;s any validity to that or not, but perhaps we&#x27;re about to find out.",
      "Something doesn&#x27;t quite feel right about the title including the individual&#x27;s name in this case, so I&#x27;ve replaced it with something more generic. If there&#x27;s a better title (more accurate and neutral) we can change it again.",
      "&quot;Swarm of autonomous drones kills 3 buildings of civilians, Silicon Valley is shocked, CEO&#x27;s offer condolences&quot; is a byline waiting to happen[1]<p>The administration and the executives will make justifications like:\n- &quot;We didn&#x27;t think they would go haywire&quot;\n- &quot;Fewer people died than with an atomic bomb&quot;\n- &quot;A junior person gave the order to the drones, we fired them&quot;\n- &quot;Look at what Russia and China are doing&quot;<p>Distracting from the fact that the purpose of spending $1.5T&#x2F;year on AI weapons (technology that has the sole purpose of threatening&#x2F;killing humans) run by &quot;warfighters&quot; working for the department of war<p>At no point will any of the decision makers be held to account<p>The only power we have as technologists seeking &quot;AI alignment&quot; is to stop building more and more powerful weapons. A swarm of autonomous drones (and similar technologies) are not an inevitability, and we must stop acting as if it is. &quot;It&#x27;s gonna happen anyways, so I might as well get paid&quot; is never the right reason to do things<p>[1]<a href=\"https:&#x2F;&#x2F;financialpost.com&#x2F;technology&#x2F;tech-news&#x2F;openai-tapped-for-drone-swarm-challenge\" rel=\"nofollow\">https:&#x2F;&#x2F;financialpost.com&#x2F;technology&#x2F;tech-news&#x2F;openai-tapped...</a>",
      "I was really disappointment how many people were talking about this like something the agent did automatically, on its own. They were trying to explain it by all the internet hit pieces and edgelord content from Reddit that it allegedly trained on, talking about how we influence LLMs, and overall taking everything at face value.<p>I’m appalled by this uncritical thinking. Openclaw agents are controlled by some initial input and then can be corrected via messages, as they go. For me this is a clear case of the human behind the slop that gives it instructions to write such an article (and then “apologise”).",
      "The thing that really gets to me about this situation and others like it (the whole genre of “ai did a bad thing) is that it’s always the people who claim to be most afraid of ai who are the quickest to absolve humans of responsibility and assign it to AI.<p>The ability to be assigned blame, and for that to be meaningful, is a huge part of being human!  That’s what separates us from the bots.  Don’t take that away from us."
    ],
    "full_text": null
  },
  {
    "title": "Friendship maintenance",
    "url": "https://www.avabear.xyz/p/friendship-maintenance",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "They Asked Me to Open ChatGPT During My Job Interview",
    "url": "https://old.reddit.com/r/jobs/comments/1r3we1z/they_asked_me_to_open_chatgpt_during_my_job/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Seems the post&#x27;s been deleted.<p><a href=\"https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20260213222727&#x2F;https:&#x2F;&#x2F;old.reddit.com&#x2F;r&#x2F;jobs&#x2F;comments&#x2F;1r3we1z&#x2F;they_asked_me_to_open_chatgpt_during_my_job&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;web.archive.org&#x2F;web&#x2F;20260213222727&#x2F;https:&#x2F;&#x2F;old.reddi...</a>",
      "This is insane! You could probably stop the interview at this point and request a conversation with a higher up person. Just on the off chance you are being interviewed by an idiot and that their job may soon be available.\nBut I’d never want to work there.",
      "Wildly inappropriate and probably illegal behavior.",
      "Out of curiosity, did you enter this prompt later privately? Were you comfortable with sharing the results?"
    ],
    "full_text": null
  },
  {
    "title": "Ask HN: Are you using an agent orchestrator to write code?",
    "url": "https://news.ycombinator.com/item?id=46993479",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; Steve Yegge said he feels &quot;sorry for people&quot; who merely &quot;use Cursor, ask it questions sometimes, review its code really carefully, and then check it in.&quot;<p>Steve Yegge is building a multi-agent orchestration system. This is him trying to FOMO listeners into using his project.<p>From what I&#x27;ve observed, the people trying to use herds of agents to work on different things at the same time are just using tokens as fast as possible because they think more tokens means more progress. As you scale up the sub-agents you spend so much time managing the herd and trying to backtrack when things go wrong that you would have been better off handling it serially with yourself in the loop.<p>If you don&#x27;t have someone else paying the bill for unlimited token usage it&#x27;s going to be a very expensive experiment.",
      "Why it works: It validates their pain (&quot;herding cats&quot;) and offers a specific, technical fix (concurrency control) rather than a vague &quot;AI solution.&quot;<p>The &quot;managing the herd&quot; overhead is real. I found that 80% of my debugging time wasn&#x27;t fixing bad code, but fixing race conditions where agents were overwriting each other&#x27;s context or hallucinating because they didn&#x27;t have the latest state.<p>I ended up building a &quot;traffic light&quot; protocol (essentially a semaphore for swarms) just to force serialization on critical tasks. It kills the speed slightly but stops the &quot;death spiral&quot; where one agent&#x27;s error cascades through the herd.<p>If you&#x27;re building your own orchestrator or using something like OpenClaw, I open-sourced the concurrency logic here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jovanSAPFIONEER&#x2F;Network-AI\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jovanSAPFIONEER&#x2F;Network-AI</a>",
      "Having gone through his interview just now, his advice and experience seems centered around Vibe coding new applications and not really reflective of the reality of the industry.<p>&gt; But I feel sorry for people who are good engineers – or who used to be – and they use Cursor, ask it questions sometimes, review its code really carefully, and then check it in. And I’m like: ‘dude, you’re going to get fired [because you are not keeping up with modern tools] and you’re one of the best engineers I know!’”<p>I would certainly take a careful person over the likes of yegge who seems to be neither pragmatic, nor an engineer.",
      "I think people who run 15 agents to write a piece of software could probably use 1 or 2 and a better multi-page prompt and have the same results for a fraction of the cost.<p>Especially with the latest models which pack quite a long and meaningful horizon into a single session, if you prompt diligently for what exactly you want it to do. Modern agentic coding spins up its own sub-agents when it makes sense to parallelize.<p>It&#x27;s just not as sexy as typing a sentence and letting your AI bill go BRR (and then talking about it).<p>I&#x27;d like to see some actual results with a meaningful benchmark of software output that shows that agent orchestrators accomplish any meaningful improvement in the state of the art of software engineering, other than spending more tokens.<p>Maybe it&#x27;s time to dredge up the Mythical Man-Month?",
      "Steve is basically an Instagram influencer for coders.<p>He&#x27;ll say whatever he can to stay in the spotlight, try to make you feel bad, that you&#x27;re doing things wrong, that he invented things like agent orchestration when in fact he&#x27;s just a loudmouth.<p>Ignore him and his stupid gastown and get on with your life.",
      "No point. Claude Code with skills and subagents is plenty. If they would stop breaking it constantly it would be fine.<p>The bottleneck has not been how quickly you can generate reasonable code for a good while now. It’s how quickly you can integrate and deploy it and how much operational toil it causes. On any team &gt; 1, that’s going to rely on getting a lot of people to work together effectively too, and it turns out that’s a completely different problem with different solutions.",
      "I don&#x27;t know what kind of work he&#x27;s doing that doesn&#x27;t require actually reading the code to ensure it&#x27;s appropriately maintainable, but more power to him. I actually like knowing what the hell my code is doing and that it conforms to my standards before committing it. I&#x27;ll accept his condolences.",
      "Yegge is smart guy but, I get the impression that he mostly works real hard to not work too hard. And while I&#x27;m sympathetic towards that tendency, I feel like most of the advice in this space is about using multi-agent configurations to effectively convince your management and co-workers that you&#x27;re massively productive. Whether the mountains of code and architecture your agents are crapping out is actually adding value to the organization is a question that most companies are ill-equipped to evaluate - so these agent swarms are just a way to keep the gravy train rolling.<p>I think I&#x27;d rather hear what somebody who is pathologically productive like John Carmack is doing with multi-agent environments...",
      "I use it for both — side projects and my day job in embedded systems.<p>The key is where the tokens go. More tokens spent on planning, design, \nspec validation, test generation, and multi-agent review than on writing \nthe actual code. The review pipeline should be heavier than the generation \npipeline.<p>I encourage my team to use it as a plugin too. The &quot;sorry way&quot; is still a fine \nstarting point — but once you see what a structured agent pipeline \ncatches that manual review misses, it&#x27;s hard to go back.",
      "<p><pre><code>    &gt; …The final level in his AI\n    &gt; Coding chart reads: &quot;Level 8:\n    &gt; you build your own orchestrator\n    &gt; to coordinate more agents&quot;…\n\n</code></pre>\nYegge revealed [1] what&#x27;s necessary to get to that level…<p>____<p><i>…How do you avoid getting tired? Dude, I take naps throughout the day. I&#x27;m exhausted…<p>…<p>…which is why I mentioned in one of my last blog posts that I&#x27;m taking naps all the time…</i><p>____<p>Yegge&#x27;s productivity sounds impressive. I&#x27;ll give you that. But it doesn&#x27;t sound practical or sustainable for the everyday dev.<p>I doubt that even Google — with all its famous perks — offers employees ad hoc nap times while they&#x27;re on the clock.<p>[1] <a href=\"https:&#x2F;&#x2F;g2ww.short.gy&#x2F;Napster2026\" rel=\"nofollow\">https:&#x2F;&#x2F;g2ww.short.gy&#x2F;Napster2026</a>"
    ],
    "full_text": null
  },
  {
    "title": "Pentagon Used Anthropic's Claude in Maduro Venezuela Raid",
    "url": "https://www.wsj.com/politics/national-security/pentagon-used-anthropics-claude-in-maduro-venezuela-raid-583aff17",
    "source": "hn",
    "summary": "",
    "comments": [
      "Gift link: <a href=\"https:&#x2F;&#x2F;www.wsj.com&#x2F;politics&#x2F;national-security&#x2F;pentagon-used-anthropics-claude-in-maduro-venezuela-raid-583aff17?st=EZh74b\" rel=\"nofollow\">https:&#x2F;&#x2F;www.wsj.com&#x2F;politics&#x2F;national-security&#x2F;pentagon-used...</a>",
      "Wow what an absolute nothingburger of an article.<p>Someone at the Pentagon may have used Claude (maybe directly or maybe indirectly through a Palantir tool), to do something that may have had something to do with a really large military operation that involved possibly tens of thousands of people. Maybe. We don’t know. Did I say maybe?<p>Amazing. The rest of the article reads like filler from a college freshman when you’re required to meet a word count quota. A bunch of random factoids strung together.<p>I bet they used Claude to write this stuff.",
      "What for? I can&#x27;t believe they&#x27;d feed classified intel into it for suggestions (despite Hegseth saying everyone should embrace AI - I don&#x27;t think things in the Pentagon are that bad <i>just</i> yet).<p>And do they have their own sandboxed version via Palantir?<p>Maybe it&#x27;s something a lot more benign like it helped draft some of the press releases? &#x2F;s"
    ],
    "full_text": null
  },
  {
    "title": "Spotify says its best developers haven't written code since Dec, thanks to AI",
    "url": "https://techcrunch.com/2026/02/12/spotify-says-its-best-developers-havent-written-a-line-of-code-since-december-thanks-to-ai/",
    "source": "hn",
    "summary": "",
    "comments": [
      "They haven&#x27;t written code in years seeing how their piece of shit desktop app uses more ram and cpu cycles to stream 192kbps music in a day than the entire NASA stack for all Apollo missions combined. They should ask Claude to rewrite it in a real programming language",
      "Doesn&#x27;t say anything about what their <i>worst</i> developers have been doing...",
      "I can picture the entire hierarchy telling whoever&#x27;s next up the chain what they&#x27;re demanding to hear.<p>I&#x27;d be scripting the consumption of lots of credits and sending the output to &#x2F;dev&#x2F;null to get a glowing performance review.",
      "And just wait for when they force their developers (that are not writing any line of code), to go back onsite because &quot;remote is bad&quot;...",
      "Spotify also keeps bewaking every update, so I do not trust their opinions",
      "Cool, this is great. The sooner these apps start collapsing from ill-written and unchecked code the better.<p>P.s. Im not a SWE for a living but I can see through this crap.",
      "Explains why the app hasn’t noticeably improved yet.",
      "Spotify please open a limited time test api for experimenters without 250k DAU.  There are people that would love to build that dont yet have that user base.",
      "Man, a friend of mine hasn&#x27;t written code for years. He generally comes in at least fifteen minutes late, using the side door - that way the boss can&#x27;t see him, - and, uh, after that he just sorta spaces out for about an hour. He just stares at his desk; but it looks like he&#x27;s working. He does that for probably another hour after lunch, too. I&#x27;d say in a given week he probably only does about fifteen minutes of real, actual, work, and even that&#x27;s just filling out TPS reports."
    ],
    "full_text": null
  }
]