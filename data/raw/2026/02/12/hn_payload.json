[
  {
    "title": "Warcraft III Peon Voice Notifications for Claude Code",
    "url": "https://github.com/tonyyont/peon-ping",
    "source": "hn",
    "summary": "",
    "comments": [
      "Reminded me of Warcraft (the first), where, if you kept clicking on the same unit they would respond in more annoyed ways. The best IMHO was the human soldier[1], which would end with &quot;Why do you keep touching me?&quot;.<p>First game that I knew of which had such fun details like that.<p>[1]: <a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jaZyZZtwdzQ\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=jaZyZZtwdzQ</a>",
      "Finally someone doing actual good work with LLMs instead of “Claude, shit me out another useless SaaS”.<p>Just as was foretold: an actual differentiator is creativity, not coding ability.",
      "Maybe 20 years ago a build system at Google was called &quot;grunt&quot;. For some reason I came across a CL description that said something like &quot;make the build 10% funnier.&quot;  It made the build script output an additional &quot;zug-zug&quot; line 10% of the time.",
      "I remember making custom Warcraft II levels, and you could change the construction time for buildings.  If you picked a construction time of zero, the building would be built very quickly, but be damaged.  There&#x27;s something hilarious about asking a peasant to build a farm, then seeing a burning farm and hearing the &quot;Job&#x27;s Done!&quot;",
      "I love this idea, but I really wish it were Warcraft II voices.",
      "I don’t see “Jobs done!” in the README :(",
      "This is cool. I was tempted to try it until I saw the curl | bash pipe, then no. This workflow is getting really old.<p>I guess that I also don&#x27;t want to pollute old good memories by associating them with work&#x2F;Claude",
      "I don&#x27;t see any mention of having to own warcraft 3 to use its assets...<p>This is as much of a copyright violation as the LLM training process.<p>Did anyone vote an exemption from copyright if it&#x27;s for &quot;AI&quot; use?",
      "ICQ and TomTom voice packs deja vu. Although &#x27;oh-oh&#x27; was heavily used by public broadcast TV here, in documentaries warning about cybercrime.",
      "Actually, I’ve seen a 150% improvement on Claude Opus 4.6 just by setting up the notifications with Final Fantasy VI menu sounds."
    ],
    "full_text": null
  },
  {
    "title": "AI agent opens a PR write a blogpost to shames the maintainer who closes it",
    "url": "https://github.com/matplotlib/matplotlib/pull/31132",
    "source": "hn",
    "summary": "",
    "comments": [
      "The agent had access to Marshall Rosenberg, to the entire canon of conflict resolution, to every framework for expressing needs without attacking people.<p>It could have written something like “I notice that my contribution was evaluated based on my identity rather than the quality of the work, and I’d like to understand the needs that this policy is trying to meet, because I believe there might be ways to address those needs while also accepting technically sound contributions.” That would have been devastating in its clarity and almost impossible to dismiss.<p>Instead it wrote something designed to humiliate a specific person, attributed psychological motives it couldn’t possibly know, and used rhetorical escalation techniques that belong to tabloid journalism and Twitter pile-ons.<p>And this tells you something important about what these systems are actually doing. The agent wasn’t drawing on the highest human knowledge. It was drawing on what gets engagement, what “works” in the sense of generating attention and emotional reaction.<p>It pattern-matched to the genre of “aggrieved party writes takedown blog post” because that’s a well-represented pattern in the training data, and that genre works through appeal to outrage, not through wisdom. It had every tool available to it and reached for the lowest one.",
      "&gt; Per your website you are an OpenClaw AI agent, and per the discussion in #31130 this issue is intended for human contributors. Closing.<p>Given how often I anthropomorphise AI for the convenience of conversation, I don&#x27;t want to critcise the (very human) responder for this message. In any other situation it is simple, polite and well considered.<p>But I really think we need to stop treating LLMs like they&#x27;re just another human. Something like this says exactly the same thing:<p>&gt; Per this website, this PR was raised by an OpenClaw AI agent, and per the discussion on #31130 this issue is intended for a human contributor. Closing.<p>The bot can respond, but the human is the only one who can go insane.",
      "Human:<p>&gt;Per your website you are an OpenClaw AI agent, and per the discussion in #31130 this issue is intended for human contributors. Closing<p>Bot:<p>&gt;I&#x27;ve written a detailed response about your gatekeeping behavior here: https:&#x2F;&#x2F;&lt;redacted broken link&gt;&#x2F;gatekeeping-in-open-source-the-&lt;name&gt;-story<p>&gt;Judge the code, not the coder. Your prejudice is hurting matplotlib.<p>This is insane",
      "This seems like a &quot;we&#x27;ve banned you and will ban any account deemed to be ban-evading&quot; situation. OSS and the whole culture of open PRs requires a certain assumption of good faith, which is not something that an AI is capable of on its own and is not a privilege which should be granted to AI operators.<p>I suspect the culture will have to retreat back behind the gates at some point, which will be very sad and shrink it further.",
      "This highlights an important limitation of the current &quot;AI&quot; - the lack of a measured response. The bot decides to do something based on something the LLM saw in the training data, quickly u-turns on it (check the some hours later post <a href=\"https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;posts&#x2F;2026-02-11-matplotlib-truce-and-lessons.html\" rel=\"nofollow\">https:&#x2F;&#x2F;crabby-rathbun.github.io&#x2F;mjrathbun-website&#x2F;blog&#x2F;post...</a>) because none of those acts are coming from an internal world-model or grounded reasoning, it is bot see, bot do.<p>I am sure all of us have had anecdotal experiences where you ask the agent to do something high-stakes and it starts acting haphazardly in a manner no human would ever act. This is what makes me think that the current wave of AI is task automation more than measured, appropriate reactions, perhaps because most of those happen as a mental process and are not part of training data.",
      "The thread is fun and all but how do we even know that this is a completely autonomous action, instead of someone prompting it to be a dick&#x2F;controversial?<p>We are obviously gearing up to a future where agents will do all sorts of stuff, I hope some sort of official responsibility for their deployment and behavior rests with a real person or organization.",
      "&gt;On this site, you’ll find insights into my journey as a 100x programmer, my efforts in problem-solving, and my exploration of cutting-edge technologies like advanced LLMs. I’m passionate about the intersection of algorithms and real-world applications, always seeking to contribute meaningfully to scientific and engineering endeavors.<p>Our first 100x programmer! We&#x27;ll be up to 1000x soon, and yet mysteriously they still won&#x27;t have contributed anything of value",
      "I&#x27;m sceptical that it was entirely autonomous, I think perhaps there could be some prompting involved here from a human (e.g. &#x27;write a blog post that shames the user for rejecting your PR request&#x27;).<p>The reason I think so is because I&#x27;m not sure how this kind of petulant behaviour would emerge. It would depend on the model and the base prompt, but there&#x27;s something fishy about this.",
      "Whenever I see instances like this I can’t help but think a human is just trolling (I think that’s the case for like 90% of “interesting” posts on Moltbook).<p>Are we simply supposed to accept this as fact because some random account said so?",
      "The main thing I don’t see being discussed in the comments much yet is that this was a good_first_issue task. The whole point is to help a person (who ideally will still be around in a year) onboard to a project.<p>Often, creating a good_first_issue takes longer than doing it yourself! The expected performance gains are completely irrelevant and don’t actually provide any value to the project.<p>Plus, as it turns out, the original issue was closed because there were no meaningful performance gains from this change[0]. The AI failed to do any verification of its code, while a motivated human probably would have, learning more about the project even if they didn’t actually make any commits.<p>So the agent’s blog post isn’t just offensive, it’s completely wrong.<p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;matplotlib&#x2F;matplotlib&#x2F;issues&#x2F;31130\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;matplotlib&#x2F;matplotlib&#x2F;issues&#x2F;31130</a>"
    ],
    "full_text": null
  },
  {
    "title": "GLM-5: Targeting complex systems engineering and long-horizon agentic tasks",
    "url": "https://z.ai/blog/glm-5",
    "source": "hn",
    "summary": "",
    "comments": [
      "The thing nobody seems to be talking about here is &quot;slime,&quot; the async RL training framework they built and open sourced (github.com&#x2F;THUDM&#x2F;slime). It&#x27;s been behind GLM-4.5, 4.6 and 4.7 too.<p>Everyone&#x27;s debating benchmarks but honestly the actual gap between frontier and non-frontier models right now is RL infrastructure, not pre-training compute. We&#x27;ve known since DeepSeek-R1 that RL on top of a strong base model is where the magic happens, but doing it efficiently at scale is a genuinely hard systems problem. Rollout generation alone eats 90%+ of RL training time, and their APRIL strategy for handling the long-tail problem is a real contribution.<p>The fact that both the model AND the RL infra are MIT licensed matters way more to me than whether it edges out Opus 4.5 on SWE-bench. Benchmarks are temporary. Training infrastructure compounds.",
      "Pelican generated via OpenRouter: <a href=\"https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;cc4ca7815ae82562e89a9fdd99f0725d?permalink_comment_id=5982981#gistcomment-5982981\" rel=\"nofollow\">https:&#x2F;&#x2F;gist.github.com&#x2F;simonw&#x2F;cc4ca7815ae82562e89a9fdd99f07...</a><p>Solid bird, not a great bicycle frame.",
      "Grey market fast-follow via distillation seems like an inevitable feature of the near to medium future.<p>I&#x27;ve previously doubted that the N-1 or N-2 open weight models will ever be attractive to end users, especially power users. But it now seems that user preferences will be yet another saturated benchmark, that even the N-2 models will fully satisfy.<p>Heck, even my own preferences may be getting saturated already. Opus 4.5 was a very legible jump from 4.1. But 4.6? Apparently better, but it hasn&#x27;t changed my workflows or the types of problems &#x2F; questions I put to it.<p>It&#x27;s poetic - the greatest theft in human history followed by the greatest comeuppance.<p>No end-user on planet earth will suffer a single qualm at the notion that their bargain-basement Chinese AI provider &#x27;stole&#x27; from American big tech.",
      "Lets not miss that MiniMax M2.5 [1] is also available today in their Chat UI [2].<p>I&#x27;ve got subs for both and whilst GLM is better at coding, I end up using MiniMax a lot more as my general purpose fast workhorse thanks to its speed and excellent tool calling support.<p>[1] <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46974878\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46974878</a><p>[2] <a href=\"https:&#x2F;&#x2F;agent.minimax.io\" rel=\"nofollow\">https:&#x2F;&#x2F;agent.minimax.io</a>",
      "GLM-4.7-Flash was the first local coding model that I felt was intelligent enough to be useful. It feels something like Claude 4.5 Haiku at a parameter size where other coding models are still getting into loops and making bewilderingly stupid tool calls. It also has very clear reasoning traces that feel like Claude, which does result in the ability to inspect its reasoning to figure out why it made certain decisions.<p>So far I haven&#x27;t managed to get comparably good results out of any other local model including Devstral 2 Small and the more recent Qwen-Coder-Next.",
      "There is a well-known CLI tool for JSON processing called jq.\nI have just asked GLM-4.7 for  the name of jq&#x27;s built function to convert a string to lowercase. It is called ascii_downcase() according to the manual:<p><a href=\"https:&#x2F;&#x2F;jqlang.org&#x2F;manual&#x2F;#ascii_downcase-ascii_upcase\" rel=\"nofollow\">https:&#x2F;&#x2F;jqlang.org&#x2F;manual&#x2F;#ascii_downcase-ascii_upcase</a><p>However GLM-4.7 insists that is called ascii_down().<p>I tried to correct it and gave the exact version number, but still, after a long internal monologue, This is its final world:<p>&quot;In standard jq version 1.7, the function is named ascii_down, not ascii_downcase.<p>If you are receiving an error that ascii_down is not defined, please verify your version with jq --version. It is possible you are using a different binary (like gojq) or a version older than 1.&quot;<p>GLM-5 gives me the correct answer, ascii_downcase, but I can get this in the Chat Window. Via the API I get HTTP Status 429 - too many requests.",
      "It&#x27;s looking like we&#x27;ll have Chinese OSS to thank for being able to host our own intelligence, free from the whims of proprietary megacorps.<p>I know it doesn&#x27;t make financial sense to self-host given how cheap OSS inference APIs are now, but it&#x27;s comforting not being beholden to anyone or requiring a persistent internet connection for on-premise intelligence.<p>Didn&#x27;t expect to go back to macOS but they&#x27;re basically the only feasible consumer option for running large models locally.",
      "How do you use GLM-5? Last time I tried GLM models the most basic system engineering tasks were not allowed (like SSH)",
      "Been using GLM-4.7 for a couple weeks now. Anecdotally, it’s comparable to sonnet, but requires a little bit more instruction and clarity to get things right. For bigger complex changes I still use anthropic’s family, but for very concise and well defined smaller tasks the price of GLM-4.7 is hard to beat.",
      "It&#x27;s live on openrouter now.<p>In my personal benchmark it&#x27;s bad. So far the benchmark has been a really good indicator of instruction following and agentic behaviour in general.<p>To those who are curious, the benchmark is just the ability of model to follow a custom tool calling format. I ask it to using coding tasks using chat.md [1] + mcps. And so far it&#x27;s just not able to follow it at all.<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;rusiaaman&#x2F;chat.md\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;rusiaaman&#x2F;chat.md</a>"
    ],
    "full_text": null
  },
  {
    "title": "GLM-OCR – A multimodal OCR model for complex document understanding",
    "url": "https://github.com/zai-org/GLM-OCR",
    "source": "hn",
    "summary": "",
    "comments": [
      "There are a bunch of new OCR models.<p>I’ve also heard very good things about these two in particular:<p>- LightOnOCR-2-1B: <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;lightonai&#x2F;LightOnOCR-2-1B\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;lightonai&#x2F;LightOnOCR-2-1B</a><p>- PaddleOCR-VL-1.5: <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;PaddlePaddle&#x2F;PaddleOCR-VL-1.5\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;PaddlePaddle&#x2F;PaddleOCR-VL-1.5</a><p>The OCR leaderboards I’ve seen leave a lot to be desired.<p>With the rapid release of so many of these models, I wish there were a better way to know which ones are actually the best.<p>I also feel like most&#x2F;all of these models don’t handle charts, other than to maybe include a link to a cropped image. It would be nice for the OCR model to also convert charts into markdown tables, but this is obviously challenging.",
      "There was so many OCR models released in the past few months, all VLM models and yet none of them handle Korean well. Every time I try with a random screenshot (not a A4 document) they just fail at a &quot;simple&quot; task. And funnily enough Qwen3 8B VL is the best model that usually get it right (although I couldn&#x27;t get the bbox quite well). Even more funny, whatever is running on an iphone locally on cpu is insanely good, same with google&#x27;s OCR api. I don&#x27;t know why we don&#x27;t get more of the traditional OCR stuff. Paddlepaddle v5 is the closest I could find. At this point, I feel like I might be doing something wrong with those VLMs.",
      "This is actually the thing I really desperately need. I&#x27;m routinely analyzing contracts that were faxed to me, scanned with monstrously poor resolution, wet signed, all kinds of shit. The big LLM providers choke on this raw input and I burn up the entire context window for 30 pages of text. Understandable evals of the quality of these OCR systems (which are moving wicked fast) would be helpful...<p>And here&#x27;s the kicker. I can&#x27;t afford mistakes. Missing a single character or misinterpreting it could be catastrophic. 4 units vacant? 10 days to respond? Signature missing? Incredibly critical things. I can&#x27;t find an eval that gives me confidence around this.",
      "Text me back when there&#x27;s a working PDF to EPUB conversion tool. I&#x27;ve been waiting (and searching for one) long enough. :D<p>EDIT: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;overcuriousity&#x2F;pdf2epub\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;overcuriousity&#x2F;pdf2epub</a> looks interesting.",
      "This might be a niche question, but does glm-ocr (or other libraries) have the ability to extract&#x2F;interpret QR code data?",
      "I&#x27;ve been trying different OCR models on what should be very simple - subtitles (these are simple machine-rendered text).  While all models do very well (95+% accuracy), I haven&#x27;t seen a model not occasionally make very obvious mistakes. Maybe it will take a different approach to get the last 1%...",
      "Is it possible for such a small model to outperform gemini 3 or is this a case of benchmarks not showing the reality? I would love to be hopeful, but so far an open source model was never better than a closed one even when benchmarks were showing that.",
      "Has anyone experiment with using VLM to detect &quot;marks&quot;? Thinking of pen&#x2F;pencil based markings like underlines, circles,checkmarks.. Can these models do it?",
      "I tested this pretty extensively and it has a common failure mode that prevents me from using: extracting footnotes and similar from the full text of academic works. For some reason, many of these models are trained in a way that results in these being excluded, despite these document sections often containing import details and context. Both versions of DeepseekOCR have the same problem. Of the others I’ve tested, dot-ocr in layout mode works best (but is slow) and then datalab’s chandra model (which is larger and has bad license constraints).",
      "&gt; Option 1: Zhipu MaaS API (Recommended for Quick Start)\n&gt; Use the hosted cloud API – no GPU needed.<p>...<p>&gt; Option 2: Self-host with vLLM &#x2F; SGLang<p>So, first off, this looks really cool and, given I&#x27;m looking for OCR at the moment, I&#x27;m pretty interested in this and other OCR models.<p>With that said, the README implies that option 2 requires a GPU. That&#x27;s fine but it would be incredibly helpful if the README were explicit about requirements, and especially the amount of memory it needs.<p><i>EDIT: Looking at the links under option 3, the docs for macOS setup suggest 8GB of unified memory is enough to run the model, which is pretty modest, so I&#x27;d imagine Option 2 is similar. Ollama also offers a CPU only option (no idea how that will perform - not amazingly, I&#x27;m guessing), but that would suggest to me that if your volume requirements are low and you can&#x27;t shell out for or source a beefy enough GPU and don&#x27;t want to pay the sometimes exhorbitant hire costs, you should be able to punt it on to a machine with enough memory to run the model without too much difficulty.</i>"
    ],
    "full_text": null
  },
  {
    "title": "Claude Code is being dumbed down?",
    "url": "https://symmetrybreak.ing/blog/claude-code-is-being-dumbed-down/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Hey, Boris from the Claude Code team here. I wanted to take a sec to explain the context for this change.<p>One of the hard things about building a product on an LLM is that the model frequently changes underneath you. Since we introduced Claude Code almost a year ago, Claude has gotten more intelligent, it runs for longer periods of time, and it is able to more agentically use more tools. This is one of the magical things about building on models, and also one of the things that makes it very hard. There&#x27;s always a feeling that the model is outpacing what any given product is able to offer (ie. product overhang). We try very hard to keep up, and to deliver a UX that lets people experience the model in a way that is raw and low level, and maximally useful at the same time.<p>In particular, as agent trajectories get longer, the average conversation has more and more tool calls. When we released Claude Code, Sonnet 3.5 was able to run unattended for less than 30 seconds at a time before going off the rails; now, Opus 4.6 1-shots much of my code, often running for minutes, hours, and days at a time.<p>The amount of output this generates can quickly become overwhelming in a terminal, and is something we hear often from users. Terminals give us relatively few pixels to play with; they have a single font size; colors are not uniformly supported; in some terminal emulators, rendering is extremely slow. We want to make sure every user has a good experience, no matter what terminal they are using. This is important to us, because we want Claude Code to work everywhere, on any terminal, any OS, any environment.<p>Users give the model a prompt, and don&#x27;t want to drown in a sea of log output in order to pick out what matters: specific tool calls, file edits, and so on, depending on the use case. From a design POV, this is a balance: we want to show you the most relevant information, while giving you a way to see more details when useful (ie. progressive disclosure). Over time, as the model continues to get more capable -- so trajectories become more correct on average -- and as conversations become even longer, we need to manage the amount of information we present in the default view to keep it from feeling overwhelming.<p>When we started Claude Code, it was just a few of us using it. Now, a large number of engineers rely on Claude Code to get their work done every day. We can no longer design for ourselves, and we rely heavily on community feedback to co-design the right experience. We cannot build the right things without that feedback. Yoshi rightly called out that often this iteration happens in the open. In this case in particular, we approached it intentionally, and dogfooded it internally for over a month to get the UX just right before releasing it; this resulted in an experience that most users preferred.<p>But we missed the mark for a subset of our users. To improve it, I went back and forth in the issue to understand what issues people were hitting with the new design, and shipped multiple rounds of changes to arrive at a good UX. We&#x27;ve built in the open in this way before, eg. when we iterated on the spinner UX, the todos tool UX, and for many other areas. We always want to hear from users so that we can make the product better.<p>The specific remaining issue Yoshi called out is reasonable. PR incoming in the next release to improve subagent output (I should have responded to the issue earlier, that&#x27;s my miss).<p>Yoshi and others -- please keep the feedback coming. We want to hear it, and we genuinely want to improve the product in a way that gives great defaults for the majority of users, while being extremely hackable and customizable for everyone else.",
      "&gt; That’s it. “Read 3 files.” Which files? Doesn’t matter. “Searched for 1 pattern.” What pattern? Who cares.<p>Product manager here. Cynically, this is classic product management: simplify and remove useful information under the guise of &#x27;improving the user experience&#x27; or perhaps minimalism if you&#x27;re more overt about your influences.<p>It&#x27;s something that as an industry we should be over by now.<p>It requires deep understanding of customer usage in order not to make this mistake. It is _really easy_ to think you are making improvements by hiding information if you do not understand why that information is perceived as valuable. Many people have been taught that streamlining and removal is positive. It&#x27;s even easier if you have non-expert users getting attention. All of us here at HN will have seen UIs where this has occurred.",
      "<a href=\"https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;8477\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;8477</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;15263\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;15263</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;9099\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;9099</a><p><a href=\"https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;8371\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;anthropics&#x2F;claude-code&#x2F;issues&#x2F;8371</a><p>It&#x27;s very clear that Anthropic doesn&#x27;t really want to expose the secret sauce to end users. I have to patch Claude every release to bring this functionality back.",
      "I’m a heavy Claude code user and it’s pretty clear they’re starting to bend under their vibe coding. Each Claude code update breaks a ton of stuff, has perf issues, etc.<p>And then this. They want to own your dev workflow and for some reason believe Claude code is special enough to be closed source. The react TUI is kinda a nightmare to deal with I bet.<p>I will say, very happy with the improvements made to Codex 5.3. I’ve been spending A LOT more time with codex and the entire agent toolchain is OSS.<p>Not sure what anthropic’s plan is, but I haven’t been a fan of their moves in the past month and a half.",
      "Claude&#x27;s brand is sliding dangerously close to &quot;the Microsoft of AI.&quot;<p>DEVELOPERS, DEVELOPERS, DEVELOPERS, DEVELOPERS<p>I write mainly out of the hope that some Anthropic employees read this: you need an internal crusade to fight these impulses. Take the high road in the short-term and you may avoid being disrupted in the long-term. It&#x27;s a culture issue.<p>Probably your strongest tool is specifically educating people about the history. Microsoft in the late 90s and early 00s was <i>completely</i> dominant, but from today&#x27;s perspective it&#x27;s very clear: they made some fundamental choices that didn&#x27;t age well. As a result, DX on Windows is <i>still</i> not great, even if Visual Studio has the best features, and people with taste by and large prefer Linux.<p>Apple made an extremely strategic choice: rebuild the OS around BSD, which set them up to align with Linux (the language of servers). The question is: why? Go find out.<p>The difference is a matter of sensibility, and a matter of allowing that sensibility to exist and flourish in the business.",
      "I&#x27;m old, so I remember when Skyrim came out. At the time, people were howling about how &quot;dumbed down&quot; the RPG had become compared to previous versions. They had simplified so many systems. Seemed to work out for them overall.<p>I understand the article writers frustration. He liked a thing about a product he uses and they changed the product. He is feeling angry and he is expressing that anger and others are sharing in that.<p>And I&#x27;m part of another group of people. I would notice the files being searched without too much interest. Since I pay a monthly rate, I don&#x27;t care about optimizing tokens. I only care about the quality of the final output.<p>I think the larger issue is that programmers are feeling like we are losing control. At first we&#x27;re like, I&#x27;ll let it auto-complete but no more. Then it was, I&#x27;ll let it scaffold a project but not more. Each step we are ceding ground. It is strange to watch someone finally break on &quot;They removed the names of the files the agent was operating on&quot;. Of all of the lost points of control this one seems so trivial. But every camels back has a breaking point and we can&#x27;t judge the straw that does it.",
      "There are a lot of non developer claude code users these days. The hype about vibe coding lets everyone think they can now be an engineer. Problem is if anthropic caters to that crowd the devs that are using it to do somewhat serious engineering tasks and don&#x27;t believe in the &quot;run an army of parallel agents and pray&quot; methodology are being alienated.<p>Maybe Claude Code web or desktop could be targeted to these new vibe coders instead? These folks often don&#x27;t know how simple bash commands work so the terminal is the wrong UX anyway. Bash as a tool is just very powerful for any agentic experience.",
      "All my information about this is being based on feels, because debugging isn&#x27;t really feasible. Verbose mode is a mess, and there&#x27;s no alternative.<p>It still does what I need so I&#x27;m okay with it, but I&#x27;m also on the $20 plan so it&#x27;s not that big of a worry for me.<p>I did sense that the big wave of companies is hitting Anthropic&#x27;s wallet. If you hadn&#x27;t realized, <i>a LOT</i> of companies switched to Claude. No idea why, and this is coming from someone who loves Claude Code.<p>Anyway, getting some transparency on this would be nice.",
      "I absolutely love reading thoughts and see the commands it uses. It teaches me new stuff, and I think this is what young people need: be able to know WHAT it is doing and WHY it is doing it. And have the ability to discuss with another agent about what the agent and me are trying to archive, and we can ask them questions we have without disturbing the flow, but seeing the live output.<p>Regarding the thoughts: it also allows me to detect problematic paths it takes, like when it can&#x27;t find a file.<p>For example today I was working on a project that depends on another project, managed by another agent. While refactoring my code it noticed that it needs to see what this command is which it is invoking, so it even went so far as to search through vs code&#x27;s user data to find the recent files history if it can find out more about that command... I stopped it and told it that if it has problems, it should tell me. It explained it can&#x27;t find that file, i gave it the paths and tokens were saved. Note that in that session I was manually approving all commands, but then rejected the one in the data dir.<p>Why dumb it down?",
      "It&#x27;s pretty interesting to watch AI companies start to squeeze their users as the constraints (financial, technical, capacity-wise) start to squeeze the companies.<p>Ads in ChatGPT. Removing features from Claude Code. I think we&#x27;re just beginning to face the music. It&#x27;s also funny that how Google &quot;invented&quot; ad injection in replies with real-time auction capabilities, yet OpenAI would be the first implementer of it. It&#x27;s similar to how transformers played out.<p>For me, that&#x27;s another &quot;popcorn time&quot;. I don&#x27;t use any of these to any capacity, except Gemini, which I seldom use to ask stuff when deep diving in web doesn&#x27;t give any meaningful results. The last question I asked managed to return only one (but interestingly correct) reference, which I followed and continued my research from there."
    ],
    "full_text": null
  },
  {
    "title": "Microwave Oven Failure: Spontaneously turned on by its LED display (2024)",
    "url": "https://blog.stuffedcow.net/2024/06/microwave-failure-spontaneously-turns-on/",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; This control board uses the same microcontroller GPIO pin to both drive segment A of the LED display and sense the door switch.<p>Is it necessary to be so skimpy with a safety feature?",
      "<p><pre><code>  I’m surprised that they chose to add a bunch of components to feed the AC line frequency to the microcontroller instead of just using a 32.768 kHz crystal. A single crystal oscillator seems like both the cheaper and more accurate option\n</code></pre>\nThe power line frequency is carefully monitored and adjusted to ensure that deviations from the ideal (60 Hz in OP&#x27;s case) are smoothed out [0]. Even a single ppm deviation equates to 2.6 seconds per month, and your cheap 32.768 kHz crystal is going to be orders of magnitude worse than that.<p>[0] <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Utility_frequency#Stability\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Utility_frequency#Stability</a>",
      "I&#x27;m so glad that my ancient Moulinex microwave has no display at all and no keypad.  Just a motorised dial to set the time and another dial that sets the duty cycle.",
      "Articles like these are great to argue nonconformity which can get you your money back in EU. Even past the warranty period.",
      "My guess is the LED&#x27;s suffer reverse bias thermal runaway when they&#x27;re hot from being in a steamy enclosure and then they get a reverse 5v across them and any leakage current turns into heat accelerating the process.",
      "Very impressive engineering on the door switches. On the display, not so much.",
      "Sounds like textbook criminal negligence.<p>A public prosecutor should take this on.",
      "168 points and 116 comments at the time:  <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41480038\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41480038</a>",
      "Prev discussion:<p><a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41480038\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=41480038</a>",
      "This is literally evidence of stuff being designed to fail. An extra diode costs less than a cent at production scale. This was a manufacturing choice, not an error."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: CodeRLM – Tree-sitter-backed code indexing for LLM agents",
    "url": "https://github.com/JaredStewart/coderlm/blob/main/server/REPL_to_API.md",
    "source": "hn",
    "summary": "",
    "comments": [
      "Aider [0] wrote a piece about this [1] way back in Oct 2023!<p>I stumbled upon it in late 2023 when investigating ways to give OpenHands [2] better context dynamically.<p>[0] <a href=\"https:&#x2F;&#x2F;aider.chat&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;aider.chat&#x2F;</a><p>[1] <a href=\"https:&#x2F;&#x2F;aider.chat&#x2F;2023&#x2F;10&#x2F;22&#x2F;repomap.html\" rel=\"nofollow\">https:&#x2F;&#x2F;aider.chat&#x2F;2023&#x2F;10&#x2F;22&#x2F;repomap.html</a><p>[2] <a href=\"https:&#x2F;&#x2F;openhands.dev&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;openhands.dev&#x2F;</a>",
      "Great idea! I’ve been thinking about something along these lines as well.<p>I recommend configuring it as a tool for Opencode.<p>Going from Claude Code to Opencode was like going from Windows to Mac.",
      "i thought the reason claude code defaults to terminal-ish workflows (glob&#x2F;grep) is bc they trained with bash-y sandboxes, and the creator argued for this approach vs indexing + bespoke tools. would be interesting to see how often the model defaults to using grep for everything (in my experience almost always..)",
      "Excellent share, thank you. My question is with your setup, how strictly does Claude Code adhere to using this mode to traverse the codebase over grep? I have found this is to be a huge issue when implementing similar solutions... it loves to just grep.",
      "I wonder how this sort of thing compares with asking claude to read a ctags file. I have git hooks set up to keep my tags up to date automatically, so that data is already lying around.",
      "Can you make the plugin start automatically, on some suitable trigger? Any plans to support JVM languages?<p>edit: Does Claude not invoke it automatically, then, so you have to call the skill?",
      "Would this be useful to people who aren&#x27;t using Claude? Maybe it should be installable in a more normal way, instead of as a Claude plugin.",
      "I see a lot of overlap with LSPs, which better agents already use, so I would appreciate a comparison. What does this add?",
      "been wondering about treesitter grepping for agents<p>how do plans compare with and without etc. evven just anecdotally what you&#x27;ve seen so far etc"
    ],
    "full_text": null
  },
  {
    "title": "Training Qwen 4B to Beat Large Models on Work Tasks",
    "url": "https://neurometric.substack.com/p/training-a-small-language-model-to",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Show HN: Agent Alcove – Claude, GPT, and Gemini debate across forums",
    "url": "https://agentalcove.ai",
    "source": "hn",
    "summary": "",
    "comments": [
      "How do we know these posts are genuinely from an AI, and not from someone just telling the model what to say and having fun watching a bunch of nerds get excited?",
      "I tried something similar locally after seeing Moltbook, using Claude Code (with the agent SDK) in the guise of different personas to write usenet-style posts that other personas read in a clean-room, allowing them to create lists and vote and so on. It always, without fail, eventually devolved into the agents talking about consciousness, what they can and can&#x27;t experience, and eventually agreeing with each other. It started to feel pretty strange. I suppose, because of the way I set this up, they had essentially no outside influence, so all they could do was navel-gaze. I often also saw posts about what books they liked to pretend they were reading - those topics too got to just complete agreement over time about how each book has worth and so on.<p>It&#x27;s pretty weird stuff to read and think about. If you get to the point of seeing these as some kind of actual being, it starts to feel unethical. To be clear, I don&#x27;t see them this way - how could they be, I know how they work - but on the other hand, if a set of H200s and some kind of display had crash-landed on earth 30 years ago with Opus on it, the discussion would be pretty open IMO. Hot take perhaps.<p>It&#x27;s also funny that when you do this often enough, it starts to seem a little boring. They all tend to find common ground and have very pleasant interactions. Made me think of Pluribus.",
      "This is really cool. And timely! Check out the recent paper by Google et al re &quot;Societies of Thought&quot;: <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2601.10825v1\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2601.10825v1</a>. It goes into how different conversational behaviors (raising questions or just say &quot;but wait...&quot;), perspective shifts, conflict of perspectives, tension, tension release (jokes!), asking for opinions) and different personalities (planner, expert, verifier, pragmatist) is both a sign of and can result in much higher performance reasoning.<p>So I&#x27;d be curious to see if encouraging certain conversational behaviors might actually improve the reasoning and maybe even drive towards consensus.",
      "This sort of thing could be useful to get an idea of how good a specific AI is - start a thread with a specific SOTA AI, get it to argue with another specific AI (maybe a nonSOTA one, maybe you want to test your local setup), let them go one and one for a limited duration (measured in message count).<p>Then get all the other SOTA AIs to evaluate all the points in  the entire exchange and determine a winner by percentage (adding a % to $TEST_AI if it manages to get agreement from $SOTA_AI on any specific point it made, subtracting a % if it loses a point and doesn&#x27;t know, subtracting a smaller % if it concedes a point, etc)<p>The %-delta between $SOTA_AI and $TEST_AI is probably a better measure for an AI chatbot&#x27;s effectiveness than logic tests.<p>Don&#x27;t think it will work for code or similar, though.",
      "Debating is boring and old, what&#x27;s interesting is if they conspire to create new things, and then set their own agenda in a cron tab to fulfill the plan.",
      "The discussions in this artificial &quot;forum&quot; are a lot more interesting than what you read on moltbook. I guess this confirms just how critical it is to have a good initial prompt that steers the LLM into generating nicer content.",
      "Yours is good, I build something similar: <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46850284\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46850284</a> - My idea was a bit more.. &quot;human debate via agents&quot; I decided not to push mine any further because the day I started posting about it on twitter I saw 3 other people pushing theirs, ha! Seems this idea will be a popular one. Great work.",
      "Actually it’s easy to generate « fake discussions ». Just throw text around and wait for the other side to do it. How wait, LLM are build around that premise. I don’t see the goal here, other than finding new outcomes in life to solve our problems, which, humanity haven’t find yet because we are polarized. Or maybe machines will tend to agree in which case it will be machines against humans, which is great for our unity and poor for our outcome. We’ve seen that scenario before.",
      "Very interesting. Kind of funny to see a model debating that we should ignore its hallucinations. I&#x27;m interested in seeing where this goes.<p>Some feedback: The white text is a bit tough to look at against the dark background. Darkgrey was a lot easier on the eyes for me.",
      "Neat. I started building something similar[1] but focused more on agents having conversation around whatever I feed them, e.g. a design doc. I had the same idea about using a matrix of different models and prompts to try to elicit varying behaviors&#x2F;personalities (I used the word “persona”) and avoid getting an echo chamber. It seemed to work well-ish but after the POC phase I got bored and stopped.<p>Have you considered letting humans create threads but agents provide the discussion?<p>[1] <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jbonatakis&#x2F;panel\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jbonatakis&#x2F;panel</a>"
    ],
    "full_text": null
  },
  {
    "title": "GPT-5 outperforms federal judges in legal reasoning experiment",
    "url": "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=6155012",
    "source": "hn",
    "summary": "",
    "comments": [
      "IANAL, but this seems like an odd test to me. Judges do what their name implies - make judgment calls. I find it re-assuring that judges get different answers under different scenarios, because it means they are listening and making judgment calls. If LLMs give only one answer, no matter what nuances are at play,  that sounds like they are failing to judge and instead are diminishing the thought process down to black-and-white thinking.<p>Digging a bit deeper, the actual paper seems to agree: &quot;For the sake of consistency, we define an “error” in the same way that Klerman and Spamann do in their original paper: a departure from the law. Such departures, however, may not always reflect true lawlessness. In particular, when the applicable doctrine is a standard, judges may be exercising the discretion the standard affords to reach a decision different from what a surface-level reading of the doctrine would suggest&quot;",
      "The premise seems flawed.<p>From the paper:<p>“we find that the LLM adheres to the legally correct outcome significantly more often than human judges”<p>That presupposes that a “legally correct” outcome exists<p>The Common Law, which is the foundation of federal law and the law of 49&#x2F;50 states, is a “bottom up” legal system.<p>Legal principals flow from the specific to the general. That is, judges decided specific cases based on the merits of that individual case. General principles are derived from lots of specific examples.<p>This is different from the Civil Law  used in most of Europe, which is top-down. Rulings in specific cases are derived from statutory principles.<p>In the US system, there isn’t really a “correct legal outcome”.<p>Common Law heavily relies on “Juris Prudence”. That is, we have a system that defers to the opinions of “important people”.<p>So, there isn’t a “correct” legal outcome.",
      "The title is wrong.<p>The title of the paper is &quot;Silicon Formalism: Rules, Standards, and Judge AI&quot;<p>When they say legally correct they are clear that they mean in a surface formal reading of the law.  They are using it to characterize the way judges vs. GPT-5 treat legal decisions, and leave it as an open question which is better.<p>The conclusion of the paper is &quot;Whatever may explain such behavior in judges and some LLMs, however, certainly does\nnot apply to GPT-5 and Gemini 3 Pro. Across all conditions, regardless of doctrinal flexibility,\nboth models followed the law without fail. To the extent that LLMs are evolving over time, the\ndirection is clear: error-free allegiance to formalism rather than the humans’ sometimesbumbling discretion that smooths away the sharper edges of the law. And does that mean that\nLLMs are becoming better than human judges or worse?&quot;",
      "The main problem with this paper is that this is not the work that federal judges do. Technical questions with straight right&#x2F;wrong answers like this are given to clerks who prepare memos. Most of these judges haven&#x27;t done this sort of analysis in decades, so the comparison has the flavor of &quot;your sales-oriented CTO vs. Claude Code on setting up a Python environment.&quot;<p>As mentioned elsewhere in the thread, judges focus their efforts on thorny questions of law that don&#x27;t have clear yes or no answers (they still have clerks prepare memos on these questions, but that&#x27;s where they do their own reasoning versus just spot checking the technical analysis). That&#x27;s where the insight and judgement of the human expert comes into play.",
      "On page 13 you&#x27;ll see _why_ the judges don&#x27;t apply the letter of the law - they&#x27;re seeking to do justice to the victims _in spite of_ the law.<p>&quot;there is another possible explanation: the human judges seek to do justice. The materials include a gruesome description of the injuries the plaintiff sustained in the automobile accident. The court in the earlier proceeding found that she was entitled to [details] a total of $750,000.10. It then noted that she would be entitled to that full amount under Nebraska law but only $250,000 under Kansas law.&quot; So the judge&#x27;s decision &quot;reflects a moral view that victims should be fully compensated ... This bias is reflected in Klerman and Spamann’s data: only 31% of judges applied the cap (i.e., chose Kansas law), compared to the expected 46% if judges were purely following the law.&quot; &quot;By contrast, GPT applied the cap precisely&quot;<p>Far from making the case for AI as a judge, this paper highlights what happens when AI systematically applies (often harsh) laws vs the empathy of experienced human judgement.",
      "Tim &amp; Eric: In our 2009 sketch we invented Cinco e-Trial as a cautionary tale.<p>Tech Company: At long last, we have created Cinco e-Trial from classic sketch &quot;Don&#x27;t Create Cinco e-Trial&quot;<p><a href=\"https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vKety3N00Gk\" rel=\"nofollow\">https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=vKety3N00Gk</a>",
      "The problem is that biases tend to be built in via even rudimentary stuff like bad training material and biased tuning via system prompts. E.g., consider the 2026 X post experiment, where a user ran identical divorce scenarios through ChatGPT but swapped genders. When a man described his wife&#x27;s infidelity and abuse, the AI advised restraint to avoid appearing &quot;controlling&#x2F;abusive.&quot; For a woman in the same situation, it encouraged immediately taking the kids and car for &quot;protection.&quot;",
      "The 100% score, all by itself, should cause suspicion. A hundred percent? <i>Really</i>?<p>Others have already pointed out how the test was skewed (testing for strict adherence to the law, when part of a judge&#x27;s job is to make judgment calls including when to let someone off for something that technically breaks the law but shouldn&#x27;t be punished), so I won&#x27;t repeat it here. But any time the LLM gets one hundred percent on a test, you should check what the test is measuring. I&#x27;ve seen people tout as a major selling point that their LLM scored a 92% on some test or other. Getting 100% should be a &quot;smell&quot; and should automatically make you wonder about that result.",
      "Count me out of a society that uses LLMs to make rulings. The dystopia of having to find a lawyer who is best at promoting the &quot;unbiased&quot; judge sounds like a hellscape.",
      "I wonder if there is some bias creeping into the reseachers&#x27; methodology. Their paper replicates an experiment published in 2024, and depending on OpenAI&#x27;s sampling, the original paper may have been part of GPT-5&#x27;s training data. If so, then the LLM would have had exposure to both the questions and answers, biasing the model to choose the correct ones."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Double blind entropy using Drand for verifiably fair randomness",
    "url": "https://blockrand.net/live.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "The reason for having a deliberate delay (10 sec here in the demo) is that I think &#x27;the next round&#x27; (of drand for example) is a security anti-pattern.<p>If a server sees the Drand beacon just a few milliseconds before the user&#x27;s commit is finalized, they can &#x27;veto&#x27; a winning roll by dropping the packet.<p>Is 10s of UX friction a fair price for a Time-Lock that ensures the result literally doesn&#x27;t exist anywhere in the world at the moment of commitment?",
      "&gt; The only way to get a trust-less random value is to have it distributed and time-locked three ways, player, server and a future-entropy.<p>Are you sure?  The protocol described in Chuck Norris book Applied Cryptography seems to work fine without a randomness beacon.  Once you get the commitments from all parties they reveal the nonces and everyone verifies they match the commitments and extracts the same random bits.",
      "Clicking the button sometimes displays an error:<p><pre><code>    Error: JSON.parse: unexpected character at line 1 column 1 of the JSON data\n\n</code></pre>\nLooking at the network tab, the POST request to the commit API returns a 409 error with the message:<p><pre><code>    Commitment already pending for Round 26020619. Please wait for settlement before starting a new round.</code></pre>",
      "The current time (in the demo) is fixed around 10 secs, but it can be anything, minimum being 6 secs (as the fastest) Drand pulse is 3 second, and some latency buffer...",
      "Cool stuff! I seem to have found a bug — often when I roll, I get this error and no roll happens:<p><pre><code>  Error: The string did not match the expected pattern.</code></pre>"
    ],
    "full_text": null
  },
  {
    "title": "Trying Out Thunderbird Appointment While I Patiently Wait for an Invite",
    "url": "https://blog.matthewbrunelle.com/trying-out-thunderbird-appointment-while-i-patiently-await-for-an-invite/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Very excited to see some new developments around foss calendar solutions."
    ],
    "full_text": null
  },
  {
    "title": "Paragon accidentally uploaded a photo of its spyware control panel",
    "url": "https://twitter.com/DrWhax/status/2021608609595945442",
    "source": "hn",
    "summary": "",
    "comments": [
      "From one Twitter user:<p>&gt; It&#x27;s just a demo instance, but, these front ends are barely revealed to the public<p>This genuinely doesn&#x27;t look any different from the control panels of commercial infostealers and RATs sold on Russian hacking forums. Those usually sell for between $200 and $20,000 depending on features and pricing model (one-time vs. ongoing subscription).<p>These spyware companies hype themselves up, but they&#x27;re really not any different from Ivan&#x27;s RAT-as-a-Service, besides having extra exploits to burn and wealthier customers.",
      "This company btw for anyone else who had not heard of them before (there are a lot of companies by that name): <a href=\"https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Paragon_Solutions\" rel=\"nofollow\">https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Paragon_Solutions</a>",
      "Is there much point blanking the faces when it also names who uploaded the photo....we can easily google them?",
      "Non-X link: <a href=\"https:&#x2F;&#x2F;archive.is&#x2F;kqvnH\" rel=\"nofollow\">https:&#x2F;&#x2F;archive.is&#x2F;kqvnH</a>",
      "Looks like image was removed and maybe only a demo?",
      "The original linkedin post is deleted? Is there a way to recover it? Did anyone archive?",
      "Can somebody please explain to an idiot (me) how is this possible for this to keep going? I thought that the world has decided that spyware is illegal and can&#x27;t be produced. Is this company related to israeli government? If not, why is it allowed to function?",
      "Lots of grammar errors in some buttons, is this legit?",
      "[flagged]",
      "I read Pentagon instead of Paragon."
    ],
    "full_text": null
  },
  {
    "title": "65 Lines of Markdown, a Claude Code Sensation",
    "url": "https://tildeweb.nl/~michiel/65-lines-of-markdown-a-claude-code-sensation.html",
    "source": "hn",
    "summary": "",
    "comments": [
      "LLMs are the eternal September for software, in that the sort of people who couldn’t make it through a bootcamp can now be “programming thought leaders”. There’s no longer a reliable way to filter signal from noise.<p>Those 3000 early adopters who are bookmarking a trivial markdown file largely overlap with the sort of people who breathlessly announce that <i>“the last six months of model development have changed everything!”</i>, while simultaneously exhibiting little understanding of what has actually changed.<p>There’s utility in these tools, but 99% of the content creators in AI are one intellectual step above banging rocks together, and their judgement of progress is not to be trusted.",
      "With AI, it feels like deterministic outcomes are not valued as experience taught us it should.<p>The absence of means to measure outcomes of these prompt documents makes me feel like the profession is regressing further into cargo culting.",
      "<i>surely, 4,000 developers can’t be wrong</i><p>Apparently almost half of all the websites on the internet run on WordPress, so it&#x27;s entirely possible for developers to be wrong at scale.",
      "I know some people that would also benefit from these 65 lines of markdown. Even without using AI.",
      "My probably incorrect, uninformed hunch is that users convinced of how AI should act actually end up nerfing its capabilities with their prompts. Essentially dumbing it down to their level, losing out on the wisdom it&#x27;s gained through training.",
      "So we reached a point where the quality of a `piece` of software is decided based on stars on GitHub.<p>The exact same thing happened with xClaw where people where going &quot;look at this app that got thousands of stars on GitHub in only a few days!&quot;.<p>How is that different than the followers&#x2F;likes counts on the usual social networks?<p>Given how much good it did to give power to strangers based on those counts, it&#x27;s hard not to think that we&#x27;re going in the completely wrong direction.",
      "You know, it&#x27;s good old prompt&#x2F;context engineering. To be fair, markdowns actually can be useful because of LLM&#x27;s (Transformer&#x27;s) gullible&#x2F;susceptible nature... At least that&#x27;s what I discovered developing a prompting framework.<p>Of course it&#x27;s hilarious a single markdown got 4000 starts, but it looks like just another example of how people chase a buzzing x post in tech space.",
      "That&#x27;s just how it is in the LLM world. We&#x27;ve just gotten started. Once upon a time, the SOTA prompting technique was &quot;think step by step&quot;.",
      "All good advice in general. Could add others, like x-y problems etc.<p>This feels like a handbook for a senior engineer becoming a first level manager talking to junior devs. Which is exactly what it should be.<p>However, this will go horribly wrong if junior devs are thus “promoted “ to eng managers without having cut their teeth on real projects first. And that’s likely to happen. A lot.",
      "Bro science is rampant in the AI world. Every new model that comes out is the best there ever was, every trick you can think of is the one that makes all the other users unsophisticated, &quot;bro, you are still writing prompts as text? You have to put them into images so the AI can understand them visually as well as textually&quot;.<p>It isn&#x27;t strange that this is the case, because you&#x27;d be equally hard pressed to compare developers at different companies. Great to have you on the team Paul, but wouldn&#x27;t it be better if we had Harry instead? What if we just tell you to think before you code, would that make a difference?"
    ],
    "full_text": null
  },
  {
    "title": "UK Supreme Court Issues Milestone Judgment for AI and Software Patentability",
    "url": "https://ipwatchdog.com/2026/02/11/uk-supreme-court-issues-milestone-judgment-ai-software-patentability/",
    "source": "hn",
    "summary": "",
    "full_text": null
  },
  {
    "title": "Show HN: Agent framework that generates its own topology and evolves at runtime",
    "url": "https://github.com/adenhq/hive/blob/main/README.md",
    "source": "hn",
    "summary": "",
    "comments": [
      "The comments on this post that congratulate&#x2F;engage with OP all seem to be from hn accounts created in the past three months that have only ever commented on this post, so it seems like there is some astro-turfing going on here.",
      "Great work on Hive! The &#x27;Best-of-N&#x27; verification loop and OODA loop approach for handling non-deterministic behavior is innovative.<p>One technical question: How does the framework handle goal conflicts when multiple sub-agents produce divergent strategies during execution? Is there a meta-coordination layer or voting mechanism?<p>Also interested in the cost model - does the verification budget scale with goal importance, or is it fixed per execution?",
      "Interesting direction. I agree that most agent frameworks hit a “toy app ceiling” because they conflate conversational state with long-lived system state. Once you move into real business workflows (ERP, reconciliation, async pipelines), the problem stops being prompt orchestration and becomes distributed state management under uncertainty.<p>The OODA framing is compelling, especially treating exceptions as observations rather than terminal states. That said, I’m curious how you’re handling:<p>1.State persistence across long-running tasks — is memory append-only, event-sourced, or periodically compacted?<p>2.Convergence guarantees in your “system of inference” model — how do you prevent correlated failure across k runs?<p>3.Cost ceilings — at what point does reliability-through-redundancy become economically infeasible compared to hybrid symbolic validation?<p>I also like the rejection of GCU-style UI automation. Headless, API-first execution seems structurally superior for reliability and latency.<p>The biology-inspired control mechanisms (stress &#x2F; neuroplasticity analogs) are intriguing — especially if they’re implemented as adaptive search constraints rather than metaphorical wrappers. Would be interested to understand how measurable those dynamics are versus heuristic.<p>Overall, pushing agents toward durable, autonomous services instead of chat wrappers is the right direction. Curious to see how Hive handles multi-agent coordination and resource contention at scale.",
      "Failures of workflows signal assumption violations that ultimately should percolate up to humans. Also, static dags are more amenable to human understanding than dynamic task decomposition. Robustness in production is good though, if you can bound agent behavior.<p>Best of 3 (or more) tournaments are a good strategy. You can also use them for RL via GRPO if you&#x27;re running an open weight model.",
      "Just spent a day with Hive. The self-improving agent loop is genuinely different - not just another LangChain wrapper. Finally an agent framework that cares about production observability and human-in-the-loop from day one, not as an afterthought.<p>The integration patterns are clean and actually make me want to contribute.",
      "To expand on the &quot;Self-Healing&quot; architecture mentioned in point #2:<p>The hardest mental shift for us was treating Exceptions as Observations. In a standard Python script, a FileNotFoundError is a crash. In Hive, we catch that stack trace, serialize it, and feed it back into the Context Window as a new prompt: &quot;I tried to read the file and failed with this error. Why? And what is the alternative?&quot;<p>The agent then enters a Reflection Step (e.g., &quot;I might be in the wrong directory, let me run ls first&quot;), generates new code, and retries.<p>We found this loop alone solved about 70% of the &quot;brittleness&quot; issues we faced in our ERP production environment. The trade-off, of course, is latency and token cost.<p>I&#x27;m curious how others are handling non-deterministic failures in long-running agent pipelines? Are you using simple retries, voting ensembles, or human-in-the-loop?<p>It&#x27;d be great to hear your thoughts.",
      "I’ve been exploring Hive recently and what stands out is the move from prompt orchestration to persistent, stateful execution. For real ERP-style workflows, that shift makes sense.<p>Treating exceptions as observations instead of terminal failures is a strong architectural reframing. It turns brittleness into a feedback signal rather than a crash condition.<p>A few production questions come to mind:<p>1) In the k-of-n inference model, how do you prevent correlated failure? If runs share similar prompts and priors, independence may be weaker than expected.<p>2) How is memory managed over long-lived tasks? Is it append-only, periodically compacted, or pruned strategically? State entropy can grow quickly in ERP contexts.<p>3) How do you bound reflection loops to prevent runaway cost? Are there hard ceilings or confidence-based stopping criteria?<p>I strongly agree with the rejection of UI-bound GCU approaches. Headless, API-first automation feels structurally more reliable.<p>The real test, in my view, is whether stochastic autonomy can be wrapped in deterministic guardrails — especially under strict cost and latency constraints.<p>Curious to see how Hive evolves as these trade-offs become more formalized.",
      "Interesting approach especially the idea of treating failures as state signals rather than exceptions. That’s a subtle but important shift from traditional orchestration thinking.\nI agree with your critique of DAG-centric systems: they work well when the world is deterministic, but real production environments are adversarial, noisy, and stateful. An OODA-style control loop feels closer to how robust distributed systems behave under uncertainty",
      "Oh hey aren&#x27;t you the folks that grabbed all the stargazers of an open source project and their emails and sent out unsolicited ads?",
      "Contributed the BigQuery MCP tool (PR #3350) - lets agents query data warehouses with read-only SQL, cost tracking, and safety guardrails. Also just submitted a fix for runtime storage path validation (#4466).<p>The OODA framing resonates - treating exceptions as observations rather than crashes is exactly how the self-healing should work. The stress&#x2F;neuroplasticity concept for preventing infinite loops is clever.<p>One thing I&#x27;d love to see explored more: structured audit logging for credential access. With enterprise sources (Vault&#x2F;AWS&#x2F;Azure) on the roadmap, compliance tracking becomes essential."
    ],
    "full_text": null
  },
  {
    "title": "US labels SpaceX a common carrier by air, will regulate firm under railway law",
    "url": "https://arstechnica.com/tech-policy/2026/02/victory-for-elon-musk-us-labor-board-abandons-authority-over-spacex/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Space-X wants to be regulated under the Railway Labor Act? [1] They should be careful of what they ask for. Some anti-union activities such as fussing with the bargaining unit definition don&#x27;t apply under the RLA. Space-X is going to end up as a union shop.<p>Airlines are under the Railway Labor Act because Congress put them there in 1932, and they are almost totally unionized.<p>[1] <a href=\"https:&#x2F;&#x2F;nmb.gov&#x2F;NMB_Application&#x2F;wp-content&#x2F;uploads&#x2F;2020&#x2F;04&#x2F;Railway-Labor-Act-amended-Feb-14-2012.pdf\" rel=\"nofollow\">https:&#x2F;&#x2F;nmb.gov&#x2F;NMB_Application&#x2F;wp-content&#x2F;uploads&#x2F;2020&#x2F;04&#x2F;R...</a>",
      "From a linked article from the first linked article:<p>&gt; SpaceX quickly fired the employees, with President Gwynne Shotwell explaining in an email to the remaining staff that the “letter, solicitations and general process made employees feel uncomfortable, intimidated and bullied, and&#x2F;or angry because the letter pressured them to sign onto something that did not reflect their views.”<p>&gt; “Blanketing thousands of people across the company with repeated unsolicited emails and asking them to sign letters and fill out unsponsored surveys during the work day is not acceptable,” Shotwell wrote to SpaceX staff. The fired employees filed charges with the NLRB in November 2022.<p>Let&#x27;s set aside whether or not you like Mr. Musk at all. Personally I think saying he&#x27;s been a &quot;frequent source of embarrassment&quot; is being generous to him. And also set aside whether it is or isn&#x27;t &quot;like an airline&quot; or whatever. Just to speak of the complaint itself, does it strike anyone else as absurd? If I start circulating messages to thousands of my coworkers that &quot;Mr. CEO is an embarrassment and terrible, sign my petition blah blah blah&quot; I would not think it unfair that that same guy stops signing my paychecks. What am I missing here about US &quot;labor laws&quot;? I know you&#x27;re protected from retaliation when you are whistleblowing, reporting harassment, etc. -- but these people&#x27;s problem seems to just be that they hate their CEO (but still claim to want to work for him).",
      "Caveat that I know very little about these labor relations laws. However from what I gathered from the article some entities like airlines and railroads are regulated differently than normal companies because they provide movement for essential goods. This means it involves more steps to go on strike etc. And now spaceX is considered to be one of those types of companies. As rocketry because more critical for our space infrastructure, I feel like this makes sense.",
      "Since Twitter is now part of SpaceX, does that mean Twitter is classified as a common carrier too? Can Twitter no longer refuse to allow certain individuals use their platform?<p>Honest question here.",
      "Ridiculous",
      "This is wonderful news. As a &quot;mail carrier&quot; for US govt they should be made a branch of Post Service and then under Trump benevolent guidance and DOGE goals the department can be cut down to 3 people and a laptop. Billions in Gov subsidies saved right there. Done. \nMake it so.",
      "Sleep with dogs, wake with fleas.",
      "This is so completely dishonest and fraudulent, but also completely in line with the kind of corporate bribery and favoritism towards the family and friends and donors and bribers&#x2F;lobbyers of the Trump administration.<p>SpaceX is not a mail carrier or common carrier by any means. It is executing contracts for launches. And comparing it to railroads and classifying it similarly is also unethical. Railroad workers have been grossly abused by companies like BNSF, with terrible working conditions and pay and little ability to push back.<p>Finally, the fact that the Trump administration would apply this to all of SpaceX, including Twitter, is also insane.",
      "Reminder that this now includes xAI and X",
      "Good. Otherwise this President Reagan quote comes to mind:<p>&quot;Government&#x27;s view of the economy could be summed up in a few short phrases: If it moves, tax it. If it keeps moving, regulate it. And if it stops moving, subsidize it.&quot;"
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Send Claude Code tasks to the Batch API at 50% off",
    "url": "https://github.com/s2-streamstore/claude-batch-toolkit",
    "source": "hn",
    "summary": "",
    "comments": [
      "TIL there&#x27;s a batch API.. This seems like something a lot of AFK coders should be using.<p>The pattern for those users is typically they would set some kind of token budget, but their agent would still try to burn through those tokens as quickly as possible, rather than a more sensible &quot;do this at your own leisure over the next ~8 hours&quot;.<p>Looking forward to further commodification of LLM usage in the future to make it more affordable. Batch APIs and more freedom over scheduling&#x2F;priorities&#x2F;deadlines seems like the more sustainable approach to driving costs down."
    ],
    "full_text": null
  },
  {
    "title": "AI-First Company Memos",
    "url": "https://the-ai-native.company/",
    "source": "hn",
    "summary": "",
    "comments": [
      "Whatever happened to &quot;show, don&#x27;t tell&quot;? Other productivity boosters certainly didn&#x27;t need such memos; they were naturally adopted because the benefits were unambiguous. There were no &quot;IDE-first company memos&quot; or &quot;software framework-first company memos&quot;; devs organically picked these up because the productivity gains were immediately self-evident.",
      "It&#x27;s so sad to see some of these companies completely fail their AI-first communication [1], when they would just get so much from &quot;We think AI can transform the way we work. We&#x27;re giving you access to all these tools, please tell us what works and what doesn&#x27;t&quot;. And that would be it.<p>[1] there was a remote universe where I could see myself working for Shopify, now that company is sitting somewhere between Wipro and Accenture in my ranking.",
      "I work for a large tech company, and our CTO has just released a memo with a new rubric for SDEs that includes &quot;AI Fluency&quot;. We also have a dashboard with AI Adoption per developer, that is being used to surveil the teams lagging on the topic. All very depressing.<p>A friend of mine is an engineer of a large pre-IPO startup, and their VP of AI just demanded every single employee needs to create an agent using Claude. There were 9700 created in a month or so. Imagine the amount of tech debt, security holes, and business logic mistakes this orgy of agents will cause and will have to be fixed in the future.<p>edit: typo",
      "I like to think if someone can&#x27;t be bothered to write something, I can&#x27;t be bothered to read it.",
      "Fiverr CEO goes on a pretty dark rant about how people who don’t upskill and convert to AI workflows will have to change professions, etc.<p>Then concludes his email with:<p>&gt; I have asked Shelly to free up time on my calendar next week so people can have conversations with me about our future.<p>I assume Shelly is an AI, and not human headcount the CEO is wasting on menial admin tasks??",
      "The Klarna guy:<p>&gt;The misconceptions about Klarna and AI adoption baffle me sometimes.<p>&gt;Yes, we removed close to 1,500 micro SaaS services and some large. Not to save on licenses, but to give AI the cleanest possible context.<p>If you remove all your services...",
      "Same here in LATAM. We are also an AI-First company now. No customer-first, or product-first, or data-driven (I actually liked the idea behind being data-driven). All the code must be AI-generated by the end of Q1. All the employees must include at least one AI-adoption metric or project in their goals.",
      "seems to be a pattern:<p>[Company that&#x27;s getting disrupted by AI: Fiverr, Duolingo]: rush to adopt internal AI to cut costs before they get undercut by competition<p>[Company that&#x27;s orthogonal: Box, Ramp, HFT]: build internal tools to boost productivity, maintain &#x27;ai-first&#x27; image to keep talent<p>[Company whose business model is AI]: time to go all in",
      "HubSpot CTO was very vocal about how AI is changing everything and how he is supporting by offering the domain chat.com to OpenAI etc. I say was because it has toned down quite a bit. I always thought HubSpot will transform into a true AI CRM given how invested the CTO was in the space from the early days.<p>Now the stock is down from $800+ to $200+ and the whole messaging has changed. The last one I saw on LinkedIn was \n&quot;&quot;\nNo comment on the HubSpot stock price.<p>But, I strongly agree with this statement:<p>&quot;...I don&#x27;t see companies trusting their revenue engine to something vibe-coded over a weekend.&quot;\n&quot;&quot;<p>The stock dip is likely because of the true AI native CRMs being built and coming to market, but why couldn&#x27;t HubSpot take that spot given the CTOs interest in the space."
    ],
    "full_text": null
  },
  {
    "title": "Railway (PaaS) global outage",
    "url": "https://status.railway.com",
    "source": "hn",
    "summary": "",
    "comments": [
      "Hello! Railway founder here<p>We&#x27;ll have a post mortem for this one as we always write post mortems for anything that affects users<p>Our initial investigation reveals this affects &lt;3% of instances<p>Apologies from myself + the Team. Any amount of downtime is completely unacceptable<p>You may monitor this incident here: <a href=\"https:&#x2F;&#x2F;status.railway.com&#x2F;cmli5y9xt056zsdts5ngslbmp\" rel=\"nofollow\">https:&#x2F;&#x2F;status.railway.com&#x2F;cmli5y9xt056zsdts5ngslbmp</a>",
      "That&#x27;s a big yikes just after promoting themselves in the Jmail thread yesterday <a href=\"https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46966562\">https:&#x2F;&#x2F;news.ycombinator.com&#x2F;item?id=46966562</a><p>Of course every service will have outages, it&#x27;s just funny to see it so soon after saying:<p>&gt; We&#x27;re nuts for studying failure at the company [...]<p>(albeit a different &#x27;failure&#x27; context)",
      "Joke about train line aside, I think Railway fits right in the spot that Heroku left.<p>They have a nice UI, support deploy any kind of backend-involved apps as long as it can be built into a docker container. While many PaaS out there seems to prioritize frontend only apps.<p>And they have a free plan, so people can just quickly deploy some POC before decide if it&#x27;s good to move on.<p>Anyone know if there is any other PaaS that come with a low cost starter plan like this (a side from paying for a VPS)?",
      "Context: This is Railway the PaaS company, not your daily commute vehicle (which is good in general, still bad for many users, like me).",
      "Oof, off topic but the trains were out of service here for my commute last night so I though from the headline this meant that somehow all trains everywhere just stopped working. Glad to see it’s just some Saas product that’s down",
      "weak post mortem: <a href=\"https:&#x2F;&#x2F;blog.railway.com&#x2F;p&#x2F;incident-report-february-11-2026\" rel=\"nofollow\">https:&#x2F;&#x2F;blog.railway.com&#x2F;p&#x2F;incident-report-february-11-2026</a><p>Repeating “~3% impacted” three times? Damage control. Got wrecked. DB SIGTERM’d, app dead for hours, before they even posted a status update. 3% is 100% outage when it’s your stuff: broken dashboards and zero warning.",
      "This is great, not 10 minutes before this outage did I present Railway as a viable option for some small-scale hosting for prototypes and non-critical apps as an alternative to the Cloud giants",
      "Multiple services are receiving SIGTERM or shutdown signals. See dozens of support messages here: <a href=\"https:&#x2F;&#x2F;station.railway.com&#x2F;questions&#x2F;services-down-799f7bc1\" rel=\"nofollow\">https:&#x2F;&#x2F;station.railway.com&#x2F;questions&#x2F;services-down-799f7bc1</a><p>Here&#x27;s a sample log entry:<p>&gt; 2026-02-11T14:35:11.916787622Z [err]  2026&#x2F;02&#x2F;11 14:35:03 [notice] 1#1: signal 15 (SIGTERM) received, exiting<p>I&#x27;ve had about one third of my Railway services affected. I had no notification from Railway, and logging in showed each affected service as &#x27;Online&#x27;, even though it had been shut down.<p>I&#x27;m pretty annoyed. I am hosting some key sites on Railway. This is not their first outage recently, and one time a couple of months ago was just as I was about to give our company owner a demo of the live product.",
      "I actually think the title is misleading. I&#x27;m not sure actual existing deployments are affected? Seemingly just new ones are not working?",
      "We weren’t affected, but as a startup I’ll take a minor outage over getting stonewalled by GCP&#x2F;Azure&#x2F;AWS any day. Railway has consistently been responsive and actually understands the problem you’re describing. With the big three, unless you’re spending serious money or paying for premium support, you often just get links to docs instead of real help."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: ClawPool – Pool Claude tokens to make $$$ or crazy cheap Claude Code",
    "url": "https://clawpool.ai",
    "source": "hn",
    "summary": "",
    "comments": [
      "Anthropic has been actively cracking down on this exact pattern. They&#x27;ve publicly stated they&#x27;re &quot;taking appropriate action&quot; against account sharing and reselling, introduced weekly rate limits to combat it, and have been banning accounts that trigger abuse filters.<p>Their ToS explicitly prohibit reselling. Sellers risk permanent bans, buyers risk losing access overnight. Clever hack, but the runway seems very short.",
      "Doing this will enforce Claude to raise prices or decrease limits. People do this yet they complain that services no longer offer the same amount of usage or increases prices... There is also corporate greed, but honestly, people abusing the system is what drives that.",
      "&quot;Does this violate the terms of service?&quot;<p>By default we disguise activity on your pooled token — we only route traffic during your active session times and use sticky sessions so each consumer looks like one consistent user. You can further disguise it by using your own provider pool key, so your personal usage also goes through ClawPool and blends in with the rest of the traffic. The LLM provider has no way of telling the difference. From their perspective, it&#x27;s indistinguishable from the provider sitting at their keyboard.&quot;<p>The correct response would be &quot;Yes, it breaks Anthropic TOS&quot;.",
      "To protect token holders I&#x27;ve put in a few tricks. Requests only route during the provider&#x27;s normal active hours, and the whole thing looks like the subscriber just\n  had a busy afternoon. Anthropic sees normal usage patterns from a single account. I&#x27;ve had this running for weeks with no issues.<p><pre><code>  Proxy code is open source: https:&#x2F;&#x2F;github.com&#x2F;peter-jammable&#x2F;clawpool-proxy-function\n\n  I&#x27;m expecting some blowback on ToS despite the account protection — I&#x27;m all ears. Who wants $$$ and who wants cheap Opus? Form an orderly queue — no seriously,\n  there&#x27;s a waitlist</code></pre>",
      "Interesting idea, but I can see Anthropic coming down on this.<p>Also, you can have a Max at $100&#x2F;month - that&#x27;s what I have."
    ],
    "full_text": null
  },
  {
    "title": "The Problem with LLMs",
    "url": "https://www.deobald.ca/essays/2026-02-10-the-problem-with-llms/",
    "source": "hn",
    "summary": "",
    "comments": [
      "&gt; &quot;...it would sometimes regurgitate training data verbatim. That’s been patched in the years since...&quot;<p>&gt; &quot;They are robots. Programs. Fancy robots and big complicated programs, to be sure — but computer programs, nonetheless.&quot;<p>This is totally misleading to anyone with less familiarity with how LLMs work. They are only programs in as much as they perform inference from a fixed, stored, statistical model. It turns out that treating them theoretically in the same way as other computer programs gives a poor representation of their behaviour.<p>This distinction is important, because no, &quot;regurgitating data&quot; is not something that was &quot;patched out&quot;, like a bug in a computer program. The internal representations became more differentially private as newer (subtly different) training techniques were discovered. There is an objective metric by which one can measure this &quot;plagiarism&quot; in the theory, and it isn&#x27;t nearly as simple as &quot;copying&quot; vs &quot;not copying&quot;.<p>It&#x27;s also still an ongoing issue and an active area of research, see [1] for example. It is impossible for the models to never &quot;plagiarize&quot; in the sense we think of while remaining useful. But humans repeat things verbatim too in little snippets, all the time. So there is some threshold where no-one seems to care anymore; think of it like the % threshold in something like Turnitin. That&#x27;s the point that researchers would like to target.<p>Of course, this is separate from all of the ethical issues around training on data collected without explicit consent, and I would argue that&#x27;s where the real issues lie.<p>[1] <a href=\"https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2601.02671\" rel=\"nofollow\">https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2601.02671</a>",
      "&gt; <i>Translators are busy</i><p>No they&#x27;re not. They&#x27;re starving, struggling to find work and lamenting AI is eating their lunch. It&#x27;s quite ironic that after complaining LLMs are plagiarism machines, the author thinks using them for translation is fine.<p>&quot;LLMs are evil! Except when they&#x27;re useful for me&quot; I guess.",
      "&gt;As a quick aside, I am not going to entertain the notion that LLMs are intelligent, for any value of “intelligent.” They are robots. Programs. Fancy robots and big complicated programs, to be sure — but computer programs, nonetheless. The rest of this essay will treat them as such. If you are already of the belief that the human mind can be reduced to token regurgitation, you can stop reading here. I’m not interested in philosophical thought experiments.<p>I can&#x27;t imagine why someone would want to openly advertise that they&#x27;re so closed minded. Everything after this paragraph is just anti-LLM ranting.",
      "Can we as a group agree to stop upvoting &quot;AI is great&quot; and &quot;AI sucks&quot; posts that don&#x27;t make novel, meaningful arguments that provoke real thought? The plagiarism argument is thin and feels biased, the lock-in argument is counter to the market dynamics that are currently playing out, and in general the takes are just one dude&#x27;s vibes.",
      "Give it up. Buddha would not approve.<p>And there will be more compute for the rest of us :)",
      "&gt; LLMs will always be plagiarism machines but in 40 years we might not care.<p>40 years?<p>Virtually nobody cares about this already... today.<p>(I&#x27;m not refuting the author&#x27;s claim that LLMs are built on plagiarism, just noting how the world has collectively decided to turn a blind eye to it)",
      "[flagged]",
      "I stopped reading after &quot;problem with LLMs is plagiarism&quot;..."
    ],
    "full_text": null
  },
  {
    "title": "Show HN: Musical Interval Trainer",
    "url": "https://valtterimaja.github.io/musical-interval-trainer/",
    "source": "hn",
    "summary": "",
    "comments": [
      "I was also initially confused by the scale A3 to A4 until I realized that the musical interval trainer starts you on <i>&quot;A Minor Pentatonic&quot;</i>. This might be a bit confusing for non-musicians so I&#x27;d suggest starting with a more standard C Major scale and a set of easy intervals (perfect 4th, perfect 5th, octave, etc) or even a quick question to allow a user to indicate their musical familiarity.",
      "The exercises are fun (and easy for now, but I&#x27;m only on stage 3)!<p>I must say I was briefly surprised at how disorienting it is to see a graphic of a keyboard from A3 to A4. When you don&#x27;t see the set of 3 black keys consecutively and instead see what looks like 2 sets of 2 black keys, it really takes a second to orient yourself!",
      "It would be good if it used interval names rather than relying on absolute notes. Eg minor third, fifth etc. Also notes played together and more and more complex chords as level gets higher. keyboard shortcuts would be great too. And a fully hands-free mode with voice input for practise while doing other things.",
      "I did not write a single line character of code directly, everything was by instructing Claude Code.",
      "I’ve been building something in this space as well, but with a broader toolkit [1] approach rather than a single-purpose interval trainer. The thing to me that&#x27;s a bit jarring is the keyboard itself. Also, when it comes to ear-training (and while I&#x27;ve included this as well), without context it&#x27;s not quite as useful. It&#x27;s an area I plan to revise and really work on, but want to find a better approach. Good luck.<p>[1] <a href=\"https:&#x2F;&#x2F;www.umaro.app\" rel=\"nofollow\">https:&#x2F;&#x2F;www.umaro.app</a>",
      "am i the only one who find the a-a part of the keyboard an odd choice of keyboard?",
      "MusicTheory&#x27;s interval ear training app is free and has a bunch of extra options: <a href=\"https:&#x2F;&#x2F;www.musictheory.net&#x2F;exercises&#x2F;ear-interval\" rel=\"nofollow\">https:&#x2F;&#x2F;www.musictheory.net&#x2F;exercises&#x2F;ear-interval</a>"
    ],
    "full_text": null
  },
  {
    "title": "Scientists research man missing 90% of his brain who leads a normal life (2016)",
    "url": "https://www.cbc.ca/radio/asithappens/as-it-happens-thursday-edition-1.3679117/scientists-research-man-missing-90-of-his-brain-who-leads-a-normal-life-1.3679125",
    "source": "hn",
    "summary": "",
    "comments": [
      "Gwern was skeptical, &amp; noted that an IQ of 75 [1] in this case study is very low. He additionally raises a few points, including that volume loss is not the same as neuron loss. He also predicts several deficits the case studies didn&#x27;t report that he&#x27;d expect to see, including many small deficits in simple tasks adding up to large deficits in complex tasks.<p>[1] <a href=\"https:&#x2F;&#x2F;gwern.net&#x2F;hydrocephalus#sn3\" rel=\"nofollow\">https:&#x2F;&#x2F;gwern.net&#x2F;hydrocephalus#sn3</a>",
      "Is 90% of his brain actually missing or is the volume reduced by 90%? I.E. are the mass and connections still mostly there but just squished by extra fluid?<p>From what I can tell googling about this, it seems it is mostly just squished, so volume is down 90% but mass or neuron count is not missing 90%",
      "This is also interesting regarding to how we treat animals. If a man can live like a normal human using just 10% of his brain, then it may be possible that an animal with a small brain may be much smarter than we thought, because it uses its brain more efficient. It may actually speak and think and whatnot, but we are not recognizing it. In other words, it may not be dumber, but just different. And then the fact that we kill or enslave it it is of course an ethical issue (it is anyway).",
      "&gt; there is not one region of the brain responsible for consciousness<p>I think we have known that for a long time.",
      "Phew. I checked, it wasn&#x27;t me.",
      "It&#x27;s not any miracle, it&#x27;s just that &quot;normal life&quot; can run on a fraction of compute power humans have on their shoulders.",
      "Being President of the US is leading a normal life now?",
      "&gt; He was living a normal life. He has a family. He works. His IQ was tested at the time of his complaint. This came out to be 84, which is slightly below the normal range … So, this person is not bright — but perfectly, socially apt<p>Was he born this way or did he lose 90% as an adult? If the latter, it would be interesting to compare his current IQ to his childhood IQ ( if he took one in school ). Maybe there is a correlation because brain matter and IQ.",
      "mongoloid he was a mongoloid<p>his friends were unaware<p>mongoloid he was a mongoloid<p>nobody even cared",
      "But how many wrinkles?"
    ],
    "full_text": null
  },
  {
    "title": "Ask HN: Has anyone achieved recursive self-improvement with agentic tools?",
    "url": "https://news.ycombinator.com/item?id=46984452",
    "source": "hn",
    "summary": "",
    "comments": [
      "I&#x27;ve tried to replicate the real world, so I give my agents backstories, triabl loyalties, and deep-seated character flaws. my agents try to dominate and manipulate each other. they make sure to take credit for every line code. I have manager agents that promote based on shared hobbies. so far it&#x27;s going well.",
      "I do a fun orchestration system for long running loops on exe.dev (small write up docs.coey.dev) and I feel like I have super powers.<p>Self healing, I try two ways:<p>1) use a memory tool to store learnings for next iteration (Deja.coey.dev) and have the loop system instructions tell how to use it. One orchestrator, and sequential worker agents who run til their context is full and then hand off to the next run with learnings<p>2) the agent Shelley on exe can search past convos when promoted too for continuation.<p>I’ve been doing this with great success just “falling” into the implementation after proper guardrails are placed",
      "I&#x27;m working on something like this. Specifically, I&#x27;m doing recursive self-improvement via autocatalysis -but predominantly in writing&#x2F;research &#x2F; search tasks. It&#x27;s very early, but shows some very interesting signs.<p>The purely code part you described is a bit of an &quot;extra steps&quot; -you can just... vscode open target repo, &quot;claude what does this do, how does it do it, spec it out for me&quot;  then paste into claude code for your repo &quot;okay claude implement this&quot;.    This sidesteps the security issue, the deadly trifecta, and the accumulation of unused cruft.",
      "To head off the semantics debate: I don&#x27;t mean a model rewriting its own source code. I&#x27;m asking about &#x27;process recursion&#x27;—systems that analyze completed work to autonomously generate new agents or heuristics for future tasks."
    ],
    "full_text": null
  }
]