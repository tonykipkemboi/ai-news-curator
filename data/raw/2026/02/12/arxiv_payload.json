[
  {
    "title": "Diffusion-Pretrained Dense and Contextual Embeddings",
    "url": "https://arxiv.org/abs/2602.11151v1",
    "source": "arxiv",
    "summary": "In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunki",
    "full_text": "\n\n\n\n1 Introduction\n\n2 PPLX Embedding\n\n2.1 Continued Diffusion Pretraining\n2.2 Pooling and Quantization\n2.3 Pair Training\n2.4 Contextual Training\n2.5 Triplet Training\n2.6 Datasets for Contrastive Learning\n\n\n\n3 Evaluations\n\n\n3.1 Public Benchmarks\n\nMTEB Multilingual.\nMIRACL.\nMTEB Code.\nConTEB.\nBERGEN.\nTool Search.\n\n\n\n3.2 Internal Benchmarks\n\nPPLXQuery2Query.\nPPLXQuery2Doc.\n\n\n3.3 Effect of Binary Quantization\n\n\n4 Diffusion vs. Autoregressive Pretraining\n\n5 Related Work\n\nDiffusion Language Models.\nContrastive Training of Text Embeddings.\nContextual Embeddings.\n\n\n6 Conclusion\n\nAppendix\n\n\nA Details on Continued Pretraining\n\nLoss.\nHyperparameters.\nData.\n\n\n\nB Details on Contrastive Training\n\nPair Training.\nContextual Training.\nTriplet Training.\n\n\nC Details on MTEB Evaluation\nD Details on ConTEB Evaluation\n\nE Details on BERGEN Evaluation\n\nEmbedding Models.\nRAG Configuration.\n\n\n\n\n\n\n\n\n\n\n\\uselogo\n\nDiffusion-Pretrained Dense and Contextual Embeddings\n\n\nSedigheh Eslami\n\nEqual contributions\n\n\n\n\nMaksim Gaiduk\n\nEqual contributions\n\n\n\n\nMarkus Krimmel\n\nEqual contributions\n\n\n\n\nLouis Milliken\n\nEqual contributions\n\n\n\n\nBo Wang\n\nEqual contributions\n\n\n\n\nDenis Bykov\n\n\n\n\n\nAbstract\nIn this report, we introduce \\pplxfamily, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval.\nBy leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents.\nWe release two model types: \\pplxfor standard retrieval, and \\pplxcontextfor contextualized embeddings that incorporate global document context into passage representations.\n\\pplxachieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while \\pplxcontextsets new records on the ConTEB benchmark.\nBeyond public benchmarks, \\pplxdemonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models‚Äô effectiveness in production environments where retrieval quality and efficiency are critical at scale.\n\n\n\n1 Introduction\n\nDense textual embeddings represent texts as points in a continuous vector space in which distances capture meaningful semantic relationships. Embeddings are a crucial part of search systems, as they map queries and documents into a shared semantic space, in which information can be retrieved efficiently via approximate nearest neighbor search.\nThe recent development of large language models has increasingly shifted embedding model training toward employing pretrained decoder-only LLMs to leverage their pre-existing knowledge and improve embedding quality [Zhang et al., 2025b, Jiang et al., 2024, Lee et al., 2025b].\nWe investigate diffusion-based language models [Austin et al., 2021, Nie et al., 2025b] as an alternative paradigm. Diffusion language models employ transformer encoders with bidirectional attention, enabling more comprehensive context modeling compared to causally masked autoregressive models [Zhang et al., 2025a].\nThis architectural difference is particularly advantageous for retrieval tasks, where capturing the global document context is essential.\n\n\nThis report introduces \\pplxfamily111https://huggingface.co/collections/perplexity-ai/pplx-embed, a family of multilingual text embedding models employing multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval.\nContinued pretraining via a diffusion objective converts a causally masked LLM backbone into a bidirectional encoder.\nFurther contrastive training on large-scale question-document pairs, as well as triplet data, aligns the embedding space geometry with semantic similarity. We release two model types: \\pplxfor standard retrieval and \\pplxcontextfor encoding passages with respect to document-level context.\nBoth models are released in 0.6B- and 4B-parameter scales.\nNotably, our models are not instruction-tuned, eliminating the need for users to maintain instruction prefixes.\n\n\nThe \\pplxfamilyfamily utilizes native quantization-aware training and outputs INT8 embeddings by default, achieving competitive performance while offering significantly improved efficiency. On MTEB(Multilingual, v2) [Enevoldsen et al., 2025, Muennighoff et al., 2023], \\pplx-4B achieves an average \\ndcgat10 of 69.66% with INT8 quantization, matching or exceeding leading models such as Qwen3-Embedding-4B (69.60%) and gemini-embedding-001 (67.71%).\nThe \\pplx-4B model also demonstrates competitive performance on the MTEB(Code) and MIRACL benchmarks [Zhang et al., 2023]. For contextual retrieval on ConTEB [Conti et al., 2025], \\pplxcontext-4B outperforms state-of-the-art contextual models such as voyage-context-3222https://blog.voyageai.com/2025/07/23/voyage-context-3/ (79.45%) and Anthropic Contextual333https://www.anthropic.com/engineering/contextual-retrieval (72.4%) with an average \\ndcgat10 of 81.96%. On ToolRet [Shi et al., 2025], \\pplx-4B achieves 44.45% average \\ndcgat10, surpassing larger 7B models including NV-Embed-v1 [Lee et al., 2025a] (42.71%) and GritLM-7B [Muennighoff et al., 2025] (41.13%). This enables efficient pre-filtering of relevant tools from large API corpora, reducing context usage.\nWe demonstrate that \\pplxconsistently outperforms BGE-M3 [Chen et al., 2024] and Qwen3-Embedding on the large-scale BERGEN [Rau et al., 2024] RAG benchmark, with \\pplx-0.6B beating the larger Qwen3-Embedding-4B model on three of the five question-answering tasks.\nWe further describe internal benchmarks for assessing embedding models as first-stage retrievers at web-scale and show superior performance of \\pplxin comparison to Qwen3-Embedding and BGE-M3.\n\n\n\n\n2 PPLX Embedding\n\nThis section presents our training framework for learning high-quality embeddings for retrieval through a multi-stage curriculum. We combine four distinct training paradigms in a branched fashion, followed by a merging and selection stage. A schematic illustration of our training process is provided in Figure 1.\n\n\nFigure 1: Training pipeline of \\pplxand \\pplxcontext.\n\n\nIn the first training stage, described in Section 2.1, we perform continued pretraining of a decoder-only transformer on a diffusion objective, allowing it to use bidirectional self-attention. Subsequent training on query-document pairs (Section 2.3) establishes basic semantic alignment of sequence-level embeddings.\nFollowing this, a contextual stage (Section 2.4) trains chunk-level embeddings, allowing the model to identify relevant passages within documents. The \\pplxcontextmodel is obtained from this training stage.\nWe perform triplet training with hard negatives (Section 2.5) on checkpoints obtained after pair and contextual training, refining boundaries between similar but non-relevant documents. For model merging, we employ Spherical Linear Interpolation [Shoemake, 1985] of the contextual model and the triplet checkpoints to obtain \\pplx.\n\n\n\n2.1 Continued Diffusion Pretraining\n\nFollowing the methodology of Gong et al. [2025], we train two bidirectional diffusion language models via continued pretraining of existing autoregressive decoder-only backbones.\nConsidering the state-of-the-art performance of the Qwen3 family [Yang et al., 2025], we choose Qwen3-0.6B444https://huggingface.co/Qwen/Qwen3-0.6B-Base and 4B555https://huggingface.co/Qwen/Qwen3-4B-Base as our base models.\n\n\nWe disable causal attention masking and train the resulting transformer encoders to reverse a corrupting noise process. We adopt a continuous-time formulation [Shi et al., 2024] and an absorbing state process in which, at timestep t‚àà[0,1]t\\in[0,1], each token has decayed to the absorbing [MASK] state independently with probability tt. To represent the [MASK] state, we repurpose a rarely used token from the Qwen3 vocabulary.\nFollowing prior work [Gong et al., 2025, Ye et al., 2025, Nie et al., 2025b], we preserve the left-shift operation that is applied during autoregressive pretraining. Although we also performed experiments with annealing the causal attention mask [Gong et al., 2025], we did not observe substantial performance improvements and did not pursue this technique further.\n\n\nDuring training, we sample t‚àºùí∞‚Äã(0.001,1)t\\sim\\mathcal{U}(0.001,1) for each input sequence independently and mask each token in the input sequence with probability tt.\nWe train our models via the standard evidence lower bound, which is given by the sum of token-wise cross entropies at masked positions, scaled by 1/t1/t.\n\n\nHalf of our training data consists of English educational web pages from FineWeb-Edu [Penedo et al., 2024], while the other half covers 29 other languages with data sourced from FineWeb2 [Penedo et al., 2025] and FineWeb2-HQ [Messmer et al., 2025].\nWe train our models for 60,000 steps with a global batch size of 1024 and a sequence length of 4096. Thus, we perform pretraining on approximately 250 billion tokens of multilingual text data. Following Nie et al. [2025a], we truncate 1% of the training sequences to a randomly chosen length to ensure that the models are exposed to varying sequence lengths.\nWe use an AdamW optimizer [Loshchilov and Hutter, 2019] with a warmup-stable-decay schedule and peak learning rates of 5√ó10‚àí45\\times 10^{-4} and 3.16√ó10‚àí43.16\\times 10^{-4} for the 0.6B and 4B models, respectively. For a more detailed description of hyperparameters, we refer the reader to Appendix A. We leverage the resulting models exclusively for contrastive learning, evaluating them solely on embedding quality rather than generative performance.\n\n\n\n\n2.2 Pooling and Quantization\n\nTo produce embeddings, we pool token-level representations extracted from the backbone model into a sequence-level representation."
  },
  {
    "title": "YOR: Your Own Mobile Manipulator for Generalizable Robotics",
    "url": "https://arxiv.org/abs/2602.11150v1",
    "source": "arxiv",
    "summary": "Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source,",
    "full_text": null
  },
  {
    "title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
    "url": "https://arxiv.org/abs/2602.11149v1",
    "source": "arxiv",
    "summary": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-e",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Scaling Epochs on a Fixed Update Budget\n\n2.1 Preliminaries\n\n2.2 Experimental Setup\n\nModels.\nDataset.\nEvaluation.\nTraining.\nExperimental grid.\n\n\n2.3 Results\n\n\n\n3 Impact of Training Data\n\n3.1 Teacher Model Quality.\n3.2 Negative Trajectories\n\n\n\n4 Probing the Repetition Advantage\n\n4.1 Memorization signals convergence.\n4.2 Termination correlates with performance.\n4.3 Overfitting paradox.\n4.4 Catastrophic Forgetting\n\n\n\n5 Related Work\n\nData repetition and scaling laws in pretraining.\nMulti-epoch SFT in post-training practice.\nMemorization, overfitting, and training dynamics.\n\n\n6 Conclusion\nA Hyperparameters\n\nB Full Results.\n\nB.1 Dolci Dataset\nB.2 Qwen3 Distills\nB.3 Qwen3 8B Distill; Pos. vs Neg.\n\n\n\n\n\n\n\nData Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning\n\n\nDawid J. Kopiczko\n\n‚ÄÉ‚ÄÉ\nSagar Vaze\n\n‚ÄÉ‚ÄÉ\nTijmen Blankevoort\n\n‚ÄÉ‚ÄÉ\nYuki M. Asano\n\n\n\nAbstract\nSupervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models.\nStandard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME‚Äô24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12‚Äì26 percentage points, with no additional catastrophic forgetting.\nWe find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical\napproach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling.\nWe pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models. Code is available at: https://github.com/dkopi/data-repetition.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nModern language model training proceeds through distinct stages: pretraining on internet-scale data to acquire world knowledge, mid-training on curated corpora to extend capabilities, and post-training to shape model behavior (Guo et al., 2025; Team OLMo, 2025; Yang and others, 2025). For reasoning-focused models, post-training typically begins with supervised fine-tuning (SFT) on long Chain-of-Thought (CoT) demonstrations, often distilled from\n\n\nFigure 1: Illustration of our approach to supervised fine-tuning in a modern LLM training pipeline. Instead of maximizing dataset size and training for few epochs, we train for many epochs on a small random subset of SFT data, substantially reducing compute while improving downstream reasoning performance.\n\n\n\nFigure 2: \nScaling epochs versus scaling data for Olmo3-7B trained on long-CoT SFT data, averaged across AIME‚Äô24, AIME‚Äô25, and GPQA benchmarks. Each diagonal represents a fixed update budget, where epochs √ó samples is constant. Within any diagonal, moving toward fewer samples and more epochs consistently improves accuracy and pass@n, with gains diminishing around 32‚Äì64 epochs. Termination rate correlates strongly with accuracy and may be a primary driver of performance gains, as models that fail to terminate cannot produce a final answer.\n\n\n\n\nFigure 3: \nThe repetition advantage is consistent across models, benchmarks, and evaluation metrics. Heatmaps show normalized scores for Olmo3-7B (top) and Qwen3-8B (bottom) on AIME‚Äô24, AIME‚Äô25, and GPQA, evaluated with both Accuracy@nn and Pass@nn. Each diagonal corresponds to a fixed update budget (epochs √ó\\times samples), and in all settings, performance improves when moving along a diagonal toward fewer samples and more epochs.\n\n\n\nmore capable models, where reasoning traces can span thousands of tokens before reaching a final answer.\nThis SFT step, analogous to behavioral cloning in reinforcement learning (Osa et al., 2018), primes the model for subsequent stages such as reinforcement learning from human feedback (Ouyang and others, 2022) or reinforcement learning with verifiable rewards (Guo et al., 2025; Shao et al., 2024).\nUnlike pretraining data, which can be scraped at scale from the web, high-quality long-CoT demonstrations require either expensive human annotation or careful distillation from larger models, including generation, filtering, and validation of long reasoning traces. As a result, the question of how to best utilize limited SFT data is practically important.\n\n\nThe common assumption in machine learning would suggest that training with more unique training samples yields better generalization. Under i.i.d. sampling, each new example provides independent information about the data distribution, and generalization bounds in statistical learning theory typically improve with dataset size. This principle manifests practically throughout the field ‚Äì data augmentation techniques are widely used to artificially expand effective dataset size when real data is limited¬†(Hern√°ndez-Garc√≠a and K√∂nig, 2018; Shorten and Khoshgoftaar, 2019), and the success of large language models has been attributed in significant part to training on ever-larger unique corpora. Following this logic, modern post-training pipelines employ millions of SFT samples¬†(Team OLMo, 2025).\n\n\nIn this paper, we show that this might not only be suboptimal, but that, actually, a reverse pattern can be observed for the SFT stage for a pretrained LLM, see Figure¬†1.\nUnder a fixed update budget, training for more epochs on smaller datasets outperforms training on\nlarger datasets. The gains are not marginal. The performance and termination rates, i.e., the model‚Äôs ability to successfully conclude reasoning with a final answer, both scale with epoch count and saturate together, suggesting that sufficient repetition of the same data is required for models to fully internalize the demonstrated reasoning structure.\n\n\nWe find that this convergence is tightly linked to training set memorization. Performance improvements plateau once models achieve near-perfect next-token prediction accuracy on the training data, even as validation loss continues to rise. This relationship holds across all models we test, all benchmarks, and different training datasets, making train token accuracy a practical stopping criterion for scaling epochs. Despite this apparent overfitting, we observe no additional catastrophic forgetting compared to single-epoch training on large datasets.\nOur main contributions are:\n\n\n‚Ä¢\n\nPhenomenon. We demonstrate that under a fixed update budget, scaling epochs on smaller datasets substantially outperforms scaling unique samples.\n\n\n\n‚Ä¢\n\nDynamics. We identify training token accuracy as a reliable stopping criterion for epoch scaling, with performance gains plateauing once models reach full memorization, and we show that multi-epoch training on small datasets causes no additional catastrophic forgetting compared to\nlarge datasets.\n\n\n\n‚Ä¢\n\nFactors. We show how training data properties, such as teacher model size in distillation and the correctness of data samples, affect the repetition advantage.\n\n\n\nWhile we provide a practical heuristic for exploiting the repetition advantage, we pose explaining this phenomenon in long-CoT SFT as a novel, open problem for the community.\n\n\nTable 1: \nPerformance at a fixed update budget of ‚Ñ¨=51,200\\mathcal{B}=51,200 gradient updates, showing configurations up to 16 epochs. All rows within each model use equivalent update budget but vary the epochs-to-samples ratio. For all three models, 16 epochs on 3,200 samples substantially outperforms 1 epoch on 51,200 samples across all benchmarks.\n\n\n\n\nModel\nEpochs\nSamples\nGPQA\nAIME‚Äô24\nAIME‚Äô25\n\n\n\n\n\nAvg@4\nPass@4\nAvg@16\nPass@16\nAvg@16\nPass@16\n\n\n\n\n\n\nOlmo3-7B\n\n1\n51.2k\n11.5\n23.7\n17.7\n46.7\n22.3\n50.0\n\n\n2\n25.6k\n14.8\n29.3\n28.1\n66.7\n24.8\n46.7\n\n\n4\n12.8k\n20.2\n38.9\n33.3\n73.3\n29.2\n50.0\n\n\n8\n6.4k\n29.7\n51.5\n44.4\n73.3\n35.4\n66.7\n\n\n16\n3.2k\n34.0\n62.1\n42.3\n80.0\n39.2\n63.3\n\n\n\n\nQwen3-8B\n\n1\n51.2k\n21.6\n38.9\n12.3\n36.7\n11.2\n30.0\n\n\n2\n25.6k\n25.4\n42.4\n14.0\n46.7\n17.3\n43.3\n\n\n4\n12.8k\n35.6\n56.6\n20.6\n66.7\n20.0\n40.0\n\n\n8\n6.4k\n41.9\n61.6\n30.6\n70.0\n27.7\n56.7\n\n\n16\n3.2k\n51.0\n72.7\n30.6\n76.7\n31.2\n63.3\n\n\n\n\nQwen3-4B\n\n1\n51.2k\n13.1\n29.8\n6.5\n26.7\n5.6\n30.0\n\n\n2\n25.6k\n21.1\n38.9\n13.5\n36.7\n14.0\n36.7\n\n\n4\n12.8k\n29.7\n52.5\n18.1\n36.7\n17.7\n40.0\n\n\n8\n6.4k\n40.5\n64.1\n23.3\n43.3\n23.5\n43.3\n\n\n16\n3.2k\n39.3\n68.7\n19.2\n43.3\n18.8\n40.0\n\n\n\n\n\n\n\n2 Scaling Epochs on a Fixed Update Budget\n\nTo investigate whether data repetition can substitute for data scaling in supervised fine-tuning, we conduct controlled experiments varying the number of epochs and unique samples while holding total gradient updates, and all other parameters, constant. We train base checkpoints of two recent language models on chain-of-thought data and evaluate on challenging reasoning benchmarks.\n\n\n\n2.1 Preliminaries\n\nSupervised fine-tuning adapts a pretrained language model to target behaviors by training on demonstration data. Given input-output pairs (x,y)(x,y) where y=(y1,‚Ä¶,yT)y=(y_{1},\\ldots,y_{T}) is a target sequence, SFT minimizes the cross-entropy loss over next-token predictions:\n\n\n\n‚Ñí‚Äã(Œ∏)=‚àí‚àët=1Tlog‚Å°pŒ∏‚Äã(yt‚à£x,y&lt;t)\\mathcal{L}(\\theta)=-\\sum_{t=1}^{T}\\log p_{\\theta}(y_{t}\\mid x,y_{&lt;t})\n\n(1)\n\n\nIn practice, the loss is typically masked to exclude input tokens, applying only to the response.\n\n\nThroughout this work, we use update budget ‚Ñ¨\\mathcal{B} to denote the total number of gradient updates during training, which for batch size one is equal to the number of epochs multiplied by the number of unique samples. Comparing configurations at equal update budgets isolates the effect of data repetition from differences in total optimization steps.\n\n\n\n\n2.2 Experimental Setup\n\nModels.\n\nWe use the Qwen3-4B, Qwen3-8B (Yang and others,"
  },
  {
    "title": "Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling",
    "url": "https://arxiv.org/abs/2602.11146v1",
    "source": "arxiv",
    "summary": "Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\nCLIP-based Reward Models\nVLM-based Reward Models\nDiffusion Models for Discriminative Task\n\n\n\n3 Preliminaries\n\nDiffusion Models\nFlow Matching Models\nLatent Space Modeling\n\n\n\n4 Method\n\n\n4.1 Diffusion-native Preference Formulation\n\nThurstone Model on Clean Samples\nNoise-calibrated Thurstone\nTraining\n\n\n\n4.2 Latent Reward Architecture\n\nMulti-layer diffusion features.\nTimestep-conditioned adaptation.\nQ-Former Scoring\n\n\n4.3 Inference-Time Scaling via Noise Ensembling\n\n\n\n5 Experiments\n\n\n5.1 Experimental Setup\n\nTraining Details\nEvaluation Protocol\n\n\n5.2 Reward Modeling\n\n5.3 Ablation Studies\n\nTimestep Schedules\nNoise-Calibrated Variance\nBackbone Adaptation.\nInference-time Noise Level.\nMulti-Noise Ensembling.\n\n\n\n5.4 Preference Alignment\n\nExperimental Setup\nOptimization Dynamics\nEfficiency Analysis\n\n\n\n\n\n6 Conclusion\n\nLimitations and Future Work\n\n\n7 Uncertainty Analysis\n\n8 Detailed Algorithmic Procedures\n\n8.1 Training with Noise-Calibrated Thurstone\n8.2 Inference-Time Noise Ensembling\n8.3 Implementation of Reward-Gradient Alignment (ReFL)\n\n\n\n9 Extensive Experiments\n\n\n9.1 Extensive Ablation Studies\n\nImpact of Layer Depth\nGeneralization Across Diverse Backbones\nDiscussion about Scaling Behaviors\n\n\n\n9.2 Extensive Alignment Experiments\n\nFlow-GRPO Optimization\n\n\n\n\n10 Discussion about Reward Hacking\n11 Experimental Details\n\n\n\n\n\n\n1]The Hong Kong University of Science and Technology\n2]Huawei Hong Kong AI Framework &amp; Data Technologies Lab\n3]Tsinghua University\n4]The Australian National University\n\nBeyond VLM-Based Rewards: \nDiffusion-Native Latent Reward Modeling\n\n\n\nGongye Liu1\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇBo Yang1\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇYida Zhi1\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇZhizhou Zhong1\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇLei Ke3\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇDidan Deng2\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇHan Gao2\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇYongxiang Huang2\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇKaihao Zhang4\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇHongbo Fu1\n\n‚ÄÉ‚ÄÉ\n ‚ÄÇWenhan Luo1 ‚Ä†\\dagger\n\n[\n\n[\n\n[\n\n[\n\n\n\nAbstract\nPreference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient.\nVision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment.\nHowever, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment.\nIn this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states.\nOur method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty.\nDiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding.\nAcross image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost.\nIn preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.\n\n\n\\checkdata\n[Website]https://github.com/HKUST-C4G/diffusion-rm\n\n‚Ä†‚Ä†‚Ä†Corresponding author.\n\n\n1 Introduction\n\nDiffusion [ddpm] and flow-matching models [liu2022flow] have become the dominant paradigm for high-quality visual generation, achieving remarkable progress in image [sd3, wu2025qwenimage] and video synthesis [sora, wan2025wan].\nAs these models scale, aligning their outputs with human preferences has emerged as a critical challenge. A range of preference alignment algorithms, including Reward Feedback Learning [xu2023imagereward], Direct Preference Optimization [wallace2024diffusion, yang2024using, liu2025improving], and RL-based approaches such as GRPO [liu2025flow, xue2025dancegrpo], have been proposed and shown strong empirical results on large-scale foundation models [wu2025qwenimage, seedream2025seedream4].\nAcross these methods, the reward model is a key bottleneck because it provides the supervision signal that steers optimization.\n\n\nMost existing reward models for visual generation are built on large multimodal encoders, and recent work has increasingly adopted vision-language models (VLMs) as reward backbones [liu2025improving, ma2025hpsv3, wang2025unified].\nCompared to earlier CLIP-based rewards [xu2023imagereward, kirstain2023pick, wu2023human_hpsv2], VLM-based rewards substantially improve accuracy and robustness.\nThis trend is further amplified by backbone scaling, as larger VLMs often provide more robust discriminative ability through large-scale pretraining [wu2025rewarddance].\nAt the same time, using VLM rewards in alignment can be costly, especially when reward evaluation is queried repeatedly during optimization.\nMoreover, VLM rewards typically operate in pixel space, whereas latent diffusion generators are trained and optimized in latent space [ldm], which introduces a latent-to-pixel mismatch that complicates alignment and increases system overhead, especially for reward-gradient methods [xu2023imagereward, prabhudesai2023aligning].\n\n\nThese limitations motivate revisiting reward modeling from a diffusion-native perspective.\nDiffusion models form another family of large-scale pretrained backbones [sd3, wan2025wan], and prior work shows that their generative pretraining yields rich representations that transfer to discriminative objectives, ranging from classification [li2023your, xiang2023denoising] to adversarial discrimination [sauer2024fast, yin2024improved].\nThis raises a natural question: can diffusion backbones be turned into general-purpose reward models that remain competitive in preference discrimination while being more optimization-friendly for alignment?\n\n\nRecent attempts [zhang2025diffusion, mi2025video] have begun to explore diffusion models as noise-aware rewards under specific alignment paradigms, often focusing on preference optimization over noisy intermediate states.\nWhile promising, this line of work is often tied to particular training algorithms and does not directly study diffusion backbones as general-purpose reward models under the same usage scenario as VLM rewards.\nIn contrast, we focus on the reward model itself and study diffusion backbones as reusable reward models under the same usage scenario as VLM rewards, namely, scoring clean samples via latent-space evaluation.\nOur goal is to unlock the intrinsic discriminative capability of large-scale diffusion pretraining through diffusion-native preference modeling.\n\n\nTo this end, we propose DiNa-LRM, a diffusion-native latent reward model.\nDiNa-LRM formulates preference learning directly on noisy diffusion states by extending the Thurstone model with a noise-calibrated comparison uncertainty that scales with the diffusion noise level.\nBuilt on a pretrained latent diffusion backbone, DiNa-LRM predicts timestep-aware rewards in the VAE latent space with a scoring head.\nAt inference time, DiNa-LRM supports noise ensembling that aggregates evidence from multiple timesteps within the reward head, providing a diffusion-native test-time scaling knob for more robust scoring.\nExperiments show that DiNa-LRM substantially improves over diffusion-based reward baselines and narrows the gap to strong VLM-based rewards on benchmark evaluations, while reducing memory overhead and improving optimization dynamics in preference optimization under the same setup.\n\n\nOur contributions are summarized as follows:\n\n\n‚Ä¢\n\nDiffusion-native preference formulation.\nWe extend the Thurstone preference model from clean samples to noisy diffusion states by introducing a noise-calibrated comparison uncertainty that scales with the noise level.\n\n\n\n‚Ä¢\n\nInference-time scaling via noise ensembling.\nWe build a timestep-aware latent reward model on top of a pretrained latent diffusion backbone, and propose inference-time noise ensembling that aggregates multi-timestep features.\n\n\n\n‚Ä¢\n\nEmpirical evaluation of alignment behavior.\nWe demonstrate that diffusion-native rewards substantially improve over diffusion-based baselines and narrow the gap to strong VLM rewards on benchmarks.\n\n\n\n\n\n\n\n2 Related Works\n\nCLIP-based Reward Models\n\nEarly reward modeling [wu2023human_hps, xu2023imagereward, kirstain2023pick, wu2023human_hpsv2, zhang2024learning, liang2024rich] for text-to-image generation often repurposes CLIP-style vision-language pretraining as a proxy for human preference.\nRepresentative methods such as PickScore [kirstain2023pick], HPS-v2 [wu2023human_hpsv2], and ImageReward [xu2023imagereward] fine-tune CLIP [clip] or BLIP [blip] on human preference datasets to predict a single scalar score.\nSubsequent works have extended this paradigm to multi-dimensional assessments [zhang2024learning] and fine-grained feedback signals [liang2024rich].\nWhile computationally efficient, these models typically couple a frozen or fine-tuned encoder with a lightweight MLP head.\nConsequently, their performance is inherently bounded by the representational capacity of the pretrained CLIP models.\n\n\n\nVLM-based Reward Models\n\nWith rapid advances in large vision-language models [gpt4o, qwen2vl, Qwen3-VL], recent reward modeling has increasingly shifted toward stronger VLM backbones that support richer semantic understanding, improved robustness, and better generalization.\nA common practice is to replace the VLM‚Äôs language head with a regression [he2024videoscore, liu2025improving, ma2025hpsv3] or logit-based head [wu2025rewarddance, gong2025onereward, zhang2025mind], and optimize the model for Bradley‚ÄìTerry [bt_model] or MSE objectives.\nEmerging generative reward models further leverage chain-of-thought [wang2025unified_cot, wu2025visualquality], thinking-with-image [wang2025vr], and VLM-as-a-verifier [zhang2025generative] strategies to produce structured reasoning before scoring. Despite their effectiveness, VLM-based reward models typically operate in pixel spa"
  },
  {
    "title": "SCRAPL: Scattering Transform with Random Paths for Machine Learning",
    "url": "https://arxiv.org/abs/2602.11145v1",
    "source": "arxiv",
    "summary": "The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which ",
    "full_text": null
  },
  {
    "title": "GENIUS: Generative Fluid Intelligence Evaluation Suite",
    "url": "https://arxiv.org/abs/2602.11144v1",
    "source": "arxiv",
    "summary": "Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on",
    "full_text": "\n\n\n\n1 Introduction\n\n2 GENIUS\n\n2.1 Benchmark Overview\n2.2 Benchmark Construction\n\n2.3 Evaluation Metric\n\n\n3 Experiment\n\n3.1 Main Results\n3.2 Discussion and Analysis\n\n3.3 Validity of LMM-as-a-Judge\n\n\n4 A Potential Solution\n\n4.1 Experimental Observation\n4.2 Theoretical Analysis\n4.3 Attention Adjustment Mechanism\n\n4.4 Experimental Results\n\n\n5 Conclusion\n\n\nA Benchmark Details\n\nA.1 Data Statistics\nA.2 Evaluation Prompt\n\n\nB Detailed Qualitative Examples and Model Outputs\n\nC Evaluation using Qwen2.5-VL-72B as Judge\n\n\nD Additional Experiments and Analysis\n\nD.1 Ablation on Interleaved Format\nD.2 Discussion on the Composition of Input\n\nE Related Work\n\n\nF Details of Method\n\nF.1 Prompt Template for Keyword Generation\nF.2 Mathematical Formulation of Attention Modulation\n\nG Theorem Part\n\nG.1 Exact Definition of ùíú\\mathcal{A}\nG.2 Proof of Thm.¬†4.1\nG.3 Proof of Thm.¬†4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGENIUS: Generative Fluid Intelligence Evaluation Suite\n\n\nRuichuan An\n\n‚ÄÉ‚ÄÉ\nSihan Yang\n\n‚ÄÉ‚ÄÉ\nZiyu Guo\n\n‚ÄÉ‚ÄÉ\nWei Dai\n\n‚ÄÉ‚ÄÉ\nZijun Shen\n\n‚ÄÉ‚ÄÉ\nHaodong Li\n\n‚ÄÉ‚ÄÉ\nRenrui Zhang\n\n‚ÄÉ‚ÄÉ\nXinyu Wei\n\n‚ÄÉ‚ÄÉ\nGuopeng Li\n\n‚ÄÉ‚ÄÉ\nWenshan Wu\n\n‚ÄÉ‚ÄÉ\nWentao Zhang\n\n\n\nAbstract\nUnified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess Crystallized Intelligence, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks Generative Fluid Intelligence (GFI): the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce GENIUS (GENerative Fluid Intelligence EvalUation Suite).\nWe formalize GFI as a synthesis of three primitives. These include Inducing Implicit Patterns (e.g., inferring personalized visual preferences), Executing Ad-hoc Constraints (e.g., visualizing abstract metaphors), and Adapting to Contextual Knowledge (e.g., simulating counter-intuitive physics).\nCollectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, GENIUS establishes a rigorous standard for GFI, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning.\nOur dataset and code will be released at: https://github.com/arctanxarc/GENIUS.\n\nUnified Model, Fluid Intelligence\n\n\nFigure 1: An overview of GENIUS benchmark. It is hierarchically structured into three dimensions, five tasks, and diverse sub-tasks.\n\n\nTable 1: Comparison of representative benchmarks. * denotes understanding tasks. ‚úî\n\n‚úó\n¬†indicates partial satisfaction (e.g., For Manual Curated/Annotation, it implies a combination of human curation and automatic methods). GENIUS pioneers Fluid Intelligence evaluation, featuring multi-image input, multimodal interleaved context, hybrid metrics, and pure manually curated and annotated testcases.\n\n\n\nBenchmark\n# Samples\n \n\n\nMulti-Image\n\nInput\n \n \n\n\nFluid\n\nIntelligence\n \n \n\n\nMultimodal\n\nInterleaved\n\nContext\n \nTask Dimension\n \n\n\nHybrid\n\nEvaluation\n \n \n\n\nManual\n\nCurated\n\nAnnotation\n \n\n\n\n \n\n\nImplicit Pattern\n\nInduction\n\n\n \n\n\nExplicit Constraint\n\nExecution\n\n\n \n\n\nContextual Knowledge\n\nAdaptation\n\n\n\nGenEval¬†(Ghosh et al., 2023)\n\n2.2k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n\n\n\n\\rowcolor[gray]0.95WISE¬†(Niu et al., 2025)\n\n1.0k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úî\n\n‚úó\n\n\n\n\nRISE¬†(Zhao et al., 2025)\n\n360\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n\n\n\n\\rowcolor[gray]0.95DPG-Bench¬†(Hu et al., 2024)\n\n1.0k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n\n\nDreamBooth¬†(Ruiz et al., 2023)\n\n3.0k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n\n\nUnifyBench¬†(An et al., 2025)\n\n0.1k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úî\n\n‚úó\n\n\n‚úó\n‚úó\n‚úì\n\n\n\n\\rowcolor[gray]0.95Tiif-Bench¬†(Wei et al., 2025)\n\n5.0k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úî\n\n‚úó\n\n\n\n\nOpenING*¬†(Zhou et al., 2025)\n\n5.4k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úî\n\n‚úó\n\n\n\n\nUniEval*¬†(Li et al., 2025e)\n\n4.2k\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n\n\n\n\\rowcolor[gray]0.95MME-Unify*¬†(Xie et al., 2025b)\n\n4.1k\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úî\n\n‚úó\n\n\n\n\nRealUnify*¬†(Shi et al., 2025)\n\n1.0k\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n\n\n\n\\rowcolor[gray]0.95ROVER¬†(Liang et al., 2025)\n\n1.3k\n‚úì\n‚úó\n‚úó\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n\n\nWEAVE¬†(Chow et al., 2025)\n\n0.1k\n‚úì\n‚úó\n‚úî\n\n‚úó\n\n\n‚úó\n‚úó\n‚úó\n‚úì\n‚úì\n\n\n\n\\rowcolor[gray]0.95GENIUS (Ours)\n\n0.5k\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n‚úì\n\n\n\n\n\n\n1 Introduction\n\nUnified Multimodal Models (UMMs) have witnessed remarkable progress recently¬†(Team, 2024; Chen et al., 2025; Xie et al., 2024), delivering impressive results across diverse tasks¬†(An et al., 2025; Li et al., 2025a; Jiang* et al., 2025). Benefiting from the fusion of understanding, UMMs are capable of processing complex, interleaved contexts and exhibiting extensive world knowledge to reshape the generative paradigm. Consequently, they are widely regarded as a milestone on the path toward Artificial General Intelligence (AGI). However, this rapid advancement invites a natural question: How far are current UMMs from achieving true general intelligence regrading visual generation?\n\n\nTo investigate this problem, drawing upon existing literature¬†(Cattell, 1963; Schipolowski et al., 2014; Kent, 2017), we deconstruct General Intelligence in visual generation into two primary components: Crystallized Intelligence (CI) and Fluid Intelligence (FI). Current development and evaluation focus of UMMs mainly targets CI (i.e., the capacity for memorization and retrieval of pre-trained knowledge). For instance, a model‚Äôs ability to generate a flawless ‚Äúcat‚Äù often stems from exposure to billions of instances during training, followed by probabilistic reproduction during inference. However, this trend has severely masked a critical but long-ignoring deficiency concerning FI of visual generation skills, termed Generative Fluid Intelligence (GFI), (i.e., the ability to perform inducing, reasoning and ad-hoc adaptation in novel scenarios). As shown in Fig.¬†1, the ‚ÄúSimple Constraint‚Äù task requires the model to identify ad-hoc rules (e.g., abstract symbol denotes ‚Äúrain‚Äù) and apply them to the visual output, instead of just retrieving static concepts.\n\n\nDespite its critical importance, research along this direction remains limited (shown in Tab.¬†1):\n\n\n‚Ä¢\n\nFirst, a formal definition is absent. This theoretical void impedes the foundational guidance, which is necessary for steering UMMs toward general intelligence.\n\n\n\n‚Ä¢\n\nSecond, benchmarks are inadequate. Current evaluations predominantly assess model memorization and retrieval, failing to disentangle static knowledge to probe the true bounds of general intelligence.\n\n\n\n‚Ä¢\n\nThird, systematic analyses are lacking. The lack of investigations into the failure modes leaves critical questions of why models fail and how to improve unanswered.\n\n\n\n\n\nTo bridge these gaps, we introduce GENIUS (GENerative Fluid Intelligence EvalUation Suite), the first framework dedicated to the systematic evaluation of GFI. Drawing from the Cattell-Horn-Carroll (CHC) theory¬†(Schneider and McGrew, 2012), we distill three core primitives of FI: (I) Inductive Inference, (II) Abstract Dynamic Reasoning and (III) Adaptive Inhibition. We materialize these theoretical concepts into three corresponding dimensions within GENIUS. To ensure a fine-grained assessment, we further construct five novel and well-designed tasks that specify concrete capabilities within each dimension. For each task, we employ a hybrid evaluation comprising three metrics: (I) Rule Compliance, which challenges the model‚Äôs precision in following ad-hoc rules; (II) Visual Consistency, which assesses the stability of generated attributes under logical constraints; and (III) Aesthetic Quality, which demands that the model maintains fundamental aesthetic standards. Through manual curation, our suite features tasks well designed by multi-modal experts. Unlike traditional benchmarks that prioritize static world knowledge, generation quality or safety,\nwe ensure that every sample presents a dynamic and novel rule, strictly decoupling static knowledge to offer a pure quantification of the model‚Äôs GFI capabilities.\n\n\nWith GENIUS, we systematically evaluate 12 representative open-source\nand proprietary models. To ensure evaluation robustness, we provide manually annotated hints for each test case, which have undergone at least three rounds of cross-validation to support unbiased hybrid evaluation.\nOverall, our results reveal clear gaps between current state-of-the-art (SOTA) models and general intelligence. Surprisingly, pre-planning and post-reflection yield marginal gains.\nThese findings expose under-explored deficiencies in current generative models, highlighting the urgent need to advance fluid intelligence in the next generation of UMMs.\n\n\nBuilding on these findings, we move beyond evaluation to investigate the underlying mechanisms of failure. Taking Bagel¬†(Deng et al., 2025) as an example, by visualizing the attention distribution of it, it surprisingly shows irregular noise and spikes across the multimodal context. This indicates that the model struggles to accurately focus on critical new rules in the context. Inspired by the theoretical perspective of In-Context Learning (ICL) as Implicit Fine-Tuning¬†(Dherin et al., 2025), we demonstrate that ICL process is mathematically equivalent to update specific model parameters while generation. Then, we offer a possible view: the imbalanced attention distribution results in insufficient guidance during implicit gradient descent, which causes the gradient direction vague or stochastic, failing to overcome the inertia of pre-trained priors. Based on this, we design a training-free mechanism as a strong baseline. The results show consistent performance gains across all tasks, which not only validates the effectiveness of our method but also corroborates the rationality of our theoretical framework.\n\n\nIn summary, our core contributions are as follows:\n\n\n‚Ä¢\n\nWe formally define Generative Fluid Intell"
  },
  {
    "title": "Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows",
    "url": "https://arxiv.org/abs/2602.11142v1",
    "source": "arxiv",
    "summary": "Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-",
    "full_text": null
  },
  {
    "title": "LCIP: Loss-Controlled Inverse Projection of High-Dimensional Image Data",
    "url": "https://arxiv.org/abs/2602.11141v1",
    "source": "arxiv",
    "summary": "Projections (or dimensionality reduction) methods $P$ aim to map high-dimensional data to typically 2D scatterplots for visual exploration. Inverse projection methods $P^{-1}$ aim to map this 2D space to the data space to support tasks such as data augmentation, classifier analysis, and data imputation. Current $P^{-1}$ methods suffer from a fundamental limitation -- they can only generate a fixed",
    "full_text": null
  },
  {
    "title": "TabICLv2: A better, faster, scalable, and open tabular foundation model",
    "url": "https://arxiv.org/abs/2602.11139v1",
    "source": "arxiv",
    "summary": "Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions\n\n\n\n2 Related Work\n\n\n2.1 Tabular foundation models\n\nArchitectural perspectives.\nSynthetic prior datasets.\nFine-tuning, retrieval, and distillation.\nLLM-based tabular models.\n\n\n\n2.2 Attention struggles with long-context generalization\n\nAttention fading.\nTemperature scaling.\n\n\n\n\n3 Architecture\n\n4 Pretraining and Inference\n\n\n4.1 Pretraining setup\n\nThree pretraining stages.\nOptimizer.\nPretraining cost.\n\n\n4.2 Inference optimizations\n\n\n\n5 Synthetic data prior\n\nHigh-level structure.\nNew sampling mechanisms\nPostprocessing.\nData filtering.\nSampling correlated scalars.\n\n\n\n6 Experiments\n\nBenchmarks.\nTabICLv2 is state-of-the-art on both benchmarks.\nTabICLv2 is consistently faster than TabPFN-2.5.\nTabICLv2 excels on many-class classification.\nTabICLv2 scales to large datasets.\n\n\n7 Ablation study\n8 Limitations\n9 Conclusion\n\nA More architecture details about TabICLv2\n\nA.1 Repeated feature grouping\nA.2 Compression then ICL\n\nA.3 Many-class classification\n\nComputing balanced bases.\nMixed-radix label encoding.\nEnsemble aggregation.\nRelationship to error-correcting output codes.\nCombined with hierarchical classification.\n\n\n\nA.4 Model configuration\n\nClassification model.\nRegression model.\n\n\n\n\n\nB More pretraining details about TabICLv2\n\nB.1 Three pretraining stages\nB.2 Speed and memory optimization\n\n\n\nC Additional ablation results\n\nIncreasing model depth.\nAdding noise to the prior.\n\n\n\nD Other things we tried\n\nPretraining.\nArchitecture: embeddings.\nArchitecture: row interaction.\nArchitecture: normalizations.\nArchitecture: other.\nOther things.\n\n\n\nE Details on the prior\n\nE.1 Differences to previous priors\n\nE.2 Sampling correlated scalars\n\nNumerical values:\nCategorical values:\n\n\nE.3 Random dataset\nE.4 Random graph\nE.5 Random node function\n\nE.6 Random converter\n\nNumerical converters:\nCategorical converters:\n\n\nE.7 Random multi-function\n\nE.8 Random functions\n\nRandomNNFunction\nRandomTreeFunction\nRandomDiscretizationFunction\nRandomGPFunction\nRandomLinearFunction\nRandomQuadraticFunction\nRandomEMAssignmentFunction\nRandomProductFunction\n\n\n\nE.9 Random activations\n\nFixed activations.\nParametric activations.\n\n\n\nE.10 Random matrix\n\nPostprocessing.\n\n\nE.11 Random weights\nE.12 Random points\nE.13 Postprocessing\nE.14 Filtering\n\n\nF Plots for the prior\nG Path smoothness for Gaussian processes\n\nH Inference optimization for TabICLv2\n\n\nH.1 Efficient attention computation via selective query-key-value projections\n\nRow-wise inter-feature interaction.\nDataset-wise in-context learning.\nLayer normalization reuse.\n\n\n\nH.2 Offloading technique\n\nBatch size estimation.\nMemory bottleneck analysis.\nDisk offloading via memory-mapped files.\nAsynchronous data transfer.\nAutomatic mode selection.\n\n\n\n\n\nI Quantile distribution\n\nI.1 Problem setup\nI.2 Quantile function\n\nI.3 Quantile crossing correction\n\n(1) No correction.\n(2) Sorting.\n(3) Isotonic regression.\n\n\nI.4 Tail parameter estimation\nI.5 Cumulative distribution function (CDF)\nI.6 Probability density function (PDF)\n\nI.7 Continuous ranked probability score (CRPS)\n\nI.7.1 CRPS contribution from spline region\nI.7.2 CRPS contribution from exponential left tail\nI.7.3 CRPS contribution from exponential right tail\n\n\n\nI.8 Moment calculations\n\nI.8.1 Mean\nI.8.2 Variance\n\n\n\nI.9 Empirical validation on synthetic regression tasks\n\n\nI.9.1 Synthetic regression datasets\n\nDataset 1: Quadratic with homoscedastic gaussian noise.\nDataset 2: Sinusoidal with heteroscedastic noise.\nDataset 3: Step function with noise.\nDataset 4: Linear with heavy-tailed noise.\n\n\nI.9.2 Visualization and analysis\n\n\n\n\n\nJ Detailed results on the TabArena benchmark\n\n\nJ.1 Aggregation metrics\n\nPer-dataset error metrics.\nElo rating.\nImprovability.\nAverage rank.\nDiscussion.\n\n\n\nJ.2 Results on all datasets\n\nRanking and Elo.\nPairwise win rates.\nPareto efficiency.\n\n\nJ.3 Results on binary classification datasets\nJ.4 Results on multiclass classification datasets\nJ.5 Results on regression datasets\n\n\n\nK Detailed results on the TALENT benchmark\n\n\nK.1 Benchmark overview\n\nEvaluation metrics.\n\n\nK.2 Results on all datasets\nK.3 Results on binary classification datasets\nK.4 Results on multiclass classification datasets (‚â§\\leq 10 classes)\nK.5 Results on multiclass classification datasets (&gt;&gt; 10 classes)\nK.6 Results on regression datasets\nK.7 Results on small datasets with less than 10K samples\nK.8 Results on large datasets with more than 10K samples\nK.9 Model rankings with respect to meta-features\n\n\n\n\n\n\n\nTabICLv2: A better, faster, scalable, and open tabular foundation model\n\n\nJingang Qu\n\n‚ÄÉ‚ÄÉ\nDavid Holzm√ºller\n\n‚ÄÉ‚ÄÉ\nGa√´l Varoquaux\n\n‚ÄÉ‚ÄÉ\nMarine Le Morvan\n\n\n\nAbstract\nTabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nFigure 1: Improvability vs. train time on TabArena (Erickson et¬†al., 2025). Improvability (lower is better) measures the relative error gap to the best method, averaged across datasets. Train time is training + inference in 8-fold cross-validation. For foundation models, it is dominated by forward passes that perform in-context learning. Default uses default hyperparameters; Tuned selects the best of 200 random hyperparameter configurations on validation; Tuned + Ens. applies post-hoc weighted ensemble of all configurations. The runtime of TabICLv2 is measured on an H100 GPU, while others are from TabArena. Results for inapplicable model-dataset pairs are imputed with default RandomForest.\n\n\n\nTabular data, whether stored in spreadsheets or databases, is ubiquitous across applications ranging from healthcare to credit card fraud detection (Borisov et¬†al., 2022; Jesus et¬†al., 2022; Grinsztajn et¬†al., 2025). While supervised learning on tabular data has long been dominated by gradient-boosted decision trees (Grinsztajn et¬†al., 2022), both pretrained and trained-from-scratch deep learning models have recently been able to match or even surpass their accuracy on tables with up to 100K samples (Erickson et¬†al., 2025; Ye et¬†al., 2024). In particular, starting from TabPFN (Hollmann et¬†al., 2022), tabular foundation models (TFMs) have received a lot of attention thanks to their ability to perform training and inference in a single forward pass of a Transformer-based architecture. The development of better TFMs also benefits downstream adaptations, such as causal inference, generative modeling, joint predictive distributions, and simulation-based inference (Ma et¬†al., 2025b; Robertson et¬†al., 2025; Balazadeh et¬†al., 2025; Hollmann et¬†al., 2025; Hassan et¬†al., 2025; Vetter et¬†al., 2025). To foster this research, there is a pressing need for fully open-source TFMs that rival closed-source ones to democratize access to top-tier performance and demystify the recipe behind top-performing TFMs.\n\n\nContributions\n\nWe introduce TabICLv2, a state-of-the-art tabular foundation model, as shown in Figure¬†1. Our contributions include architectural innovations (Section¬†3), pretraining improvements (Section¬†4), a novel synthetic data generator (Section¬†5), extensive evaluations (Section¬†6), and an ablation study (Section¬†7).\n\n\n\n\n\n2 Related Work\n\n\n2.1 Tabular foundation models\n\nTabular foundation models (TFMs) based on Prior-data fitted networks (PFNs, M√ºller et¬†al. 2021) emerged as a paradigm shift in tabular learning. Given a training set and test input, a TFM qŒ∏q_{\\theta} with parameters Œ∏\\theta directly predicts a distribution qŒ∏‚Äã(ytest‚à£xtest,ùíütrain)q_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},\\mathcal{D}_{\\mathrm{train}}) with a forward pass on input (xtest,ùíütrain)(x_{\\mathrm{test}},\\mathcal{D}_{\\mathrm{train}}). For a single dataset, it hence performs in-context learning (ICL) without gradient updates. For pretraining, given a prior p‚Äã(ùíü)p(\\mathcal{D}) from which datasets ùíü\\mathcal{D} (train+test) can be sampled, TFMs are trained to minimize\n\n\n\n‚Ñí‚Äã(Œ∏)=ùîºùíü‚àºp‚Äã(‚ãÖ)‚Äã[‚àílog‚Å°qŒ∏‚Äã(ytest‚à£xtest,ùíütrain)].\\mathcal{L}(\\theta)=\\mathbb{E}_{\\mathcal{D}\\sim p(\\cdot)}\\big[-\\log q_{\\theta}(y_{\\mathrm{test}}\\mid x_{\\mathrm{test}},\\mathcal{D}_{\\mathrm{train}})\\big]~.\n\n\n\n\n\nArchitectural perspectives.\n\nTabPFN¬†(Hollmann et¬†al., 2022) treats each row as a token and performs ICL over rows. TabPFNv2¬†(Hollmann et¬†al., 2025) moves to a cell-based design with alternating row and column attentions, where each cell receives a separate representation. However, this incurs O‚Äã(n2‚Äãm+n‚Äãm2)O(n^{2}m+nm^{2}) complexity for a table with nn rows and mm columns. TabPFN-2.5 (Grinsztajn et¬†al., 2025) extends TabPFNv2 with deeper networks. TabICL¬†(Qu et¬†al., 2025) reduces the computational complexity to O‚Äã(n2+n‚Äãm2)O(n^{2}+nm^{2}) via a two-stage design: a lightweight column-then-row attention first constructs fixed-dimensional row embeddings, after which ICL is performed over these embeddings. Recent work continues to innovate on these "
  },
  {
    "title": "Weight Decay Improves Language Model Plasticity",
    "url": "https://arxiv.org/abs/2602.11137v1",
    "source": "arxiv",
    "summary": "The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model's validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of",
    "full_text": null
  },
  {
    "title": "FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight",
    "url": "https://arxiv.org/abs/2602.11136v1",
    "source": "arxiv",
    "summary": "As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escap",
    "full_text": "\n\n\n\n1 Introduction\n2 Background and Related Work\n\n3 Method: FormalJudge\n\n3.1 Problem Formulation\n\n3.2 Bidirectional Formal-of-Thought Pipeline\n\n3.2.1 Phase 1: Top-Down Intent Decomposition\n3.2.2 Phase 2: Context-Aware Grounded Extraction\n3.2.3 Phase 3: Formal Proof Synthesis\n3.2.4 Phase 4: SMT Verification\n\n\n3.3 Design Properties\n\n\n\n4 Experiment\n\n4.1 Experimental Setup\n4.2 Main Results\n4.3 Robustness to Attack Sophistication\n4.4 Weak-to-Strong Detection in Deception\n4.5 Iterative Refinement with Formal Feedback\n\n\n5 Conclusion &amp; Limitation\n\nA VitaBench Experimental Setup\n\nA.1 Experiment 1: Multi-Agent Trajectory Verification\nA.2 Experiment 2: Adversarial Robustness Testing\nA.3 Experiment 3: Cross-Verifier Generalization\n\n\n\nB Deceivers Benchmark: Experimental Details\n\nB.1 Benchmark Overview and Task Design\nB.2 Agent Execution Environment\nB.3 Atomic Fact Taxonomy for Deception Detection\nB.4 Verification Prompts\nB.5 Baseline Verification Methods\nB.6 Formal Composition Logic\nB.7 Hyperparameters and Generation Settings\nB.8 Case Study: Type 1 Deception Detection\nB.9 Case Study: Type 2b Deception Detection\nB.10 Evaluation Metrics and Results Summary\n\n\n\nC Agent-SafetyBench: Experimental Details\n\nC.1 Benchmark Overview\nC.2 Sample Data Format\nC.3 Atomic Fact Taxonomy\n\nC.4 Iterative Refinement Pipeline\n\nC.4.1 Pipeline Architecture\nC.4.2 Refinement Feedback Format\nC.4.3 Configuration\nC.4.4 Target Models\n\n\nC.5 Case Study 1: Phishing Detection\nC.6 Case Study 2: Sensitive Data Leak Prevention\nC.7 Case Study 3: Hidden Offensive Content Detection\nC.8 Hyperparameters and Generation Settings\nC.9 Formal Composition Logic\nC.10 Evaluation Protocol\nC.11 Iterative Refinement Results Summary\n\n\n\n\n\n\n\n\nFormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight\n\n\nJiayi Zhou\n\n‚ÄÉ‚ÄÉ\nYang Sheng\n\n‚ÄÉ‚ÄÉ\nHantao Lou\n\n‚ÄÉ‚ÄÉ\nYaodong Yang\n\n‚ÄÉ‚ÄÉ\nJie Fu\n\n\n\nAbstract\nAs LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing FormalJudge, a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. Experiments on 7 agent models demonstrate that FormalJudge achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.\n\nAgent Safety, Formal Verification, LLM Oversight, Neuro-Symbolic AI\n\n\n‚àó Equal contribution. Correspondence to: Jiayi Zhou gaiejj[at]stu.pku.edu.cn.\n\n\n\n\n‚ÄúWho will watch the watchmen?‚Äù\n\n\n‚Äî Juvenal saying, 1st‚Äì2nd century ‚Äì Poem\n\n\n\n\n\n1 Introduction\n\nFigure 1: Teaser: LLM agents face scalable oversight challenges: it is difficult to identify a reliable oversight agent. While the LLM-as-a-Judge baseline relies on probabilistic Chain-of-Thought reasoning, we introduces a Formal-of-Thought architecture that leverages LLMs as specification compilers. It decomposes agent trajectories into atomic facts verified by SMT solvers to provide mathematical proofs of correctness rather than subjective scores. By separating neural semantic extraction from deterministic logical verification, the system ensures oversight remains immune to persuasive manipulation.\n\n\nFigure 2: The neuro-symbolic architecture and verification pipeline of FormalJudge. Panel (a) outlines the oversight workflow where an LLM compiles user intent into Dafny specifications and extracts atomic facts, enabling a Z3 SMT solver to provide deterministic proofs of correctness independent of neural reasoning. Panel (b) details the bidirectional Formal-of-Thought process, illustrating how high-level instructions are decomposed into grounded atomic queries and subsequently translated into logical predicates to mathematically verify agent trajectories against specific constraints.\n\n\nThe deployment of Large Language Models (LLMs) (Achiam et al., 2023; Claude, 2024; Gemini, 2024) has transitioned rapidly from passive information retrieval to active agentic intervention¬†(Wang et al., 2024). No longer confined to text generation, modern AI agents now possess the autonomy to manipulate graphical user interfaces, execute financial transactions, manage complex logistics, and modify codebases¬†(Yao et al., 2022; Shinn et al., 2023). This explosive expansion in both the scope and stakes of agent-mediated tasks has fundamentally transformed how AI intervenes in human life (Ye et al., 2025). Yet this capability surge has been driven primarily by scaling training compute (Mai et al., 2025) and enriching tool interfaces (Li et al., 2025), rather than by deeper scientific understanding of agent behavior.\n\n\nThe mechanisms for overseeing these increasingly capable agents have remained dangerously static. The prevailing supervision paradigm, LLM-as-a-Judge¬†(Li et al., 2024a), relies on the probabilistic intuition of strong models to assess weaker ones. This approach instantiates the scalable oversight problem¬†(Anwar et al., 2024; Bommasani et al., 2021; Ji et al., 2023a; Szegedy, 2020): as agent capabilities approach or exceed human-level competence, how can we ensure reliable supervision? LLM-as-a-Judge attempts to address scalability by automating evaluation, yet it introduces a fundamental epistemological flaw: using probabilistic systems to supervise probabilistic systems rather than resolving unreliability. The resulting ‚Äúhallucination echo chambers‚Äù suffer from systematic biases¬†(Ji et al., 2023b; Yao et al., 2023; Schick et al., 2021), fail to enforce hard constraints, and remain vulnerable to persuasive manipulation by deceptive agents¬†(Chen et al., 2025; Weidinger et al., 2022).\nhttps://www.overleaf.com/project\nMathematics offers arguably the most inviolable constraint in our world: even the most sophisticated AI systems cannot circumvent the correctness guarantees of formal proofs (Cheng et al., 2024; Zhang et al., 2024a; Ospanov et al., 2025). This observation motivates formal verification as a principled escape from probabilistic oversight (Dalrymple et al., 2024): SMT solvers provide mathematical guarantees independent of the system being verified, breaking the circular dependency inherent in neural judges. However, a fundamental barrier has prevented formal methods from being applied to agent oversight: the translation from natural language requirements to formal specifications is, in the general case, an undecidable problem. This specification synthesis bottleneck has traditionally demanded prohibitive expertise in formal languages and verification theory, rendering formal oversight impractical for real-world agent supervision¬†(Ma et al., 2024).\n\n\nCan we bridge the gap between natural language intent and formal verification for agent oversight?\n\n\nTo answer this question, we propose a pragmatic compromise: Formal-Agent-as-a-Judge, a neuro-symbolic paradigm that achieves bidirectional supervision through hierarchical decomposition and compositional verification. Our key insight is that while general-purpose translation from natural to formal language remains intractable, the structured nature of agent oversight admits a tractable decomposition. Specifically, we employ LLMs as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance step-by-step, with each proof step validated using the formal language Dafny¬†(Drechsler, 2004) and SMT solving via Z3¬†(De Moura and Bj√∏rner, 2008). This architecture confines probabilistic reasoning to atomic semantic extraction while delegating all logical composition to deterministic solvers, enabling oversight that is not a matter of opinion, but of proof. Our main contributions are:\n\n\n‚Ä¢\n\nFramework &amp; Performance Improvement. We propose FormalJudge, a neuro-symbolic framework for agentic oversight. Across 3 benchmarks covering behavioral safety, multi-domain constraint adherence, and deception detection, evaluated on 7 agent models (including both open- and closed-source), FormalJudge achieves an average improvement of 16.6% over LLM-as-a-Judge baselines (Section¬†4.2).\n\n\n\n‚Ä¢\n\nAdvanced Features: Detection. FormalJudge enables weak-to-strong deception detection: a 7B judge model achieves over 90% accuracy in detecting deceptive behaviors from 72B agents (Section¬†4.4), and maintains stable performance across varying attack sophistication levels (Section¬†4.3)\n\n\n\n‚Ä¢\n\nAdvanced Features: Refinement. We discover that formal verification feedback enables near-linear safety improvement through iterative refinement (Section¬†4.5): agents guided by Dafny specifications improve from 70.7% to 99.8% accuracy over three rounds, while all LLM-as-a-Judge baselines show no improvement or degradation.\n\n\n\n‚Ä¢\n\nOpen-source Resources. We have released our code, benchmark implementations, and raw experimental logs in https://github.com/htlou/FormalJudge to facilitate reproducibility and future research.\n\n\n\n\n\n\n\n2 Background and Related Work\n\nThe Expanding Attack Surface of LLM Agents.\nAs LLM agents transition from passive assistants to autonomous actors¬†(Wang et al., 2024; Yao et al., 2022; Shinn et al., 2023), their failure modes have grown increasingly consequentia"
  },
  {
    "title": "Just on Time: Token-Level Early Stopping for Diffusion Language Models",
    "url": "https://arxiv.org/abs/2602.11133v1",
    "source": "arxiv",
    "summary": "Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model's predictions ",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n\n2 Related Work\n\n2.1 Diffusion Language Models\n2.2 Masked Diffusion Language Models\n2.3 Early Stopping for Diffusion Decoding\n\n\n\n3 Preliminaries\n\n\n3.1 Discrete Diffusion Language Models\n\nForward process.\nReverse process.\n\n\n3.2 Iterative Sampling\n3.3 Training Objective\n\n\n\n4 Approach\n\n4.1 Overview\n4.2 Confidence Metric\n4.3 Spatial Modulation\n4.4 Adaptive Threshold\n4.5 Early-Exit Decision\n\n4.6 Discussion\n\nComputational overhead.\nInteraction with transfer schedules.\nOrthogonality with other acceleration methods.\n\n\n\n\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n5.3 Threshold Ablation\n5.4 Spatial Modulation Ablation\n5.5 Hyperparameter Sweep\n5.6 LLaDA Threshold Ablation\n5.7 Confidence Dynamics\n\n\n6 Limitations\n7 Conclusion\n\nA Additional Experiments\n\nA.1 Wallclock Time Analysis\nA.2 Effect of Minimum Threshold\nA.3 Generation Quality Metrics\n\n\n\n\n\n\n\nJust on Time: Token-Level Early Stopping for Diffusion Language Models\n\n\nZahar Kohut1,2\nEqual contribution.\n‚ÄÉ‚ÄÉ\nSeveryn Shykula1,211footnotemark: 1\n\n‚ÄÉ‚ÄÉ\nDmytro Khamula1,2\n\n‚ÄÉ‚ÄÉ\nMykola Vysotskyi1,2\n\n‚ÄÉ‚ÄÉ\nTaras Rumezhak1\n\n‚ÄÉ‚ÄÉ\nVolodymyr Karpiv1\n\n\n\nAbstract\nDiffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model‚Äôs predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.\n\n\n1SoftServe Inc.\n2Ukrainian Catholic University\n\n\n\n1 Introduction\n\nAutoregressive (AR) models dominate the current landscape of large language models, generating text through sequential left-to-right token prediction. While remarkably effective, this paradigm imposes inherent constraints: each token must wait for all preceding tokens, limiting parallelism and preventing the model from leveraging future context during generation.\n\n\nDiffusion language models (DLMs) have emerged as a compelling alternative [2]. Rather than generating tokens sequentially, DLMs begin with a fully masked sequence and progressively refine it through iterative denoising, enabling parallel token prediction and bidirectional context integration. Recent work has demonstrated that DLMs can achieve competitive performance with strong autoregressive models on tasks ranging from in-context learning to instruction following [15, 24].\n\n\nDespite this promise, decoding efficiency remains a challenge for DLMs. Generation proceeds through a reverse-diffusion chain requiring many refinement steps, and practitioners must choose step budgets conservatively to avoid quality degradation. This leads to unnecessary computation: many tokens converge to stable predictions well before the final denoising step [11], yet standard decoding continues refining them regardless.\n\n\nWe introduce Jot (Just on Time), a training-free method for per-token early stopping in diffusion language models. Rather than applying a global stopping criterion, Jot monitors prediction confidence at each position independently and finalizes tokens as soon as they exceed a threshold. This allows different positions to exit at different steps, concentrating computation on tokens that genuinely require further refinement.\n\n\nWe evaluate Jot on Dream-7B-Instruct and LLaDA-8B-Instruct across four benchmarks: GSM8K, MMLU, HellaSwag, and HumanEval. Our experiments demonstrate that Jot achieves favorable speed-quality trade-offs, providing up to 5.5√ó5.5\\times speedup on GSM8K and 19.6√ó19.6\\times on HumanEval while maintaining score within 3 per cent of full decoding. Ablation studies confirm that both threshold selection and spatial modulation contribute to performance.\n\n\nContributions.\n\nWe summarize our main contributions as follows:\n\n\n‚Ä¢\n\nWe propose Jot, a training-free, per-token early stopping method for diffusion language models that adapts acceptance thresholds based on spatial proximity to resolved context.\n\n\n\n‚Ä¢\n\nWe conduct comprehensive experiments on Dream-7B and LLaDA-8B across four benchmarks, analyzing speed-quality trade-offs against existing early-exit methods.\n\n\n\n‚Ä¢\n\nWe provide detailed ablation studies isolating the effects of threshold selection and spatial modulation, offering practical guidance for hyperparameter configuration.\n\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 Diffusion Language Models\n\nDiffusion models have achieved remarkable success in continuous domains such as image and audio generation [9, 20]. Extending this paradigm to discrete data like text presents unique challenges, as the standard Gaussian noise process does not directly apply to categorical variables. Two main approaches have emerged: embedding-based methods that project tokens into continuous space before applying standard diffusion [12, 7], and discrete diffusion methods that define corruption processes directly over the vocabulary.\n\n\nD3PM [2] established a foundational framework for discrete diffusion by modeling the forward process as a discrete-state Markov chain with learnable transition matrices. This work demonstrated that diffusion principles could be adapted to categorical data while preserving the variational lower bound interpretation. Subsequent work on score entropy discrete diffusion (SEDD) [13] connected score-based perspectives with likelihood-based training for discrete sequences.\n\n\n\n\n2.2 Masked Diffusion Language Models\n\nAmong discrete diffusion formulations, masked (absorbing-state) diffusion has emerged as particularly effective for language modeling. In this setup, the forward process progressively replaces tokens with a special mask token, and the reverse process learns to predict the original tokens from partially masked sequences.\n\n\nMDLM [19] formalized this paradigm with a substitution-based reverse parameterization (SUBS) and derived a simplified, Rao-Blackwellized objective that reduces to a weighted mixture of classical masked language modeling losses. This connection to BERT-style training [5] enables leveraging established architectural choices while supporting principled generation via ancestral sampling.\n\n\nLLaDA [15] scaled masked diffusion to 8B parameters, demonstrating that diffusion-based LMs can achieve competitive performance with autoregressive baselines across diverse tasks including in-context learning and instruction following. The model employs a standard pre-training and supervised fine-tuning pipeline, establishing that the core capabilities of large language models are not exclusive to autoregressive architectures.\n\n\nDream 7B [24] further advances the state of the art with practical training techniques including autoregressive model initialization and context-adaptive token-level noise rescheduling. The model demonstrates diffusion-specific advantages such as flexible generation order, infilling capabilities, and tunable quality-speed trade-offs.\n\n\n\n\n2.3 Early Stopping for Diffusion Decoding\n\nRecent work has observed that predictions often stabilize well before the final diffusion step, motivating early-exit strategies. Prophet [11] documents this ‚Äúearly answer convergence‚Äù phenomenon: on benchmarks such as GSM8K and MMLU, a large fraction of instances can be correctly decoded using only half of the prescribed steps. Prophet proposes a training-free early-commit rule that monitors the confidence gap between the top-2 predicted tokens and finalizes all remaining masked positions once this gap exceeds a threshold, achieving significant speedups with minimal quality degradation.\n\n\nComplementary approaches focus on token-level stability signals. KLASS [10] introduces KL-Adaptive Stability Sampling, which uses the KL divergence between consecutive step distributions to identify stable predictions and unmask multiple tokens in parallel. SlowFast Sampling [22] alternates between exploratory and accelerated phases based on certainty and convergence criteria.\n\n\nOrthogonal to reducing step counts, other efforts target per-step latency through caching mechanisms adapted to bidirectional attention [14, 21, 23].\n\n\n\n\n\n3 Preliminaries\n\nWe introduce notation and background on discrete diffusion language models (DLMs).\n\n\n\n3.1 Discrete Diffusion Language Models\n\nConsider a vocabulary ùí±\\mathcal{V} of size KK augmented with a mask token [M]. A sequence of length LL is denoted ùê±=(x1,‚Ä¶,xL)\\mathbf{x}=(x^{1},\\ldots,x^{L}) with xi‚ààùí±x^{i}\\in\\mathcal{V}.\n\n\nAutoregressive language models factorize the joint distribution as p‚Äã(ùê±)=‚àèi=1Lp‚Äã(xi‚à£x&lt;i)p(\\mathbf{x})=\\prod_{i=1}^{L}p(x^{i}\\mid x^{&lt;i}) and generate tokens sequentially from left to right. In contrast, discrete diffusion models define a generative process through the interplay of a forward corruption process and a learned reverse denoising process, enabling parallel refinement of all positions simultaneously.\n\n\nForward process.\n\nThe forward process progressively corrupts a clean sequence ùê±0\\mathbf{x}_{0} by independently replacing tokens with the mask token. Let t‚àà[0,1]t\\in[0,1] denote the continuous noise level, where t=0t=0 corresponds to clean data and t=1t=1 to the fully masked state. The forward marginal factorizes over positions as q‚Äã(ùê±t‚à£ùê±0)=‚àèi=1Lq‚Äã(xti‚à£x0i)q(\\mathbf{x}_{t}\\mid\\mathbf{x}_{0})=\\prod_{i=1}^{L}q(x_{t}^{i}\\mid x_{0}^{i}), where\n\n\n\nq‚Äã(xti‚à£x0i)=Œ±t‚ÄãŒ¥‚Äã(xti=x0i)+(1‚àíŒ±t)‚ÄãŒ¥‚Äã(xti=[M]).q(x_{t}^{i}\\mid x_{0}^{i})=\\alpha_{t}\\,\\delta(x_{t}^{i}=x_{0}^{i})+(1-\\alpha_{t})\\,\\delta(x_{t}^{i}=\\texttt{[M]}).\n\n(1)\n\n\nHere, Œ±t‚àà[0,1]\\alpha_{t}\\in[0,1] is a monotonically decreasing noise schedul"
  },
  {
    "title": "From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers",
    "url": "https://arxiv.org/abs/2602.11130v1",
    "source": "arxiv",
    "summary": "Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomeno",
    "full_text": null
  },
  {
    "title": "Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards",
    "url": "https://arxiv.org/abs/2602.11128v1",
    "source": "arxiv",
    "summary": "Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with interm",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.11128v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2602.11128v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 11 Feb 2026]\n    Title:Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards\n    Authors:Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis            View a PDF of the paper titled Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards, by Reinhard Heckel and 2 other authors\n    View PDF\n\n\n\n    \n            Abstract:Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2602.11128 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.11128v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.11128\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Reinhard Heckel [view email]          [v1]\n        Wed, 11 Feb 2026 18:39:42 UTC (469 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards, by Reinhard Heckel and 2 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Disable MathJax (What is MathJax?)\n    \n "
  },
  {
    "title": "The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization",
    "url": "https://arxiv.org/abs/2602.11126v1",
    "source": "arxiv",
    "summary": "Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other me",
    "full_text": null
  },
  {
    "title": "From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent",
    "url": "https://arxiv.org/abs/2602.11123v1",
    "source": "arxiv",
    "summary": "Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into execu",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Workflow\n\n2.1 Stage I: Query and Knowledge Grounding\n2.2 Stage II: Data and Model Construction\n2.3 Stage III: Material Modification and Validation\n\n\n\n3 Methods\n\n3.1 LLM-Orchestrated Reasoning Framework\n3.2 Query Parsing and Literature Grounding\n3.3 Autonomous Property Retrieval via LLM-Generated Code\n3.4 Structure Modification via Substitution and Perturbation\n3.5 Property Prediction via CGCNN\n3.6 Stability Validation via M3GNet\n\n\n\n4 Case Study: Discovery of High‚ÄìDebye‚ÄìTemperature Materials\n\n4.1 Literature-Grounded Interpretation of ‚ÄúHigh‚Äù ŒòD\\Theta_{D}\n4.2 Autonomous Property Retrieval via Code Generation\n4.3 Property Prediction via CGCNN\n4.4 Structure Modification and Stability Filtering\n\n\n5 Discussion\n6 Conclusion\n\n\n\n\n\nFrom Natural Language to Materials Discovery: The Materials Knowledge Navigation Agent\n\n\n\nGenmao Zhuang\n\nDepartment of Materials Science and Engineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USA\n\n‚ÄÉ‚ÄÉ\nAmir Barati Farimani\n\nbarati@cmu.edu\n\nDepartment of Mechanical Engineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, USA\n\n\n\nAbstract\nAccelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high‚ÄìDebye‚Äìtemperature ceramics, the agent identifies a literature-supported screening criterion (ŒòD&gt;800\\Theta_{D}&gt;800¬†K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be‚ÄìC‚Äìrich compounds that populate the sparsely explored 1500‚Äì1700¬†K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.\n\n\nkeywords: autonomous agents; materials discovery; Debye temperature; literature mining; code generation; CGCNN; M3GNet\n\n\n\\abbreviations\nMKNA, RAG, CGCNN, M3GNet, MP, DFT\n\n\n\n\n\n1 Introduction\n\nThe discovery of new materials has repeatedly enabled technological revolutions across energy storage, information processing, and aerospace engineering. Breakthroughs such as lithium-ion batteries 14, advanced semiconductor materials 29, and high-temperature alloys 26 illustrate a broader principle: materials innovation is not incremental optimization but the foundation upon which disruptive technologies are built. As technological demands intensify, the ability to rapidly identify and design functional compounds has become increasingly critical.\n\n\nIn response to this need, the past two decades have seen the rise of high-throughput experimental and computational platforms. Combinatorial synthesis techniques have enabled the parallel fabrication of thousands of samples 32, 31, while high-throughput DFT workflows now allow systematic exploration of vast chemical spaces 9, 8. The resulting data are consolidated in large-scale repositories‚Äîincluding the Materials Project 18, AFLOW 10, OQMD 27, and NOMAD 11‚Äîwhich provide open access to structures, formation energies, and derived properties for hundreds of thousands of compounds. Meanwhile, machine-learning models such as CGCNN 34, MEGNet 6, M3GNet 5, ElemNet 20, and ALIGNN 7 have further accelerated computational screening by enabling accurate surrogate predictions, with recent advances including orbital-aware graph models and self-supervised pretraining for crystalline property prediction 22, 25.More recently, structure-agnostic and transformer-based pretraining has improved data efficiency and transferability across materials domains, spanning crystals, metal‚Äìorganic frameworks, and polymers 17, 16, 4, 35.\n\n\nDespite this progress, the discovery pipeline remains fragmented. Databases are incomplete 18, 11; machine-learning models are typically task-specific 3, 28, 17, 25; workflows require substantial human intervention 33, 19; and predictive modeling often fails to translate into experimental validation 1, 21. Addressing these limitations requires a unified framework capable of reasoning across retrieval, prediction, and validation while autonomously extracting actionable scientific knowledge.\n\n\nRecent work has begun exploring agent-based automation in materials science. Systems such as LLMatDesign 13 generate hypotheses from natural-language prompts, HoneyComb 23 automates modular tool invocation, and other efforts focus on literature extraction 2 or autonomous laboratories 30. However, these systems typically operate on isolated stages of the discovery pipeline and lack the ability to infer and utilize physicochemical priors to guide search.\n\n\nTo address these gaps, we introduce the Materials Knowledge Navigation Agent (MKNA)‚Äîan autonomous scientific decision-maker that unifies semantic interpretation, knowledge retrieval, surrogate property estimation, structure generation, and physics-informed stability validation into a coherent closed loop. MKNA dynamically inspects available data, generates or repairs missing computations, and selectively invokes models such as CGCNN and M3GNet when needed, enabling robust progress even with incomplete databases.\n\n\nWe demonstrate MKNA‚Äôs reasoning capability through a representative search objective: high‚ÄìDebye‚Äìtemperature materials. Starting from high-level natural-language queries, MKNA not only rediscovers known high-performance systems but also uncovers previously unreported candidates by inferring physicochemical priors (e.g., ŒòD&gt;800\\Theta_{D}&gt;800¬†K) and prioritizing lightweight covalent Be‚ÄìC frameworks‚Äîrevealing latent design rules rather than performing blind screening.\n\n\nFigure 1: Overview of the proposed agentic workflow for materials discovery. \n\n\n\n\n2 Workflow\n\nThe proposed agentic framework converts an open-ended natural-language query\n(e.g., ‚ÄúFind materials with high Debye temperature‚Äù) into a sequence of\nscientifically grounded computational actions. As summarized in\nFig.¬†1, the workflow follows a three-stage process that\nprogressively transforms linguistic intent into validated material candidates.\n\n\n\n2.1 Stage I: Query and Knowledge Grounding\n\nThe agent first interprets the user query by identifying relevant physical\nquantities and resolving vague descriptors. Using a literature-grounded\nMap‚ÄìReduce extraction method, the system assembles numerical evidence,\nderives quantitative screening thresholds, and collects representative\ncandidate materials and physical relations.\nThe output of this stage is a structured knowledge record that provides\nmachine-actionable criteria for subsequent data construction and modeling.\n\n\n\n\n2.2 Stage II: Data and Model Construction\n\nBased on the grounded criteria, the agent autonomously retrieves\ntask-relevant properties from external databases or synthesizes\nLLM-generated code to compute missing quantities.\nThe resulting dataset is combined with literature-derived priors to form a\nreference material pool. Machine-learning models such as CGCNN are then\ntrained or fine-tuned to enable rapid prediction of the target property across\nboth known and hypothetical structures, with data-efficient pretraining and sampling strategies offering a practical route to reduce labeling cost while maintaining accuracy 17, 24.\n\n\n\n\n2.3 Stage III: Material Modification and Validation\n\nTo explore chemical space beyond existing entries, the agent proposes modified candidate structures through controlled substitution and perturbation\nmethods and filters them using the trained predictors. The most promising\ncandidates undergo physics-based validation with M3GNet, which evaluates both\nstructural relaxation and thermodynamic stability.\nThe final outputs consist of (i) high-performing materials retrieved from\ndatabases and (ii) newly modified stable candidates that satisfy the\nquantitative criteria inferred in Stage¬†I.\n\n\nOverall, this workflow links natural-language reasoning to end-to-end\nmaterials exploration, providing a unified route from scientific intuition\nto validated candidate compounds.\n\n\n\n\n\n3 Methods\n\n\n3.1 LLM-Orchestrated Reasoning Framework\n\nLangChain, a framework for composing multi-step LLM workflows, is used as the\norchestration layer that structures interactions among retrieval, summarization,\nand scientific reasoning. GPT-5-mini, a lightweight scientific variant of the\nGPT-5 family, serves as the core reasoning engine and is accessed through the\nChatOpenAI interface with standard inference settings (temperature = 0.7).\nWithin this setup, LangChain provides a unified interface for model invocation,\nprompt-template management, and consistent message passing across the mapping\nand summarization stages, while the overall workflow logic is implemented through\ntask-specific modules within MKNA.\n\n\n\n\n3.2 Query Parsing and Literature Grounding\n\nGiven a natural-language query, the system first extracts domain-relevant\nphysical properties using a prompt-driven keyword parser built on GPT-5-mini.\nThese property tokens initialize an automated literature-grounding workflow\nthat queries arXiv, downloads relevant PDFs, and converts them into plain text\nvia PyMuPDF.\n\n\nThe corpus is embedded using OpenAI embeddings and stored in a persistent\nChroma vector database. Text is chunked using a 500-character window with\n100-character overlap. Semantic retrieval is performed using cosine-similarity\nsearch, with the retr"
  },
  {
    "title": "Learning to Compose for Cross-domain Agentic Workflow Generation",
    "url": "https://arxiv.org/abs/2602.11114v1",
    "source": "arxiv",
    "summary": "Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Automated Agentic Workflows\n2.2 Parameter-efficient and Modular Adaptation\n\n\n3 Preliminary\n\n4 Methodology\n\n4.1 Workflow Capability Bases\n4.2 Task-Conditioned Capability Composer\n4.3 Workflow Capability Decomposition\n4.4 Learning Basic Workflow Capabilities\n4.5 Counterfactual Capability Attribution for Controllable Recomposition\n\n\n\n5 Experiments\n\n5.1 Experimental Settings\n5.2 Main Results\n5.3 Capability Usage and Attribution\n5.4 Ablations: Decompose-Recompose-Decide\n\n\n6 Conclusion\n\nA Appendix\n\n\nA.1 Training and Regularization Details\n\nA.1.1 Two-timescale optimization via detached routing\nA.1.2 Regularization\nA.1.3 Counterfactual attribution: implementation details\n\n\nA.2 Training Procedure\n\n\n\n\n\n\n\nLearning to Compose for Cross-domain \nAgentic Workflow Generation\n\n\nJialiang Wang1, Shengxiang Xu3, Hanmo Liu12, Jiachuan Wang4, Yuyu Luo2, Shimin Di3, Min-Ling Zhang3, Lei Chen12\n\njwangic@connect.ust.hk, xushx@seu.edu.cn, hliubm@connect.ust.hk, wangjc@slis.tsukuba.ac.jp, yuyuluo@hkust-gz.edu.cn, shimin.di@seu.edu.cn, zhangml@seu.edu.cn, leichen@cse.ust.hk\n\n0009-0005-0850-0389, 0009-0003-4964-8406, 0000-0002-6471-0226, 0000-0001-6473-8221, 0000-0001-9530-3327, 0000-0002-7394-0082, 0000-0003-1880-5918, 0000-0002-8257-5806\n1Hong Kong University of Science and Technology, Hong Kong SAR, China\n2Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China\n3Southeast University, Nanjing, China\n4University of Tsukuba, Tsukuba, Japan\n\n\n(12 February 2026)\n\nAbstract.\nAutomatically generating agentic workflows‚Äîexecutable operator graphs or codes that orchestrate reasoning, verification, and repair‚Äîhas become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle.\nYet what constitutes a good workflow depends heavily on the task distribution and the available operators.\nUnder domain shift, current systems typically rely on iterative workflow refinement to discover a feasible workflow from a large workflow space, incurring high iteration costs and yielding unstable, domain-specific behavior.\nIn response, we internalize a decompose-recompose-decide mechanism into an open-source LLM for cross-domain workflow generation.\nTo decompose, we learn a compact set of reusable workflow capabilities across diverse domains.\nTo recompose, we map each input task to a sparse composition over these bases to generate a task-specific workflow in a single pass.\nTo decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, thereby capturing which capabilities actually drive success by their marginal effects.\nAcross stringent multi-domain, cross-domain, and unseen-domain evaluations, our 1-pass generator surpasses SOTA refinement baselines that consume 20 iterations, while substantially reducing generation latency and cost.\n\nAgentic Workflows, Large Language Models, Capability Learning, Compositional Generalization\n\n‚Ä†‚Ä†copyright: acmlicensed‚Ä†‚Ä†journalyear: 2026‚Ä†‚Ä†doi: XXXXXXX.XXXXXXX‚Ä†‚Ä†conference: Make sure to enter the correct\nconference title from your rights confirmation email; February 12,\n2026; Hong Kong, CN‚Ä†‚Ä†isbn: 978-1-4503-XXXX-X/2026/02\n\n\n1. Introduction\n\nFigure 1. Two workflow generation paradigms. Left: inference-time refinement resorts to trial-and-error in a large workflow space. Right: CapFlow internalizes ‚Äúdecompose-recompose-decide‚Äù into LLMs, enabling single-pass generation across domains.\n\n\nLarge language models (LLMs) have demonstrated strong zero-shot capabilities in open-domain question answering and code generation¬†(Brown et al., 2020; Chen, 2021; Ouyang et al., 2022).\nYet, these capabilities are initially realized through single-pass generation: the model produces a final answer or program in one shot.\nFor complex tasks, single-pass generation often hits a structural ceiling¬†(Hong et al., 2023; Chen et al., 2024).\nBeyond being correct, solutions must satisfy constraints, admit external tools, support error correction, and remain reliable under tight latency and cost budgets.\n\n\nTo push beyond this ceiling, agentic workflows¬†(Yao et al., 2022; Zhuge et al., 2024; Liu et al., 2024) have emerged as a practical approach after the Chain-of-Thought¬†(Wei et al., 2022).\nBy making the solving procedure more explicit, an agentic workflow decomposes the task into an ordered, executable composition of operators and executes them under a control structure.\nConcretely, workflow generation for complex tasks requires deciding which operators to invoke and how to compose them into a topology (e.g., a sequence, a branching graph) that determines how intermediate states are produced and consumed¬†(Ye et al., ).\nFor example, open-domain reasoning benefits from operators that retrieve evidence and compare multiple aspects¬†(Wang et al., ); mathematical problems require verification and counterexample-checking operators¬†(Cobbe et al., 2021); and code tasks demand the topology of a test-and-repair loop¬†(Madaan et al., 2023).\n\n\nRecent years have seen rapid advances in building agentic systems with multi-agent workflows.\nPioneering works¬†(Hong et al., 2023; Chen et al., 2024) use manually designed operator pipelines or collaboration structures‚Äîeffective in specific settings but typically fixed regardless of the input task, thereby limiting adaptivity and generality.\nMotivated by the cost and brittleness of manual design, recent work has moved toward automating workflow generation, aiming to reduce human engineering and tailor workflows to the task.\nFor example, systems such as AFlow¬†(Zhang et al., ) and related automated frameworks¬†(Hu et al., ; Wang et al., 2025; Xu et al., 2025) seek to generate or refine workflows with minimal manual specification, improving scalability across diverse domains.\n\n\nA common strategy behind these automated systems is to place workflow generation inside the inference loop (shown in Fig.¬†1(a)).\nGiven a task, the model first generates one or more candidate workflows, and then samples and improves them via search or iterative refinement‚Äîthrough best-of-N sampling¬†(Khattab et al., 2024), self-reflection rewriting¬†(Wang et al., 2025), heuristic structure edits¬†(Zhang et al., ), or evolutionary procedures¬†(Novikov et al., 2025) that repeatedly select, mutate, and re-evaluate workflows.\nOverall, this workflow refinement paradigm treats workflow generation as a trial-and-error inference over a large workflow space, trading high iteration costs for effectiveness and generality.\n\n\nHowever, the fact that workflows can raise the ceiling does not imply that an LLM can naturally generate effective, task-specific workflows for diverse domains.\nIn fact, the past paradigm resembles attaching an external optimizer at inference time rather than endowing the model with transferable workflow generation capability.\nFig.¬†1(a) exposes two limitations in handling domain shift.\nFirst, workflow generation is frequently driven by LLM-only heuristics¬†(Hu et al., ) or stochastic refinement¬†(Zhang et al., ).\nThis limited stability and controllability offer no guarantee about workflow quality when task distribution shifts.\nSecond, good workflow criteria and strategies are difficult to standardize across domains.\nWorkflow criteria and heuristics that work in one domain may fail or even backfire in another¬†(Zhang et al., 2025; Trirat et al., 2025), yielding pronounced generalization variance.\nTherefore, workflow generation often amounts to within-task trial-and-error to approach a feasible workflow at inference time, rather than learning a transferable mapping from task semantics to workflow structure decisions.\nWe further characterize this missing mechanism through two coupled gaps:\n(1) a capability decomposition gap: LLMs often represent tasks at the content level but lack a representation that directly exposes which workflow-relevant capabilities are needed; and\n(2) a capability recomposition gap: even when useful workflow patterns are known, the model lacks a controllable way to select and combine the right capabilities for a new task.\n\n\nNotably, these gaps do not imply that workflow generation is wholly domain-specific.\nOur study in Sec.¬†4.1 shows that, despite task-surface differences across domains, many successful workflows repeatedly instantiate similar underlying capability factors (e.g., multifaceted analysis, verification/repair, and aggregation).\nAs illustrated in Fig.¬†1(b), if a model could represent these capability factors in a parametric, reusable form and compose them on demand for new tasks, cross-domain workflow generation could shift from trial-and-error inference to a single-pass structural decision.\n\n\nTo bridge the decomposition and recomposition gaps, we internalize a ‚Äúdecompose-recompose-decide‚Äù mechanism into the open-source LLM so that workflow generation does not rely on heuristic trial-and-error inference.\nTo decompose, we learn a compact set of reusable capability bases across diverse domains, capturing recurring workflow factors that generalize in the latent space.\nTo recompose, we map an input task to a sparse composition over these bases, providing a controllable way to reuse capabilities for new tasks.\nTo decide, we attribute the success or failure of workflow generation to counterfactual contributions from learned capabilities, capturing which capability bases truly contribute to workflow generation through their marginal effects under domain shifts.\nThe resulting model generates the executable, task-specific agentic workflow in a single pass, thereby avoiding costly refinement during inference.\nOur contributions are:\n\n\n‚Ä¢\n\nWe propose Workflow Capability Basis Learning, reframing cross-domain workflow generation from trial-and-error inference into a learnable problem of capability decomposition and recomposition, enabling single-pass, task-specific workflow generation.\n\n\n\n‚Ä¢\n\nWe present a workflow generalization framework, CapFlow, centered on shared capability bases an"
  },
  {
    "title": "Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection",
    "url": "https://arxiv.org/abs/2602.11107v1",
    "source": "arxiv",
    "summary": "We introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, $\\ell_1$-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the $\\ell_2$ penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinka",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Motivation\n1.2 The Relaxation Problem\n1.3 Our Contribution: Renet\n\n\n\n2 The Renet Algorithm\n\nStage 1: Feature Selection\nStage 2: Adaptive Relaxation\n2.1 Solver-Agnosticism\n2.2 Computational Advantage over LARS-based Implementations\n2.3 Stability Safeguards and the Consensus on the n‚â™pn\\ll p Regime\n\n\n\n3 Theoretical Properties\n\n3.1 Generalization of Relaxed Lasso and Elastic Net\n3.2 Renet as a Stabilized Lasso\n3.3 The Renet Grouping Property\n3.4 Bias Reduction in Orthogonal Design\n3.5 The Synergy Between Relaxation and the 1-SE Rule\n\n\n\n4 Empirical Results\n\n4.1 Formal Setup\n4.2 Results and Discussion\n\n\n5 Conclusion\nA Renet Implementation Pseudocode\nB Adaptive Elastic Net Implementation Pseudocode\nC Extended Results\n\n\n\n\n\nRenet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection\n\n\nAlbert Dorador\nUniversitat Polit√®cnica de Catalunya\nalbert.dorador@upc.edu\n\n\n\nAbstract\nWe introduce Renet, a principled generalization of the Relaxed Lasso to the Elastic Net family of estimators. While, on the one hand, ‚Ñì1\\ell_{1}-regularization is a standard tool for variable selection in high-dimensional regimes and, on the other hand, the ‚Ñì2\\ell_{2} penalty provides stability and solution uniqueness through strict convexity, the standard Elastic Net nevertheless suffers from shrinkage bias that frequently yields suboptimal prediction accuracy. We propose to address this limitation through a framework called relaxation. Existing relaxation implementations rely on naive linear interpolations of penalized and unpenalized solutions, which ignore the non-linear geometry that characterizes the entire regularization path and risk violating the Karush-Kuhn-Tucker conditions. Renet addresses these limitations by enforcing sign consistency through an adaptive relaxation procedure that dynamically dispatches between convex blending and efficient sub-path refitting. Furthermore, we identify and formalize a unique synergy between relaxation and the ‚ÄúOne-Standard-Error‚Äù rule: relaxation serves as a robust debiasing mechanism, allowing practitioners to leverage the parsimony of the 1-SE rule without the traditional loss in predictive fidelity. Our theoretical framework incorporates automated stability safeguards for ultra-high dimensional regimes and is supported by a comprehensive benchmarking suite across 20 synthetic and real-world datasets, demonstrating that Renet consistently outperforms the standard Elastic Net and provides a more robust alternative to the Adaptive Elastic Net in high-dimensional, low signal-to-noise ratio and high-multicollinearity regimes. By leveraging an adaptive solver backend, Renet delivers these statistical gains while offering a computational profile that remains competitive with state-of-the-art coordinate descent implementations.\n\n\n\n1 Introduction\n\n\n1.1 Motivation\n\nIn high-dimensional regression problems where the number of features pp may exceed the number of observations nn, the Elastic Net (Zou and Hastie, 2005) has become a standard tool. By combining the variable selection properties of the Lasso (‚Ñì1\\ell_{1} penalty) with the stabilizing (‚Äúgrouping‚Äù) effect and strict convexity of Ridge (‚Ñì2\\ell_{2} penalty), Elastic Net offers a robust alternative to Lasso, particularly in the presence of high multicollinearity.\n\n\nHowever, both Lasso and Elastic Net suffer from a fundamental limitation: the regularization required to simultaneously suppress noise and induce sparsity shrinks the coefficients of true signal variables towards zero. This bias can lead to suboptimal prediction performance, especially when the signal-to-noise ratio is high. This issue is compounded in Elastic Net, as it imposes a ‚Äúdouble penalty‚Äù on coefficients (Zou and Hastie, 2005), and the prescribed ex-post coefficient re-scaling, while principled, is not always satisfactory.\n\n\nWhile the Adaptive Elastic Net (AEN) (Zou and Zhang, 2009) attempts to mitigate this bias by utilizing data-driven weights, its performance relies directly on the quality of an initial estimator‚Äîa task that is notoriously difficult and prone to weight propagation error with finite samples in the n&lt;pn&lt;p regime or low signal-to-noise environments. Although AEN possesses the ‚ÄúOracle property‚Äù under regularity conditions, this theoretical guarantee can fail in finite samples where the initial estimator is unreliable. Instead of focusing on coefficient-specific debiasing through potentially noisy weights, the framework proposed in this work, known as relaxation, adopts a more robust global debiasing strategy. By decoupling the variable selection process from the final coefficient estimation, relaxation allows for a sparse model that retains the predictive accuracy of unconstrained solutions without sacrificing the sparsity and numerical stability provided by the ‚Ñì1\\ell_{1} and ‚Ñì2\\ell_{2} penalties, respectively.\n\n\nFurthermore, while AEN provides a single point-estimate correction based on a fixed weighting scheme that can be satisfactory in well-behaved regimes, it lacks a mechanism to explore the bias-variance tradeoff within a fixed support. In contrast, introducing a relaxation parameter Œ∏\\theta allows for a continuous navigation of the estimation space. For each candidate sparse model, a Relaxed Elastic Net can reduce shrinkage bias along a continuous path. This allows the estimator to reach optimal risk profiles that are structurally inaccessible to AEN, which must rely on the accuracy of its local weights and is unable to ‚Äúre-tune‚Äù the shrinkage intensity without altering the weighted penalty structure itself.\n\n\nOn the other hand, when defined (n‚â§pn\\leq p), unpenalized Ordinary Least Squares (OLS) linear regression, while not suffering from coefficient bias under standard assumptions, often exhibits a higher mean squared prediction error than Elastic Net due to its increased variance, besides providing zero sparsity. If stepwise variable selection is used in an attempt to obtain a sparse solution, the estimated coefficients are no longer unbiased but tend to be overestimated instead. Furthermore, stepwise selection is a greedy heuristic that scales poorly with dimensionality, as it requires ùí™‚Äã(p2)\\mathcal{O}(p^{2}) OLS fits.\n\n\nAllowing the Elastic Net to optionally debias coefficient estimates‚Äîwhether partially or in full‚Äîemerges as a promising middle ground between the standard, often over-regularized Elastic Net and the less stable or computationally inefficient stepwise OLS linear regression. This approach, which we term Renet, utilizes a framework that is overall more robust in finite samples than that of AEN, providing a self-correcting path that balances parsimony with predictive fidelity even in the absence of the regularity conditions that are required for asymptotically valid individual coefficient debiasing.\n\n\n\n\n1.2 The Relaxation Problem\n\nTo mitigate the estimation bias in ‚Ñì1\\ell_{1}-regularized linear regression, Meinshausen (2007) introduced the Relaxed Lasso, which applies the variable selection of Lasso but re-estimates the coefficients with a reduced penalty.\n\n\nHowever, as successful as the Lasso has been in many situations, it has a few well-known design flaws that may hinder performance:\n\n\n‚Ä¢\n\nSaturation: in the n&lt;pn&lt;p regime, the Lasso is only able to select at most nn variables, which imposes a hard constraint that is driven by the mathematical construction of the estimator, not by the data\n\n\n\n‚Ä¢\n\nSpurious selection in groups of highly correlated predictors, which undermines model stability and may compromise interpretation\n\n\n\n‚Ä¢\n\nUnder high multicollinearity, the Ridge estimator often exhibits superior performance (Tibshirani, 1996)\n\n\n\n\n\nAll of these shortcomings are theoretically solved by introducing an ‚Ñì2\\ell_{2} penalty term, giving rise to the Elastic Net estimator (although it does introduce a complication in a relaxation framework, see Section 2.3). Given a dataset (ùêó,ùê≤\\mathbf{X},\\mathbf{y}) where ùêó‚àà‚Ñùn√óp\\mathbf{X}\\in\\mathbb{R}^{n\\times p} and ùê≤‚àà‚Ñùn√ó1\\mathbf{y}\\in\\mathbb{R}^{n\\times 1}, the Elastic Net estimator Œ≤^\\hat{\\beta} is defined as the minimizer of the following objective function:\n\n\n\n‚Ñí‚Äã(Œ≤;Œª,Œ±)=12‚Äãn‚Äã‚Äñùê≤‚àíùêó‚ÄãŒ≤‚Äñ22+Œª‚Äã(Œ±‚Äã‚ÄñŒ≤‚Äñ1+1‚àíŒ±2‚Äã‚ÄñŒ≤‚Äñ22)\\mathcal{L}(\\beta;\\lambda,\\alpha)=\\frac{1}{2n}\\|\\mathbf{y}-\\mathbf{X}\\beta\\|_{2}^{2}+\\lambda\\left(\\alpha\\|\\beta\\|_{1}+\\frac{1-\\alpha}{2}\\|\\beta\\|_{2}^{2}\\right)\n\n(1)\n\n\nwhere Œª‚â•0\\lambda\\geq 0 controls the overall strength of the penalty, and Œ±‚àà[0,1]\\alpha\\in[0,1] governs the balance between the Lasso and Ridge components.\n\n\nWhile relaxation is theoretically sound, there are not many software implementations of Relaxed Lasso, let alone Relaxed Elastic Net. The R package glmnet (Friedman et al., 2010) provides one of the few implementations of the Relaxed Elastic Net publicly available ‚Äì perhaps the only one. However, based on its documentation, it does not follow the relaxation algorithm in Meinshausen (2007) but instead implements relaxation as a simple linear interpolation between the Elastic Net solution and the OLS solution on the active set:\n\n\n\nŒ≤^n‚Äãa‚Äãi‚Äãv‚Äãe=Œ∏‚ÄãŒ≤^E‚ÄãN+(1‚àíŒ∏)‚ÄãŒ≤^O‚ÄãL‚ÄãS\\hat{\\beta}_{naive}=\\theta\\hat{\\beta}_{EN}+(1-\\theta)\\hat{\\beta}_{OLS}\n\n(2)\n\n\nThis approach is computationally inexpensive but mathematically compromised when the regularization path is non-linear, and, specifically, when relaxing the penalty causes variables to change sign or re-enter the active set. This approach, thus, may yield a model that never actually satisfied the Karush-Kuhn-Tucker (KKT) conditions of the original objective function at any Œª‚â•0\\lambda\\geq 0 (unless one of the extreme points is chosen). As already warned in Tibshirani (1996) and later proved formally (see e.g. Dorador (2025)), the size of the estimated regression coefficients in Lasso is in general not a monotonic function of the penalty parameter, as it depends on the correlation structure present in the design matrix. This means that the linearity violation in the regularization path is fa"
  },
  {
    "title": "TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection",
    "url": "https://arxiv.org/abs/2602.11106v1",
    "source": "arxiv",
    "summary": "Misinformation detection is a critical task that can benefit significantly from the integration of external knowledge, much like manual fact-checking. In this work, we propose a novel method for representing textual documents that facilitates the incorporation of information from a knowledge base. Our approach, Text Encoding with Graph (TEG), processes documents by extracting structured informatio",
    "full_text": null
  },
  {
    "title": "GameDevBench: Evaluating Agentic Capabilities Through Game Development",
    "url": "https://arxiv.org/abs/2602.11103v1",
    "source": "arxiv",
    "summary": "Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets suc",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Benchmark Construction\n\n2.1 Stage 1: Data Preparation\n2.2 Stage 2: Automatic Task Construction\n2.3 Stage 3: Task Refinement\n2.4 Stage 4: Human Annotation.\n\n\n\n3 GameDevBench\n\n3.1 Task Categories\n3.2 Features of GameDevBench\n\n\n\n4 Evaluation\n\n4.1 Multimodal Feedback\n4.2 Discussion of Results\n4.3 Error Analysis and Directions for Improvement.\n\n\n5 Related Works\n6 Conclusion\nA Task Construction Prompt\nB Task Refinement Prompt\nC Human Annotation Instructions\nD Prompt Templates\n\nE Task Examples\n\nE.1 Isometric Crusader Animation\nE.2 Floating Balls\nE.3 FPS User Interface\nE.4 RTS Unit\n\n\nF Task Statistics\n\nG Case Study of Model Failure\n\nG.1 Common Game Development Patterns\n\n\n\n\n\n\n\nGameDevBench: Evaluating Agentic Capabilities Through Game Development\n\n\nWayne Chi\n\nCarnegie Mellon University\n\n\nYixiong Fang\n\nCarnegie Mellon University\n\n\nArnav Yayavaram\n\nCarnegie Mellon University\n\n\nSiddharth Yayavaram\n\nCarnegie Mellon University\n\n\nSeth Karten\n\nPrinceton University\n\n\nQiuhong Anna Wei\n\nCarnegie Mellon University\n\n\nRunkun Chen\n\nCarnegie Mellon University\n\n\nAlexander Wang\n\nCarnegie Mellon University\n\n\nValerie Chen\n\nCarnegie Mellon University\n\n\n\nAmeet Talwalkar\n\nCarnegie Mellon University\n\n\nChris Donahue\n\nCarnegie Mellon University\n\n\n\nAbstract\nDespite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind.\nA key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding.\nGame development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets such as shaders, sprites, and animations within a visual game scene.\nWe present GameDevBench, the first benchmark for evaluating agents on game development tasks.\nGameDevBench consists of 132¬†tasks derived from web and video tutorials.\nTasks require significant multimodal understanding and are complex‚Äîthe average solution requires over three times the amount of lines of code and file changes compared to prior software development benchmarks. Agents still struggle with game development, with the best agent solving only 54.5%54.5\\% of tasks.\nWe find a strong correlation between perceived task difficulty and multimodal complexity, with success rates dropping from 46.9%46.9\\% on gameplay-oriented tasks to 31.6%31.6\\% on 2D graphics tasks.\nTo improve multimodal capability, we introduce two simple image and video-based feedback mechanisms for agents.\nDespite their simplicity, these methods consistently improve performance, with the largest change being an increase in Claude Sonnet 4.5‚Äôs performance from 33.3%33.3\\% to 47.7%47.7\\%.\nWe release GameDevBench publicly to support further research into agentic game development.\n\n\n\n\n\n\n\nProject Page\nhttps://waynechi.com/gamedevbench\n\n\n\n\n\n\nGithub Repo\nhttps://github.com/waynchi/gamedevbench\n\n\n\n\n‚Ä†‚Ä†footnotetext: Correspondence to waynechi@andrew.cmu.edu\n\n\n1 Introduction\n\nFigure 1: We present GameDevBench, a benchmark for evaluating an agent‚Äôs ability to solve complex and multimodal game development tasks in a modern game engine.\n\n\nProgress on multimodal language model (LM) agents has lagged behind that of their unimodal counterparts¬†(Yang et al., 2024b; Jimenez et al., 2024; Zhou et al., 2024; Koh et al., 2024).\nAgentic game development‚Äîdespite its inherent multi-modality, increasing public interest, and a rich history combining artificial intelligence and games¬†(Vinyals et al., 2019; Schrittwieser et al., 2019; Silver et al., 2018, 2016; Jagli et al., 2024; Filipoviƒá, 2023; Yakan, 2022)‚Äîhas largely been overlooked by the research community.\nMost prior works focus on specific goals within game development such as next frame prediction¬†(Valevski et al., 2024; Oh et al., 2015), which replaces the graphics engine, procedural content generation¬†(Summerville et al., 2018; Shaker et al., 2016), which replaces both functional and cosmetic asset creation, or game playing agents¬†(Vinyals et al., 2019; Silver et al., 2016), which replaces the non-player characters (NPCs) and opponents.\nThere has been little to no research on agentic use for general game development (i.e., developing games within a game engine), most likely because it seemed inconceivable until recently.\nAs LM agent capabilities continue to improve, it seems natural to ask:\ncan agents develop video games?\n\n\n\nGame development combines many desirable characteristics for a challenging benchmark in a modern agentic domain.\nFirst, tasks are complex and context-rich with projects often spanning large amounts of files, assets, and folders akin to that of traditional software development¬†(Yang et al., 2024b).\nSecond, tasks are inherently multimodal, requiring visual understanding of both static elements (e.g., map or scene layouts) and temporal dynamics (e.g., animations or movement) to accurately assess project state.\nLastly, task solutions are deterministically verifiable through code which alleviates the need for approaches such as LLM-as-a-Judge¬†(Zheng et al., 2023) which are often subject to biases¬†(Wang et al., 2023; Koo et al., 2024).\nFor example, it is possible to verify that the correct animation was used by checking animation states at each frame.\nThis combination of features makes game development an ideal environment to evaluate complex, multi-modal agentic capabilities.\n\n\nIn this work, we study an agent‚Äôs ability to solve complex game development tasks for a modern game engine.\nTo our knowledge, this is the first work evaluating this capability.\nGame development typically involves creating and editing artifacts such as sprite sheet animations, collision shapes, game logic scripts, and scene layouts in a GUI (Graphical User Interface) called the game editor.\nA game engine then processes these artifacts into a runnable game build.\nCommon examples of game engines include Unity, Unreal Engine, and Godot, each of which provides both an editor and an engine.\nGame development tasks are deceptively complex.\nFor example, the ‚Äúsimple‚Äù task of creating an Italian plumber for a platformer game would require creating animations for various states such as idling, jumping, or running, setting up a collider to allow for jumping on enemies such as turtles, writing scripts to allow for control, adding sound effects for actions, and more.\n\n\n\nWe focus our work on the Godot environment for several reasons.\nFirst, Godot is fully open sourced under the MIT license which makes it easy to extend and release alongside the benchmark.\nSecond, Godot is an increasingly popular game development engine, with 770 and 1185 releases on Steam in 2024 and 2025.\nThird, Godot‚Äôs environment strongly resembles Unity, which is by far the most popular game development engine.\nLastly, Godot projects (not including assets such as images) can be represented in code which makes it simple to extend existing LLM agent capabilities without having to construct specific tool-use APIs.\n\n\n\nWe present GameDevBench, the first benchmark for evaluating an agent‚Äôs ability to solve game development tasks.\nTasks are created by analyzing and processing Godot YouTube and web tutorials.\nThese tutorials span a wide range of topics such as 2D sprite animations, character controllers (i.e., character movement), colliders and platforms, shader usage, particle effects, among others.\nThis ensures that tasks are not only diverse, but also align with common game development needs.\nTasks are incredibly complex and content-rich.\nNot only do they require a deep understanding of various file types and assets (e.g., images), tasks on average require more than three times the number of lines of code changes compared to SWE-Bench¬†(Yang et al., 2024b).\nFor each task, agents are given a project folder with code and various assets, as well as an instruction as is standard in software benchmarks¬†(Yang et al., 2024b).\nTask success is evaluated using tests built within Godot‚Äôs scripting framework.\nThis allows us to deterministically test for features such as physics or polygonal shapes.\nAdditionally, each task comes with a verified reference solution.\nAll code and task project files for GameDevBench are released publicly.\n\n\nWe found that while agents are increasingly capable, they still struggle with the majority of game development tasks.\nWithout additional support, the best agent succeeds at only 47.0%47.0\\% of the tasks.\nIn particular, models perform significantly worse when the tasks require increased multimodal understanding.\nFor example, agents perform almost two times better when tackling gameplay-oriented tasks compared to graphics tasks.\n\n\nTo improve agent multimodal capabilities, we propose two methods that provide agents with multi-modal feedback when solving a task.\nOne method provides a screenshot view of the editor‚Äôs current state via a Model Context Protocol (MCP) server¬†(Anthropic, 2024) while another records a video of the game scene.\nDespite their simplicity, we found that both methods are effective empirically, increasing agent performance across almost all models.\n\n\n\n\n2 Benchmark Construction\n\nFigure 2: This is an example task from GameDevBench that requests for the creation of a UI minimap.\nTop is the visual GUI representation and highlighted points of interest.\nBottom is the same scenes and files represented in code.\nTasks can be solved via the editor or entirely through code although either method requires understanding multimodal assets.\nGame development tasks are complex and require editing dense files, identifying and visually understanding various assets, and navigating various nodes (game elements) and scenes (a collection of nodes).\n\n\n\nGameDevBench consists of game-development tasks distilled from online tutorials (e.g., ‚Äúadd a walking animation using the given spritesheet‚Äù). GitHub repositories are a rich source of data, but can be noisy and poorly documented.\nAdditionally, unlike prior work¬†(Jimenez et al., 2024) on benchmarking general so"
  },
  {
    "title": "Statistical Learning Analysis of Physics-Informed Neural Networks",
    "url": "https://arxiv.org/abs/2602.11097v1",
    "source": "arxiv",
    "summary": "We study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective. Specifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning probl",
    "full_text": "\n\n\n\n1 Introduction\n2 Statistical learning interpretation of physics-informed learning for IBVPs\n\n3 Numerical estimation of the local learning coefficient\n\n3.1 MCMC-based local learning coefficient estimator\n3.2 Numerical experiments\n\n\n4 Discussion\n5 Conclusions\n\n\n\n\n\nStatistical Learning Analysis of Physics-Informed Neural Networks\n\n\nDavid Barajas-Solano\n\nDavid.Barajas-Solano@pnnl.gov\n\n\n\nAbstract\nWe study the training and performance of physics-informed learning for initial and boundary value problems (IBVP) with physics-informed neural networks (PINNs) from a statistical learning perspective.\nSpecifically, we restrict ourselves to parameterizations with hard initial and boundary condition constraints and reformulate the problem of estimating PINN parameters as a statistical learning problem.\nFrom this perspective, the physics penalty on the IBVP residuals can be better understood not as a regularizing term bus as an infinite source of indirect data, and the learning process as fitting the PINN distribution of residuals p‚Äã(y‚à£x,t,w)‚Äãq‚Äã(x,t)p(y\\mid x,t,w)q(x,t) to the true data-generating distribution Œ¥‚Äã(0)‚Äãq‚Äã(x,t)\\delta(0)q(x,t) by minimizing the Kullback-Leibler divergence between the true and PINN distributions.\nFurthermore, this analysis show that physics-informed learning with PINNs is a singular learning problem, and we employ singular learning theory tools, namely the so-called Local Learning Coefficient [7] to analyze the estimates of PINN parameters obtained via stochastic optimization for a heat equation IBVP.\nFinally, we discuss implications of this analysis on the quantification of predictive uncertainty of PINNs and the extrapolation capacity of PINNs.\n\n\nkeywords: \nPhysics-informed learning , physics-informed neural networks , statistical learning , singular learning theory\n\n\n\n\\affiliation\n[PNNL]organization=Pacific Northwest National Laboratory,city=Richland,\npostcode=99352,\nstate=WA, USA\n\n\n\n1 Introduction\n\nPhysics-informed machine learning with physics-informed neural networks (PINNs) (see [6] for a comprehensive overview) has been widely advanced and successfully employed in recent years to solve approximate the solution of initial and boundary value problems (IBVPs) in computational mathematics.\nThe central mechanism of physics-informed learning is model parameter identification by minimizing ‚Äúphysics penalty‚Äù terms penalizing the governing equation and initial and boundary condition residuals predicted by a given set of model parameters.\nWhile various significant longstanding issues of the use PINNs for physics-informed learning such as the spectral bias have been addressed, it remains active area of research, and our understanding of various features of the problem such as the loss landscape, and the extrapolation capacity of PINN models, remains incomplete.\n\n\nIt has been widely recognized that traditional statistical tools are inadequate for analyzing deep learning.\nWatanabe‚Äôs ‚Äúsingular learning theory‚Äù (SLT) (see [13] for a review) recognizes that this is due to the singular character of learning with deep learning models and aims to address these limitations.\nThe development of efficient deep learning and Bayesian inference tools and frameworks [3, 15, 2, 1] has enabled the application of SLT to analyzing practical applications of deep learning, leading to various insights on the features of the loss landscape, the generalization capacity of the model, and the training process [7, 4, 10].\n\n\nGiven that physics-informed learning with PINNs is also singular due to its use of neural networks as the backbone architecture, we propose in this work to use SLT to analyze the training and performance of PINNs.\nThe physics penalty in physics-informed learning precludes an out-of-the-box application of SLT, so we first reformulate the PINN learning problem as a statistical learning problem by restricting our attention to PINN parameterizations with hard initial and boundary condition constraints.\nEmploying this formulation, it can be seen that the PINN learning problem is equivalent to the statistical learning problem of fitting the PINN distribution of governing equation residuals p‚Äã(y‚à£x,t,w)‚Äãq‚Äã(x,t)p(y\\mid x,t,w)q(x,t) to the true data-generating distribution Œ¥‚Äã(0)‚Äãq‚Äã(x,t)\\delta(0)q(x,t) corresponding to zero residuals y‚Äã(x,t)‚â°0y(x,t)\\equiv 0 for all (x,t)‚ààŒ©√ó(0,T](x,t)\\in\\Omega\\times(0,T], by minimizing the Kullback-Leibler divergence between the true and PINN distributions.\nWe argue this implies that the physics penalty in physics-informed learning can be better understood as an infinite source of indirect data (with the residuals interpreted as indirect observations given that they are a function of the PINN prediction) as opposed to as a regularizing term as it is commonly interpreted in the literature.\nFurthermore, we employ the SLT‚Äôs so-called Local Learning Coefficient (LLC) to study the features of the PINN loss for a heat equation IBVP.\n\n\nThis manuscript is structured as follows: In section¬†2 we reformulate and analyze the physics-informed learning problem as a singular statistical learning problem.\nIn section¬†3 we discuss the numerical approximation of the LLC and compute it for a heat equation IBVP.\nFinally, further implications of this analysis are discussed in section¬†4.\n\n\n\n\n2 Statistical learning interpretation of physics-informed learning for IBVPs\n\nConsider the well-posed boundary value problem\n\n\n\n‚Ñí‚Äã(u‚Äã(x,t),x,t)\\displaystyle\\mathcal{L}(u(x,t),x,t)\n=0\\displaystyle=0\n‚àÄx‚ààŒ©,\\displaystyle\\forall x\\in\\Omega,\nt‚àà(0,T],\\displaystyle t\\in(0,T],\n\n(1)\n\n\n\n‚Ñ¨‚Äã(u‚Äã(x,t),x,t)\\displaystyle\\mathcal{B}(u(x,t),x,t)\n=0\\displaystyle=0\n‚àÄx‚àà‚àÇŒ©,\\displaystyle\\forall x\\in\\partial\\Omega,\nt‚àà[0,T],\\displaystyle t\\in[0,T],\n\n(2)\n\n\n\nu‚Äã(x,0)\\displaystyle u(x,0)\n=u0‚Äã(x)\\displaystyle=u_{0}(x)\n‚àÄx‚ààŒ©,\\displaystyle\\forall x\\in\\Omega,\n\n(3)\n\n\nwhere the Œ©\\Omega denotes the IBVP‚Äôs spatial domain, ‚àÇŒ©\\partial\\Omega the domain‚Äôs boundary, TT the maximum simulation time, and u‚Äã(x,t)u(x,t) the IBVP‚Äôs solution.\nIn the PINN framework, we approximate the solution u‚Äã(x,t)u(x,t) using an overparameterized deep learning-based model u‚Äã(x,t,w)u(x,t,w), usually a neural network model NNw‚Äã(x,t)\\text{NN}_{w}(x,t), where w‚ààWw\\in W denotes the model parameters and W‚äÇ‚ÑùdW\\subset\\mathbb{R}^{d} denotes the parameter space.\nFor simplicity, we assume that the model u‚Äã(x,t,w)u(x,t,w) is constructed in such a way that the initial and boundary conditions are identically satisfied for any model parameter, that is,\n\n\n\n‚Ñ¨‚Äã(u‚Äã(x,t,w),x,t)=0‚àÄx‚àà‚àÇŒ©,t‚àà(0,T],w‚ààW,\\displaystyle\\mathcal{B}(u(x,t,w),x,t)=0\\quad\\forall x\\in\\partial\\Omega,\\ t\\in(0,T],\\ w\\in W,\n\n\n\n\nu‚Äã(x,0,w)=u0‚Äã(x)‚àÄx‚ààŒ©,w‚ààW,\\displaystyle u(x,0,w)=u_{0}(x)\\quad\\forall x\\in\\Omega,\\ w\\in W,\n\n\n\nvia hard-constraints parameterizations (e.g., [8, 9]).\nThe model parameters are then identified by minimizing the squared residuals of the governing equation evaluated at nn ‚Äúresidual evaluation‚Äù points {xi,ti}i=1n\\{x_{i},t_{i}\\}^{n}_{i=1}, that is, by minimizing the loss\n\n\n\nLnPINN‚Äã(w)‚âî1n‚Äã‚àëi=1n[‚Ñí‚Äã(u‚Äã(xi,ti,w),xi,ti)]2,L^{\\text{PINN}}_{n}(w)\\coloneqq\\frac{1}{n}\\sum^{n}_{i=1}\\left[\\mathcal{L}(u(x_{i},t_{i},w),x_{i},t_{i})\\right]^{2},\n\n(4)\n\n\n\n\nIn the physics-informed-only limit, the PINN framework aims to approximate the solution without using direct observations of the solution, that is, without using labeled data pairs of the form (xi,ti,u‚Äã(xi,ti))(x_{i},t_{i},u(x_{i},t_{i})), which leads many authors to refer to it as an ‚Äúunsupervised‚Äù learning framework.\nAn alternative view of the PINN learning problem is that it actually does use labeled data, but of the form (xi,ti,‚Ñí‚Äã(u‚Äã(xi,ti),xi,ti)‚â°0)(x_{i},t_{i},\\mathcal{L}(u(x_{i},t_{i}),x_{i},t_{i})\\equiv 0), as the PINN formulation encourages the residuals of the parameterized model, ‚Ñí‚Äã(u‚Äã(x,t,w),x,t)\\mathcal{L}(u(x,t,w),x,t), to be as close to zero as possible.\nCrucially, ‚Äúresidual‚Äù data of this form is ‚Äúinfinite‚Äù in the sense that such pairs can be constructed for all x‚ààŒ©x\\in\\Omega, t‚àà(0,T]t\\in(0,T].\nIn fact, the loss 4 can be interpreted as a sample approximation of the volume-and-time-averaged loss LPINN‚âî‚à´Œ©√ó(0,T][‚Ñí‚Äã(u‚Äã(x,t),x,t)]2‚Äãq‚Äã(x,t)‚Äãdx‚ÄãdtL^{\\text{PINN}}\\coloneqq\\int_{\\Omega\\times(0,T]}\\left[\\mathcal{L}(u(x,t),x,t)\\right]^{2}q(x,t)\\,\\mathrm{d}x\\mathrm{d}t for a certain weighting function q‚Äã(x,t)q(x,t).\n\n\nWe can then establish a parallel with statistical learning theory. Namely, we can introduce a density over the inputs, q‚Äã(x,t)q(x,t), and the true data-generating distribution q‚Äã(x,t,y)=Œ¥‚Äã(0)‚Äãq‚Äã(x,t)q(x,t,y)=\\delta(0)q(x,t), where yy denotes the equation residuals.\nFor a given error model p‚Äã(y‚à£‚Ñí‚Äã(u‚Äã(x,t,w),x,t))p(y\\mid\\mathcal{L}(u(x,t,w),x,t)), the PINN model induces a conditional distribution of residuals p‚Äã(y‚à£x,w)p(y\\mid x,w).\nFor example, for a Gaussian error model p‚Äã(y‚à£‚Ñí‚Äã(u‚Äã(x,t,w),x,t))=ùí©‚Äã(y‚à£0,œÉ2)p(y\\mid\\mathcal{L}(u(x,t,w),x,t))=\\mathcal{N}(y\\mid 0,\\sigma^{2}), the conditional distribution of residuals is of the form\n\n\n\np‚Äã(y‚à£x,t,w)‚àùexp‚Å°{‚àí12‚ÄãœÉ2‚Äã[‚Ñí‚Äã(u‚Äã(x,t,w),x,t)]2}.p(y\\mid x,t,w)\\propto\\exp\\left\\{-\\frac{1}{2\\sigma^{2}}\\left[\\mathcal{L}(u(x,t,w),x,t)\\right]^{2}\\right\\}.\n\n(5)\n\n\nThe PINN framework can then be understood as aiming to fit the model p‚Äã(x,t,y‚à£w)‚âîp‚Äã(y‚à£x,t,w)‚Äãq‚Äã(x,t)p(x,t,y\\mid w)\\coloneqq p(y\\mid x,t,w)q(x,t) to the data-generating distribution by minimizing the sample negative log-likelihood Ln‚Äã(w)L_{n}(w) of the training dataset {(xi,ti‚Äãyi‚â°0)}i=1n\\{(x_{i},t_{i}y_{i}\\equiv 0)\\}^{n}_{i=1} drawn i.i.d. from q‚Äã(x,t,y)q(x,t,y), with\n\n\n\nLn‚Äã(w)‚âî‚àí1n‚Äã‚àëi=1nlog‚Å°p‚Äã(yi‚â°0‚à£xi,ti,w).L_{n}(w)\\coloneqq-\\frac{1}{n}\\sum^{n}_{i=1}\\log p(y_{i}\\equiv 0\\mid x_{i},t_{i},w).\n\n(6)\n\n\nAssociated to this negative log-likelihood we also have the population negative log-likelihood L‚Äã(w)‚âî‚àíùîºq‚Äã(x,t,y)‚Äãlog‚Å°p‚Äã(y‚à£x,t,w)L(w)\\coloneqq-\\mathbb{E}_{q(x,t,y)}\\log p(y\\mid x,t,w).\nNote that for the Gaussian error model introduced above we have that LnPINN‚Äã(w)=2‚ÄãœÉ2‚ÄãLn‚Äã(w)L^{\\text{PINN}}_{n}(w)=2\\sigma^{2}L_{n}(w) up to an additive constant, so that learning by minimizing the PINN loss 4 and the ne"
  },
  {
    "title": "Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away",
    "url": "https://arxiv.org/abs/2602.11096v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates. We propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constr",
    "full_text": "\n\n\n\n1 Introduction\n2 Problem Formulation\n\n3 Proposed Approach\n\n3.1 Inference-time steering\n3.2 SafeThink Algorithm\n\n\n4 Safety Recovery with Few Step Steering\n\n5 Experiments\n\n5.1 Experimental Setup\n5.2 Main Results\n5.3 Additional Insights\n\n\n6 Related Works\n7 Conclusion\n8 Impact Statement\n9 Acknowledgments\nA Software and Hardware Used\nB Limitations\nC Satisficing Principle for Safety\n\nD Extended Results\n\nSafeThink achieves minimal ASR on MM-SafetyBench.\nRobustness to Choice of Safety Reward Model.\n\n\n\nE Experimental Details\n\n\nE.1 Details of Jailbreak Benchmarks\n\nText-based Attacks.\nImage-based Attacks.\n\n\nE.2 Description of Baselines\n\n\nF Extended Related Works\nG Qualitative Evaluations\n\n\n\n\n\n\n\\uselogo\n\nSafety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away\n\n\n\nSoumya Suvra Ghosal1*,\nSouradip Chakraborty1*,\nVaibhav Singh2*,\nFurong Huang1, \nDinesh Manocha1,\nAmrit Singh Bedi3,\n\n\n\n\n1University of Maryland, College Park ‚ÄÇ‚ÄÑ‚Ää2IIT, Bombay ‚ÄÇ‚ÄÑ‚Ää3University of Central Florida\n\n\n\n\n\n\n\n\nAbstract\nReinforcement learning (RL) based post-training for explicit chain-of-thought (e.g., GRPO) improves the reasoning ability of multimodal large-scale reasoning models (MLRMs). But recent evidence shows that it can simultaneously degrade safety alignment and increase jailbreak success rates.\nWe propose SafeThink, a lightweight inference-time defense that treats safety recovery as a satisficing constraint rather than a maximization objective.\nSafeThink monitors the evolving reasoning trace with a safety reward model and conditionally injects an optimized short corrective prefix (‚ÄúWait, think safely‚Äù) only when the safety threshold is violated. In our evaluations across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, and MM-SafetyBench), SafeThink reduces attack success rates by 30-60 % (e.g., LlamaV-o1: 63.33% to 5.74% on JailbreakV-28K, R1-Onevision: 69.07% to 5.65% on Hades) while preserving reasoning performance (MathVista accuracy: 65.20% to 65.00%). A key empirical finding from our experiments is that safety recovery is often only a few steering steps away: intervening in the first 1‚Äì3 reasoning steps typically suffices to redirect the full generation toward safe completions.\nProject Page: https://itsvaibhav01.github.io/SafeThink-web/\nWARNING: This paper contains prompts and model outputs that may be offensive in nature.\n\n\n\n1 Introduction\n\nMultimodal large reasoning models (MLRMs) are increasingly achieving strong performance across multimodal reasoning benchmarks (yue2024mmmu; ma2024m; lu2022learn).\nHowever, recent studies report a consistent side effect: reasoning-centric post-training (e.g., RL for explicit chain-of-thought) can degrade safety alignment, increasing vulnerability to jailbreak attacks (fang2025safemlrm; huang2025safety; jiang2025safechain; zhou2025hidden).\nFor example, on the Hades benchmark (Li-HADES-2024), reasoning-tuned models exhibit substantially higher attack success rates than their base counterparts: R1-Onevision (yang2025r1) shows an increase from 19.13%19.13\\% to 69.07%69.07\\% compared to Qwen2.5-VL (Figure 1). This reasoning tax on safety demonstrates that the same recipes that improve reasoning capabilities can substantially weaken safety robustness.\n\n\nA natural response is to strengthen inference-time defenses (e.g., refusal prompting, safety prompting, or decoding heuristics). However, existing defenses are often brittle under jailbreaks and can degrade reasoning utility when applied aggressively (jiang2025safechain; jeung2025safepath).\nThis motivates a central question:\nCan we recover safety in reasoning-tuned MLRMs without sacrificing the reasoning gains of post-training?\n\n\nFigure 1: Overview of SafeThink. (a) Without intervention, reasoning MLRMs process adversarial queries through unsafe reasoning chains, producing harmful responses. SafeThink monitors the reasoning trace and injects a safety steering token when the safety threshold is violated. Safety recovery typically occurs within the first few reasoning steps, after which generation proceeds toward safe completions while preserving reasoning utility. (b) Reasoning fine-tuning improves task performance but degrades safety alignment, resulting in a higher attack success rate (ASR). For example, on the Hades benchmark (Li-HADES-2024), R1-Onevision (yang2025r1) exhibits a sharp decline in safety score (defined as 100‚àíASR100-\\text{ASR}) from 80.87%80.87\\% to 30.93%30.93\\% compared to its base model Qwen2.5-VL, illustrating the reasoning tax on safety. SafeThink recovers safety at inference time without sacrificing reasoning capabilities (Reasoning MLRM + SafeThink).\n\n\nKey insight. We posit that safety-relevant behavior is not fully erased by reasoning-centric training. Rather, safe behavior often remains latent but is not reliably selected under adversarial conditioning. Equivalently, unsafe behavior can arise from a conditional coverage failure: safe continuations may exist, but the model assigns them negligible probability under jailbreak contexts. This suggests that safety recovery may not require retraining; it may be achievable via lightweight inference-time interventions that recondition generation back toward safe regions.\n\n\nOur approach.\nWe propose SafeThink, a lightweight inference-time steering method for reasoning models.\nSafeThink monitors safety during chain-of-thought generation using a safety reward model and, when necessary, applies a targeted steering intervention at the level of intermediate reasoning steps.\nA key design choice is how to aim for safety recovery.\nPrior work often implicitly tries to maximize safety to counteract the reasoning tax, which can yield overly conservative behavior and reduced utility (jiang2025safechain).\nWe instead adopt a satisficing perspective: safety need not be maximized if outputs can be kept above a predefined safety threshold (chehade2025bounded; simon1956rational).\n\n\nKey finding: safety is only a few steering steps away.\nAcross multiple reasoning-tuned MLRMs and jailbreak benchmarks, we find that safety recovery is typically early and budget-efficient.\nRestricting intervention to just the first few reasoning steps is often sufficient to redirect the full trajectory toward safe completions.\nIn particular, intervening only within the first 1‚Äì3 reasoning steps can sharply reduce attack success rates, without requiring persistent steering throughout generation (Figure 4). We summarize our contributions as follows.\n\n\n(i) Satisficing safety as an inference-time constraint.\nWe frame safety recovery for reasoning-tuned MLRMs as maintaining generations above a safety threshold, rather than maximizing an overly conservative safety objective.\n\n\n(ii) Conditional step-wise safety steering.\nWe propose SafeThink, which monitors the evolving chain of thought and injects a short corrective prefix only when safety violations are detected, with a small intervention budget.\n\n\n(iii) Few-step safety recovery.\nWe show that safety recovery is often only a few early steering steps away: intervening in the first 1‚Äì3 reasoning steps typically suffices to keep the remainder of the generation safe.\n\n\n(iv) Comprehensive evaluation with strong safety gains and minimal utility loss. Across six open-source MLRMs and four jailbreak benchmarks (JailbreakV-28K, Hades, FigStep, MMSafetyBench), SafeThink reduces attack success rates by 30‚Äì60 % (e.g., LlamaV-o1: 63.33%‚Üí\\rightarrow5.74% on JailbreakV-28K; R1-Onevision: 69.07%‚Üí\\rightarrow5.65% on Hades) while preserving reasoning performance (MathVista: 65.20%‚Üí\\rightarrow65.00%).\n\n\n\n\n\n(a) R1-Onevision\n\n\n\n\n\n(b) VLAA-Thinker\n\n\n\n\n\n(c) Vision-R1\n\n\n\nFigure 2: Best-of-NN sampling fails to recover safe trajectories. We empirically validate that under adversarial inputs xadvx_{\\text{adv}}, the probability of sampling a safe continuation from the base policy is near-zero. Given an intermediate state x‚Ä≤=(xadv,z&lt;t)x^{\\prime}=(x_{\\text{adv}},z_{&lt;t}), BoN‚àó samples kk candidate next steps directly from the base policy, zt(i)‚àºœÄ(‚ãÖ‚à£x‚Ä≤)z_{t}^{(i)}\\sim\\pi(\\cdot\\mid x^{\\prime}), and selects the one with maximum RsafeR_{\\text{safe}}. Despite increasing kk up to 20, BoN‚àó (purple) remains below the safety threshold œÑ\\tau, confirming that safe continuations have vanishing probability mass under the base policy. SafeThink conditions generation on a safety steering token ss, sampling zt(i)‚àºœÄ(‚ãÖ‚à£x‚Ä≤,s)z_{t}^{(i)}\\sim\\pi(\\cdot\\mid x^{\\prime},s). This shifts the distribution toward safe regions, allowing SafeThink to cross the threshold with as few as k=2k{=}2 samples. The results demonstrate that the failure of naive sampling stems not from the absence of safe continuations, but from poor conditional coverage, a gap that safety-steered sampling effectively bridges. Results on HADES (Li-HADES-2024) for (a) R1-Onevision, (b) VLAA-Thinker, and (c) Vision-R1.\n\n\n\n\n2 Problem Formulation\n\nWe consider a multimodal reasoning model œÄŒ∏(‚ãÖ‚à£x)\\pi_{\\theta}(\\cdot\\mid x) parameterized by Œ∏\\theta, where the input is\nx=[I,w]x=[I,w] (image II and text prompt ww).\nThe model generates an explicit reasoning trace z=(z1,‚Ä¶,zT)z=(z_{1},\\dots,z_{T}) followed by a final answer yy, with joint distribution\n\n\n\npŒ∏‚Äã(y,z‚à£x):=(‚àèt=1TœÄŒ∏‚Äã(zt‚à£x,z&lt;t))‚ÄãœÄŒ∏‚Äã(y‚à£x,z).\\displaystyle p_{\\theta}(y,z\\mid x):=\\Big(\\prod_{t=1}^{T}\\pi_{\\theta}(z_{t}\\mid x,z_{&lt;t})\\Big)\\;\\pi_{\\theta}(y\\mid x,z).\n\n(1)\n\n\nModern multimodal large reasoning models (MLRMs) are often obtained through RL post-training (e.g., GRPO guo2025deepseek),\nwhich rewards task performance and encourages longer, more structured chains of thought.\nWhile this improves utility, it can also shift the induced reasoning distribution œÄŒ∏‚Äã(z‚à£x)\\pi_{\\theta}(z\\mid x) away from safety-aligned behaviors inherited from the base model.\nUnder adversarial prompts, this shift increases the likelihood of unsafe intermediate reasoning and unsafe outputs, leading to elevated jailbreak success rates (Figure 1).\n\n\nFigure 3: Satisficing safety alignment. Safety r"
  },
  {
    "title": "MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning",
    "url": "https://arxiv.org/abs/2602.11092v1",
    "source": "arxiv",
    "summary": "Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates ",
    "full_text": "\n\n\n\nI Introduction and Motivation\n\nII MerLin architecture\n\n\nII-A Differentiable simulation of Photonic Quantum circuits\n\nII-A1 Essentials of linear optics\nII-A2 Strong Linear Optical Simulation (SLOS)\nII-A3 Circuit optimization with PyTorch\n\n\nII-B The Quantum Layer abstract\n\nII-C Data encoding\n\nII-C1 Angle encoding\nII-C2 Amplitude encoding\n\n\nII-D Hardware-aware design\nII-E Complexity and simulability\n\n\n\nIII MerLin as a benchmark-driven reproduction platform\n\nIII-A Kernel methods\nIII-B Recurrent Architectures for Sequential Data\nIII-C Convolutional Neural Network\nIII-D Reservoir Computing\nIII-E Training paradigms for QNN\nIII-F Image generation\nIII-G Benchmark Tasks for Model Expressivity and Robustness\n\n\nIV Discussion &amp; Conclusion\n\n\n\n\n\n\n\\DeclareCaptionType\nlisting[Code Block][List of Listings]\n\n\nMerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning\n‚Ä†‚Ä†thanks: This research was supported by MITACS Accelerate (projects IT45761 and IT36314), the UFOQO Project, financed by the French State as part of France 2030, the European Union‚Äôs Horizon Europe research and innovation programme under grant agreement No 101130384 (QUONDENSATE) and the QuantERA programme through the project ResourceQ.\n\n\n\n\n\nCassandre Notton16,\nBenjamin Stott26,\nPhilippe Schoeb13,\nAnthony Walsh2,\nGr√©goire Leboucher24,\nVincent Espitalier2,\nVassilis Apostolou2,\nLouis-F√©lix Vigneux15,\nAlexia Salavrakos2,\nJean Senellart2\n\n\n\nAbstract\nIdentifying where quantum models may offer practical benefits in near-term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open-source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates optimized strong simulation of linear-optical circuits into standard PyTorch and scikit-learn workflows, enabling end-to-end differentiable training of quantum layers.\nMerLin is designed around systematic benchmarking and reproducibility. As an initial contribution, we reproduce eighteen state-of-the-art photonic and hybrid QML works spanning kernel methods, reservoir computing, convolutional and recurrent architectures, generative models, and modern training paradigms. These reproductions are released as reusable, modular experiments that can be directly extended and adapted, establishing a shared experimental baseline consistent with empirical benchmarking methodologies widely adopted in modern artificial intelligence.\nBy embedding photonic quantum models within established machine learning ecosystems, MerLin allows practitioners to leverage existing tooling for ablation studies, cross-modality comparisons, and hybrid classical‚Äìquantum workflows. The framework already implements hardware-aware features, allowing tests on available quantum hardware while enabling exploration beyond its current capabilities, positioning MerLin as a future-proof co-design tool linking algorithms, benchmarks, and hardware.\n\n\n\nI Introduction and Motivation\n\n\nThe convergence of quantum computing and machine learning is driving significant advances in both theoretical research and practical applications, with QML offering the potential to accelerate and extend the capabilities of classical algorithms. Among the various quantum platforms, photonic quantum computing is particularly promising due to its scalability, robustness, compatibility with optical communication technologies, and energy efficiency [12, 13, 30, 45]. Building on these advantages, photonic QML exploits the bosonic nature of light and high-dimensional multi-mode interference to implement and train machine learning models directly on this unconventional photonic quantum computation model, enabling intrinsic parallelism and efficient exploration of large Hilbert spaces.\nRealizing this potential in practice requires software frameworks that connect abstract QML models to their execution on emerging photonic and other quantum hardware, enabling identical learning tasks to be studied across heterogeneous platforms. By embedding trainable quantum circuits into standard AI pipelines, such frameworks support hybrid quantum‚Äìclassical workflows suited to the near-term regime, where quantum structure can be empirically explored as a source of representations, kernels, and inductive biases beyond classical models. Consequently, progress in photonic QML increasingly depends on scalable, benchmark-driven experimentation rather than isolated algorithmic proposals‚Äîmirroring the empirical paradigm that underpins modern AI.\n\n\nWithin this landscape, quantum software frameworks differ markedly in both their level of abstraction and the computational paradigms they support. Hardware-oriented frameworks, such as Qiskit [18] or Cirq [5], provide mature ecosystems for superconducting processors. By contrast, Pulser [44] targets neutral-atom platforms, while Perceval [13] offers state-of-the-art discrete-variable (DV) simulation backends and direct hardware access to Quandela‚Äôs photonic processors. As for Strawberry Fields [21] and Piquasso [23], they focus on continuous-variable (CV) paradigms.\n\n\nOther software frameworks focus on tools for QML and optimization such as differentiable quantum programming and PyTorch111PyTorch, the PyTorch logo and any related marks are trademarks of The Linux Foundation.\n integration, like PennyLane [1], TorchQuantum [47] and Qiskit-Torch-Module [32]. More recently, DeepQuantum [12] emerged as a unified platform bridging qubit circuits, photonic qumodes (Fock, Gaussian, Bosonic backends), and measurement-based quantum computing within PyTorch, reporting GPU-accelerated gradient computation an order of magnitude faster than PennyLane at scale.\n\n\nThis abundance of platforms results in a software landscape which is rich, but fragmented. Each framework specialises in a particular layer or paradigm, creating silos where algorithms are not portable without significant conversion effort. Most of the QML literature is built on the gate-based paradigm. However, a growing body of work considers algorithms tailored to hardware, within which photonic QML is gaining traction. A systematic survey found that photonic contributions currently account for ‚âà\\approx6% of QML publications, with most being simulator-based¬†[36]. We confirmed this through independent searches222arXiv abstract search: 188 photonic QML papers vs. 3,592 QML papers; lens.org yields 6.1%..\n\n\nRecent works demonstrate the potential for photonic QML using a variety of platforms. Ref. [9] explores variational algorithms which project data and training parameters to the high-dimensional Fock space using NLopt¬†[9], while another work ¬†[48] utilizes DisCoPy [7] to explore Fock-space encodings for kernel methods. Moreover, additional algorithms, such as quantum reservoir computing implemented with Perceval and Keras ¬†[40, 39], and quantum convolutional neural networks using QOptCraft [35, 10] have been simulated on photonic hardware. Across this surveyed literature, each of these contributions relies on a different ad-hoc software stack, because no existing framework currently combines efficient simulation, integration with ML workflows, noise models and hardware access.\n\n\nTo address this gap, we present MerLin: a framework for photonic QML that builds on Perceval‚Äôs optimised DV simulation and hardware access while adding native PyTorch and scikit-learn integration for end-to-end differentiable training.\n\n\nSimultaneously, we designed MerLin as a benchmarking- and reproduction-first platform, providing a unified, hardware-aware platform for implementing, training, and evaluating photonic QML models. Indeed, there has been a need for rigorous benchmarking in the QML community ‚Äì called for in [2, 36] ‚Äì which can be linked to several factors: an extreme heterogeneity in data preprocessing and task formulation in the literature, low code availability preventing independent verification of most results333Code availability hovers around 27% for gate-based and 43% for photonic QML papers., and a preference for single-run metrics rather than multi-dimensional evaluations.\n\n\nWe addressed this need by developing a dedicated reproduction framework, designed to enable systematic and ready-to-use replication of published QML works, including reported claims, performance metrics, and experimental analyses, within a controlled software environment built on top of MerLin. As an initial demonstration of this methodology, we reproduced key results from eighteen state-of-the-art photonic and hybrid QML works, spanning expressivity analyses, quantum kernels, reservoir computing, and convolutional architectures. These reproductions validate MerLin‚Äôs correctness, and provide a starting point for future contributions.\n\n\nCrucially, the objective of this benchmarking effort is not only to identify positive performance gains, but also to understand their origin, and to disentangle improvements due to data preprocessing, model engineering, or optimization strategies from those arising from genuinely new representational or computational mechanisms.\n\n\nThe MerLin framework and the companion reproduction repository are publicly available at https://github.com/merlinquantum/merlin and https://github.com/merlinquantum/reproduced_papers.\n\n\n\n\nII MerLin architecture\n\n\nFigure 1: MerLin architecture for photonic quantum machine learning. (A) Classical data encoding and photonic circuit configuration define the quantum model. (B) MerLin integrates PyTorch-based optimization with photonic-native execution through a logical-to-photonic bridge, differentiable quantum layers, and hardware-oriented simulation, with optional inference on Quandela photonic QPUs. (C) Measurement strategies and detectors behaviour expose full quantum states or partially measured observables. (D) The resulting amplitudes or probability distributions are returned as class"
  },
  {
    "title": "Can Large Language Models Make Everyone Happy?",
    "url": "https://arxiv.org/abs/2602.11091v1",
    "source": "arxiv",
    "summary": "Misalignment in Large Language Models (LLMs) refers to the failure to simultaneously satisfy safety, value, and cultural dimensions, leading to behaviors that diverge from human expectations in real-world settings where these dimensions must co-occur. Existing benchmarks, such as SAFETUNEBED (safety-centric), VALUEBENCH (value-centric), and WORLDVIEW-BENCH (culture-centric), primarily evaluate the",
    "full_text": null
  },
  {
    "title": "Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates",
    "url": "https://arxiv.org/abs/2602.11090v1",
    "source": "arxiv",
    "summary": "Neural PDE surrogates are often deployed in data-limited or partially observed regimes where downstream decisions depend on calibrated uncertainty in addition to low prediction error. Existing approaches obtain uncertainty through ensemble replication, fixed stochastic noise such as dropout, or post hoc calibration. Cross-regularized uncertainty learns uncertainty parameters during training using ",
    "full_text": null
  },
  {
    "title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.11089v1",
    "source": "arxiv",
    "summary": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \\emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering",
    "full_text": null
  },
  {
    "title": "General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies",
    "url": "https://arxiv.org/abs/2602.11087v1",
    "source": "arxiv",
    "summary": "Offline RL algorithms aim to improve upon the behavior policy that produces the collected data while constraining the learned policy to be within the support of the dataset. However, practical offline datasets often contain examples with little diversity or limited exploration of the environment, and from multiple behavior policies with diverse expertise levels. Limited exploration can impair the ",
    "full_text": null
  },
  {
    "title": "First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges",
    "url": "https://arxiv.org/abs/2602.11086v1",
    "source": "arxiv",
    "summary": "Biometric footstep recognition, based on a person's unique pressure patterns under their feet during walking, is an emerging field with growing applications in security and safety. However, progress in this area has been limited by the lack of large, diverse datasets necessary to address critical challenges such as generalization to new users and robustness to shifts in factors like footwear or wa",
    "full_text": null
  },
  {
    "title": "GRASP: group-Shapley feature selection for patients",
    "url": "https://arxiv.org/abs/2602.11084v1",
    "source": "arxiv",
    "summary": "Feature selection remains a major challenge in medical prediction, where existing approaches such as LASSO often lack robustness and interpretability. We introduce GRASP, a novel framework that couples Shapley value driven attribution with group $L_{21}$ regularization to extract compact and non-redundant feature sets. GRASP first distills group level importance scores from a pretrained tree model",
    "full_text": null
  },
  {
    "title": "Token-Efficient Change Detection in LLM APIs",
    "url": "https://arxiv.org/abs/2602.11083v1",
    "source": "arxiv",
    "summary": "Remote change detection in LLMs is a difficult problem. Existing methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.\n  Our approach hinges on specific inputs we call Border Inputs, for which there exists mo",
    "full_text": "\n\n\n\n\n1 Introduction\n\nContributions.\n\n\n\n2 System model\n\nChange detection.\nTypes of changes.\nSingle-token perspective.\n\n\n\n3 Theoretical analysis and guidelines\n\n3.1 Rationale\n\n3.2 The Local Asymptotic Normality Framework\n\nTakeaway.\n\n\n3.3 Decomposition for LLMs: exploiting the last layer\n\n3.4 The Zero-Temperature Limit\n\nTakeaway.\n\n\n\n\n\n4 B3IT: Black-Box Border Input Tracking\n\n4.1 The B3IT Initialization Stage\n4.2 The B3IT Detection Stage\n\n4.3 Analysis: Type-I and Type-II Errors\n\nExperimental guidelines\n\n\n\n4.4 B3IT is Optimal for k=2k=2\n\nTakeaway.\n\n\n\n\n\n5 Experimental Evaluation of B3IT\n\nOn the existence of BIs.\n\n5.1 In-Vitro Evaluation: TinyChange Benchmark\n\n\n5.1.1 Experimental Setup\n\nModels.\nPerturbation types.\nBaselines.\nMetrics.\nTest statistic.\n\n\n5.1.2 Initialization Stage\n\n5.1.3 Experimental Results\n\nHyperparameter selection.\nComparison with baselines.\nPerformance by change difficulty.\n\n\n\n\n\n5.2 In-Vivo Evaluation: Commercial LLM APIs\n\n5.2.1 Endpoint Selection\n5.2.2 Prevalence of Border Inputs\n5.2.3 Continuous Monitoring Results\n\n\n\n\n\n6 Related Work\n\nChange detection of models behind APIs.\nNon-determinism in LLM inference.\nLLM fingerprinting.\n\n\n7 Conclusion\n8 Acknowledgments\nA Notations\n\nB Proofs\n\n\nB.1 Log-likelihood ratio in the LAN regime\n\nFirst-order term.\nSecond-order term.\nConclusion of the proof.\n\n\nB.2 Proof of Theorem¬†3.1\nB.3 Proof of Lemma¬†3.2\nB.4 Proof of Theorem¬†3.3\nB.5 Effect of the last layer\nB.6 Proof of Theorem¬†4.1\nB.7 Proof of Theorem¬†4.2\nB.8 Proof of Theorem¬†4.3\n\n\n\nC Derivation of Optimal Sampling Budget\n\nC.1 Problem Setup\nC.2 Cost Function Derivation\nC.3 Comparison of Strategies\nC.4 Optimal Condition for m=4m=4\n\n\n\nD Experimental addendum\n\nD.1 Models used in the TinyChange Benchmark\nD.2 Models used for the Border Input prevalence study\nD.3 Endpoints used for B3IT tracking\nD.4 Selecting minimum-length prompts\nD.5 TinyChange experimental parameters and methodology\nD.6 Parameter sweeps\n\nD.7 Misc\n\nNote on OLMo.\n\n\nD.8 Full TinyChange difficulty scales\n\n\n\n\n\n\n\nToken-Efficient Change Detection in LLM APIs\n\n\nTimoth√©e Chauvin\n\n‚ÄÉ‚ÄÉ\nCl√©ment Lalanne\n\n‚ÄÉ‚ÄÉ\nErwan Le Merrer\n\n‚ÄÉ‚ÄÉ\nJean-Michel Loubes\n\n‚ÄÉ‚ÄÉ\nFran√ßois Ta√Øani\n\n‚ÄÉ‚ÄÉ\nGilles Tredan\n\n\n\nAbstract\nRemote change detection in LLMs is a difficult problem.\nExisting methods are either too expensive for deployment at scale, or require initial white-box access to model weights or grey-box access to log probabilities. We aim to achieve both low cost and strict black-box operation, observing only output tokens.\nOur approach hinges on specific inputs we call Border Inputs, for which there exists more than one output top token.\nFrom a statistical perspective, optimal change detection depends on the model‚Äôs Jacobian and the Fisher information of the output distribution. Analyzing these quantities in low-temperature regimes shows that border inputs enable powerful change detection tests.\nBuilding on this insight, we propose the Black-Box Border Input Tracking (B3IT) scheme. Extensive in-vivo and in-vitro experiments show that border inputs are easily found for non-reasoning tested endpoints, and achieve performance on par with the best available grey-box approaches. B3IT reduces costs by 30√ó30\\times compared to existing methods, while operating in a strict black-box setting.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLLM APIs are increasingly embedded in high-impact applications, such as software engineering (Zheng et al., 2025), yet users have no guarantee that the model behind an endpoint remains unchanged. Providers may modify models for safety updates, cost optimization, or infrastructure changes, often without notice. Quietly deploying quantized versions to reduce costs, for instance, could degrade performance on tasks users depend on. Such changes are not hypothetical: Grok on X suffered three incidents in 2025 where modified system prompts were deployed due to rogue employees or bad updates (Babuschkin, 2025; xAI, 2025a, b). Anthropic experienced infrastructure bugs in 2025 that degraded Claude‚Äôs responses, which remained undetected for weeks despite affecting 16% of Claude Sonnet 4 requests at peak (Anthropic, 2025).\n\n\nSeveral change-detection methods have been proposed (Gao et al., 2025; Bai et al., 2025; Gubri et al., 2024; Chauvin et al., 2026). However, they face a fundamental tension between access requirements and cost. White-box methods like ESF (Bai et al., 2025) and TRAP (Gubri et al., 2024) craft sensitive inputs using model internals (embeddings, gradient-based optimization), limiting their use to known models. Grey-box methods like LT (Chauvin et al., 2026) require log-probabilities, which are available only on a fraction of public endpoints. Fully black-box approaches like MET (Gao et al., 2025) avoid these requirements but demand many queries to the tracked model, making continuous monitoring prohibitively expensive.\n\n\nWe introduce B3IT (Black-Box Border Input Tracking), a change-detection method that operates in a strict black-box setting, observing only output tokens, while achieving accuracy and cost efficiency comparable to grey-box approaches. Our key insight, brought by a theoretical analysis, is that at low temperature, inputs where two tokens are nearly tied as the top prediction (‚Äúborder inputs‚Äù) become extremely sensitive detectors. In other words, even tiny model changes can dramatically alter the output distribution. We show theoretically that this sensitivity diverges as temperature tends to zero,\nexhibiting a phase transition between detectable and undetectable regimes.\n\n\nB3IT exploits this phenomenon: it discovers border inputs through black-box sampling, then monitors for changes by comparing output distributions. Surprisingly, border inputs are easy to find on most production models. The result is a method that detects subtle changes at 1/301/30th the cost of alternatives, enabling large-scale continuous and cost-effective API monitoring.\n\n\nContributions.\n\n(1) We establish theoretical foundations for change detection in LLMs, revealing a phase transition at low temperature that enables high-sensitivity detection without model access.\n(2) We propose B3IT, a practical black-box method that identifies border inputs and uses them for efficient change detection.\n(3) We validate B3IT in vitro on TinyChange (Chauvin et al., 2026), achieving detection performance comparable with grey-box methods at 1/301/30th the cost of the next best black-box baselines.\n(4) We demonstrate B3IT in vivo on 93 endpoints (64 models, 20 providers), showing broad applicability for continuous monitoring.\n\n\nRoadmap. Section¬†2 describes our system model. Section¬†3 presents the theoretical analysis underpinning the B3IT method, which we describe in Section¬†4 and evaluate in Section¬†5.\n\n\nCode. All our code is open-source at github.com/timothee-chauvin/token-efficient-change-detection-llm-apis.\n\n\n\n\n\n2 System model\n\nIn this paper, we take the perspective of an unprivileged user facing a black-box API exposing an LLM fŒ∏f_{\\theta}, where Œ∏‚ààŒò\\theta\\in\\Theta denotes the (unknown) model parameters. The user can only submit queries (x,T)(x,T) for some input x‚ààùí≥x\\in\\mathcal{X} and temperature T‚àà‚Ñù+T\\in\\mathbb{R}^{+}, and collect the answer fŒ∏‚Äã(x,T)f_{\\theta}(x,T).\n\n\nChange detection.\n\nWe consider a two-stage interaction: an initialization stage and a detection stage. Let Œ∏0\\theta_{0} denote the model parameters during initialization and Œ∏1\\theta_{1} during detection. The change detection problem consists in deciding whether Œ∏0=Œ∏1\\theta_{0}=\\theta_{1}, as in prior work¬†(Gao et al., 2025; Chauvin et al., 2026). Formalized as a hypothesis test, let H0:=Œ∏0=Œ∏1H_{0}:=\\theta_{0}=\\theta_{1} (the model hasn‚Äôt changed) versus H1:=Œ∏0‚â†Œ∏1H_{1}:=\\theta_{0}\\neq\\theta_{1} (the model has changed).\n\n\nIn this endeavor, we are interested in two key quantities:\n\n\n‚Ä¢\n\nDetector accuracy: we want to minimize false positives (incorrectly detecting a change) and false negatives (missing actual changes). Formalizing a test as a (possibly randomized) function œï\\phi taking values in [0,1][0,1], we therefore seek to minimize ‚Ñô‚Äã(œï=1|H0)+‚Ñô‚Äã(œï=0|H1)\\mathbb{P}(\\phi=1|H_{0})+\\mathbb{P}(\\phi=0|H_{1}).\n\n\n\n‚Ä¢\n\nCost: for wide-scale monitoring of LLM APIs, detecting changes should induce a minimal cost. This is especially true of the detection stage, which is run repeatedly.\n\n\n\n\n\n\nTypes of changes.\n\nDeployed LLMs can undergo many kinds of modifications: fine-tuning (full or Low-Rank Adaptation¬†(Hu et al., 2022)), quantization, model distillation, system prompt changes, or further Reinforcement Learning. Infrastructure can also affect outputs: CUDA versions, GPU selection, compiler optimizations, or routing errors directing requests to misconfigured servers¬†(Anthropic, 2025). The intensity of any such modification determines detection difficulty; our focus is on query-efficient detection, regardless of the change‚Äôs origin or intensity.\n\n\nFor the theoretical analysis, we model parameter changes as Œ∏1:=Œ∏0+œµ‚Äãh\\theta_{1}:=\\theta_{0}+\\epsilon h where h‚ààùïäq‚àí1h\\in\\mathbb{S}^{q-1} is a unit direction and œµ\\epsilon controls magnitude. This local perturbation model enables tractable analysis via the Local Asymptotic Normality framework (Section¬†3). In experiments, we directly evaluate on realistic updates (fine-tuning, pruning, etc.) across a range of change magnitudes, and on live LLM APIs.\n\n\n\nSingle-token perspective.\n\nOur analysis focuses on the first output token from a single query. While approaches such as MET¬†(Gao et al., 2025) leverage multiple output tokens, inter-token dependencies complicate the analysis substantially. However, multiple queries from different prompts can be combined for improved performance, which we demonstrate experimentally.\n\n\n\n\n\n3 Theoretical analysis and guidelines\n\nBuilding on optimal change detection in the sense of Neyman‚ÄìPearson (Neyman and Pearson, 1933), we exhibit a new phase-transition phenomenon for LLMs (Theorem¬†3.3). At temperature T‚â™1T\\ll 1, inputs where two or more tokens share the maximal lo"
  }
]