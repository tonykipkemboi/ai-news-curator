[
  {
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "url": "https://arxiv.org/abs/2602.06043v1",
    "source": "arxiv",
    "summary": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on d",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Method\n\nProblem Setting\n\n3.1 Motivation\n\nMotivation\n\n\n\n3.2 Share: Continual Shared Subspace Adaptation\n\n3.2.1 Step 1 - Initialization\n3.2.2 Step 2 - Continual Adaptation\n3.2.3 Step 3 - Merging and Finetuning\n3.2.4 Ablation: How to decide kk, pp and œÜ\\varphi?\n\n\n3.3 Theoretical Analysis\n\n\n\n4 Experimental Evaluation\n\n\n4.1 Continual Natural Language Understanding\n\nSetup\n\n\n\n4.2 Continual Image Classification\n\nSetup\n\n\n\n4.3 Continual 3D Object Pose Estimation\n\nSetup\n\n\n4.4 Text to Image Generation\n\n4.5 Continual Asynchronous Learning and Serving of LoRAs at Scale\n\nSetup\n\n\n\n\n5 Conclusion\n6 FAQs\n\n7 Share: Shared Subspace Adaptation\n\n7.1 Theoretical Analysis\n\n\n\n8 Experiments\n\n\n8.1 Continual Natural Language Understanding\n\nHyperparameters\n\n\n\n8.2 Continual Image Classification\n\nHyperparameters\n\n\n8.3 Continual Text-to-Image Generation\n8.4 Continual Asynchronous Learning and Serving of LoRAs at Scale\n8.5 Computational Complexity\n\n8.6 Ablation\n\nHow to decide k,p,œÜk,p,\\varphi?\n8.6.1 Limitations\n\n\n\n\n9 Future Work and Broader Impact\n\n\n\n\n\nShared LoRA Subspaces for almost Strict Continual Learning\n\n\nPrakhar Kaushik‚Äâ‚Äâ, Ankit Vaidya11footnotemark: 1, Shravan Chaudhari, Rama Chellappa, Alan Yuille \n\nDepartment of Computer Science\nJohns Hopkins University\nBaltimore, MD, USA \n{pkaushi1,schaud35,avaidya7,rchella4,ayuille1}@jhu.edu\nhttps://toshi2k2.github.io/share/\nequal contributionCorresponding author: prakhark2@gmail.com\n\n\nAbstract\nAdapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into\nthis evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100√ó parameter reduction and 281√ó memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.\n\n\n\n1 Introduction\n\nFigure 1: Evidence of a Shared Foundational Subspace in Continual Learning. Linear CKA similarity analysis reveals a universal weight subspace (orange) emerging during sequential learning. Three independent trajectories (red, green, blue), starting from different GLUE task subsets, show monotonic convergence to this shared subspace, reaching near-perfect alignment (&gt;0.95&gt;0.95) by task T=5T=5. Shaded regions show standard deviation across experiments. These results demonstrate: (1) the existence of a common foundational weight subspace that efficiently encodes cross-task knowledge, and (2) our method‚Äôs ability to discover it through continual adaptation without catastrophic forgetting. This convergence reveals how low-rank adapters naturally bias models toward shared weight structures that generalize across diverse tasks.\n\n\nFigure 2: Share. Our continual reparameterization where only principal coefficients œµt\\epsilon^{t} are trained. a. Initialization We initialize the principal factors (Œ±0,Œ≤0\\alpha^{0},\\beta^{0}) of our Share model using available LoRA¬†[9] adapters (A,BA,B). b. Continual Adaptation Few top œÜ‚â™k\\varphi\\ll k factors, shown as Œ±0‚Üí1,Œ≤0‚Üí1\\alpha^{0\\to 1},\\beta^{0\\to 1}, and temporary coefficients œµ0‚Üí1\\epsilon^{0\\to 1} are fine-tuned when new data is incrementally received. Merging &amp; Fine-tuning The factors Œ±0,Œ≤0\\alpha^{0},\\beta^{0} and temporary factors Œ±0‚Üí1,Œ≤0‚Üí1\\alpha^{0\\to 1},\\beta^{0\\to 1} are merged using the initialization procedure, and Œ±1,Œ≤1,œµŒ±,Œ≤i‚Äã‚àÄi‚àà[0,1]\\alpha^{1},\\beta^{1},\\epsilon^{i}_{\\alpha,\\beta}\\quad\\forall i\\in[0,1] are analytically recalculated. œµ1\\epsilon^{1} can then be further fine-tuned to boost performance.\n\n\nAdapting large pretrained models, like LLMs, VLMs and Diffusion models, for continual learning presents significant challenges, notably catastrophic forgetting and the substantial resources required for retraining. Traditional fine-tuning methods often require retraining all model parameters, leading to inefficiencies, especially as model sizes increase. As these models scale, they require more resources and memory, making them inaccessible to ordinary researchers and increasing environmental impact¬†[22]. Parameter-efficient finetuning techniques, such as LoRA¬†[9], address some of these issues by introducing trainable low-rank matrices into each layer of the model, effectively reducing the number of parameters that need adjustment during finetuning. However, while LoRA reduces computational demands, it lacks mechanisms for continual learning and knowledge integration, often requiring separate adapters for each task, which can be inefficient and hinders cross-task knowledge sharing which improves robustness and domain generalization¬†[30, 29].\n\n\nRecent advancements have explored integrating parameter-efficient tuning with continual learning strategies. For instance, methods like O-LoRA¬†[41] propose learning new tasks in orthogonal subspaces to mitigate forgetting. However, these approaches do not fully leverage shared knowledge across tasks, limiting forward (and backward) knowledge transfer, as they require training individual models, or experts, for new tasks. All such methods fall short of Strict Continual Learning¬†[13], which requires models to learn continually, without data replay, additional models, or increase in model size, much like humans. Our work tries to remedy this.\n\n\nRecently, Universal Weight Subspace Hypothesis¬†[12] has proven that neural network weights often converge to layerwise, shared subspace across tasks and datasets, which can be employed for efficient training, inference and model merging. Method like EigenLoRAx¬†[15] have applied this concept for very efficient finetuning achieving equal or better performance to LoRA at fraction of the cost. However, ¬†[15] extract the shared subspace beforehand, and the question of continually improving or learning the shared ‚Äùuniversal‚Äù subspace is left unanswered. In this work, we show, with theoretical analysis, that our simple method, Share, is capable of approximating the shared subspace in an almost strict continual setup.\n\n\nIn this paper, we introduce Share, a novel approach to Parameter-Efficient Continual Finetuning (PaCT) that learns and dynamically updates a shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that captures core knowledge from past tasks and incrementally integrates new information by identifying and expanding essential subspace directions. Each new task is projected into this evolving subspace, facilitating forward knowledge transfer, while older knowledge is analytically reprojected to minimize catastrophic interference. Interestingly, we also observe instances of backward knowledge transfer due to presence of this subspace. This approach achieves up to 100√ó parameter reduction and 281√ó memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, 3D object pose estimation, natural language understanding, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.\n\n\nTo our knowledge, Share is among the earliest works to present a viable solution for Parameter-Efficient Continual Finetuning (for almost strict continual learning) applicable to diverse and complex models.\n\n\nOur main contributions are as follows:\n\n\n‚Ä¢\n\nWe introduce a replay-free, (almost strict) continual learning method for large pretrained models that leverages a shared, low-rank foundational subspace of adapters to achieve compute and memory-efficient learning.\n\n\n\n‚Ä¢\n\nShare enables continual learning from hybrid streams of both data and LoRA adapters, seamlessly merging information into a single model.\n\n\n\n‚Ä¢\n\nShare requires orders of magnitude fewer trainable parameters (up to 100√ó reduction) for finetuningand offers up to 281√ó memory savings compared to traditional LoRA methods.\n\n\n\n‚Ä¢\n\nA single set of continually learned Share principal factors can replace hundreds of LoRA adapters, facilitating scalable and efficient model deployment.\n\n\n\n‚Ä¢\n\nWe demonstrate Share‚Äôs applicability across various models and modalities, including image classification, 3D object pose estimation, natural language understanding, commonsense reasoning, math reasoning, and text-to-image generative models.\n\n\n\n\n\nThese contributions position Share as a scalable and practical solution for efficient continual learning in large-scale AI systems, addressing critical needs in the deployment of adaptable machine learning models.\n\n\n\n\n2 Related Work\n\nEfficient Replay Free Continual Learning‚ÄÉWhile continual learning addresses catastrophic forgetting¬†[6], its application to large models remains challenging, pa"
  },
  {
    "title": "Pseudo-Invertible Neural Networks",
    "url": "https://arxiv.org/abs/2602.06042v1",
    "source": "arxiv",
    "summary": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear ",
    "full_text": null
  },
  {
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "url": "https://arxiv.org/abs/2602.06039v1",
    "source": "arxiv",
    "summary": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditi",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 LLM-Based Multi-Agent Collaboration\n2.2 Selective and Dynamic Communication Topologies\n\n\n\n3 Methods\n\n3.1 Preliminaries\n\n3.2 Per-Round Agent Execution\n\n3.2.1 Single-Pass Inference and Descriptor Generation\n3.2.2 Synchronization Barrier and Context Update\n\n\n\n3.3 Dynamic Topology via Semantic Matching\n\n3.3.1 Semantic Alignment Quantification\n3.3.2 Sparse Graph Construction\n3.3.3 Topology Adaptation and Routing Semantics\n\n\n\n3.4 Topology-Aware Message Ordering\n\n3.4.1 Dependency Graph Definition\n3.4.2 Adaptive Topological Sequencing\n\n\n\n3.5 Meta-Control and Workflow Orchestration\n\n3.5.1 Global State Aggregation\n3.5.2 Manager Policy and Halting\n3.5.3 Bi-Level Feedback Loop\n\n\n\n\n\n4 Experiments\n\n4.1 Datasets\n4.2 Settings\n\n\n\n5 Results\n\n5.1 Main results across benchmarks\n5.2 Effect of communication rounds\n5.3 Topology evolution and interpretability\n5.4 Ablation on Q-K similarity threshold\n\n\n6 Conclusion\n\nA Complexity Analysis\n\nA.1 Computational Complexity\n\n\n\nB Implementation Details\n\n\nB.1 System Prompts and Templates\n\nB.1.1 Code Generation Agents\nB.1.2 Mathematical Reasoning Agents\n\n\nB.2 Model Configurations\n\n\nC Pseudo-code for DyTopo\n\nD Additional Experimental Results\n\nD.1 Token Usage and Latency Analysis\n\n\n\nE Qualitative Analysis\n\nE.1 Topology Evolution Trace: A Case Study\nE.2 Examples of Semantic Key-Query Matching\n\n\n\n\n\n\n\nDyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching\n\n\nYuxing Lu\n\n‚ÄÉ‚ÄÉ\nYucheng Hu\n\n‚ÄÉ‚ÄÉ\nXukai Zhao\n\n‚ÄÉ‚ÄÉ\nJiuxin Cao\n\n\n\nAbstract\nMulti-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager‚Äôs round goal, each agent outputs lightweight natural-language query (need) and key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nMulti-agent systems built from prompted large language models have become a practical paradigm for multi-round reasoning¬†(Tran et al., 2025; Li et al., 2024). By instantiating several role-specialized LLM agents and allowing them to interact over multiple rounds, these systems can iteratively refine partial solutions, cross-check intermediate steps, and integrate complementary skills¬†(W√∂lflein et al., 2025; Yu et al., 2025). This collaboration style is particularly well suited to complex problem domains, where effective reasoning emerges from the coordinated interplay of specialized agents and from their abilities to collectively revise earlier assumptions as new evidence or errors are uncovered.\n\n\nA central yet often under-specified aspect of multi-agent reasoning is the communication structure: which agents exchange information with whom, and when¬†(Goldman and Zilberstein, 2003). Many existing pipelines default to a fixed, trajectory-wide interaction pattern (e.g., broadcast discussion or scripted turn-taking), effectively reusing the same topology across all rounds. However, multi-round reasoning is stage-dependent: early rounds tend to benefit from broad exploration and shared problem framing, whereas later rounds require selective, high-precision exchanges to diagnose failures and converge on a coherent solution¬†(Liu et al., 2024). This suggests that communication topology should be an adaptive object, conditioned on the round-level goal, rather than a static design choice.\n\n\nFigure 1: Comparison of communication topologies. (A) Single-agent prompting. (B) Fixed-topology communication reused across rounds. (C) DyTopo dynamically rewires a directed agent graph each round based on the round goal and semantic relevance.\n\n\nWe propose DyTopo, a manager-guided multi-round multi-agent framework in which the manager specifies a round-level goal and determines whether to terminate the interaction. Given the round goal and the agents‚Äô self-described information needs and capabilities, DyTopo induces a directed communication graph at each round and routes messages only along the activated links. This allows the system to shift from broad exploration to targeted verification as reasoning progresses.\n\n\nA key ingredient enabling DyTopo‚Äôs round-by-round routing is a semantic key-query matching scheme across agents. Conditioned on the manager‚Äôs round goal, each agent provides short natural-language descriptors that summarize what it can provide to others (a ‚Äúkey‚Äù) and what it currently seeks (a ‚Äúquery‚Äù). DyTopo semantically matches queries to keys to induce the directed communication graph for each round, and routes messages only along the activated links. This decouples what agents generate from how their information is routed, enabling communication patterns that adapt over rounds. This topology-driven routing yields two advantages. First, it improves collaboration by organizing information flow around the current round goal rather than a static neighborhood. Second, it provides an interpretable coordination trace: edges are activated based on explicit descriptors and semantic relevance, so the evolving graphs can be inspected to reveal how pathways reconfigure over time and which patterns correlate with success or failure.\n\n\nWe evaluate DyTopo on multi-round code generation and mathematical reasoning tasks, comparing against single-agent prompting, multi-agent baselines with fixed or random communication topologies, and strong recent agentic frameworks. The results show that dynamic communication topologies consistently improve task performance and remain robust under different experiment settings. We further characterize the method through analyses of performance versus the number of rounds, qualitative visualizations of topology evolution over time, and ablations over the semantic matching hyperparameters that control link activation.\n\n\n\n\n2 Related Work\n\n\n2.1 LLM-Based Multi-Agent Collaboration\n\nA growing line of work studies how to compose multiple prompted LLM instances into a cooperative system via natural-language interaction. Early frameworks emphasize role specialization and structured dialogue: CAMEL proposes role-playing agents guided by inception prompting to autonomously collaborate on tasks¬†(Li et al., 2023), while AutoGen provides a programmable framework for building applications from multiple conversable agents with customizable interaction patterns¬†(Wu et al., 2024). MetaGPT further incorporates human-inspired standardized operating procedures (SOPs) to coordinate multiple role agents and reduce cascading errors in long workflows¬†(Hong et al., 2023). Complementary to role-based cooperation, multi-agent deliberation improves reasoning and factuality by having multiple model instances propose and critique solutions over multiple rounds¬†(Du et al., 2023). Finally, agent systems are often coupled with tools or external models, where an LLM acts as a controller that decomposes tasks and delegates to specialized executors (Shen et al., 2023). While these approaches demonstrate gains from collaboration, they typically rely on fixed or dense communication patterns, leaving open how to adaptively route information among agents at inference time.\n\n\n\n\n2.2 Selective and Dynamic Communication Topologies\n\nSelective communication has long been studied in multi-agent learning and neural routing. In cooperative MARL, targeted messaging methods such as TarMAC learn what to communicate and whom to address, enabling multi-round coordination with interpretable communication patterns¬†(Das et al., 2019). In large-scale neural architectures, conditional computation and routing activate only a small subset of experts per token to scale capacity efficiently¬†(Fedus et al., 2022), and content-based sparse attention constructs query-dependent sparse interaction patterns among tokens¬†(Roy et al., 2021). Recently, these principles have been adapted to LLM-based agent teams to reduce redundant interactions and design task-aware connectivity. AgentPrune identifies communication redundancy in multi-agent pipelines and prunes low-value messages on the induced spatio-temporal message-passing graph¬†(Zhang et al., 2024a). Beyond pruning, G-Designer generates task-conditioned agent communication topologies¬†(Zhang et al., 2024b), and GTD casts topology synthesis as a guided diffusion process to optimize performance-cost-robustness trade-offs¬†(Jiang et al., 2025). Our work complements this direction by studying an explicitly interpretable inference-time routing mechanism: agents output textual Need and Offer descriptors and a directed topology is constructed each round via semantic similarity, enabling controlled multi-round message passing and topology-level analysis.\n\n\n\n\n\n3 Methods\n\nFigure 2: DyTopo round-by-round routing via semantic matching. At each round tt, each worker agent outputs a query and a key descriptor. A semantic matching module embeds these descriptors, computes pairwise similarity, and induces a directed graph G(t)G^{(t)}. Private messages produced at round tt are routed according to G(t)G^{(t)} after a synchronization barrier and are appended to recipients‚Äô memories for round t+1t{+}1. The Manager provides round goals and updates the next-round context, yielding a closed-loop adaptation across rounds.\n\n\nWe formalize DyTopo as a Dynamic Computation Graph (DCG), ùí¢={G(t)}t=0T‚àí1\\mathcal{G}=\\{G^{(t)}\\}_{t=0}^{T-1}, where TT is"
  },
  {
    "title": "CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction",
    "url": "https://arxiv.org/abs/2602.06038v1",
    "source": "arxiv",
    "summary": "To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, e",
    "full_text": "\n\n\n\nI Introduction\n\nII Related Work\n\nII-A LLM-based Decentralized Multi-Agent Cooperation\nII-B Conformal Prediction and Calibration\n\n\nIII Problem Formulation\n\nIV Method\n\nIV-A LLM-Based Object Relevance Reasoning\nIV-B Message Generation with Conformal Prediction\nIV-C Exploration with Communication\nIV-D Confidence Check\nIV-E Stop Criterion\n\n\n\nV Experiments\n\nV-A MM-EQA Benchmark\nV-B Evaluation Metrics and Baselines\nV-C Implementation Details\nV-D Results and Analysis\n\n\nVI Conclusion\n\n\n\n\n\nCommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction\n\n\n\nXiaopan Zhang‚àó, Zejin Wang‚àó, Zhixu Li, Jianpeng Yao, Jiachen Li‚Ä°\n‚àóEqual contribution  ‚Ä°Corresponding authorX. Zhang, Z. Wang, Z. Li, J. Yao, and J. Li are with the Trustworthy Autonomous Systems Laboratory at the University of California, Riverside, CA, USA. {xzhan006, jiachen.li}@ucr.edu.\n\n\nAbstract\nTo complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects.\nReal-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively.\nBeyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments.\nTo address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy.\nTo address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA.\nOur framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability.\nTo evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions.\nExperimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.\n\n\n\nI Introduction\n\n\nModern service robots are designed to understand human instructions and complete tasks in real-world household environments (e.g., ‚ÄúTurn off the TV if it is currently on,‚Äù ‚ÄúBring the red pillow from the living room to the bedroom‚Äù). This process involves interpreting natural language commands, generating and answering relevant questions for scene understanding and reasoning (e.g., ‚ÄúIs the TV turned on?‚Äù ‚ÄúWhat is the color of the pillow?‚Äù), and manipulating target objects accordingly.\nA crucial step is answering these questions, a task known as Embodied Question Answering (EQA)¬†[1], which requires robots to efficiently explore the 3D environment from a random starting location and actively gather information until a confident answer can be provided.\nPrior studies have investigated this in single-agent settings [1, 2, 3, 4, 5]. In contrast, we envision future households with multiple heterogeneous service robots, each with distinct capabilities and non-transferable assignments. While they cannot take over each other‚Äôs tasks, they can access all generated questions and share observations and interpretations to enhance exploration efficiency. We define this cooperative information-gathering setting as a multi-agent multi-task EQA (MM-EQA) problem, a novel challenge that facilitates multi-robot collaboration in real-world scenarios.\n\n\nFigure 1: In a household setting, robots exchange observations and reasoning to collaboratively complete their assigned tasks. Each agent generates confident and goal-directed messages using calibrated outputs from LLMs. The bottom-left image shows a bird‚Äôs-eye view of Robot 1‚Äôs navigation path after incorporating information received from Robot 2. The top-right sequence captures both robots‚Äô camera views at different timestamps.\n\n\nWhile existing single-agent EQA solutions can be adapted to a multi-agent setting by having each robot work independently, this naive approach is inefficient.\nCommunication enables mutual assistance, improving exploration and increasing the likelihood of faster task completion.\nHowever, uncalibrated communication could hinder efficiency by sharing irrelevant or misleading information.\nTherefore, it is critical to ensure that messages are accurate and pertinent to the recipient‚Äôs tasks.\nThis work tackles the MM-EQA problem by designing a communication framework that enhances multi-agent exploration efficiency and task performance.\n\n\nLarge language Models (LLMs) have shown great potential in solving EQA tasks due to their remarkable ability to understand natural-language queries, reason, and provide answers in natural language¬†[6].\nIn the context of MM-EQA, natural language is an ideal communication protocol, as LLMs are inherently trained to engage in dialogues. Several LLM-based communication methods have been proposed in other domains¬†[7, 8], but these cannot be directly adapted to our MM-EQA setting.\nAdditionally, LLMs often produce miscalibrated and overconfident outputs¬†[9], which can result in irrelevant or misleading information.\nThis can hinder cooperation efficiency, as agents may share inaccurate data, reducing overall exploration effectiveness¬†[10].\n\n\nOur work tackles this challenge and develops an LLM-based communication framework for MM-EQA. Our key insight is that an agent should only communicate information it confidently deems relevant to its partner agents‚Äô tasks (see Fig. 1).\nWe propose CommCP, a novel decentralized LLM-based communication framework that employs conformal prediction (CP) [11, 12] to calibrate the confidence of LLM‚Äôs outputs.\nOur framework ensures that the outputs generated by LLMs are more reliable and reduces the negative impact of irrelevant or misleading information to partner agents, ultimately enhancing the overall task performance and efficiency of the multi-agent system.\nTo evaluate our proposed framework, we create a novel MM-EQA benchmark based on realistic scenarios and the Habitat-Matterport 3D (HM3D) dataset [13].\nThe experimental results show that our approach enhances the task success rate and shortens completion time by a large margin over baselines.\n\n\nThe main contributions of this paper are as follows:\n\n\n‚Ä¢\n\nWe formulate the information-gathering process of completing assignments provided in natural language as a novel multi-agent multi-task embodied question answering (MM-EQA) problem, where multiple robots work as a team, handling EQA tasks in a shared environment and communicating to exchange information or answers.\n\n\n\n‚Ä¢\n\nWe propose CommCP, a novel LLM-based decentralized communication framework for MM-EQA, where conformal prediction is employed to calibrate the generated messages to reduce distractions to other agents and improve communication reliability and efficiency.\n\n\n\n‚Ä¢\n\nWe create a novel MM-EQA benchmark with photo-realistic scenarios from the HM3D dataset to validate the effectiveness of the proposed framework. This benchmark is released to facilitate future studies.\n\n\n\n\n\n\n\nII Related Work\n\n\n\nII-A LLM-based Decentralized Multi-Agent Cooperation\n\n\nLLM-based multi-agent cooperation has gained increasing attention recently [14, 15], with various systems developed for multi-agent tasks [16, 17, 18, 7, 19, 20].\nUnlike single-agent or centralized systems, decentralized cooperative systems involve peer-to-peer communication, where agents interact directly, an architecture common in world simulation applications [21, 22].\nIn these systems, communication typically takes the form of natural language text generated by LLMs, with content varying by application, such as sharing environmental observations, coordinating actions, or reallocating tasks.\nHowever, the effectiveness of LLM-generated communication remains underexplored. As noted in [6], decentralized communication often incurs costs such as bandwidth limitations or delays. Thus, agents must communicate efficiently and avoid unnecessary or redundant messages. Current approaches lack mechanisms to assess communication quality and rely solely on raw LLM outputs, leading to inefficiencies, especially when agents act on incomplete or uncertain information.\n\n\n\n\nII-B Conformal Prediction and Calibration\n\n\nRecent research has highlighted the miscalibration issue in LLMs, where models may exhibit overconfidence or under-confidence in their text outputs.\nThis presents a huge challenge as foundation models are applied to embodied tasks where agents may have miscalibrated confidence in their decisions. Previous work [12, 23] has employed conformal prediction [11] to formally quantify an LLM‚Äôs uncertainty in a robot planning context, which ensures that the robot‚Äôs plans are executed with calibrated confidence.\nExplore until Confident¬†[2] extends this approach by applying multi-step conformal prediction in EQA tasks to determine when the VLM is sufficiently confident when a visual language model (VLM) is sufficiently confident to stop exploration.\nTo our best knowledge, we are the first to employ conformal prediction to enhance multi-agent communication through calibrating confidence during collaborative exploration, which is a setting not addressed by prior work.\n\n\n\n\n\nIII Problem Formulation\n\n\nConsider a scenario where NaN_{a} robots are deployed in a 3D scene with multiple different assignments, each starting from an initial pose g0ig^{i}_{0}, and aiming to answer the questions q1:Nqiq^{i}_{1:N_{q}} related to its assignments.\nThe objective is to maximize the success rate while minimizing the exploration time, with all answers required within a time horizon TmaxT_{\\text{max}}.\nEach robot "
  },
  {
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "url": "https://arxiv.org/abs/2602.06036v1",
    "source": "arxiv",
    "summary": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential an",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 Speculative Decoding\n2.2 Diffusion Language Models\n2.3 Diffusion-based Speculative Decoding\n\n\n\n3 Preliminaries\n\n3.1 Speculative Decoding Speedup\n3.2 Autoregressive vs. Diffusion Drafting\n\n\n\n4 Method\n\n4.1 Inference\n4.2 Training\n\n\n\n5 Experiments\n\n5.1 Instruct Models\n5.2 Reasoning Models\n5.3 Performance on SGLang\n\n5.4 Ablation Study\n\n5.4.1 Training Data\n5.4.2 Number of Draft Layers\n5.4.3 Number of Target Hidden Features\n5.4.4 Training-Inference Time Block Size\n\n\n\n\n6 Conclusion\n\nA Appendix\n\nA.1 Training Implementation\nA.2 Diffusion Drafter without Target Feature\n\nA.3 Further Ablations\n\nA.3.1 Loss Decay\nA.3.2 Random Sampling of Masked Blocks\n\n\n\n\n\n\n\n\n\nDFlash: Block Diffusion for Flash Speculative Decoding\n\n\nJian Chen\n\n‚ÄÉ‚ÄÉ\nYesheng Liang\n\n‚ÄÉ‚ÄÉ\nZhijian Liu\n\n\n\nAbstract\nAutoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM. However, existing methods still rely on autoregressive drafting, which remains sequential and constrains practical speedups.\nDiffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models.\nIn this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. We show that speculative decoding provides a natural and effective setting for diffusion models. By generating draft tokens in a single forward pass, DFlash enables efficient drafting, and by conditioning the draft model on context features extracted from the target model, it achieves high-quality drafts with higher acceptance rates.\nExperiments show that DFlash achieves over 6√ó\\times lossless acceleration across a range of models and tasks, delivering up to 2.5√ó\\times higher speedup than the state-of-the-art speculative decoding method EAGLE-3.\nLinks:‚Äâ Code (GitHub) || Models (Hugging Face)\n\n\nhttps://z-lab.ai/projects/dflash\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have enabled a wide range of powerful applications, including conversational agents¬†(Yang et al., 2025; Guo et al., 2025) and automated programming tools. Despite their success, LLM inference remains dominated by a sequential, token-by-token generation process, where each output depends on the full preceding context. This inherent seriality creates a major performance bottleneck: inference is slow, memory-bound, and fails to fully utilize modern GPUs. With the recent emergence of long Chain-of-Thought (CoT) reasoning models (OpenAI et al., 2024; Guo et al., 2025), this bottleneck has become increasingly critical, as prolonged inference times now dominate the generation process.\n\n\nSpeculative decoding (Leviathan et al., 2023; Li et al., 2025c, 2024, b; Cai et al., 2024) has emerged as a primary solution to this bottleneck. This paradigm employs a lightweight draft model to speculate a sequence of future tokens, which are then verified in parallel by the large target model. While this approach achieves lossless acceleration and has been widely integrated into production frameworks, state-of-the-art methods like EAGLE-3 (Li et al., 2025b) still rely on autoregressive drafting. This serial drafting process is not only inherently inefficient but also susceptible to error accumulation, which effectively caps achievable speedups at approximately 2‚àí-3√ó\\times.\n\n\nRecently, Diffusion LLMs (dLLMs) (Nie et al., 2025) offer a promising alternative to autoregressive LLMs by enabling parallel text generation and bidirectional context modeling. Block diffusion models (Arriola et al., 2025; Cheng et al., 2025; Wu et al., 2025) can denoise a block of masked tokens simultaneously. However, current open-source dLLMs typically underperform their autoregressive counterparts in terms of generation quality. Furthermore, maintaining acceptable output quality often necessitates a high number of denoising steps, which significantly diminishes their raw inference speed (Qian et al., 2026).\n\n\nThis landscape reveals a critical trade-off: autoregressive models deliver superior performance but suffer from sequential latency, while diffusion models allow for fast, parallel generation but often at the cost of accuracy. A natural research question follows: Can we combine the strengths of both paradigms while mitigating their respective weaknesses? A compelling solution lies in leveraging diffusion models for high-speed, parallel drafting, while relying on high-quality autoregressive models for verification to ensure the final output remains lossless.\n\n\nFigure 1: Speedup comparison between DFlash, EAGLE-3 against Autoregressive Decoding on Qwen3-8B¬†(Yang et al., 2025) with the Transformers backend. Overall, DFlash achieves more than 2.5√ó higher speedup than EAGLE-3.\n\n\nHowever, utilizing diffusion for drafting is non-trivial, and existing methods are either impractical or offer limited speedups. Methods such as DiffuSpec¬†(Li et al., 2025a) and SpecDiff-2¬†(Sandler et al., 2025) utilize massive (e.g., 7B parameter) draft models. This significant memory footprint is often prohibitively expensive for real-world serving. Furthermore, while these large drafters offer relatively high quality draft tokens and acceptance lengths, the high drafting latency limits their practical speedups to a modest 3‚àí-4√ó\\times. In contrast, PARD (An et al., 2025) trains small autoregressive models to mimic diffusion-style parallel generation, and then perform speculative decoding for target LLMs. However, the resulting small models lack the modeling capacity of the target LLMs, leading to limited acceptance lengths and a speedup ceiling of approximately 3√ó\\times.\n\n\nIs there truly ‚Äúno free lunch‚Äù? Can we build a diffusion drafter that is both lightweight and highly accurate?\n\n\nIn this paper, we introduce DFlash, a speculative decoding framework that uses a lightweight block diffusion model to achieve both fast and high-quality drafting. Our key insight is simple: the target knows best. As observed by Samragh et al. (2025), large autoregressive LLMs‚Äô hidden features implicitly contain information about multiple future tokens. DFlash utilizes these hidden features as context, conditioning the draft model to predict future blocks of tokens in parallel. In effect, the draft model becomes a diffusion adapter that efficiently leverages the deep context features modeled by the large target model. Instead of requiring a tiny draft model to reason from scratch, DFlash fuses the reasoning capabilities of the target model with the parallel generation speed of a small diffusion drafter.\n\n\nWe evaluate DFlash across a wide range of models and benchmarks, and demonstrate its practical benefits under realistic serving setups using SGLang¬†(Zheng et al., 2024). As shown in Figure¬†1, DFlash achieves up to a 6.1√ó\\bm{\\times} speedup on Qwen3-8B¬†(Yang et al., 2025), and is nearly 2.5√ó\\bm{\\times} faster than the state-of-the-art EAGLE-3 across most benchmarks. We believe DFlash represents a significant step forward in accelerating LLM inference and democratizing high-performance AI.\n\n\n\n\n2 Related Work\n\n\n2.1 Speculative Decoding\n\nSpeculative decoding accelerates LLM inference by mitigating the sequential bottleneck of autoregressive generation. Early methods¬†(Leviathan et al., 2023) employ a smaller draft model to propose token sequences that are verified in parallel by a larger target model. Medusa¬†(Cai et al., 2024) eliminates the external draft model by augmenting the base LLM with multiple prediction heads and using tree attention for parallel verification. The EAGLE series¬†(Li et al., 2025c, 2024, b) further improves speculative decoding by exploiting feature-level context from the frozen target model. EAGLE-1 predicts future hidden-state distributions to boost acceptance rates, EAGLE-2 introduces adaptive drafting trees, and EAGLE-3 refines training objectives to scale speedups.\n\n\nDespite these advances, most existing methods rely on autoregressive drafting, which remains inherently sequential, limiting their speedups.\n\n\n\n\n2.2 Diffusion Language Models\n\nDiffusion large language models (dLLMs) offer an alternative to autoregressive generation by predicting masked tokens in parallel. LLaDA¬†(Nie et al., 2025) was the first to scale dLLMs to billions of parameters, achieving performance comparable to LLaMA-3.1-8B¬†(Grattafiori et al., 2024). However, fully parallel diffusion models suffer from fixed-length generation and lack efficient KV cache support. Block diffusion models¬†(Arriola et al., 2025) address these issues by denoising sequences block-by-block, blending parallelism with autoregressive structure. Building on this idea, Fast-dLLM v2¬†(Wu et al., 2025) and SDAR¬†(Cheng et al., 2025) adapt pre-trained autoregressive LLMs into block-diffusion variants, enabling parallel generation while preserving generation quality on specific tasks. Nevertheless, existing dLLMs generally underperform state-of-the-art autoregressive models and often require many denoising steps, which limits their practical inference speed.\n\n\n\n\n2.3 Diffusion-based Speculative Decoding\n\nRecent work explores using diffusion models as drafters within speculative decoding. TiDAR¬†(Liu et al., 2025) jointly trains diffusion and autoregressive objectives, enabling parallel ‚Äúthinking‚Äù via diffusion and sequential ‚Äútalking‚Äù via autoregressive decoding, though final generation quality is not yet lossless.\n\n\nOther approaches repurpose autoregressive models for diffusion-style drafting. Samragh et al. (2025) observe that autoregressive LLMs implicitly encode future-token information and train a LoRA adapter to enable parallel drafting, while retaining the base model for verification.\n\n\nDiffuSpec¬†(Li et al., 2025a) and SpecDiff-2¬†(Sandler et al., 2"
  },
  {
    "title": "Can vision language models learn intuitive physics from interaction?",
    "url": "https://arxiv.org/abs/2602.06033v1",
    "source": "arxiv",
    "summary": "Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an envi",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Methods\n\n3.1 Datasets\n3.2 Tasks\n\n3.3 Fine-Tuning Methods\n\nGroup-Relative Policy Optimization\nSupervised Fine-Tuning\nPEFT Hyperparameters\n\n\n3.4 Reward Functions\n\n\n\n4 Results\n\n4.1 Post-training performance improvement\n4.2 Generalization to related tasks\n4.3 Generalization to real images\n4.4 Decodability analysis\n\n4.5 Ablations\n\n4.5.1 Other models\n4.5.2 Other RL implementations\n\n\n\n\n5 Discussion\n6 Conclusion\n\nA Appendix\n\n\nA.1 Data examples\n\nA.1.1 Top block dataset\nA.1.2 Side block dataset\nA.1.3 Lerer dataset\n\n\nA.2 Main result tables\nA.3 Reward function visualisation\nA.4 Task Prompts\n\nA.5 Training logs\n\nA.5.1 Qwen3-VL-8B\nA.5.2 Qwen2.5-VL-7B\n\n\nA.6 Decoding analysis\nA.7 Attention maps\nA.8 Bigger model and another RL implementation\n\nA.9 Additional checks on Qwen2.5-VL-7B\n\nA.9.1 Additional supervised fine-tuning\nA.9.2 Longer training horizon\nA.9.3 Training ablations\nA.9.4 Blocked and interleaved joint training\n\n\nA.10 Generalization to real images\n\n\n\n\n\n\n\nCan vision language models learn intuitive physics from interaction?\n\n\nLuca M. Schulze Buschoff\n\n‚ÄÉ‚ÄÉ\nKonstantinos Voudouris\n\n‚ÄÉ‚ÄÉ\nCan Demircan\n\n‚ÄÉ‚ÄÉ\nEric Schulz\n\n\n\nAbstract\nPre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nA central goal of machine learning research is to build machines that think and behave like people do. Lake et al. (2017) propose that human-like machine learning models must be capable of reasoning about their environment and its physical, social, and causal structure. These capabilities are often referred to as intuitive theories (Baillargeon et al., 1995; Spelke, 1990; Spelke and Kinzler, 2007). Here, we focus on intuitive physics ‚Äî the ability to understand and predict the physical properties and interactions of objects (Battaglia et al., 2012; Piloto et al., 2022).\n\n\nRecent work has established that vision language models (VLMs), models that receive visual and textual inputs, are still limited in their understanding of the physical world and its causal structure (Jin et al., 2023; Balazadeh et al., 2024). VLMs do not perform well on standard visual cognition tasks ‚Äî such as tasks testing intuitive physics ‚Äî and they do not show a good fit with human behavioral data (Schulze Buschoff et al., 2025a). While supervised fine-tuning enables models to perform well on the tasks they were fine-tuned on, they do not appear to learn generalizable intuitions about the physical world (Schulze Buschoff et al., 2025b).\n\n\nA prominent idea in cognitive science is that humans learn a robust understanding of their world by interacting with it (Gibson, 1979; Merleau-Ponty, 1945; Varela et al., 1991). The key claim is that humans learn robust, generalizable concepts for explaining and predicting their world not merely from passive observation and symbolic abstraction, but from actively interacting with their environment‚Äôs dynamics (Barsalou, 1999; Clark, 1998). Some have argued that directly experimenting with the physical properties of objects in the environment allows children to test their hypotheses about their environment (Gopnik et al., 1999). In contrast to passively observing the interactions of other people with an environment, they learn much more from trying, and often failing, to predict how the environment will evolve given their own actions (Smith, 1982; Chu and Schulz, 2020; Nicolopoulou, 1993; Smith and Gasser, 2005; Schulz and Bonawitz, 2007). While the important role of interaction is slowly being recognized in generative model training (Silver and Sutton, 2025; Motamed et al., 2025), its merit for teaching vision language models visual cognitive abilities such as intuitive physics has not yet been explored.\n\n\nIn this paper, we present a first attempt at evaluating the role of interaction for learning intuitive physics in VLMs. Interaction can be operationalized in several ways (Shapiro and Spaulding, 2025), from one- and multi-step reinforcement learning (RL) to multi-sensory robotics. We operationalize interaction in the context of one-step RL, defining an environment, action space, and reward function (Sutton et al., 1998). VLMs are presented with an image of a stack of colored blocks generated by a physics engine. They must for example respond with an action sequence to move another block to build a taller, stable tower, receiving a reward that depends on the stability of the resulting tower.\n\n\n\nWe compare models that are trained to build towers through trial-and-error (the interactive condition) with models that are shown examples of optimal action sequences to build stable towers (the non-interactive condition). Similarly to how children appear to learn generalizable physical intuitions by playing with objects (Piaget, 1952), we propose that learning to build towers through interaction with the physics of the environment will enable VLMs to learn those same intuitions.\n\n\nFollowing this line of argument, we hypothesize the following:\n\n\n1.\n\nModels in the interactive condition will generalize better to building new towers not seen in their training data, compared to the non-interactive condition.\n\n\n\n2.\n\nModels in the interactive condition will generalize better to a new task, such as judging the stability of a tower, compared to the non-interactive condition.\n\n\n\n\n\nWe test these hypotheses mainly by evaluating the textual outputs of VLMs. However, it is possible that models might have the knowledge required to solve the task, but cannot produce textual outputs in the right format. We explore this distinction between model competence and model performance (Chomsky, 1965) by decoding model activations layer-wise to see how predictive they are of key physical quantities. We thus further hypothesize that these quantities will be more decodable at later model layers in models trained in the interactive condition compared to the non-interactive condition.\n\n\nWe find no noticeable differences between the interactive and non-interactive conditions, both in and outside of the training tasks. Both methods yield models that perform at ceiling on the tasks they are trained on, but neither method produces models that reliably generalize to new physical tasks. While we find that physical quantities like tower stability are highly decodable from model activations, neither post-training method successfully converts this competence into reliable performance on new tasks.\n\n\n\n\n2 Related Work\n\nDespite recent advances in architectures and training methods, VLMs continue to struggle on simple visual tasks that are trivial for any human observer, such as counting objects in a scene or making judgements about their interactions (Rahmanzadehgervi et al., 2024; Schulze Buschoff et al., 2025a; Balazadeh et al., 2024). Campbell et al. (2024) suggest that these failures originate from the fact that the test images contain multiple objects whose higher-order relations must be tracked. There is evidence that pre-trained VLMs struggle to attend to and distinguish multiple objects at the same time (Frankland et al., 2021).\n\n\nSupervised fine-tuning (SFT) has emerged as an efficient way to overcome limitations such as these through extensive post-training on specific problems (Han et al., 2024). These methods have also proven useful for aligning models towards more human-like outputs (Binz et al., 2024; Hussain et al., 2024). However, SFT with VLMs appears to have a limited effect on their ability to learn generalizable physical intuitions (Schulze Buschoff et al., 2025b) and interact reliably with physical environments (Mecattaf et al., 2024). One plausible hypothesis is that SFT simply allows VLMs to learn useful shortcuts for specific tasks (Geirhos et al., 2020). Indeed, Motamed et al. (2025) argue that large generative video models are likely making predictions about the physical world without a proper understanding of its underlying physics. They suggest that a lack of active interaction with the physical world could be the limiting factor. Our study therefore seeks to explore whether models‚Äô understanding of physics can be improved through active interaction with an environment.\n\n\nIn line with this proposal, online reinforcement learning (RL), a paradigm in which models learn through interaction with an environment, has recently been argued to generalize more robustly than SFT (Chu et al., 2025), an analogue of offline RL (Wu et al., 2025). Online RL refers to updating the model sequentially based on actions it has taken in an environment, whereas offline RL refers to updating the model based on a fixed set of state-action pairs collected using another policy (Levine et al., 2020). Online RL has been shown to outperform offline RL methods in some cases (Ostrovski et al., 2021), but this distinction has been under-explored in the context of VLMs. Chu et al. (2025) train VLMs on arithmetic reasoning and simple navigation tasks and find that online RL trained models generalize better than models trained with SFT. In contrast to Chu et al. (2025), our work focuses on established intuitive physics tasks in cognitive science: building and judging the"
  },
  {
    "title": "PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling",
    "url": "https://arxiv.org/abs/2602.06030v1",
    "source": "arxiv",
    "summary": "Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviora",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\n2.1 LLM-Based Multi-Agent Simulation Frameworks\n2.2 ABM for Social and Health Dynamics\n2.3 Graph and Behavior-Aware Clustering Methods\n\n\n\n3 Methodology\n\n3.1 PhysicsAgentABM Architecture\n3.2 ANCHOR: Agent-Driven Multi-Stage Clustering\n3.3 Decoupled Population-to-Entity Simulation\n\n\n\n4 Experiments\n\n4.1 Experimental Setup\n\n4.2 Results\n\n4.2.1 ANCHOR Cluster Interpretation\n4.2.2 Quantitative Evaluation\n\n4.2.3 Qualitative Evaluation\n\nCOVID-19 Dynamics:\nMarket Sentiment Diffusion:\nAttention Lifecycle:\n\n\n\n\n4.3 Case Study: Singapore COVID-19 Circuit Breaker\n\n\n\n5 Cost and Scalability Analysis\n\n\n5.1 Ablation\n\nPhysicsAgentABM Architecture:\nANCHOR Component Analysis:\n\n\n\n\n6 Conclusion\nA Additional Details on Singapore COVID-19 Case Study:\n\nB Additional Details on ANCHOR\n\nB.1 Algorithm\nB.2 Additional Ablations\n\n\nC PhysicsAgentABM Calibration Analysis\nD Symbolic Reasoning Pathway\nE Neural Multimodal Pathway\n\nF Experimental Setup &amp; Design Principles\n\n\nF.1 Task Overview: Population-Level State Inference under Partial Observability\n\nObserved Inputs.\nLatent Quantities of Interest.\nModeling Objective.\nGround Truth Definition:\nUnified Task Abstraction:\n\n\n\nF.2 Dataset Overview\n\nSummary of Datasets:\nTemporal Resolution and Regime Dynamics.\nObserved Signals and Supervision.\nUnified Experimental Framing.\n\n\n\nF.3 Population &amp; Network Synthesis\n\nAgent Population:\nInteraction Graph:\n\n\n\nF.4 Latent State and Ground Truth Construction\n\nLatent State Space.\nGround Truth Signals:\n\n\n\n\n\n\n\n\n\nPhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling\n\n\nKavana Venkatesh\n\n‚ÄÉ‚ÄÉ\nYinhan He\n\n‚ÄÉ‚ÄÉ\nJundong Li\n\n‚ÄÉ‚ÄÉ\nJiaming Cui\n\n\n\nAbstract\nLarge language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to ‚àº\\sim6‚Äì8√ó\\times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.\n\n\n\n1 Introduction\n\nFigure 1: \nOverview of PhysicsAgentABM Architecture.\nInference is performed at the cluster level via symbolic and neural pathways with uncertainty-aware fusion, followed by stochastic agent-level realization. ANCHOR enables behaviorally coherent abstraction.\n\n\n\nMany complex systems of scientific and societal importance, such as infectious disease spread (Deng et al., 2020; Pastor-Satorras et al., 2015), social diffusion (Qiu et al., 2018; Kempe et al., 2003), financial contagion (Xu et al., 2021; Xiang et al., 2022), and infrastructure failure cascades (Kipf et al., 2018; Fang et al., 2021) are governed by state-dependent interactions unfolding across multiple scales (Veliƒçkoviƒá et al., 2017; Battaglia et al., 2018). Individual entities do not evolve in isolation: their behaviors are shaped by local interactions, shared group-level influences, and broader contextual signals such as policies, norms, or collective risk perception (Ying et al., 2018; Lowe et al., 2017). Accurately simulating such systems therefore requires models that can jointly capture mechanistic structure, data-driven dynamics, symbolic context, and uncertainty while remaining scalable and interpretable.\n\n\nAgent-based models (ABMs) provide a principled foundation by explicitly modeling interacting entities through mechanistic transition rules, yielding interpretability and emergent behavior (Epstein, 2012; Bonabeau, 2002). However, classical ABMs rely on hand-crafted, static rules and coarse calibration, limiting adaptation to heterogeneous populations, multimodal signals, and non-stationary regimes (Pastor-Satorras et al., 2015). Learned transition models, including probabilistic graphical models, neural state-space models, and graph neural networks have been incorporated to improve flexibility (Murphy, 2012; Krishnan and others, 2017; Kipf and Welling, 2017; Hamilton and others, 2017; Rossi et al., 2020), but often obscure mechanistic structure and exhibit poor uncertainty calibration under distribution shift (Ovadia et al., 2019). More recently, large language models (LLMs) have enabled generative agent-based models (GABMs) with rich symbolic reasoning and memory (J. S. Park, J. O‚ÄôBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein (2023); 22; 60). Parallel lines of work integrate LLMs into ABM pipelines for social simulation and conversational dynamics (38; 1; 2). Yet most LLM-based GABMs perform reasoning at the level of individual agents, incurring high computational cost, weak grounding in relational data, and unreliable stochastic behavior in the absence of principled uncertainty modeling (10; 48).\n\n\nTaken together, existing approaches reveal two fundamental gaps in generative agent-based modeling. First, inference is performed at the level of individual agents (Park et al., 2023; Gao et al., 2024) despite strong group-level, institutional, and contextual forces that drive coherent population dynamics (Granovetter, 1978; Centola, 2018). Second, symbolic reasoning and neural learning are combined heuristically (Manhaeve et al., 2018; Yi et al., 2018) rather than treated as complementary epistemic sources with explicit uncertainty modeling. Real-world systems make these gaps explicit: epidemic dynamics shift coherently within communities (Chang et al., 2021; Block et al., 2020), financial institutions co-move through shared exposures (Billio et al., 2012; Diebold and Yƒ±lmaz, 2014), and social groups align around common narratives. Capturing such structure requires population-level inference that produces calibrated priors for individual realization (Gelman et al., 1995; Ovadia et al., 2019), rather than isolated agent simulation.\n\n\nWe introduce PhysicsAgentABM, a hierarchical neuro-symbolic framework that addresses these challenges by redefining inference in generative agent-based models. PhysicsAgentABM elevates inference from individual agents to adaptive agent clusters, where shared dynamics, contextual influences, and uncertainty are modeled explicitly. Cluster-level predictions define probabilistic transition priors (Tran et al., 2017; Fortuin and R√§tsch, 2019), while individual agents stochastically realize state transitions conditioned on local attributes and neighborhood context, preserving heterogeneity without sacrificing population-level coherence (Figure¬†1). Within each cluster, a state-specialized symbolic reasoning layer coordinated by a meta-agent encodes mechanistic constraints and regime context, while a multimodal neural transition model (Baltru≈°aitis et al., 2018) captures temporal and interaction-driven regularities. These pathways are treated as distinct epistemic hypotheses and reconciled through uncertainty-aware fusion (H√ºllermeier and Waegeman, 2021) to yield calibrated, population-consistent dynamics.\n\n\nFigure 2: \nANCHOR Overview.\nAn overview of our clustering mechanism.\n\n\n\nA critical enabler of this hierarchy is clustering itself. In generative agent-based models, clustering must be semantically meaningful, transition-faithful, and adaptive to evolving dynamics. Classical graph clustering methods optimize structural criteria while ignoring behavioral and dynamical semantics (Schaeffer, 2007; Loukas, 2019), whereas existing LLM-based simulators (Park et al., 2023; Hong et al., 2023; Qian et al., 2024) use LLMs to execute agent behavior rather than to reason about abstraction. We therefore introduce ANCHOR, a novel LLM-agent-driven clustering mechanism in which symbolic agents act as semantic controllers of abstraction (Huang et al., 2022), organizing populations based on evolving cross-contextual behavioral similarity (Newman, 2006; Yang et al., 2009), interaction structure, and state-transition tendencies under graph constraints. By using LLMs to control where and how abstraction occurs rather than to simulate individual agents, ANCHOR enables scalable symbolic reasoning aligned with downstream probabilistic transition modeling. Our contributions are threefold:\n\n\n‚Ä¢\n\nWe introduce PhysicsAgentABM, a hierarchical neuro-symbolic framework for cluster-level inference via state-specialized symbolic and neural reasoning.\n\n\n\n‚Ä¢\n\nWe propose ANCHOR, the first LLM-agent-anchored clustering method that treats abstraction as a semantic control problem for transition-faithful simulation.\n\n\n\n‚Ä¢\n\nWe introduce a calibrated, timestep-aligned paradigm that decouples population-level inference from agent-level realization for robust long-horizon dynamics.\n\n\n\n\n\n\n\n2 Related Work\n\n\n2.1 LLM-Based Multi-Agent Simulation Frameworks\n\nLLMs have enabled generative multi-agent systems with language-based reasoning and coordination (Wang et al., 2024; Guo et al., 2024). Frameworks such as MetaGPT and ChatDev demonstrate role-specialized collaboration (Hong et al., 2023; Qian et al., 2024), while Generative Agents and AgentScope model large-scale social behavior (Park et al., 2023; Gao et al., 2024). Related work studie"
  },
  {
    "title": "AP-OOD: Attention Pooling for Out-of-Distribution Detection",
    "url": "https://arxiv.org/abs/2602.06031v1",
    "source": "arxiv",
    "summary": "Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes b",
    "full_text": null
  },
  {
    "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
    "url": "https://arxiv.org/abs/2602.06029v1",
    "source": "arxiv",
    "summary": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertain",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Preliminaries\n\n2.1 Bayesian Optimization\n2.2 Bayesian Experimental Design\n\n\n3 Active Inference Bridges Learning and Optimization\n4 Performance Evaluation in Learning and Optimization\n\n5 Self-Consistent Learning\n\n5.1 Interpretation of Assumptions\n5.2 Interpretation of Convergence Rate\n5.3 Practical Usability and Implications\n\n\n\n6 No-Regret Optimization\n\n6.1 Interpretation of Assumptions\n6.2 Interpretation of Cumulative Regret Bound\n6.3 Practical Usability and Implications\n\n\n\n7 Experiments for Theorem Validation\n\n7.1 Discrete Sandbox\n7.2 1D GP Bandit\n\n\n\n8 Experiments on Real-World Problems\n\n8.1 Constrained System Identification\n8.2 Composite Bayesian Optimization\n\n\n9 Conclusion and Limitation\nA Proof of Theorem 5.1\nB Proof of Theorem 6.1\n\nC Experimental Details\n\nC.1 Discrete Sandbox\nC.2 1D GP Bandit\n\nC.3 Constrained System Identification\n\nC.3.1 Plume Field Model and Parameters\nC.3.2 Task-Specific Configurations\n\n\n\nC.4 Composite Bayesian Optimization\n\nC.4.1 Heuristic Estimation through Preference Learning\nC.4.2 Task-Specific Configurations\n\n\n\n\n\n\n\n\n\nCuriosity is Knowledge: \nSelf-Consistent Learning and No-Regret Optimization with Active Inference\n\n\nYingke Li\n\n‚ÄÉ‚ÄÉ\nAnjali Parashar\n\n‚ÄÉ‚ÄÉ\nEnlu Zhou\n\n‚ÄÉ‚ÄÉ\nChuchu Fan\n\n\n\nAbstract\nActive inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement‚Äîsufficient curiosity‚Äîsimultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic‚Äìpragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.\n\nActive inference, Bayesian consistency, Regret bound\n\n\n\n1 Introduction\n\nA fundamental challenge in sequential decision-making under uncertainty is balancing exploration and exploitation: an agent must gather information to reduce uncertainty while simultaneously pursuing task objectives.\nThis tension leaves a primary challenge in classical Bayesian formulations. Bayesian Optimization (BO) is primarily goal-seeking: it selects actions to maximize reward (or minimize cost) under uncertainty, typically emphasizing exploitation tempered by uncertainty-driven exploration (Shahriari et al., 2016; Frazier, 2018). In contrast, Bayesian Experimental Design (BED) is information-seeking: it selects experiments to maximize information gain about unknown parameters, often without an explicit performance objective (Rainforth et al., 2023). However, real systems rarely permit a clean separation between these two modes; instead, seeking knowledge and achieving goals are often deeply intertwined objectives. As a sequel, a principled agent should unify goal-seeking and information-seeking within a single decision rule.\n\n\nRecently, active inference (AIF) (Friston, 2010; Friston et al., 2017) has arisen as a promising way to offer such a unification. By minimizing the Expected Free Energy (EFE), AIF casts action selection as a single variational objective that naturally decomposes into an epistemic term (expected information gain) and a pragmatic term (expected regret). These two terms are balanced by a coefficient called curiosity, which sets the trade-off between learning and optimization.\nDespite the empirical success of this new paradigm (Li et al., 2026), the formal role of curiosity in guaranteeing coherent learning and efficient optimization remains incomplete.\nIf curiosity is too low, the agent can become myopic, prematurely exploiting, and failing to resolve uncertainty. On the contrary, if curiosity is too high, the agent may be devoted to unnecessary exploration, discarding the goals, and incurring catastrophic regret.\n\n\nThis raises a critical dilemma in AIF that we formally addressed in this paper:\n\nWhen can minimizing EFE guarantee both self-consistent learning (i.e., posterior convergences to the truth) and no-regret optimization (i.e., with bounded cumulative regret)?\n\n\n\nWe establish the first theoretical guarantee of posterior consistency and bounded cumulative regret for agents minimizing EFE.\nWe prove that these two fundamental properties are not merely compatible but are jointly ensured by a single, shared mechanism: sufficient curiosity.\nThat is, we derive a lower bound of the curiosity coefficient that can prevent the epistemic value from being dominated by the pragmatic term, and prove that EFE minimization can achieve both self-consistent learning and no-regret optimization simultaneously when the curiosity coefficient is larger than this lower bound. Specifically, our contributions are threefold:\n\n\nFirst, we prove that posterior consistency is guaranteed under the sufficient curiosity condition through Theorem¬†5.1. This theorem provides a sample-complexity bound that explicitly depends on prior entropy, model discriminability, and curiosity.\n\n\nSecond, we establish a general regret bound for AIF with Gaussian Process (GP) settings in Theorem¬†6.1. This bound holds for generalized heuristic estimators and regret functions, formally linking convergence rates to smoothness, heuristic alignment, and the same sufficient curiosity condition. In particular, the classical BO-style regret analysis is a special case of our general regret bound.\n\n\nThird, we translate these theories into practical design guidelines. Using both synthetic and real-world problems, we provide concrete prescriptions for the epistemic-pragmatic balance, hyperparameter selection, and objective function design for empirical applications.\n\n\nNotably, we show that the requirements to ensure self-consistent learning and no-regret optimization depend on the same sufficient curiosity condition. This finding elevates curiosity from an ad-hoc exploration heuristic into an intrinsic regularizer that couples belief updating and decision-making. Therefore, this work reveals an essential fact that, a sufficient curiosity is the critical mechanism enabling simultaneous self-consistent learning and no-regret optimization, in other words, curiosity is knowledge.\n\n\n\n\n2 Preliminaries\n\n\n2.1 Bayesian Optimization\n\nGiven an unknown objective function f:ùí≥‚Ü¶‚Ñúf:\\mathcal{X}\\mapsto\\Re, BO seeks to identify the input x‚àóx^{\\ast} that maximizes the objective ff over an admissible set of queries ùí≥\\mathcal{X}, i.e.,\nx‚àó=arg‚Å°maxx‚ààùí≥‚Å°f‚Äã(x).x^{\\ast}=\\arg\\max_{x\\in\\mathcal{X}}f(x).\nTo achieve this goal, BO relies on a surrogate model that provides a probabilistic representation of the objective ff, and uses this information to compute an acquisition function to drive the selection of the most promising sample to query.\n\n\nSurrogate model. We assume the available information regarding the objective function ff be stored in the dataset ùíüt:={(x1,y1),‚Ä¶,(xt,yt))}\\mathcal{D}_{t}:=\\{(x_{1},y_{1}),\\dots,(x_{t},y_{t}))\\}, where yt‚àºùí©‚Äã(f‚Äã(xt),œÉ2‚Äã(xt))y_{t}\\sim\\mathcal{N}(f(x_{t}),\\sigma^{2}(x_{t})) is the noisy observation of the objective function by assuming the noise follows a zero-mean normal distribution with a standard deviation œÉ\\sigma.\nThe surrogate model depicts possible explanations of ff as f‚Äã(x)‚àºp‚Äã(f‚Äã(x)|ùíüt)f(x)\\sim p(f(x)|\\mathcal{D}_{t}) applying a joint distribution over its behavior at each sample x‚ààùí≥x\\in\\mathcal{X}.\nIn Bayesian inference, the prior distribution of the objective p‚Äã(f‚Äã(x))p(f(x)) is combined with the likelihood function p‚Äã(ùíüt|f‚Äã(x))p(\\mathcal{D}_{t}|f(x)) to compute the posterior distribution p‚Äã(f‚Äã(x)|ùíüt)‚àùp‚Äã(ùíüt|f‚Äã(x))‚Äãp‚Äã(f‚Äã(x))p(f(x)|\\mathcal{D}_{t})\\propto p(\\mathcal{D}_{t}|f(x))p(f(x)), representing the updated beliefs about f‚Äã(x)f(x).\nTypically, Gaussian processes (GPs) have been widely used as the surrogate model for BO due to their efficient posterior sampling that enables cheap, gradient-based optimization of the acquisition function to propose new query points.\nGP is specified by a joint normal distribution p‚Äã(f‚Äã(x)|ùíüt)=ùí©‚Äã(Œºt‚Äã(x),Œ∫t‚Äã(x,x‚Ä≤))p(f(x)|\\mathcal{D}_{t})=\\mathcal{N}(\\mu_{t}(x),\\kappa_{t}(x,x^{\\prime})) with mean Œºt‚Äã(x)\\mu_{t}(x) and kernel function Œ∫t‚Äã(x,x‚Ä≤)\\kappa_{t}(x,x^{\\prime}), where Œºt‚Äã(x)\\mu_{t}(x) represents the prediction and Œ∫t‚Äã(x,x‚Ä≤)\\kappa_{t}(x,x^{\\prime}) the associated uncertainty.\n\n\nAcquisition function. The surrogate model is utilized to decide the next sample xt+1‚ààùí≥x_{t+1}\\in\\mathcal{X} through the maximization of an acquisition function Œ±:ùí≥‚Ü¶‚Ñú\\alpha:\\mathcal{X}\\mapsto\\Re, i.e., xt+1=arg‚Å°maxx‚ààùí≥‚Å°Œ±‚Äã(x|ùíüt)x_{t+1}=\\arg\\max_{x\\in\\mathcal{X}}\\alpha(x|\\mathcal{D}_{t}), where Œ±‚Äã(x|ùíüt)\\alpha(x|\\mathcal{D}_{t}) provides a measure of the improvement that the next query is likely to provide with respect to (w.r.t.) the current surrogate model of the objective function.\nMany acquisition functions have been proposed, including probability of improvement (Moƒçkus, 1975), expected improvement (Jones et al., 1998), upper confidence bound, and various entropy search methods (Hennig and Christian J. Schuler, 2012; Hern√°ndez-Lobato et al., 2014; Wang and Jegelka, 2017; Hvarfner et al., 2022; Neiswanger et al., 2021),\nas well as practical approaches to optimize them (Wilson et al., 2018).\n\n\n\n\n2.2 Bayesian Experimental Design\n\nRather than optimizing an objective function f‚Äã(x)f(x), the purpose of BED is to sequentially select a set of experimental"
  },
  {
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "url": "https://arxiv.org/abs/2602.06025v1",
    "source": "arxiv",
    "summary": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control o",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.06025v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computation and Language\n    \n\n    \n      arXiv:2602.06025v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 5 Feb 2026]\n    Title:Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory\n    Authors:Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang            View a PDF of the paper titled Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory, by Haozhen Zhang and 10 other authors\n    View PDF\n\n\n\n    \n            Abstract:Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.\n    \n\n    \n    \n              \n          Comments:\n          Code is available at this https URL\n        \n\n          Subjects:\n          \n            Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG)\n        \n          Cite as:\n          arXiv:2602.06025 [cs.CL]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.06025v1 [cs.CL] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.06025\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Haozhen Zhang [view email]          [v1]\n        Thu, 5 Feb 2026 18:57:09 UTC (1,744 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory, by Haozhen Zhang and 10 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: cs.CL\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.LG\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that "
  },
  {
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "url": "https://arxiv.org/abs/2602.06023v1",
    "source": "arxiv",
    "summary": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restr",
    "full_text": "\n\n\n\nI Introduction\n\nII Background\n\nII-A Agent Policies\nII-B Learning Policies\nII-C Surrogate Modeling\n\n\nIII Data Description\n\nIV Model Description\n\nIV-A Shooter Transitions\nIV-B Shooter Events\nIV-C Robot Effects\n\n\n\nV Model Evaluation\n\nV-A Shooter Transitions\nV-B Shooter Events\nV-C Robot Effects\n\n\nVI Policy Demonstration\nVII Conclusion\n\n\n\n\n\nLearning Event-Based Shooter Models\nfrom Virtual Reality Experiments\n\n\n\nChristopher A. McClurg\n\n‚ÄÉ‚ÄÉ\nAlan R. Wagner\n\n\n\nAbstract\nVirtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.\n\n\n\nI Introduction\n\n\nSchool shootings remain a persistent and growing concern in the United States, as more than half (53.3%) of documented gun-related school incidents occurred within the past seven years¬†[23]. Exposure to such events is associated with elevated rates of post-traumatic stress disorder (PTSD) and anxiety among students¬†[29, 8]. Although numerous school-security measures have been implemented‚Äîranging from hardened infrastructure to behavioral profiling‚Äîmany lack empirical validation of their efficacy¬†[1, 27, 15]. In addition, some measures have been linked to increased student anxiety, reduced trust, and erosion of school climate¬†[4].\n\n\nEvaluating school-security measures for school shootings is difficult, as the detailed behavioral data required for validation are limited and cannot be systematically collected from real events¬†[7]. Although ecologically valid, unannounced experiments in real schools would pose unacceptable physical, ethical, and psychological risks¬†[29, 8]. Virtual reality (VR) may offer a practical alternative, allowing participants to assume the role of a shooter in a simulated school environment while detailed behavior and physiological measurements are taken¬†[18]. Prior work has demonstrated statistical equivalence between VR-generated behavior and real shooter data while also showing promise for evaluating intervention strategies¬†[19].\n\n\nDespite its advantages, conducting human-subject experiments in VR to evaluate school-security measures does not scale. Each change in experimental condition‚Äîwhether testing a new intervention or refining an existing one‚Äîrequires a separate human-subject study with a new participant cohort. To address this limitation, this paper proposes a discrete-event simulation (DES) as a scalable surrogate for conditional human-subject experiments, in which shooter behavior is modeled as a stochastic process learned from human-subject data in VR.\n\n\nThe remainder of the paper is organized as follows. Section¬†II reviews the relevant background. Section¬†III describes the participant data collected in virtual reality that was used to construct and calibrate the simulator. Sections¬†IV‚ÄìV then describe and evaluate the fidelity of the discrete-event simulator. Section¬†VI demonstrates the use of the simulator in a sample-intensive agent-learning task, which would otherwise be infeasible to carry out using a series of human-subject experiments.\n\n\n\n\nII Background\n\n\nA school-security intervention is any measure taken to reduce the likelihood or severity of a shooting incident. For many proposed interventions, an autonomous system (e.g., an alerting system, a surveillance platform, or a mobile robot) must decide how to act in response to the actions of a shooter. Developing or learning such a strategy requires understanding how agent decision-making processes are represented and modeled. Consequently, this section reviews the notion of an agent policy, the mechanisms for learning policies, and the role of surrogate simulation in supporting sample-intensive learning. We use the term agent here to generalize across the different types of autonomous systems that could be used to prevent school shootings.\n\n\n\nII-A Agent Policies\n\n\nA policy maps an agent‚Äôs state or observation to an action¬†[5, 22]. In agent-based modeling (ABM), a common framework for studying active-shooter incidents, shooter policies are typically implemented as hand-crafted behavioral rules. Examples include remaining stationary¬†[7], wandering randomly¬†[3], or moving toward the nearest civilian¬†[12, 28, 16, 17]. These rules are executed at fixed time-step intervals (typically 1‚Äì10¬†Hz), causing temporal patterns to emerge because of repeated rule execution.\n\n\nDiscrete-event simulation (DES) provides an alternative way to represent agent policies¬†[33]. Rather than selecting actions at fixed time intervals, time advances directly to the next meaningful event, such as a shooter entering a new room or the arrival of first responders. This event-driven structure naturally captures variable-duration behaviors and supports policies defined through data-driven transitions and stochastic event outcomes. As a result, DES provides a flexible and empirically-grounded alternative to traditional rule-based ABM approaches for modeling shooter behavior.\n\n\n\n\nII-B Learning Policies\n\n\nIdeally, an intervening agent‚Äôs policy could be learned from experience rather than specified in advance. An agent tasked with preventing a shooting, for example, may have the ability to intervene but must learn how and when to act in response to dynamic shooter behavior. Reinforcement learning provides a common approach for this, allowing an agent to improve its behavior through repeated interaction with an environment by selecting actions, observing outcomes, and receiving feedback in the form of a reward¬†[30]. Reinforcement learning is, however, typically sample-intensive, often requiring a large number of trials before effective policies are learned. The approach becomes impractical when policy learning requires human-subject experiments (e.g., VR-based shooter role-play) to generate each variation of the experimental conditions. Hence, the primary goal of this paper is to develop a method which allows uses previously collected data to model the shooter‚Äôs behavior in a manner that enables scalable learning of agent intervention policies without the need for iterative human-subject data collection.\n\n\n\n\nII-C Surrogate Modeling\n\n\nBecause direct policy learning in high-fidelity environments is often impractical, surrogate environments are commonly used to support scalable policy learning. The most relevant body of work in this space is simulation-to-reality (sim-to-real) research¬†[25, 36], which has largely focused on robotics. In sim-to-real settings, policies are learned in simulation and later deployed on physical robots, creating a well-known reality gap when the simulator does not adequately represent the real system. To mitigate this gap, researchers have developed techniques such as domain randomization, which introduces controlled variability into simulator parameters to promote robustness across environmental conditions¬†[32, 14, 21], and domain adaptation, which aligns simulated and real observation or action spaces using learned mappings or shared representations¬†[11, 9, 24]. Hybrid approaches that combine both strategies further improve transferability and reduce reliance on costly real-world data collection¬†[34, 31, 13].\n\n\nAlthough these examples originate in physics-based robotics simulators, the underlying multi-fidelity principles generalize. In the present context, participant data from virtual reality provides empirically-grounded shooter behavior data but is too costly for the repeated trials required for policy learning. A discrete-event simulator derived from participant data serves as a mid-fidelity surrogate: it approximates school shooter behavior while enabling large-scale, low-cost experimentation. Moreover, the stochastic variability inherent in the participant data naturally induces a form of domain randomization, exposing learned policies to a distribution of plausible human behaviors.\n\n\n\n\n\nIII Data Description\n\n\nOur surrogate simulator is calibrated using behavior data from two VR studies¬†[18, 19], in which participants role-played as an active shooter in a high-fidelity reconstruction of Columbine High School. Participants navigated the environment using a foot interface (Cybershoes) while aiming and firing with VR hand controllers. The environment was populated with non-player characters (NPCs) following a ‚ÄúRun. Hide. Fight.‚Äù protocol, along with two mobile robots attempting to intervene and slow down the shooter. The complete dataset consists of 210 five-minute episodes logged at 2¬†Hz. Each episode includes the position of participants and robots, as well as NPC state information (e.g., alive or dead) and records of shots fired. For the remainder of this paper, we refer to this data as the participant data.\n\n\nIn the present work, we focus on a subset of the participant data, namely the no-robot and robot-with-smoke conditions, each including 60 episodes. The no-robot condition did not include a robot intervention; consequently, only shooter information was collected. The robot-with-smoke cond"
  },
  {
    "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
    "url": "https://arxiv.org/abs/2602.06022v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized ",
    "full_text": null
  },
  {
    "title": "Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold",
    "url": "https://arxiv.org/abs/2602.06021v1",
    "source": "arxiv",
    "summary": "When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model's performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the gener",
    "full_text": null
  },
  {
    "title": "Mechanisms of AI Protein Folding in ESMFold",
    "url": "https://arxiv.org/abs/2602.06020v1",
    "source": "arxiv",
    "summary": "How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical feat",
    "full_text": null
  },
  {
    "title": "Multi-Token Prediction via Self-Distillation",
    "url": "https://arxiv.org/abs/2602.06019v1",
    "source": "arxiv",
    "summary": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online ",
    "full_text": null
  },
  {
    "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
    "url": "https://arxiv.org/abs/2602.06015v1",
    "source": "arxiv",
    "summary": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To u",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Results\n\nModel Scale and Prompting Strategy: LLaMA-3.1 Instruct performs well, few-shot prompting and scaling beyond 70B offer limited gains\nThinking Step by Step: Inconsistent Performance in Chain-of-Thought Style Reasoning\nThe effect of reasoning effort: Higher reasoning effort leads to improved model performance\nThe effect of Predictive Redistribution: Post-hoc, Distribution-Aware Adjustments Improve Regression Accuracy\nSubscale-based vs. Direct Construct Predictions: The Role of Contextual Cues and Scoring Format in PTSD Severity Estimation\nModel Ensembles: Leveraging Complementary Models for Improved PTSD Severity Estimation\nComparison with Human Raters\n\n\n3 Discussion\n\n4 Methods\n\n\n4.1 Data\n\nCollection\nMeasures\nProcedure\nTranscription\n\n\n\n4.2 Experiment Design\n\nPrompt\nHandling of Failure Cases\nHyperparameters\nHuman Raters\n\n\n\n4.3 Analysis\n\nSelection of few-shot examples\nBaseline Method\nLikert Scale Adjustment\nPredictive Redistribution\nReasoning Effort\nStatistical Significance Testing\n\n\n\n\n\n5 Declarations\n\nData Availability\nCode Availability\nEthics\nFunding\nAcknowledgements\nAuthor Contributions\nCompeting Interests\n\n\nEffect of Few-Shot Sampling Strategies on PCL Score Estimation Performance\nComprehensive Performance Comparison Across Model Sizes and Prompting Strategies\nOptional Plug-In Components for Prompt Customization\nPrompt Architecture for PTSD Severity Estimation Experiments\nAdditional Plug-In Components for Further Prompt Customization\nDirect score prediction prompt\nDataset Descriptive Statistics\n\n\n\n\n\n\n[1]\\fnmPanagiotis \\surKaliosis\\equalcontThese authors contributed equally to this work.\n\n\n\\equalcont\nThese authors contributed equally to this work.\n\n\n[1,2]\\fnmH. Andrew \\surSchwartz\n\n\n1]\\orgdivDepartment of Computer Science, \\orgnameStony Brook University, \\countryUSA\n\n\n2]\\orgdivCollege of Connected Computing, \\orgnameVanderbilt University, \\countryUSA\n\n\n3]\\orgdivDepartment of Psychology, \\orgnameLund University, \\countrySweden\n\n\n4]\\orgdivDepartment of Psychology, \\orgnameUniversity of Minnesota, \\countryUSA\n\n\n5]\\orgdivDepartment of Applied Mathematics and Statistics, \\orgnameStony Brook University, \\countryUSA\n\n\n6]\\orgdivStony Brook World Trade Center\nWellness Program, \\orgnameRenaissance\nSchool of Medicine at Stony Brook\nUniversity, \\countryUSA\n\n\n7]\\orgdivDepartment of Psychology, \\orgnameUniversity of Texas at Dallas, \\countryUSA\n\n\n8]\\orgdivDepartment of Medicine, \\orgnameRenaissance School of Medicine at\nStony Brook University, \\countryUSA\n\n\n9]\\orgdivDepartment of Psychiatry, \\orgnameStony Brook University, \\countryUSA\n\nA Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies\n\n\n\n\npkaliosis@cs.stonybrook.edu\n\n‚ÄÉ‚ÄÉ\n\\fnmAdithya \\surV Ganesan\n\n‚ÄÉ‚ÄÉ\n\\fnmOscar N.E. \\surKjell\n\n‚ÄÉ‚ÄÉ\n\\fnmWhitney \\surRingwald\n\n‚ÄÉ‚ÄÉ\n\\fnmScott \\surFeltman\n\n‚ÄÉ‚ÄÉ\n\\fnmMelissa A. \\surCarr\n\n‚ÄÉ‚ÄÉ\n\\fnmDimitris \\surSamaras\n\n‚ÄÉ‚ÄÉ\n\\fnmCamilo \\surRuggero\n\n‚ÄÉ‚ÄÉ\n\\fnmBenjamin J. \\surLuft\n\n‚ÄÉ‚ÄÉ\n\\fnmRoman \\surKotov\n\n‚ÄÉ‚ÄÉ\n\n\nhansen.schwartz@vanderbilt.edu\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n[\n\n\n\nAbstract\nLarge language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy.\nIn this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs.\nTo understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods.\nOur findings indicate that\n(a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative;\n(b) increased reasoning effort leads to better estimation accuracy;\n(c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations;\nand (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs.\nTaken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.\n\n\n\nkeywords: Post Traumatic Stress Disorder, Large Language Models, Prompting Strategies, Zero-shot Inference\n\n\n\n1 Introduction\n\nAccurate mental health assessment is crucial for diagnosis and treatment for conditions such as the Post-Traumatic Stress Disorder (PTSD).\nHowever, access to timely and specialized care for such conditions remain limited, with the scarcity of trained clinicians often leading to delays in diagnosis and treatment, resulting in aggravated mental health conditions¬†[owusu-etal-2025, morland-etal-2020, kuhn-etal-2020].\nDespite having standard, highly-validated self-report instruments, such as the PTSD Checklist (PCL) [blevins2015posttraumatic], many clinicians rely instead on unstructured interviews during intake and treatment sessions to form qualitative impressions¬†[aboraya-et-al].\nSuch narrative accounts are valuable because they allow patients to describe symptoms in their own words, capturing nuances that structured scales may miss¬†[crespo2016trauma, schnurr2022assessment].\nHowever,\nsystematically quantifying mental health severity from natural language communication has historically been difficult¬†[sikstrom-etal-precise, aboraya-et-al].\n\n\nRecently, language models have demonstrated accuracy that approaches a theoretical upper-bound, suggesting natural language accounts, already valued by patients and therapists, could be the solution to the assessment bottleneck¬†[kjell2022natural, kjell2024beyond].\nHowever, building on over a decade of natural language processing (NLP)¬†[crespo2016trauma, wtc_son_et_al, eichstaedt2028predicting, schnurr2022assessment, varadarajan-etal-2024-alba, mangalik2024robust], such accurate measurement has mostly been shown over supervised LLMs (either fine-tuning or utilizing supervised ML over LM-based embeddings).\nThe use of zero-shot LLMs for mental health is still developing with less consistent results¬†[hua2025scoping, brickman-et-al-llms-psych-assessments, stade-etal-2024, dergaa-chatgpt-2024] including PTSD [tu-etal-2024-automating].\nMotivated by inconsistent evaluation practices in mental-health LLMs, we systematically study the role of model‚Äôs input knowledge and its assessment procedure on its assessment performance.\n\n\n\nTo address these gaps and move beyond feasibility studies¬†[tu-etal-2024-automating, vganesan-etal-schema-2025] toward principled evaluation, we conduct a systematic study of LLMs for estimating PTSD severity from open-ended responses.\nWe vary two orthogonal axes that prior works have rarely examined in a controlled way: (1) contextual knowledge provided to the model ‚Äî symptom definitions and validated scale items, elicitation context (participant criteria and interview prompts), and task-level priors about score distributions and (2) modeling strategy ‚Äì‚Äî model choice (size and ‚Äúreasoning‚Äù capabilities), task formulation (zero- versus few-shot; direct scalar versus subscales predictions), and post-processing (calibration via predictive redistribution and ensembling).\nThis study design responds to calls for standardized¬†[hua2025scoping] and rigorous evaluation of mental-health LLMs, where existing studies are often vignette-based, and lack consistent design of prompts and modeling choices.\nWe aim to understand the knowledge and process contents in prompts that contributes for reliable severity estimation.\n\n\nPrior NLP work on PTSD and mental health often draws on social-media self-disclosures¬†[coppersmith-et-al-2014, coppersmith-natural-2018, coppersmith-etal-2014-quantifying] or in-lab tasks¬†[burdisso-etal-2024-daic].\nThese settings are useful for prototyping but typically suffer selection, social desirability, demographic and platform biases¬†[shah-etal-2020-predictive, jaidka-2018-fbvstw, salecha-large-2024, jaidka2022cross, ahsan2025elucidating], limiting clinical utility.\nExisting studies also frame PTSD as a binary diagnosis rather than a continuous severity construct¬†[coppersmith-et-al-2014, tu-etal-2024-automating].\nHowever in clinical care, symptom severity is monitored on a continuum with instruments such as the PCL-5, and recommended cut points vary by population and purpose¬†[marx2022reliable].\nTogether, these choices reduce clinical relevance and make it difficult to judge whether general-purpose LLMs actually outperform the current go-to approach ‚Äî supervised language models trained and evaluated on clinically grounded data with validated scales.\nIn contrast, we ground assessment in the PTSD Checklist, a widely used DSM-5‚Äìaligned continuous severity measure, and use semi-structured, open-ended patient language rather than social media posts.\n\n\nEvaluations of general-purpose LLMs have often appeared inconsistent¬†[ghazarian2024assessment, askari-etal-2025-assessing]. Much of this variability can be attributed to differences in the contextual information provided to the model ‚Äî what background knowledge is provided¬†[v-ganesan-etal-2023-systematic], how it is framed¬†[ceballos-arroyo-etal-2024-open, sun2024evaluating], and where it appears¬†[liu-etal-2024-lost].\nBroad NLP evidence shows that LLMs‚Äô performance shifts with the content and placement of information in long inputs, as well as with how domain knowledge is structured.\nSurveys of prompting and in-context learning similarly emphasize that supplying task-relevant knowledge is often decisive¬†[dong-etal-2024-survey].\nYet, in clinical mental-health settings, the systematic study of context provision, e.g., symptom definitions, elicitation details, and task-level priors, has been limited co"
  },
  {
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "url": "https://arxiv.org/abs/2602.06014v1",
    "source": "arxiv",
    "summary": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as",
    "full_text": null
  },
  {
    "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
    "url": "https://arxiv.org/abs/2602.06013v1",
    "source": "arxiv",
    "summary": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stoc",
    "full_text": null
  },
  {
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "url": "https://arxiv.org/abs/2602.06008v1",
    "source": "arxiv",
    "summary": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models market",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nNegotiation and Bargaining in Game Theory.\nNeural and Dialogue-Based Negotiation.\nLLMs in Economic and Auction Settings.\n\n\n\n3 Problem Settings\n\nAgent States.\nProduct and Market Context.\nDialogue-Based Negotiation.\nAction Parsing and Termination.\nEvaluation Objectives.\n\n\n\n4 AgenticPay\n\n\n4.1 Environment\n\nNegotiation Protocol.\nScenario Design.\n\n\n\n4.2 Tasks\n\nTask Definition.\nTask Categories.\n\n\n\n4.3 Agents\n\nEnvironment Public Information.\nRole-Based Private Information.\nDialogue History.\n\n\n\n4.4 Metrics\n\nScore Design.\n\n\n\n\n\n5 Experiments and Analysis\n\n\n5.1 Experimental Setup\n\nBenchmark Statistics\nInference\nModels\nEvaluation Metrics\n\n\n\n5.2 Main Results\n\nProprietary Models Dominate Negotiation Performance.\nNegotiation Efficiency Correlates with Model Capability.\nAsymmetric Buyer‚ÄìSeller Performance.\n\n\n\n5.3 Behind the Bargain: Factors Influencing Negotiation Outcomes\n\nPerformance Improves with Increased Buyer and Seller Multiplicity.\nFinancial Asset Negotiations Expose Model Limitations.\nCross-Play Exposes Systematic Buyer Disadvantage.\nPersonality Significantly Impacts Negotiation Efficiency.\nNegotiation Mode Has Minimal Impact on Top Models.\nTimeout Failures Reflect Model Capability Rather Than Task Structure.\nNear-Miss Failures Reveal Convergence Deficiencies in Open-Weight Models.\n\n\n\n\n6 Conclusion\nA Benchmark Statistics\nB Additional Experimental Details\nC Dialogue Examples\n\n\n\n\n\nAgenticPay: A Multi-Agent LLM Negotiation System \nfor Buyer‚ÄìSeller Transactions\n\nAbstract\nLarge language model (LLM)‚Äìbased agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer‚Äìseller negotiation driven by natural language. AgenticPay¬†models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay¬†as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.\n\nMachine Learning, ICML\n\n\nXianyang Liu ‚ÄÉ‚Ää  Shangding Gu* ‚ÄÉ‚Ää  Dawn Song\n\n\nFigure 1: Overview of AgenticPay. (a) Agents &amp; Task Examples: Buyer and seller agents engage in three negotiation modes: 1-to-1 (bilateral bargaining between a single buyer and seller), 1-to-N (one buyer negotiating with multiple competing sellers, or one seller negotiating with multiple competing buyers), and N-to-N (many buyers and sellers forming a matching market). (b) Framework: Core components including Environment, Task, and Agent interact to enable multi-round negotiations. (c) Dialogue Example: A sample negotiation showing the user‚Äôs product requirements, buyer‚Äìseller conversation, and final deal.\n\n\n\n1 Introduction\n\nLarge language models (LLMs) have shown remarkable performance in many domains (Comanici et al., 2025; Hurst et al., 2024; OpenAI, 2025; Gu et al., 2024; Yang et al., 2025), and are increasingly deployed as autonomous agents that need to coordinate and transact on behalf of users in economic settings such as e-commerce, procurement, and service contracting. Unlike traditional decision-making systems that operate over structured bids or fixed utility functions, these agents interact through natural language, expressing preferences, constraints, and counteroffers in multi-turn dialogues. As a result, negotiation becomes a language-mediated strategic interaction, where outcomes depend jointly on reasoning, communication, and long-horizon planning.\n\n\nDespite rapid progress in LLM capabilities, existing benchmarks for agent evaluation remain limited in their ability to capture this setting (Xia et al., 2024; He et al., 2018; Fu et al., 2023). Most prior work evaluates single-agent reasoning (Mondorf and Plank, 2024; Gu et al., 2025), tool use (Chen et al., 2025), or preference following (Sun et al., 2025b), and economic interaction is often simplified to numeric auctions or short-horizon bargaining (Chen et al., 2023; He et al., 2018; Fu et al., 2023). These abstractions fail to reflect key properties of real-world transactions: private reservation values, multi-round negotiation, heterogeneous products, and competition among multiple buyers and sellers. Consequently, it remains unclear: How effectively can current LLMs function as autonomous negotiators in diverse market environments?\n\n\nIn this work, we introduce AgenticPay, a benchmark and simulation framework for studying multi-agent buyer‚Äìseller negotiation driven by natural language, spanning settings from bilateral bargaining to many-to-many markets. AgenticPay¬†models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through iterative linguistic negotiation rather than numeric bidding alone. Negotiation is formalized as a language game, with dialogue histories mapped to actions such as price proposals and deal acceptance, enabling principled evaluation of negotiation outcomes.\n\n\nAgenticPay¬†provides a comprehensive suite of tasks that scale market complexity along three dimensions: the number of buyers, the number of sellers, and the size of the product set. Tasks range from bilateral bargaining to many-to-many markets with competing agents and multiple products, supporting both sequential and parallel negotiation regimes. To evaluate performance, we introduce metrics that jointly capture deal feasibility, efficiency, and welfare for buyers, sellers, and the market as a whole.\n\n\nUsing AgenticPay, we benchmark a diverse set of state-of-the-art proprietary and open-weight LLMs under a unified inference-only protocol. Our results reveal substantial performance gaps across models, systematic asymmetries between buyer and seller roles, and persistent challenges in long-horizon strategic reasoning. These findings highlight that strong language generation alone is insufficient for effective economic negotiation.\n\n\nOverall, AgenticPay¬†establishes a foundation for studying agentic commerce, offering a controlled yet expressive testbed for research on multi-agent negotiation, economic alignment, and the co-evolution of language and strategy in autonomous agents. Our Contributions are summarized as follows:\n\n\n‚Ä¢\n\nWe introduce AgenticPay, a scalable framework that supports a large number of tasks (over 110) ranging from bilateral bargaining to many-to-many markets, with dialogue-to-action grounding and welfare-oriented evaluation metrics. The system supports diverse deployment via vLLM 111https://github.com/vllm-project/vllm, SGLang222https://github.com/sgl-project/sglang, and cloud-based LLM APIs.\n\n\n\n‚Ä¢\n\nWe formalize language-mediated buyer‚Äìseller negotiation as a multi-agent game with private reservation values and dialogue-grounded economic outcomes. Moreover, we benchmark state-of-the-art proprietary and open-weight LLMs, uncovering persistent limitations in long-horizon strategic reasoning and negotiation efficiency.\n\n\n\n\n\nFigure 2: Overview of the AgenticPay¬†task suite. Left: Ten realistic business scenarios across four categories: Consumer, Services, Supply, and Assets. Right: Task categories illustrating the progression from bilateral bargaining to full market settings along three complexity dimensions: number of buyers, number of sellers, and product set size.\n\n\n\n\n2 Related Work\n\nNegotiation and Bargaining in Game Theory.\n\nClassical work in economics and game theory has studied bargaining and bilateral trade under incomplete information, establishing foundational results on efficiency, equilibrium, and impossibility theorems (Chatterjee and Samuelson, 1983; Myerson and Satterthwaite, 1983; Rubinstein, 1985; Ausubel et al., 2002; Blumrosen and Mizrahi, 2016). These models typically assume agents interact through scalar bids or utilities, with negotiation dynamics defined over numeric strategy spaces. While analytically tractable, such formulations abstract away the role of language and may not capture the rich communicative strategies present in real-world negotiations. More recently, a growing body of work has explored the game-theoretic behavior of large language models (Lor√® and Heydari, 2023; Fan et al., 2024; Hu et al., 2024; Raman et al., 2024; Silva, 2024; Lor√® and Heydari, 2024; Jia et al., 2025; Lu, 2025; Akata et al., 2025; Sun et al., 2025a). However, these studies primarily examine strategic reasoning or equilibrium behavior in games, and do not explicitly address language-mediated negotiation or market-based economic interaction.\n\n\n\nNeural and Dialogue-Based Negotiation.\n\nPrior work in natural language processing has explored negotiation as a dialogue task, focusing on learning strategies for offer generation, concession planning, and agreement formation (Lewis et al., 2017; He et al., 2018; Chawla et al., 2021; Joshi et al., 2021; Pacella and Marocco, 2022; Hua et al., 2024; Washio et al., 2026). More recent approaches leverage LLMs and self-play to improve negotiation behavior (Ma et al., 2024; Chen et al., 2024; Long et al., 2025), often using in-context learning or reinforcement signals derived from dialogue outcomes (Fu et al., 2023; Vahidov et al., 2025; Priya et al., 2025). These methods typically consider bilateral settings with fixed roles and limited environment structure, and may not address market-level interactions involving multip"
  },
  {
    "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
    "url": "https://arxiv.org/abs/2602.06000v1",
    "source": "arxiv",
    "summary": "Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attenti",
    "full_text": null
  },
  {
    "title": "On Computation and Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.05999v1",
    "source": "arxiv",
    "summary": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amoun",
    "full_text": null
  },
  {
    "title": "Causal Inference on Stopped Random Walks in Online Advertising",
    "url": "https://arxiv.org/abs/2602.05997v1",
    "source": "arxiv",
    "summary": "We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to ",
    "full_text": null
  },
  {
    "title": "Orthogonal Self-Attention",
    "url": "https://arxiv.org/abs/2602.05996v1",
    "source": "arxiv",
    "summary": "Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims t",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Orthogonal Self-Attention\n\n2.1 Implementation Details\n2.2 Kernel Analysis\n\n\n\n3 Basis Computation\n\n3.1 Complexity and Memory Analysis\n\n\n\n4 Initialisation for OSA\n\n4.1 Value-Output Initialisation\n4.2 Query-Key Initialisation\n4.3 Œ±\\alpha Initialisation and Jacobian Analysis\n4.4 MLP Initialisation\n\n\n5 Experiments\n6 Conclusion\nA Further Details on Complexity and Memory Analysis\nB Sampling\n\nC Proofs\n\nC.1 Proof of Theorem 2.1\nC.2 Proof of Theorem 2.2\nC.3 Proof of Theorem 3.1\nC.4 Proof of Theorem 4.1\nC.5 Proof of Theorem 4.2\nC.6 Proof of Theorem 4.3\nC.7 Proof of Theorem 4.4\n\n\nD Experimental Details\n\n\n\n\n\nOrthogonal Self-Attention\n\n\nLeo Zhang1111Corresponding author: leo.zhang@stx.ox.ac.uk‚ÄÖ and James Martens222Corresponding author: james.martens@gmail.com\n1Department of Statistics, University of Oxford\n\n\n\nAbstract\nSoftmax Self-Attention (SSA) is a key component of Transformer architectures.\nHowever, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians.\nIn this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained.\nIn particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential.\nWe show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length.\nFurthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.\n\n\n\n1 Introduction\n\nSkip connections [He et¬†al., 2016] have become an ubiquitous feature of neural network architectures from facilitating the stable training of deep models.\nHowever, despite their success, prior works [Veit et¬†al., 2016, Gromov et¬†al., 2024, Zhang et¬†al., 2024] have raised the concern that the benefits of skip connections, namely ease of training, may be obscuring deeper issues, in terms of representation learning, that skip connections induce.\nThe main point behind these criticisms is that skip connections appear to bias models away from properly utilising the full depth of their architectures.\nFor instance, Ji et¬†al. [2025a] argues that since skip connections continually reintroduce earlier features into deeper layers, they disrupt the learning of hierarchical and progressively more abstract representations, fundamentally harming representation learning.\n\n\nMotivated by this line of reasoning, we explore designing Transformers that are able to be trained stably without skip connections.\nPrevious works [He et¬†al., 2023, Ji et¬†al., 2025a] have tackled this through modifications to Softmax Self-Attention (SSA) [Vaswani et¬†al., 2017] and weight initialisations to improve signal propagation and the conditioning of the Jacobian matrix.\nHowever, these works restrict themselves to standard Softmax-based Transformers which appear to be inherently unstable without skip connections [Dong et¬†al., 2021, Ji et¬†al., 2025b] due to SSA.\n\n\nTherefore, due to the fundamental issues with SSA, in this work, we propose Orthogonal Self-Attention (OSA) which attempts to circumvent the training instability of skipless SSA by designing the attention matrix to be orthogonal.\nThis is motivated by the rank collapse phenomenon associated with SSA [Dong et¬†al., 2021, Noci et¬†al., 2022] where token representations quickly converge to a rank-1 matrix with depth.\nIn contrast, by enforcing the attention matrix to be orthogonal in OSA, we preserve the rank of representations, which should mitigate against rank collapse in skipless architectures.\n\n\nIn terms of how we implement OSA, we parametrise the attention matrix via the matrix exponential, mapping skew-symmetric matrices, computed from query-key values, to the manifold of (special) orthogonal matrices.\nTo make the use of the matrix exponential tractable in practice, we present a scheme for efficiently computing the matrix exponential through exploiting our low-rank design of the skew-symmetric matrices.\nThis allows OSA to scale linearly, in terms of computational complexity and memory cost, with sequence length, in contrast to the quadratic scaling that SSA requires.\nWe note that this does restricts the applicability of OSA to non-causal decoder-based Transformers such as ViTs [Dosovitskiy, 2020] and DiTs [Peebles and Xie, 2023].\n\n\nFinally, we analyse how OSA impacts the conditioning of the network Jacobian.\nTo make our analysis tractable, we use the same assumptions as employed in Ji et¬†al. [2025b, a], which reduces the analysis of the conditioning of the network Jacobian to the conditioning of the individual attention sub-blocks.\nMotived by these assumptions, we derive an initialisation scheme for OSA for which we prove ensures that the input-output Jacobian of OSA is well-conditioned.\n\n\n\n\n2 Orthogonal Self-Attention\n\nIn this work, we consider non-causal decoder-based Transformers (e.g. ViTs and DiTs) where we will replace the use of SSA with Orthogonal Self-Attention (OSA) and remove skip connections and the use of normalisation layers.\n\n\nLet ùêó0‚àà‚ÑùN√ód\\mathbf{X}_{0}\\in\\mathbb{R}^{N\\times d} be the initial token representations computed from the input to the architecture, where NN denotes the number of tokens and dd denotes the representation dimension.\nWe define an OSA-Transformer by the following recursion:\n\n\n\nùêó^l\\displaystyle\\hat{\\mathbf{X}}_{l}\n=M‚àíOSA‚Å°(ùêól‚àí1)\\displaystyle=\\operatorname{M-OSA}(\\mathbf{X}_{l-1})\n\n(1)\n\n\n\nùêól\\displaystyle\\mathbf{X}_{l}\n=MLP‚Å°(ùêó^l)\\displaystyle=\\operatorname{MLP}(\\hat{\\mathbf{X}}_{l})\n\n(2)\n\n\nwhere ùêól\\mathbf{X}_{l} denotes the token representations after the ll-th transformer block and MLP\\operatorname{MLP} denotes some MLP applied token-wise. Furthermore, we define M‚àíOSA\\operatorname{M-OSA} as the multihead version of OSA defined as\n\n\n\nM‚àíOSA‚Å°(ùêól‚àí1)\\displaystyle\\operatorname{M-OSA}(\\mathbf{X}_{l-1})\n=‚àëi=1hOSAl,i‚Å°(ùêól‚àí1)\\displaystyle=\\sum_{i=1}^{h}\\operatorname{OSA}_{l,i}(\\mathbf{X}_{l-1})\n\n(3)\n\n\n\n\n=‚àëi=1hùêÄi‚Äã(ùêól‚àí1)‚Äãùêól‚àí1‚Äãùêñl,iV‚Äãùêñl,iO\\displaystyle=\\sum_{i=1}^{h}\\mathbf{A}_{i}(\\mathbf{X}_{l-1})\\mathbf{X}_{l-1}\\mathbf{W}_{l,i}^{V}\\mathbf{W}_{l,i}^{O}\n\n(4)\n\n\nwhere hh is the number of heads, OSAl,i\\operatorname{OSA}_{l,i} denotes OSA\\operatorname{OSA} for a single head, ùêÄi‚ààSO‚Å°(N)‚äÇ‚ÑùN√óN\\mathbf{A}_{i}\\in\\operatorname{SO}(N)\\subset\\mathbb{R}^{N\\times N} is an attention matrix, ùêñl,iV‚àà‚Ñùd√ódv\\mathbf{W}_{l,i}^{V}\\in\\mathbb{R}^{d\\times d_{v}} and ùêñi,lO‚àà‚Ñùdv√ód\\mathbf{W}_{i,l}^{O}\\in\\mathbb{R}^{d_{v}\\times d} are the value and output weights respectively (where dv=dhd_{v}=\\frac{d}{h}), and ii indexes the heads.\nFor simplicity, we will consider OSAl,i\\operatorname{OSA}_{l,i} for a single layer and head, and we will drop the indices ll and ii for the remainder of the paper.\n\n\nFinally, we define OSA\\operatorname{OSA} for some input ùêó‚àà‚ÑùN√ód\\mathbf{X}\\in\\mathbb{R}^{N\\times d} as\n\n\n\nOSA‚Å°(ùêó)=ùêÄ‚Äã(ùêó)‚ÄãùêóùêñV‚ÄãùêñO,¬†where¬†‚ÄãùêÄ‚Äã(ùêó)=exp‚Å°(ùêí)‚Äã¬†and¬†‚Äãùêí=Œ±dv‚Äã(ùêêùêä‚ä§‚àíùêäùêê‚ä§),\\displaystyle\\operatorname{OSA}(\\mathbf{X})=\\mathbf{A}(\\mathbf{X})\\mathbf{X}\\mathbf{W}^{V}\\mathbf{W}^{O},\\text{ where }\\mathbf{A}(\\mathbf{X})=\\exp(\\mathbf{S})\\text{ and }\\mathbf{S}=\\frac{\\alpha}{\\sqrt{d_{v}}}\\left(\\mathbf{Q}\\mathbf{K}^{\\top}-\\mathbf{K}\\mathbf{Q}^{\\top}\\right),\n\n(5)\n\n\nwhere exp\\exp denotes the matrix exponential [Hall, 2013], Œ±‚àà‚Ñù\\alpha\\in\\mathbb{R} is some scalar learnable parameter, and ùêê=ùêóùêñQ,ùêä=ùêóùêñK‚àà‚ÑùN√ódv\\mathbf{Q}=\\mathbf{X}\\mathbf{W}^{Q},\\mathbf{K}=\\mathbf{X}\\mathbf{W}^{K}\\in\\mathbb{R}^{N\\times d_{v}} are the query, key matrices with the respective weights ùêñQ,ùêñK‚àà‚Ñùd√ódv\\mathbf{W}^{Q},\\mathbf{W}^{K}\\in\\mathbb{R}^{d\\times d_{v}}.\n\n\nWe note that ùêí\\mathbf{S} is defined to be skew-symmetric which ensures that ùêÄ‚Äã(ùêó)\\mathbf{A}(\\mathbf{X}) is a (special) orthogonal matrix, we use the scaling 1dv\\frac{1}{\\sqrt{d_{v}}} for normalising the dot-product of vectors in ‚Ñùdv\\mathbb{R}^{d_{v}}, and we include Œ±\\alpha to aid with our initialisation scheme we define later on.\nFurthermore, it is also easy to see that OSA\\operatorname{OSA} is permutation equivariant with respect to the token positions.\n\n\n\n2.1 Implementation Details\n\nWe note that a naive implementation of OSA\\operatorname{OSA} has computational complexity of O‚Äã(N3)O(N^{3}) due to the matrix exponential.\nHowever, Theorem 2.1 shows that we can greatly reduce this through exploiting the low-rank structure of ùêí\\mathbf{S}, as we usually have that dvd_{v} is small compared to NN and ùêí\\mathbf{S} has r‚â§2‚Äãdvr\\leq 2d_{v} where r=rank‚Å°ùêír=\\operatorname{rank}\\mathbf{S}.\nFor the proof, see Appendix C.1.\n\n\n\nTheorem 2.1.\n\n\nLet ùêÅ‚Äã(ùêó)‚àà‚ÑùN√ór\\mathbf{B}(\\mathbf{X})\\in\\mathbb{R}^{N\\times r} be a matrix where the columns provide an orthonormal basis for the subspace UU spanned by the columns of ùêê,ùêä\\mathbf{Q},\\mathbf{K}.\nThen we have:\n\n\n\nexp‚Å°(ùêí‚Äã(ùêó))=ùêàN+ùêÅ‚Äã(ùêó)‚Äã(exp‚Å°[ùêí]‚Äã(ùêó)‚àíùêàr)‚ÄãùêÅ‚Äã(ùêó)‚ä§,\\displaystyle\\exp(\\mathbf{S}(\\mathbf{X}))=\\mathbf{I}_{N}+\\mathbf{B}(\\mathbf{X})\\left(\\exp[\\mathbf{S}](\\mathbf{X})-\\mathbf{I}_{r}\\right)\\mathbf{B}(\\mathbf{X})^{\\top},\n\n(6)\n\n\nwhere\n\n\n\n[ùêí]‚Äã(ùêó)=ùêÅ‚Äã(ùêó)‚ä§‚Äãùêí‚Äã(ùêó)‚ÄãùêÅ‚Äã(ùêó)‚àà‚Ñùr√ór.\\displaystyle[\\mathbf{S}](\\mathbf{X})=\\mathbf{B}(\\mathbf{X})^{\\top}\\mathbf{S}(\\mathbf{X})\\mathbf{B}(\\mathbf{X})\\in\\mathbb{R}^{r\\times r}.\n\n(7)\n\n\n\n\n\nThis reduces the problem of computing exp‚Å°(ùêí)\\exp(\\mathbf{S}) to computing exp‚Å°[ùêí]\\exp[\\mathbf{S}] which has a computational complexity of O‚Äã(r3)‚â™O‚Äã(N3)O(r^{3})\\ll O(N^{3}).\n\n\n\n\n2.2 Kernel Analysis\n\nThe following theorem shows that OSA does not suffer from the rank collapse issue associated with SSA.\nFor the proof, see Appendix C.2\n\n\n\nTheorem 2.2.\n\n\nConsider a skipless OSA-only Transformer (i.e. without MLP blocks) with h=1h=1 at initialisation where we initialise ùêñlV‚ÄãùêñlO‚àà‚Ñùd√ód\\mathbf{W}_{l}^{V}\\mathbf"
  },
  {
    "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
    "url": "https://arxiv.org/abs/2602.05993v1",
    "source": "arxiv",
    "summary": "Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose \"Diamond Maps\", stochastic flow map ",
    "full_text": null
  },
  {
    "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
    "url": "https://arxiv.org/abs/2602.05992v1",
    "source": "arxiv",
    "summary": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, m",
    "full_text": null
  },
  {
    "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
    "url": "https://arxiv.org/abs/2602.05988v1",
    "source": "arxiv",
    "summary": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing ",
    "full_text": null
  },
  {
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "url": "https://arxiv.org/abs/2602.05986v1",
    "source": "arxiv",
    "summary": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reason",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.05986v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Computer Vision and Pattern Recognition\n    \n\n    \n      arXiv:2602.05986v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 5 Feb 2026]\n    Title:RISE-Video: Can Video Generators Decode Implicit World Rules?\n    Authors:Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen, Haoyu Cao, Xing Sun, Haodong Duan, Xue Yang            View a PDF of the paper titled RISE-Video: Can Video Generators Decode Implicit World Rules?, by Mingxin Liu and 11 other authors\n    View PDF\n\n\n\n    \n            Abstract:While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.\n    \n\n    \n    \n              \n          Comments:\n          38 pages, 16 figures, 3 tables; Code: this https URL HuggingFace: this https URL\n        \n\n          Subjects:\n          \n            Computer Vision and Pattern Recognition (cs.CV); Artificial Intelligence (cs.AI)\n        \n          Cite as:\n          arXiv:2602.05986 [cs.CV]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.05986v1 [cs.CV] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.05986\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Xue Yang [view email]          [v1]\n        Thu, 5 Feb 2026 18:36:10 UTC (38,047 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled RISE-Video: Can Video Generators Decode Implicit World Rules?, by Mingxin Liu and 11 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.CV\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\n            Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.\n          \n          \n            \n          \n        \n      \n\n    \n\n\n  \n    Which authors of this paper are endorsers? |\n    Di"
  },
  {
    "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins",
    "url": "https://arxiv.org/abs/2602.05983v1",
    "source": "arxiv",
    "summary": "The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal comp",
    "full_text": null
  },
  {
    "title": "Clifford Kolmogorov-Arnold Networks",
    "url": "https://arxiv.org/abs/2602.05977v1",
    "source": "arxiv",
    "summary": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable doma",
    "full_text": null
  },
  {
    "title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "url": "https://arxiv.org/abs/2602.05975v1",
    "source": "arxiv",
    "summary": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval com",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Work\n\nDeep Research Agents.\nLLM-based Retrievers.\nTest-time Scaling for Retrieval.\n\n\n\n3 Sage Benchmark\n\nProblem Formulation.\n\n3.1 Why Scientific Literature Search?\n\n\n3.2 Short-form Questions\n\nData Curation.\n\nEvaluation Metric.\n\n\n3.3 Open-Ended Questions\n\nData Curation.\n\nEvaluation Metric.\n\n\n3.4 Corpus Construction\n\n\n4 Experiment\n\n4.1 Web-Search Experiment Setup\n\n4.2 Web-Search Results\n\nGPT-5 leads overall on short-form questions, while open-ended questions vary more by domain and model.\nSearch quantity is not the main driver of accuracy.\nAgents adapt search effort differently across query types.\nQuery decomposition strategies differ across agents.\n\n\n\n4.3 Corpus-Search Experiment Setup\n\nRetrieval Index Construction.\nRetrieval Setup.\n\n\n\n4.4 Corpus-Search Results\n\nBM25 dominates LLM-based retrievers on short-form questions, while the gap for open-ended questions is narrower.\nIncreasing per-search top-kk consistently improves performance.\nQuery-retriever mismatch limits the value of LLM-based semantics.\nLLM-based retrievers suffer from reduced diversity under long-document constraints.\nLow-diversity decomposition blunts retriever differences on open-ended queries.\n\n\n\n4.5 Ablation\n\n\nSearch method strongly shapes which information matters.\n\n\n5 Test-Time Corpus Scaling\n\n5.1 Method\n\n5.2 Results\n\nBM25 benefits most from corpus scaling.\n\nLimited improvement on open‚Äëended questions.\n\n\n6 Conclusion\n\n\nA Appendix\n\nA.1 Query-Answer Example\nA.2 Query-Decomposition Case Study\nA.3 Document Length Distribution\n\nA.4 Comparison with BrowseComp-Plus: Retriever Behavior\n\nLonger documents and weaker answer locality in our setting.\nAsymmetric text coverage can favor dense retrievers under early-answer locality.\nAgent strength and query decomposition modulate retriever sensitivity.\n\n\n\nA.5 Further Experiments with SearchR1-32B\n\nSearchR1-32B exhibits near single-shot retrieval.\nNatural-language querying does not obviate lexical matching.\n\n\nA.6 Prompt Templates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSage: Benchmarking and Improving Retrieval for Deep Research Agents\n\n\n\nTiansheng¬†Hu1‚ÄÉYilun¬†Zhao2‚ÄÉCanyu¬†Zhang1‚ÄÉArman¬†Cohan2‚ÄÉChen¬†Zhao1,3‚Ä†\n\n1‚ÄâNYU Shanghai ‚ÄÉ2‚ÄâYale University ‚ÄÉ3‚ÄâCenter for Data Science, New York University \n\n¬†¬†¬†https://github.com/HughieHu/Sage\n\n\n\nAbstract\nDeep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce Sage, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.\nWe evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries.\nTo improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively.\n22footnotetext: Correspondence: Chen Zhao (cz1285@nyu.edu)\n\n\n\nSage: Benchmarking and Improving Retrieval for Deep Research Agents\n\n\n\n\n\nTiansheng¬†Hu1‚ÄÉ‚ÄäYilun¬†Zhao2‚ÄÉ‚ÄäCanyu¬†Zhang1‚ÄÉ‚ÄäArman¬†Cohan2‚ÄÉ‚ÄäChen¬†Zhao1,3‚Ä†\n\n1‚ÄâNYU Shanghai ‚ÄÉ2‚ÄâYale University ‚ÄÉ3‚ÄâCenter for Data Science, New York University\n\n¬†¬†¬†https://github.com/HughieHu/Sage\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: Sage task overview. Given a complex question, the deep research agent (e.g., DR Tulu) iteratively reasons, generates keyword-based sub-queries, searches for relevant papers, and outputs a final answer. We first evaluate the agents with their native web-search tool, and then modify DR Tulu‚Äôs MCP service to replace web search with retrievers that performs corpus search over our paper collection.\n\n\nLike human experts, deep research agents (OpenAI, 2025b; GoogleDeepmind, 2024; Perpelexity, 2025; Shao et al., 2025a) address complex queries by iteratively searching and synthesizing information across multiple sources. With the help of recent advances in the agentic capabilities of large language models (LLMs), these systems demonstrate strong and robust performance in benchmarks across multiple domains (Agashe et al., 2025; Zheng et al., 2025; Li et al., 2025a; Wang and Yuan, 2025; Chervonyi et al., 2025; Zhao et al., 2025).\n\n\nAt the core of deep research agents lie their retrieval stack (Zheng et al., 2025; Besrour et al., 2025). Recent advances in LLM-based retrievers have shown strong promise, particularly in their ability to follow instructions and support reasoning-intensive retrieval (Shao et al., 2025b; Muennighoff et al., 2025a; Weller et al., 2025a). However, most existing commercial deep research agents adopt proprietary search APIs over large web corpora, which rely on surface-form matching. We thus ask the following research question: Whether LLM-based retrievers can effectively contribute to deep research agent workflows?\n\n\nWe propose to systematically study the retrieval behaviors of deep research agents with a scientific literature search task. As shown in Figure¬†1, queries in this task often require a deep understanding of research concepts as well as the ability to reason across entire scholarly articles. Moreover, unlike open-domain web search, this task provides a controllable experimental environment with a fixed and well-defined corpus of scientific papers. To this end, we introduce Sage,\na deep-research benchmark for Scientific AGentic retrieval Evaluation, consisting of 1,200 queries over a corpus of 200,000 papers spanning four scientific domains. Sage includes two complementary types of questions: (1) short-form questions with a verifiable answer that often require intensive reasoning, and (2) open-ended questions that reflect practical research tasks such as searching related work.\n\n\nWe first evaluate six deep research agents, including both proprietary systems like GPT-5 (OpenAI, 2025a) and Gemini-2.5-Pro (GoogleDeepmind, 2025b) and the open-source one DR Tulu (Shao et al., 2025a). While proprietary agents perform best and DR Tulu is competitive, all systems struggle with reasoning-intensive retrieval that requires synthesizing metadata and inter-paper relationships. Using DR Tulu as the backbone agent, we further find that BM25 (Robertson et al., 1994) significantly outperforms LLM-based retrievers by about 30%. Analysis shows that the sub-queries generated by existing deep research agents are keyword-oriented. This behavior aligns well with surface-form matching, while the semantic capabilities of LLM-based retrievers falter due to mismatched query formulations.\n\n\nTo address the reasoning-intensive retrieval challenge, we propose a novel corpus-level test-time scaling framework. The key idea is to leverage LLMs to reason over each paper and enrich the corpus with additional signals that make retrieval easier for off-the-shelf retrievers. Specifically, we augment each paper with informative metadata and keywords. This approach yields substantial improvements on Sage, achieving 8% gains on short-form questions and 2% on open-ended questions.\n\n\nWe summarize our key contributions as follows:\n\n\n\n\n‚Ä¢\n\nWe introduce Sage, a reasoning intensive benchmark combining short-form queries and open-ended queries together with a large dataset.\n\n\n\n‚Ä¢\n\nWe conduct extensive evaluation and find that LLM-based retrievers collaborate poorly with deep-research agent.\n\n\n\n‚Ä¢\n\nWe introduce a new framework for corpus-level test-time scaling and achieve great improvements on both short-form and open-ended queries.\n\n\n\n\n\n\n\n2 Related Work\n\nDeep Research Agents.\n\nDeep research agents represent a new paradigm of autonomous AI systems designed to tackle complex, multi-step information-seeking tasks¬†(Huang et al., 2025).\nCommercial systems including OpenAI‚Äôs Deep Research¬†(OpenAI, 2025b), Google‚Äôs Gemini Deep Research¬†(GoogleDeepmind, 2025a) have demonstrated impressive performance on challenging benchmarks such as BrowseComp¬†Wei et al. (2025). In parallel, open-source efforts have rapidly advanced, with systems such as SearchR1, WebThinker, and Tongyi Deep Research approaching competitive performance¬†(Li et al., 2025b; Jin et al., 2025; Li et al., 2025c; Team et al., 2025). Notably, DR Tulu¬†(Shao et al., 2025a) is the first open model explicitly trained for open-ended, long-form deep research via reinforcement learning, achieving results comparable to proprietary systems on benchmarks.\nDespite these advances, existing deep research agents rely primarily on web search or proprietary retrieval backends. Whether such agents can function as plug-and-play solutions when paired with LLM-based retrievers over closed-domain corpora remains largely unexplored, which we systematically investigate in this work.\n\n\n\nLLM-based Retrievers.\n\nThe advent of large-scale contrastive learning marked a significant advancement for retrievers¬†Ni et al. (2022); Gao et al. (2023); Li et al. (2023); Wang et al. (2024); Chen et al. (2024).\nMore recently, decoder-based retrievers such as LLM2Vec¬†(BehnamGhader et al., 2024) and GritLM¬†(Muennighoff et al., 2025a) have emerged, repurposing generative LLMs for embedding tasks.\nBeyond general-purpose embeddings, recent work has explored training LLM-based retrievers to enhance specific capabilities. Promptriever¬†(Weller et al., 2025a) introduces instruction-trained retrievers that can be prompted like language models. ReasonIR¬†(Shao et al., 2025b) presents the first retriever specifically trained for reasoning-intensive tasks such as finding similar"
  }
]