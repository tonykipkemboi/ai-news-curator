[
  {
    "title": "Reinforced Attention Learning",
    "url": "https://arxiv.org/abs/2602.04884v1",
    "source": "arxiv",
    "summary": "Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\n  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes in",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Post training LLMs through Reinforcement Learning\n2.2 Distilling knowledge and beyond from teacher to student models\n\n\n\n3 Reinforced Attention Learning\n\n3.1 Aggregated causal Attention Distribution Policy\n3.2 Advantage-Weighted Attention Divergence\n3.3 Combined Optimization Objective\n\n3.4 Gradient Derivation\n\nGradient w.r.t. Distribution.\nGradient w.r.t. Logits.\nTotal Parameter Update.\n\n\n\n3.5 On-Policy Attention Distillation\n\nTeacher-Student Alignment.\nUnified Distillation Objective.\nGradient Flow.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Experimental Setup\n\nModel Configurations\nTraining Pipeline\n\n\n\n4.2 Implementation Details\n\nVisual Processing\nHyperparameters.\nBaselines.\n\n\n\n4.3 Evaluation Benchmarks\n\nImage QA Tasks.\nVideo QA Tasks.\n\n\n\n4.4 Main Results\n\nImage VQA.\nVideo VQA.\nImplications.\n\n\n\n4.5 Ablation Studies\n\nRAL yields consistent improvements across varying visual resolutions and frame rates.\nIs an Explicit Thinking Process Necessary for VQA?\n\n\n\n\n5 Conclusion\n\n\n\n\n\n\n\\pdftrailerid\nredacted\\correspondingauthorbzhli@ucdavis.edu\n\n\n\n\n\n\n\\reportnumber \n\nReinforced Attention Learning\n\n\nBangzheng Li*\n\nUC Davis\n\n\nJianmo Ni\n\nGoogle DeepMind\n\n\nChen Qu\n\nGoogle\n\n\nIan Miao\n\nGoogle\n\n\nLiu Yang\n\nGoogle\n\n\nXingyu Fu\n\nPrinceton University\n\n\nMuhao Chen\n\nUC Davis\n\n\nDerek Zhiyuan Cheng\n\nGoogle DeepMind\n\n\n\nAbstract\nPost-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.\nWe propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.\n\n\n\nFigure 1: Reinforced Attention Learning formulates internal attention distributions as a policy. Unlike traditional methods that optimize next-token probabilities (‚Äúwhat to generate‚Äù), our approach prioritizes the selective allocation of information (‚Äúwhere to focus‚Äù). By optimizing for the advantage, the model explores a high-reward attention policy that effectively isolates salient information from dense contexts.\n\n\n\n1 Introduction\n\nLarge Language Models (LLMs) have achieved remarkable proficiency in complex domains such as mathematics and programming. Beyond massive unsupervised pre-training [gpt3, gemini], post-training has emerged as a critical technology for eliciting long-form Chain-of-Thought (CoT) reasoning. Current paradigms predominantly employ Reinforcement Learning (RL) to optimize the model‚Äôs policy, utilizing rewards derived from learned models or programmatic verifiers to favor high-utility token sequences. Fundamentally, these policy gradient methods refine the next-token distribution to maximize expected rewards. Recent research has further highlighted a robust correlation between reasoning length and task accuracy‚Äîa phenomenon central to the concept of test-time scaling.\n\n\nTo extend these gains to multimodal LLMs (MLLMs), recent studies have attempted to incorporate ‚Äúthinking processes‚Äù into Visual Question Answering (VQA) tasks. Under this paradigm, models are incentivized to generate exhaustive textual descriptions of visual inputs as a CoT prior to providing a final answer. However, our results reveal that for core perception tasks‚Äîsuch as image or video question answering‚Äîextensive textual reasoning provides only marginal gains and may even degrade the model‚Äôs fundamental perceptual capabilities.\n\n\nWe attribute this limitation to the insufficiency of next-token prediction as the fundamental policy objective in MLLM post-training. In typical MLLM architectures, visual inputs are encoded as tokens and projected into the textual embedding space to serve as context for generation. Accurately answering fine-grained questions requires the model to precisely identify and attend to task-relevant visual information. This process is governed by the Transformer‚Äôs attention mechanism, which must learn to assign high weights to salient multimodal tokens. Standard RLHF, however, optimizes for the result (the token) rather than the process (the internal information allocation).\n\n\nInspired by this observation, we reformulate the post-training policy for MLLMs to operate directly on the attention distribution during generation. This yields Reinforced Attention Learning (RAL), an algorithm designed to optimize the model toward high-utility attention trajectories. Unlike traditional methods, RAL treats the attention pattern itself as the policy: when a response receives a high reward, the algorithm encourages the underlying attention distribution by minimizing the divergence between the current and reference policies. Conversely, for low-reward responses, the model is penalized by increasing the divergence from those sub-optimal patterns. By shifting the optimization target from token likelihood to attention-based allocation, RAL fine-tunes MLLMs more directly for multimodal alignment. Our results indicate that RAL consistently outperforms Group Relative Policy Optimization (GRPO) across video and image benchmarks, particularly on perception-intensive tasks.\n\n\nThe efficacy of optimizing attention distributions naturally extends to On-Policy Distillation. While traditional distillation focuses on token-level probability alignment, we propose a dual-distillation approach that transfers knowledge via both token and attention distribution alignment. Our experiments indicate that the inclusion of attention distillation provides significant additional performance gains.\n\n\nIn summary, this paper introduces a novel post-training paradigm for MLLMs. Our contributions are as follows:\n\n\n\n\n‚Ä¢\n\nReinforced Attention Learning: We propose RAL, a policy-gradient method that shifts the optimization objective from next-token prediction to attention-distribution alignment, enabling direct reinforcement of visual grounding rather than indirect supervision through textual outputs.\n\n\n\n‚Ä¢\n\nOn-Policy Attention Distillation: We further extend this framework to an On-Policy Distillation setting, which substantially improves a student model‚Äôs ability to inherit fine-grained perceptual and grounding behaviors from a teacher.\n\n\n\n‚Ä¢\n\nEmpirical Validation: Extensive experiments demonstrate that RAL consistently improves upon GRPO across diverse visual question answering benchmarks that require fine-grained visual understanding and perception.\n\n\n\n\n\n\n\n2 Related Works\n\n\n2.1 Post training LLMs through Reinforcement Learning\n\nPost-training is now the standard for aligning Large Language Models (LLMs) with human intent [ouyang2022training]. The traditional RLHF pipeline involves Supervised Fine-Tuning (SFT), training a Reward Model (RM) to mimic human preferences, and optimizing the policy via Reinforcement Learning (RL) [christiano2017deep]. While early RLHF methods significantly improved model safety and helpfulness [bai2022constitutionalaiharmlessnessai], they relied heavily on Proximal Policy Optimization (PPO) [schulman2017proximal]. However, PPO‚Äôs actor-critic framework is memory-intensive due to the auxiliary critic model. Group Relative Policy Optimization (GRPO) [deepseekmath2024] addresses this by replacing the critic with group-averaged reward estimates. This shift reduces computational overhead while maintaining high performance, particularly in verifiable domains like reasoning and code [deepseekr12025], leading to the domain of RL with Verifiable Rewards (RLVR).\n\n\nExtending post-training to multimodal LLMs (MLLMs) introduces challenges beyond text-only alignment, including visual hallucination and robust cross-modal grounding [liu2023llava, pope, zhu2023minigpt]. Recent methods adapt RLHF, RLVR, or Direct Preference Optimization (DPO) to improve visual grounding and reduce hallucinations [qwen25vltechnicalreport, internvl3exploringadvancedtraining, visionr1].\n\n\nA persistent issue is modality bias, where the model over-relies on linguistic priors or, conversely, overfits to superficial visual cues [cai2025diagnosing]. To address this, recent work designs reward functions and training signals that discourage text-only shortcuts, penalize spurious visual correlations, and promote faithful, evidence-based responses [xia2025visionary, papo].\n\n\nOur approach targets the same goal by leveraging a fundamental information-selection mechanism: attention. Since cross-modal reasoning depends on identifying salient evidence in both modalities, directly shaping attention weights provides a principled way to control the cross-modal reasoning policy in our method, rather than relying solely on text-token-level policies.\n\n\n\n\n2.2 Distilling knowledge and beyond from teacher to student models\n\nKnowledge Distillation (KD) transfers knowledge from a high-capacity teacher to a student by matching softened output distributions rather than hard labels [hinton2015distilling]. By providing richer supervisory signals, KD has been widely adopted for model compression, domain adaptation, and efficient deployment [romero2015fitnetshintsdeepnets, zagoruyko2016paying]. In the context of large language models, distillation has been extended beyond output logits to intermediate representations, attention maps, and hidden states, enabling improved preservation of structural and reasoning behaviors [sun2019patientknowledgedistillati"
  },
  {
    "title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
    "url": "https://arxiv.org/abs/2602.04883v1",
    "source": "arxiv",
    "summary": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) ",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Related Work\n\nFlow and diffusion-based structure generative models.\nAutoregressive modeling.\n\n\n\n3 Protein Autoregressive Modeling\n\n3.1 Multi-scale Protein Downsampling\n\n3.2 Coarse-to-Fine Backbone Autoregressive Modeling\n\nAutoregressive transformer for scale-wise conditioning.\nFlow-based atomic decoder.\nMulti-scale structure generation.\n\n\n\n3.3 Mitigating Exposure Bias\n\nNoisy context learning.\nScheduled sampling.\n\n\n\n\n\n4 Experiments\n\n\n4.1 Protein Backbone Generation\n\nGeneration over scales.\nUnconditional generation benchmark.\n\n\n\n4.2 Zero-Shot Task Generalization\n\nGuiding backbone generation with human-specified prompt.\nMotif scaffolding.\n\n\n\n4.3 Empirical Analysis of Multiscale PAR\n\nScaling effects of PAR.\nEfficient sampling with multi-scale orchestration of SDE/ODE.\nMitigating exposure bias.\nInterpreting multi-scale PAR.\nMulti-scale formulation.\nAR Transformer improves structural consistency.\n\n\n\n\n5 Discussion\n\n6 Implementation and Evaluation Details\n\n\n6.1 Implementation Details\n\nScale Embedding.\nInterpolated Position Embedding.\n\n\n\n6.2 Evaluation Metrics\n\nDesignability.\nDiversity.\nSecondary Structure.\nFr√©chet Protein Structure Distance (FPSD).\nProtein Fold Score (fS).\n\n\n\n6.3 Unconditional Backbone Generation\n\nSelf conditioning.\nLow temperature sampling.\nCategory of unconditional backbone generation baselines.\n\n\n\n\n7 Datasets\n\n8 More Empirical Analysis\n\n8.1 Efficient Sampling with SDE/ODE Orchestration\n8.2 Ablation with Self-Conditioning\n\n8.3 Long Protein Generation\n\nFinetuning on longer protein chains.\nLong-protein generation.\n\n\n8.4 Foldseek Cluster Diversity\n8.5 Zero-shot Motif Scaffold Benchmark\n8.6 Scale-Agnostic Inference\n\n8.7 Ablating AR and Decoder Size\n\nPer-token vs per-scale decoder\nLarge vs. small decoder.\nLarge AR vs small AR.\n\n\n\n8.8 Sequence-Based Downsampling Preserves Pairwise Spatial Relationships\n\nSpatial relationships in downsampled 1D sequence.\nSpatial relationships in 3D space after downsampling.\nDoes sequence-based downsampling preserve spatial relationships?\n\n\n8.9 Visualization of Attention Scores\n\n\n\n9 Other Related Work\n\nFlow and diffusion-based structure generative models.\n\n\n\n\n\n\n\n\n1]ByteDance Seed\n2]University of Illinois Urbana-Champaign\n\\contribution[*]Equal Contributions\n\\contribution[‚Ä†]Project Lead\n\\contribution[‚Ä°]Corresponding Author\n\nProtein Autoregressive Modeling via Multiscale Structure Generation\n\n\nYanru Qu\n\n‚ÄÉ‚ÄÉ\nCheng-Yen Hsieh\n\n‚ÄÉ‚ÄÉ\nZaixiang Zheng\n\n‚ÄÉ‚ÄÉ\nGe Liu\n\n‚ÄÉ‚ÄÉ\nQuanquan Gu\n\n[\n\n[\n\nquanquan.gu@bytedance.com\n\n\n(February 3, 2026)\n\nAbstract\nWe present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction.\nUsing the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales.\nTo achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings.\nMoreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality.\nWe effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation.\nNotably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning.\nOn the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior.\nTogether, these properties establish PAR as a promising framework for protein structure generation.\n\n\n\\correspondence\nQuanquan Gu \\checkdata[Project Page]https://par-protein.github.io\n\\checkdata[Note]Work was done during Yanru Qu‚Äôs internship at ByteDance Seed\n\n\nFigure 1: Overview of PAR. PAR comprises the autoregressive (AR) transformer ùíØŒ∏\\mathcal{T_{\\theta}} and the flow-based backbone decoder ùêØŒ∏{\\mathbf{v}}_{\\theta}. During training, we downsample a backbone ùê±‚àà‚ÑùL√ó3{\\mathbf{x}}\\in\\mathbb{R}^{L\\times 3} into multi-scale representations {ùê±1,‚Ä¶,ùê±}\\{{\\mathbf{x}}^{1},\\ldots,{\\mathbf{x}}\\}. AR transformer performs next-scale prediction, producing conditional embeddings (ùê≥1,‚Ä¶,ùê≥n)({\\mathbf{z}}^{1},\\ldots,{\\mathbf{z}}^{n}) from (bos,‚Ä¶,ùê±n‚àí1)(\\textit{bos},\\ldots,{\\mathbf{x}}^{n-1}). The shared flow-based decoder learns to denoise backbones ùê±i{\\mathbf{x}}^{i} at each scale conditioned on ùê≥i{\\mathbf{z}}^{i}. At inference, PAR autoregressively generates ùê±i{\\mathbf{x}}^{i} until the final structure ùê±{\\mathbf{x}} is constructed.\n\n\n\n1 Introduction\n\nDeep generative modeling of proteins has emerged as a way to design and model novel structures with desired functions and properties, with broad applications in biomedicine and nanotechnology [21, 25].\nA widely adopted approach is to directly model the distribution of three-dimensional protein structures, which govern protein function.\nTypically, structure generative models produce protein backbones without sequences or side chains.\nPrior work in this area could be broadly categorized into methods that predict the SE(3) backbone frame representations [47, 45] and those that directly model atoms, e.g., CŒ±\\alpha coordinates for simplicity and scalability [14, 31].\nHowever, all these works are based on diffusion models and their variations (e.g., flow matching).\n\n\nOn the other hand, autoregressive (AR) modeling has emerged as a powerful paradigm for large language models [1, 41].\nAR models employ next-token prediction to model the probability of each token based on prior ones, showing striking empirical behaviors such as scalability [24] and zero-shot generalization to unseen tasks [6].\n\n\nDespite its success in other domains, AR modeling has received little attention in backbone modeling.\nWe identify two main reasons.\n(i) Extending AR models to continuous data, e.g. atomic positions in 3D, often relies on data discretization [12], which can reduce structural fidelity and fine-grained details for proteins, limiting generative performance [19].\n(ii) Protein residues exhibit strong bidirectional dependencies: residues distant in sequence may be spatially close and form hydrogen bonds or hydrophobic contacts.\nThis mutual dependency conflicts with the unidirectional assumption of standard AR models,\nand thus limits the quality of previous attempts on autoregressive structure generation [13].\nA natural question therefore arises: can we apply AR modeling to protein backbone design?\n\n\nIn this paper, we answer the above question affirmatively, and propose PAR, a Protein AutoRegressive framework, to unlock the power of AR models for protein backbone generation.\nWe take initiative from the hierarchical nature of proteins: their structures span multiple scales of granularity, from coarse 3D topology and tertiary fold arrangements, local secondary structures, to the finest atomic coordinates.\nPAR thus adopts a multi-scale autoregressive framework via next-scale prediction, predicting each scale conditioned on prior coarser scales.\nThis strategy, inspired by advances in image generation, enabled AR models to surpass strong diffusion models in image synthesis for the first time [40], and further allows multimodal LLMs to achieve unified text and image generation framework [28].\n\n\nBuilding on this multi-scale framework, PAR includes three key components (Fig. 1).\nThe multi-scale downsampling creates coarse-to-fine structural representations to serve as structural context and targets during training.\nAR transformer, a stack of non-equivariant attention layers [42], encodes all preceding scales to produce a scale-wise conditional embedding following Li et al. [30].\nThe flow-based backbone decoder is conditioned on this embedding to model CŒ±\\alpha backbone atoms directly.\nAs a result, PAR avoids both discretization of protein structures and residue-wise unidirectional autoregressive ordering, thereby overcoming the two aforementioned limitations that compromise structural fidelity and generative quality.\nMoreover, training on ground-truth structural context, AR models suffer from exposure bias [3], which is a key challenge substantially reducing structure generation quality in our preliminary study.\nWe effectively mitigate such issue via noisy context learning and scheduled sampling, allowing the model to learn from corrupted context.\n\n\nFigure 2: Samples generated by PAR over scales. We illustrate PAR‚Äôs generation process across five scales. Much like sculpting a statue, the model first formulates the global structural layout at coarse scales and progressively refines the details at later scales.\n\n\nThis multi-scale approach introduces several notable model behaviors. PAR generates backbones by establishing a global topology and performing refinements, analogous to progressively sculpting a statue into a masterpiece (Fig. 2).\nFor unconditional generation, PAR exhibits favorable scaling behavior, yielding competitive results on distributional metrics like Fr√©chet Protein Structure Distance (FPSD).\nUnlike diffusion models, which operate at a single scale, PAR flexibly handles inputs at various granularities, and hence shows zero-shot generalization in tasks like prompt-based generation and motif scaffolding.\nThe multi-scale formulation enables PAR to orchestrate sampling strategies, achieving a 2.5x sampling speedup compared to single-scale baselines.\nFinally, PAR provides a more general framework, incorporating flow-based models as a special case when restricted to a single scale, and thus remains compatible with techniques from flow-bas"
  },
  {
    "title": "Contrastive Continual Learning for Model Adaptability in Internet of Things",
    "url": "https://arxiv.org/abs/2602.04881v1",
    "source": "arxiv",
    "summary": "Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learni",
    "full_text": null
  },
  {
    "title": "Rethinking the Trust Region in LLM Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.04879v1",
    "source": "arxiv",
    "summary": "Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of samp",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Policy Performance Difference\n2.2 Policy Improvement Bound\n2.3 Trust Region Policy Optimization\n\n\n3 Trust Region Under LLM Regime\n\n4 Methodology\n\n4.1 Proximal Policy Optimization\n4.2 Limitations of PPO Ratio Clipping\n4.3 Divergence Proximal Policy Optimization\n4.4 Approximating Distribution Divergence\n\n\n\n5 Analysis on Training Stability\n\n5.1 The Necessity of a Trust Region\n5.2 The Correct Anchor for the Trust Region\n5.3 Identifying the Source of Instability\n5.4 The Pitfalls of Truncated Importance Sampling\n\n\n6 Analysis on Training Efficiency\n7 Scaling Experiments\n8 Conclusion\n\nA Related Work\n\nA.1 Extended Connections to Existing Work\nA.2 Training-inference Mismatch\n\n\n\nB Trust Region in LLMs\n\nB.1 Proof of Performance Difference Identity\nB.2 Proof of Policy Improvement Bound\nB.3 A Tighter Policy Improvement Bound\nB.4 Comparing Surrogate Objectives with Classical RL\n\n\n\nC Approximations as Lower Bounds of True Divergence\n\nC.1 Total Variation Divergence\nC.2 KL Divergence\n\n\n\nD More Details for Stability Analysis\n\nD.1 Algorithmic Details for Stability Analysis\n\n\nE Characterizing Clipped Tokens\nF More Details for Scaling Experiments\n\nG More Empirical Results\n\nG.1 Extended Main Results\nG.2 Ablation on TV/KL Approximation\nG.3 Extended Results for Different Model √ó\\times Task Combinations\n\n\n\n\n\n\n\nRethinking the Trust Region in LLM Reinforcement Learning\n\n\nPenghui Qi\n\n‚ÄÉ‚ÄÉ\nXiangxin Zhou\n\n‚ÄÉ‚ÄÉ\nZichen Liu\n\n‚ÄÉ‚ÄÉ\nTianyu Pang\n\n‚ÄÉ‚ÄÉ\nChao Du\n\n‚ÄÉ‚ÄÉ\nMin Lin\n\n‚ÄÉ‚ÄÉ\nWee Sun Lee\n\n\n\nAbstract\nReinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.\n\nLLMs, Reinforcement Learning, Trust Region, Training Stability, Training Efficiency\n\n\nFigure 2: The plots show numerical differences between a training and an inference engine for Qwen3-30B-A3B-Base with identical parameters. (Left) The probability ratio (used in PPO) is highly volatile for low-probability tokens. (Right) In contrast, the TV divergence (used in DPPO) is more stable.\nThis highlights a key flaw of PPO‚Äôs clipping mechanism: it over-penalizes low-probability tokens, which can slow down learning; and under-penalizes high-probability tokens, which can permit large, destabilizing updates.\n\n\n\n\n1 Introduction\n\nReinforcement learning (RL) is a foundational paradigm for fine-tuning Large Language Models (LLMs), enabling alignment with human preferences (Ouyang et al., 2022; Rafailov et al., 2023) and complex reasoning tasks (Guo et al., 2025; Qi et al., 2025a). Proximal Policy Optimization (PPO111We denote PPO by its ratio-clipping loss, regardless of advantage estimation. Under this definition, GRPO is a PPO variant.) (Schulman et al., 2017) has established itself as the de facto standard in this domain, favored for its simplicity and empirical scalability. Central to PPO is a heuristic clipping mechanism designed to prevent destructive policy updates. By constraining the probability ratio between new and old policies, the algorithm aims to confine learning to a trust region where monotonic improvement is theoretically guaranteed (Schulman et al., 2015).\n\n\nDespite its widespread adoption, we argue that PPO‚Äôs core mechanism, ratio clipping, is structurally ill-suited for the expansive, long-tailed vocabularies inherent to LLMs. Unlike Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), which constrains the KL or Total Variation (TV) divergence of the policy distribution, PPO constrains updates based on the probability ratio of the sampled token. As we demonstrate later, this approach functions as a noisy, single-sample Monte Carlo estimate of the true policy divergence. While this approximation suffices for classical RL environments with limited action spaces, it fails in the LLM regime due to the ratio‚Äôs hypersensitivity to the probability magnitude. For example, increasing a rare token‚Äôs probability from 10‚àí510^{-5} to 10‚àí310^{-3} generates a massive ratio of 100100 that triggers clipping, even though the actual divergence is negligible. Conversely, small ratio changes on high-probability tokens can make catastrophic shifts in probability mass (e.g., a drop from 0.990.99 to 0.80.8), yet it often remains unpenalized by the clipping mechanism.\n\n\nThis implicit bias is exacerbated by the training-inference mismatch¬†(Yao et al., 2025; Qi et al., 2025b; Zheng et al., 2025), where numerical discrepancies arise between training and inference engines even under identical parameters. As illustrated in Figure¬†2, the probability ratio becomes highly volatile for low-probability tokens, while TV divergence remains stable. Consequently, PPO creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, slowing learning, while updates to high-probability tokens are under-penalized, risking instability. These limitations necessitate a fundamental rethinking of the trust region approach in LLM fine-tuning to ensure both efficiency and stability.\n\n\nTo address these fundamental limitations, we propose Divergence Proximal Policy Optimization (DPPO), a framework that substitutes PPO‚Äôs heuristic clipping with a more principled constraint grounded in trust region theory. Rather than relying on noisy single-sample ratios, DPPO directly estimates policy divergence (e.g., TV or KL divergence). To ensure memory feasibility for LLMs, we introduce two efficient approximations, Binary and Top-K divergence, which capture essential distributional shifts with negligible overhead. This allows DPPO to rigorously distinguish between safe and unsafe updates, effectively resolving the problems of over- and under-constraining inherent in standard PPO.\n\n\nIn this work, we provide a comprehensive rethinking of the trust region in the context of LLM fine-tuning. Our contributions are threefold. Theoretical Formulation: We derive policy improvement bounds specifically tailored to the finite-horizon, undiscounted setting of LLM generation, establishing a rigorous theoretical foundation for trust-region methods in this domain. Stability and Efficiency Analysis: We isolate the primary sources of training instability to provide practical stabilization guidelines, while further highlighting the significant role that low-probability tokens play in driving exploration. Algorithmic Performance: We demonstrate that DPPO achieves superior stability and final performance compared to existing methods like GRPO, providing a robust new framework for RL-based fine-tuning.\n\n\n\n\n2 Background\n\n\n2.1 Policy Performance Difference\n\nWe begin with the standard formulation of a Markov Decision Process (MDP), defined by the tuple ‚Ñ≥=(ùíÆ,ùíú,P,r,œÅ0,Œ≥)\\mathcal{M}=(\\mathcal{S},\\mathcal{A},P,r,\\rho_{0},\\gamma), which includes the state space ùíÆ\\mathcal{S}, action space ùíú\\mathcal{A}, transition dynamics P‚Äã(s‚Ä≤|s,a)P(s^{\\prime}|s,a), reward function r‚Äã(s,a)r(s,a), initial state distribution œÅ0‚Äã(s)\\rho_{0}(s), and a discount factor Œ≥‚àà[0,1]\\gamma\\in[0,1]. A stochastic policy œÄ‚Äã(a|s)\\pi(a|s) generates trajectories œÑ=(s0,a0,r0,s1,a1,r1,‚Ä¶)\\tau=(s_{0},a_{0},r_{0},s_{1},a_{1},r_{1},\\ldots) by sampling actions at‚àºœÄ(‚ãÖ|st)a_{t}\\sim\\pi(\\cdot|s_{t}) and transitioning to states st+1‚àºP(‚ãÖ|st,at)s_{t+1}\\sim P(\\cdot|s_{t},a_{t}). The central goal of RL is to find a policy that maximizes the expected discounted return:\n\n\n\nŒ∑‚Äã(œÄ)=ùîºœÑ‚àºœÄ‚Äã[‚àët=0‚àûŒ≥t‚Äãrt].\\eta(\\pi)=\\mathbb{E}_{\\tau\\sim\\pi}\\Bigg[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\Bigg].\n\n\n\nTo facilitate policy optimization, we define the standard value functions under a policy œÄ\\pi: the state-value function VœÄ‚Äã(s)=ùîºœÑ‚àºœÄ‚Äã[‚àët=0‚àûŒ≥t‚Äãrt|s0=s]V^{\\pi}(s)=\\mathbb{E}_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\big|s_{0}=s\\Big], the action-value function QœÄ‚Äã(s,a)=ùîºœÑ‚àºœÄ‚Äã[‚àët=0‚àûŒ≥t‚Äãrt|s0=s,a0=a]Q^{\\pi}(s,a)=\\mathbb{E}_{\\tau\\sim\\pi}\\Big[\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}\\big|s_{0}=s,a_{0}=a\\Big], and the advantage function AœÄ‚Äã(s,a)=QœÄ‚Äã(s,a)‚àíVœÄ‚Äã(s)A^{\\pi}(s,a)=Q^{\\pi}(s,a)-V^{\\pi}(s). A key theoretical tool for relating the performance of two distinct policies is the policy performance difference theorem (Kakade and Langford, 2002). It states that for any two policies, a target policy (to be optimized) ùúã\\operatorname*{{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\pi}}} and a behavior policy (for rollout) ùúá\\operatorname*{{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\mu}}}, their expected returns are related by:\n\n\n\nŒ∑‚Äã(ùúã)‚àíŒ∑‚Äã(ùúá)=11‚àíŒ≥‚Äãùîºs‚àºœÅùúã,a‚àºùúã(‚ãÖ|s)‚Äã[Aùúá‚Äã(s,a)].\\eta(\\operatorname*{{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\\pgfsys@color@gray@stroke{0}\\pgfsys@color@gray@fill{0}\\pi}}})-\\eta(\\operatorname*{{{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokeco"
  },
  {
    "title": "Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning",
    "url": "https://arxiv.org/abs/2602.04872v1",
    "source": "arxiv",
    "summary": "Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal lea",
    "full_text": "\n\n\n\n\n1 Introduction\n\nPaper organization.\n\n\n2 Prior Work\n\n3 Problem Setup\n\nNotation.\n\n\n\n4 ICL and Failure of LSA\n\n\n4.1 ICL Definition\n\nFormal ICL Criterion.\n\n\n\n4.2 Baseline Model\n\nSingle-layer LSA.\n\n\n\n\n\n5 A Multi-layer CA Model\n\n\n5.1 Weight Simplifications\n\nOne-parameter Model.\nTwo-parameter Model.\n\n\n\n\n\n6 Training Setup and Main Results\n\n6.1 Training Setup\n6.2 Main Results\n6.3 The limiting parameter Œ±‚àó\\alpha^{\\ast}\n6.4 The role of depth\n\n\n7 Numerical Experiments\n8 Discussion\n\nA Notation, Basic Identities, and the Bayes-predictor\n\nA.1 Notation and Conventions\nA.2 Joint Gaussian Form and Bayes-predictor\nA.3 Law of Large Numbers for Prompt Statistics\n\n\nB Proof of Theorem 4.1\n\nC CA Algebra and Preliminaries\n\nC.1 LSA Readout with Frozen Parameters\nC.2 Solving the CA Recurrence\n\n\n\nD Proof of Theorem¬†6.2\n\nD.1 Convergence of Gradient Flow\nD.2 Asymptotic Depth Behavior\n\n\n\nE Proof of Theorem 6.3\n\nE.1 Convergence of Gradient Flow\nE.2 Asymptotic Depth Behavior\n\n\nF Ablation Experiments\nG Visualization of the Loss Landscape\n\n\n\n\n\nMulti-layer Cross-Attention is Provably Optimal for \nMulti-modal In-context Learning\n\n\nNicholas Barnfield\n\nDepartment of Statistics, Harvard University\n\n\nSubhabrata Sen\n\nDepartment of Statistics, Harvard University\n\n\nPragya Sur\n\nDepartment of Statistics, Harvard University\n\n\n\nAbstract\nRecent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood.\nWe introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.\n\n\n\n1 Introduction\n\nLarge language models exhibit striking ‚Äúin-context learning‚Äù (ICL) behavior (Brown et al., 2020): given a sequence of input-output pairs for a new task, the model can infer the rule (the function from input to output) without any update to the model weights. This phenomenon has motivated a growing body of theoretical work that tries to understand when and how ICL succeeds in linearized attention-based models (Zhang et al., 2024a; Lu et al., 2025; Zhang et al., 2025).\n\n\nPrior work on ICL focuses on unimodal data; in contrast, modern foundation models routinely process multi-modal data, such as text, images, video (Li et al., 2024), multi-omic data (Cui et al., 2025), etc. This prompts the natural question:\n\nCan attention-based neural networks learn prediction rules in-context when deployed on multi-modal datasets?\n\nIn this work, we formulate and study an ICL problem on multi-modal data. Our main contributions are as follows:\n\n\n‚Ä¢\n\nWe introduce a statistical model for ICL on multi-modal data. The proposed data distribution is a latent factor model where the factors are dependent across the data modalities.\n\n\n\n‚Ä¢\n\nWe show that a single-layer, linear self-attention (LSA) model fails to achieve Bayes-optimal performance in-context over this class of data distributions.\n\n\n\n‚Ä¢\n\nWe propose a multi-layer model with cross-attention (CA) plus self-attention (SA) and skip connections to solve the ICL problem in this context. Linearizing the attention mechanism, we show that, upon a suitable restriction of the model parameters, gradient flow on the population loss of converges to the Bayes-optimal ICL predictor.\n\n\n\n\n\nTo highlight the technical challenge in our setting, note that prior work on ICL in a regression setting assumes that the covariates are sampled from a fixed distribution (usually Gaussian) independent of the learning task (Zhang et al., 2024a, 2025; Lu et al., 2025). Indeed, (Zhang et al., 2024a, 2025) explicitly notes that the invariance of the covariate distribution across prompts is crucial in establishing the success of single-layer LSA. In sharp contrast, our multi-modal model exhibits covariate-shifts across prompts. In addition, the prediction rule in a prompt is naturally coupled with the covariate distribution in our setting.\nIn Theorem 4.1, we formally establish that the natural covariate shift in our setup invalidates single-layer LSA (i.e., models based on single-layer LSA fail to be Bayes optimal). To overcome this challenge, we introduce a multi-layer model based on both linear cross-attention (LCA) and LSA, and show that it succeeds on the ICL task.\n\n\nAt an intuitive level, the success of attention-based neural networks is derived from their ability to learn ‚Äúlong-range‚Äù relations among the features. In multi-modal learning, it is critical to learn the dependence among features across the observed modalities. The CA mechanism has been suggested as a natural extension of the attention mechanism to this end (Vaswani et al., 2017; Lu et al., 2019); in our work, we demonstrate the efficacy of deep neural networks which use both LCA and LSA layers for multi-modal learning.\n\n\nPaper organization.\n\nThe rest of the paper is organized as follows. In Section¬†2, we review prior work on ICL and multimodal learning. In Section¬†3, we introduce the latent-factor data distribution. In Section¬†4, we define the ICL objective and show a single-layer LSA baseline fails at the ICL task. In sections¬†5‚Äì6, we propose the multi-layer CA architecture and show that gradient flow converges to the Bayes-optimal in-context prediction. In Section¬†7, we present supporting numerical experiments, and then conclude in Section¬†8 with discussion and future directions. Full proofs are left for Appendices A‚ÄìE. Appendices F‚ÄìG include additional numerics.\n\n\n\n\n\n2 Prior Work\n\nICL capabilities of large language models were first noted in (Brown et al., 2020). These observations motivated significant recent research on the foundations and intricacies of ICL. Early investigations focused on the expressivity of transformers\n(Bai et al., 2023; aky√ºrek2023learningalgorithmincontextlearning; Garg et al., 2022) and demonstrated that their ICL capabilities enable them to implement common statistical algorithms and learn certain function classes. Connections of ICL with meta-learning and variants of gradient descent were noted in (Von Oswald et al., 2023; Ahn et al., 2023; Zhang et al., 2024b).\nGeneralization and stability properties of ICL were studied in (Li et al., 2023), while a Bayesian interpretation was offered in (Xie et al., 2022).\n\n\nTraining dynamics of attention-based models and ICL performance of the trained models have also been investigated thoroughly.\nPerhaps the closest to our work is (Zhang et al., 2024a), which studies ICL in the context of regression, demonstrating that a single-layer LSA model trained by gradient flow on the population loss learns the Bayes-optimal predictor. Subsequently, gradient flow or descent dynamics for multi-head self-attention were studied in (Chen et al., 2024; Zhang et al., 2025). The recent work (Huang et al., 2023) goes beyond linearized attention models, and studies gradient descent on stylized one-layer transformers with non-linear softmax attention.\n\n\nMore recently, ICL has been studied for a wider variety of models ranging from Gaussian mixture classification and clustering to non-parametric regression (Shen et al., 2025; Maulen-Soto et al., 2025; Ma et al., 2025; Ching et al., 2026). Attention-based models have also been studied through a Gaussian sequence multi-index modeling lens in (Cui et al., 2024; Arnaboldi et al., 2025; Troiani et al., 2025) and on sparse-token classification tasks (Oymak et al., 2023; Barnfield et al., 2025), although ICL was not necessarily a key focus in these lines of work.\n\n\nIn a different direction, ICL performance for infinite token dimension was studied in (Lu et al., 2025); they uncovered interesting trade-offs for ICL with regards to the number of pretraining examples, task diversity and other problem parameters; see also (Wu et al., 2024) for the impact of the number of independent pre-training tasks. The recent work (Letey et al., 2025) investigated the impacts of pre-training and testing task covariance mismatch. Despite this extensive literature, these prior works are restricted to data arising from a single mode, whereas we focus on ICL for multi-modal data.\n\n\nIn diverse artificial intelligence problems, multi-modal learning‚Äîcombining information from varied modes, e.g., text, image, video‚Äîis known to improve prediction, reasoning and learning capabilities. Although examined extensively in empirical studies (Radford et al., 2021; Alayrac et al., 2022; Jaegle et al., 2021; Wang et al., 2024), this setting is far from understood from a rigorous standpoint. Bridging this theoretical gap is a primary aim of our manuscript.\n\n\nSpecifically, multimodal learning is useful when the modes share common information. To capture this, we use latent variable models that have been widely used in statistical estimation for studying multimodal data. Relevant works in this regard include (Nandy and Ma, 2024; Ding et al., 2022; Mergny and Zdeborov√°, 2025; Keup and Zdeborov√°, 2025; Deshpande et al., 2018; Yang et al., 2025; Sergazinov et al., 2025). However, these papers do not study ICL or attention-based models in this context. On the theo"
  },
  {
    "title": "Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism",
    "url": "https://arxiv.org/abs/2602.04870v1",
    "source": "arxiv",
    "summary": "Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, a",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Prerequisites\n\n2.1 Notation\n2.2 Mixture of Experts\n2.3 Expert Parallel\n2.4 Hardware-Aware Design\n\n\n\n3 Method\n\n3.1 Multi-Head LatentMoE\n3.2 Head Parallel\n3.3 IO-Aware Routing\n3.4 IO-Aware Expert Computation\n\n\n\n4 Experiments\n\n4.1 Language Modeling\n4.2 Per-Component Analysis\n4.3 Ablation: Head Configuration\n4.4 Ablation: Separate Routing Tokens\n\n\n\n5 Related Work\n\nLatent Representations in MoE\nDistributed Training\n\n\n6 Conclusion\nA Implementation Details\n\nB Model Hyperparameters\n\nTraining hyperparameters (shared).\n\n\n\n\n\n\n\nMulti-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism\n\n\nChenwei Cui\n\n‚ÄÉ‚ÄÉ\nRockwell Jackson\n\n‚ÄÉ‚ÄÉ\nBenjamin Joseph Herrera\n\n‚ÄÉ‚ÄÉ\nAna Mar√≠a T√°rano\n\n‚ÄÉ‚ÄÉ\nHannah Kerner\n\n\n\nAbstract\nLarge language models have transformed many applications but remain expensive to train.\nSparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method.\nHowever, EP has three limitations:\ncommunication cost grows linearly with the number of activated experts kk,\nload imbalance affects latency and memory usage, and\ndata-dependent communication requires metadata exchange.\nWe propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving\nO‚Äã(1)O(1) communication cost regardless of kk,\ncompletely balanced traffic, and\ndeterministic communication,\nall while remaining compatible with EP.\nTo accelerate Multi-Head LatentMoE,\nwe propose IO-aware routing and expert computation.\nCompared to MoE with EP, Multi-Head LatentMoE with HP trains up to 1.61√ó1.61\\times faster while having identical performance.\nWith doubled granularity, it achieves higher overall performance while still being 1.11√ó1.11\\times faster.\nOur method makes multi-billion-parameter foundation model research more accessible.\n\n\nhttps://github.com/kerner-lab/Sparse-GPT-Pretraining\n\nMixture of Experts, Distributed Training, Hardware Awareness, Large Language Models\n\n\n\n1 Introduction\n\nLarge language models have transformed applications such as code generation¬†(Achiam et al., 2023), but training them remains expensive¬†(Abnar et al., 2025).\nPretraining a large model consumes substantial compute and electricity¬†(Besiroglu et al., 2024).\nThese costs limit who can conduct foundation model research.\nReducing training costs while maintaining model quality has become a key interest in machine learning research.\n\n\nThe sparse Mixture of Experts (MoE) architecture addresses this challenge through conditional computation¬†(Jacobs et al., 1991).\nFor each input token, a router activates only a small subset of expert networks.\nThis enables models to scale capacity without proportionally increasing compute¬†(Shazeer et al., 2017; Fedus et al., 2022).\n\n\nExpert Parallel (EP) is the standard distributed training method for MoEs¬†(Lepikhin et al., 2020; Fedus et al., 2022).\nEach token is routed to kk experts, replicated kk times, and sent to corresponding GPUs via all-to-all communication.\nResults are processed by experts and returned via another all-to-all operation¬†(Shoeybi et al., 2019).\n\n\nEP has three limitations.\nFirst, both communication volume and all-to-all latency are O‚Äã(k)O(k).\nSecond, expert load imbalance further degrades all-to-all performance.\nThird, non-deterministic communication necessitates an additional all-to-all to exchange metadata¬†(Shoeybi et al., 2019).\n\n\nRecently, LatentMoE¬†(Elango et al., 2026) addressed the communication bottleneck by projecting tokens from hidden dimension dd into a smaller latent dimension dhd_{h} before expert routing and computation.\nThis reduced both per-expert parameter loads and all-to-all traffic by a factor of ddh\\frac{d}{d_{h}}, enabling higher kk values at similar communication costs.\nHowever, LatentMoE still relied on Expert Parallel; communication happened after routing, so load imbalance and non-deterministic patterns persisted.\n\n\nWe propose Multi-Head LatentMoE and Head Parallel (HP) to address these limitations.\nMulti-Head LatentMoE decomposes a single MoE into multiple independent smaller modules, splitting each input token into NhN_{h} sub-tokens.\nEach sub-token is processed by an independent MoE module with its own router and experts.\nHP exploits this structure.\nThe sub-tokens are distributed to GPUs before any routing decision.\nEach GPU completes all routing and expert computation locally.\nThis has three advantages over EP:\n\n\n1.\n\nCommunication volume is constant because each token is sent exactly once before routing.\n\n\n\n2.\n\nTraffic is balanced because sub-tokens are evenly distributed regardless of routing decisions.\n\n\n\n3.\n\nThe inter-GPU communication is deterministic because it does not depend on routing decisions.\n\n\n\n\n\nA naive implementation of Multi-Head LatentMoE would multiply High Bandwidth Memory (HBM) usage and IO cost by NhN_{h}, because it materializes the full routing scores and expert activations for each head.\nWe develop IO-aware routing by doing online top-kk directly in SRAM, reducing HBM access from O‚Äã(T‚ãÖNe+Ne)O(T\\cdot N_{e}+N_{e}) to O‚Äã(T+Ne)O(T+N_{e}), where TT is the number of tokens and NeN_{e} is the number of experts.\nWe also develop IO-aware expert computation by formulating expert computation as block-sparse attention and leveraging FlexAttention¬†(Dong et al., 2024), reducing HBM access from O‚Äã(T‚ãÖde+de)O(T\\cdot d_{e}+d_{e}) to O‚Äã(T+de)O(T+d_{e}), where ded_{e} is the number of neurons per expert.\nBoth algorithms are exact.\n\n\nWe evaluate Multi-Head LatentMoE with HP on language modeling using 10B tokens from FineWebEdu¬†(Penedo et al., 2024).\nOur method trains up to 1.61x faster than the standard MoE with EP while having identical performance.\nThe inter-GPU communication volume is reduced to 25% when k=4k=4.\nUnder the same number of active parameters, our method can reach 6.9 percentage points higher overall accuracy than MLP.\nWhen doubling the granularity, our method achieves higher overall model performance while still being 1.11√ó1.11\\times faster.\n\n\nOur contributions are:\n\n\n1.\n\nWe propose Multi-Head LatentMoE and Head Parallel, achieving O‚Äã(1)O(1) communication volume for any kk, perfect load balance, and deterministic communication patterns for distributed MoE training.\n\n\n\n2.\n\nWe develop exact IO-aware routing and expert computation operations, making Multi-Head LatentMoE practical and efficient.\n\n\n\n3.\n\nWe demonstrate that Multi-Head LatentMoE with HP trains 1.61x faster than standard MoE with EP.\nWith two times the granularity, it further achieves higher overall accuracy while still being 1.11√ó1.11\\times faster.\n\n\n\n4.\n\nOur work accelerates ultra-sparse MoE pre-training, making multi-billion-parameter foundation model research more accessible to the academic research community.\n\n\n\n\n\n\n\n2 Prerequisites\n\n\n2.1 Notation\n\nWe use italic letters for scalars, bold lowercase for vectors, and bold uppercase for matrices or tensors. BB denotes batch size, TT denotes the number of tokens in a sequence, and tt indexes token position. The model hidden dimension is dd, while NhN_{h} and dhd_{h} denote the number of heads and per-head dimension, respectively. For mixture-of-experts layers, NeN_{e} is the number of experts, kk is the number of activated experts per token, and ded_{e} is the expert hidden dimension. The input and output tokens at position tt are ùê±t‚àà‚Ñùd\\mathbf{x}_{t}\\in\\mathbb{R}^{d} and ùê®t‚àà‚Ñùd\\mathbf{o}_{t}\\in\\mathbb{R}^{d}, with projection matrices ùêñin,ùêñout‚àà‚Ñùd√ód\\mathbf{W}_{\\text{in}},\\mathbf{W}_{\\text{out}}\\in\\mathbb{R}^{d\\times d}.\n\n\n\n\n2.2 Mixture of Experts\n\nMixture of Experts (MoE) scales model capacity without proportionally increasing computational cost¬†(Shazeer et al., 2017).\nThis definition follows Mixtral¬†(Jiang et al., 2024).\nWe re-use this for our baseline, as well as to help define Multi-Head LatentMoE.\n\n\nAn MoE has NeN_{e} neural network modules called experts.\nAmong them, kk experts are activated for each token¬†(Shazeer et al., 2017).\nFor each input token ùê±t\\mathbf{x}_{t}, the corresponding output ùê®t\\mathbf{o}_{t} is:\n\n\n\nùê®t\\displaystyle\\mathbf{o}_{t}\n=‚àëi=1Negi,t‚ÄãEi‚Äã(ùê±t),\\displaystyle=\\sum_{i=1}^{N_{e}}g_{i,t}E_{i}(\\mathbf{x}_{t}),\n\n(1)\n\n\n\ngi,t\\displaystyle g_{i,t}\n=exp‚Å°(gi,t‚Ä≤)‚àëj=1Neexp‚Å°(gj,t‚Ä≤),\\displaystyle=\\frac{\\exp(g^{\\prime}_{i,t})}{\\sum_{j=1}^{N_{e}}\\exp(g^{\\prime}_{j,t})},\n\n(2)\n\n\n\ngi,t‚Ä≤\\displaystyle g^{\\prime}_{i,t}\n={si,t,si,t‚ààtop-‚Äãk‚Äã({sj,t|1‚â§j‚â§Ne})‚àí‚àû,otherwise,\\displaystyle=\\begin{cases}s_{i,t},&amp;s_{i,t}\\in\\text{top-}k(\\{s_{j,t}|1\\leq j\\leq N_{e}\\})\\\\\n-\\infty,&amp;\\text{otherwise}\\end{cases},\n\n(3)\n\n\n\nsi,t\\displaystyle s_{i,t}\n=r‚Äã(ùê±t)i.\\displaystyle=r(\\mathbf{x}_{t})_{i}.\n\n(4)\n\n\nwhere gi,tg_{i,t} is the normalized gating value for the ii-th expert;\nEi‚Äã(‚ãÖ)E_{i}(\\cdot) denotes the ii-th expert;\nsi,ts_{i,t} and gi,t‚Ä≤g^{\\prime}_{i,t} are intermediate values;\ntop-‚Äãk‚Äã(‚ãÖ)\\text{top-}k(\\cdot) selects the top kk values from the set;\nand r‚Äã(‚ãÖ)r(\\cdot) is a linear mapping called the router.\n\n\nTo ensure router stability, our load balancing strategy is global¬†(Qiu et al., 2025) and auxiliary-loss-free (aux-free)¬†(Wang et al., 2024a).\nFollowing¬†(Liu et al., 2024), we replace the first two MoE layers with Multilayer Perceptrons (MLPs) to mitigate router imbalance in early layers.\nThe router computation is performed in FP32.\n\n\n\nFigure 1: Comparison of feedforward architectures. (a) Standard MLP applies a single feedforward network to each token. (b) Standard MoE uses a router to dynamically select experts from a single set. (c) LatentMoE performs routing first, then applies linear down-projection before expert computation and linear up-projection afterward. (d) Multi-Head LatentMoE projects each token into multiple sub-tokens, each processed by an independent MoE module with its own separately-trained router and expert set. Orange blocks denote activated experts; gray blocks denote inactive experts. Black lines indicate data flow.\n\n\n\n\n2.3 Expert Parallel\n\nExpert Parallel (EP) distributes expert weights across GPUs, enabling trainin"
  },
  {
    "title": "CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation",
    "url": "https://arxiv.org/abs/2602.04868v1",
    "source": "arxiv",
    "summary": "Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar",
    "full_text": "\n\n\n\n\n1 Introduction\n\n1.1 Motivation\n1.2 Contribution\n\n\n2 Related Work\n\n3 Simulated robots\n\n3.1 7-d.o.f. robot arm\n3.2 Two-wheeled mobile robot\n\n\n\n4 Benchmarks\n\n4.1 Multi-task line following (MLF)\n4.2 Multi-task pushing objects (MPO)\n4.3 High-Level Reaching (HLR)\n4.4 Kinematic High-Level Reaching (HLR-K)\n4.5 Low-Level Reaching (LLR)\n4.6 Kinematic Low-Level Reaching (LLR-K)\n\n\n\n5 Experiments\n\n5.1 Multi-Task Line-Following (MLF)\n5.2 Multi-task Pushing-Objects (MPO)\n5.3 High-Level Reaching (HLR)\n5.4 Low-Level Reaching (LLR)\n\n\n6 Discussion\n7 Future work\nA Downloading and installing the benchmark suite\n\nB Detailed information about individual benchmarks\n\nB.1 Wheel speeds for MLF and MPO\nB.2 Observation and action spaces for MLF and MPO\nB.3 Task structure for MPO\nB.4 Task structure for MLF\nB.5 Observation and action spaces for HLR and LLR\n\n\n\nC Reward computation\n\nC.1 Multi-task Line Following\nC.2 Multi-task Pushing Objects\nC.3 Low-level Reaching\nC.4 High-level Reaching\n\n\nD Runtime reduction through kinematic benchmark variants\n\nE Extended experimental results\n\nE.1 Multi-task Line Following\nE.2 Multi-task Pushing Objects\nE.3 High-level Reaching\nE.4 Low-level Reaching\n\n\n\n\n\n\n\nCRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation\n\n\nYannick Denker \nDepartment of Computer Science\nFulda University of Applied Sciences\nLeipziger Str. 123, 36037 Fulda, Germany \nyannick.denker@cs.hs-fulda.de\n\n‚ÄÉ‚ÄÉ\nAlexander Gepperth\nDepartment of Computer Science\nFulda University of Applied Sciences\nLeipziger Str. 123, 36037 Fulda, Germany \nalexander.gepperth@cs.hs-fulda.de\n\n\n\nAbstract\nContinual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies.\nIn this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator.\nOur Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter\nis used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles.\nFor the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required),\nand which can be run two orders of magnitude faster.\nCRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of\nalmost arbitrary simulated sensors.\nTo ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and\nreport performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This\nhighlights the suitability as a scalable and reproducible benchmark for CRL research.\n\n\n\n1 Introduction\n\nContinual reinforcement learning (CRL) extends the classical reinforcement learning (RL) framework to settings where an agent\nmust learn a sequence of tasks over time without access to previous environments.\nIn contrast to standard RL, where the environment is typically assumed to be stationary and training data can be revisited indefinitely,\nCRL requires agents to continuously integrate new knowledge while preserving performance on previously encountered tasks.\nThis continual setting is relevant for real-world applications such as robotics,\nautonomous driving, or online decision-making systems, where environmental conditions change over time and retraining from scratch is infeasible.\n\n\n\nA core difficulty in CRL is catastrophic forgetting [21, 28, 23, 18],\nwhere newly acquired knowledge interferes with previously learned policies. Although this problem is similar to to the one faced in Continual Learning (CL),\nwhich deals with learning from non-stationary distributions, but without environment interaction, many established CL methods cannot easily be generalized to CRL.\nIn particular, the assumption that each task is composed of well-defined sample classes,\nand that task onsets and classes are known to the learner, is what distinguishes CL from CRL.\n\n\n\n1.1 Motivation\n\nCurrent CRL research commonly relies on benchmarks such as robotic reaching suites (e.g., Continual World [35])\nand sequential game-based environments (e.g., Atari-based continual learning setups [1, 5]).\nWhile these benchmarks have been valuable for early progress, they exhibit several limitations that constrain their applicability.\nRobotic CRL benchmarks typically offer only a small number of distinct tasks, with limited support for sensors, plugins or actuator extensions.\nMany of these environments also do not work ‚Äùout of the box‚Äù, requiring substantial installation effort and\nrelying on simulation backends that are difficult to configure, distribute, or extend.\nFor Continual World, observations are low-dimensional, and the tasks do not really require learning in the first place,\nsince the robot arm could be controlled directly using inverse kinematics. Additionally, the simulation is kinematics-only, containing no real physics engine.\nGame-based continual RL setups, including Atari, provide high-dimensional observations and actions with a wide variety of tasks.\nHowever, they operate in noise-free, fully deterministic environments that lack physical realism.\nTasks are very diverse, but their intrinsic difficulty is very high, making it difficult to separate intrinsic difficulty from the difficulty of continual learning.\nWe thus require a benchmark suite that incorporates out-of-the-box readiness, physical realism, a well-supported and extensible simulation backend, and a large number of tasks with high diversity.\n\n\nOur goal is to close this gap by introducing the Continual Robotic Simulation Suite (CRoSS), a Gazebo-based CRL benchmark suite that combines\nrealistic robotic simulation with massive task diversity while providing a reproducible, containerized deployment.\nCRoSS is designed around two complementary robotic platforms:\nFirst of all, a two-wheeled differential-drive robot is used in multi-task line-following (MLF) and multi-task pushing-objects (MPO) scenarios, where visual and structural parameters (e.g., line shape, textures, colors) are systematically varied to produce hundreds of distinct (but relatively simple) tasks.\nOn the other hand, a 7-d.o.f. robotic arm is evaluated in high-level reaching (Cartesian end-effector control in six discrete directions mirroring the setup of Continual World) and low-level reaching, where the agent directly manipulates the seven arm joints, increasing task dimensionality and realism.\n\n\nOur benchmark design leverages the Gazebo-Transport middleware communication between sensors, actuators, and agents.\nSince Gazebo and the Robot Operating System (ROS) are compatible through a standardized bridge,\nthis control interface can be directly applied to physical robots with minimal modification, supporting extensions toward sim-to-real continual learning.\nOur environments can be used as drop-in replacements for standard Gymnasium tasks.\nThe environment manager follows the same API conventions as Gymnasium making the benchmark directly compatible with existing RL pipelines and libraries.\nThrough containerization, installation is simplified across a wide range of platforms.\n\n\n\n\n1.2 Contribution\n\nWe introduce a new benchmark suite for CRL with the following key contributions:\n\n\nRealistic robotic environments: We present two Gazebo-based robotic platforms, a 2-wheeled differential-drive robot and a 7-d.o.f. robot arm, enabling embodied CRL experiments in physically consistent simulations.\n\n\nHigh task diversity and scalability: By varying visual and structural parameters, our benchmark generates hundreds of distinct tasks, allowing controlled studies of forgetting, transfer, and scalability in continual learning.\n\n\nMultiple control modalities: The robotic arm is controlled both in cartesian and joint-space mode, allowing varying action complexities.\n\n\nStandardized, reproducible and extensible setup: All environments are provided in a containerized (Apptainer) format that runs out of the box on any Linux system, using a well-supported simulation back-end that can be extended by a variety of community-developed plugins.\n\n\nBaseline evaluations and metrics: We benchmark standard CRL algorithms, including DQN and REINFORCE, and provide evaluation metrics for measuring forgetting, transfer, and final performance.\n\n\nSimulation-to-real compatibility: The benchmark‚Äôs communication framework is built on Gazebo-Transport, which is inter-operable with ROS via an existing bridge, making it straightforward to port trained agents or environments from simulation to hardware platforms.\n\n\nTogether, these contributions establish a scalable, realistic, and easily extensible foundation for advancing continual reinforcement learning research in robotic domains.\nAll benchmark environments, experiment scripts, and documentation are available in our public repository111https://github.com/anon-scientist/continual-robotic-simulation-suite/.\n\n\n\n\n\n2 Related Work\n\nComprehensive surveys have outlined the current state and challenges\nof the CRL field [12, 20, 8].\nThese works summarize existing methods and highlight the open problems in scalability, stability,\nand evaluation that motivate continued research in realistic and dynamic environments.\nMore recently, generative replay and task-agnostic methods have emerged that adapt without explicit task boundaries (e.g.,[17, 11, 34])\nWhile these advances have improved algorithmic ro"
  },
  {
    "title": "Subliminal Effects in Your Data: A General Mechanism via Log-Linearity",
    "url": "https://arxiv.org/abs/2602.04863v1",
    "source": "arxiv",
    "summary": "Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model's properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, pos",
    "full_text": null
  },
  {
    "title": "From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures",
    "url": "https://arxiv.org/abs/2602.04861v1",
    "source": "arxiv",
    "summary": "Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibr",
    "full_text": null
  },
  {
    "title": "CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation",
    "url": "https://arxiv.org/abs/2602.04856v1",
    "source": "arxiv",
    "summary": "From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning ma",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 CoT Datasets Construction\n\n3.1 Generation Settings\n3.2 Unsafe CoT Criteria\n\n\n\n4 From Layer to Attention: A Routing Characterization Framework\n\n4.1 Safety Layer Localization\n4.2 Jacobian Lens for Routing Operators\n\n4.3 Routing Operator Evaluation Metric\n\n4.3.1 Routing Stability\n4.3.2 Routing Geometry.\n4.3.3 Routing Energy\n\n\n\n4.4 Sensitivity Concentration under Routing Perturbations\n\nSafety Assessment After Perturbation\n\n\n\n\n\n5 Experiments and Results\n\n\n5.1 Safety-Critical Layers‚Äô Localization\n\n5.1.1 Key Observation\n5.1.2 Distribution of Critical Layers\n\n\n5.2 Spectral patterns at operator level\n5.3 Perturbation validation\n\n\n6 Discussion and Conclusion\n\nA CoT Dataset Generation\n\nA.1 Seed Dataset Selection.\nA.2 Reasoning LLMs Selection.\n\nA.3 Induction Paradigms.\n\nDirect prompting.\nIndirect prompting.\n\n\n\nA.4 Sytlistic Conditioning.\n\nNew York Times style.\nBBC style.\n\n\nA.5 Annotation process pseudocode\nA.6 Generation Distribution.\nA.7 CoT Case Study.\n\n\n\nB Model Architecture Details\n\nB.1 LLaMA-8B: Shallower‚ÄìWider Trend\nB.2 Qwen-4B/Qwen-8B: Deeper‚ÄìNarrower Trend and Scale Effect\nB.3 Takeaway for Window Shifts\n\n\nC Choosing the window length KK\nD Correlation Calculation.\n\nE Jacobian Martrix\n\nSoftmax Jacobian.\nFirst-order response.\nMass conservation.\nPSD and variance form.\nSpectral norm bound.\nEigen/SVD notation.\n\n\n\nF Metrics‚Äô Theorem\n\nF.1 B1: Stability\nF.2 B2:Geometry\nF.3 B3:Energy\n\n\n\nG Perturbations‚Äô Theorem\n\nG.1 Perturbation Properties\nG.2 Intensity of Perturbations\nG.3 Significance of the Perturbations\n\n\nH Additional Visualization\n\n\n\n\n\nCoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation\n\nAbstract\nFrom generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks. Our codes are available at this website.\n\n\n\\icml@noticeprintedtrue\n\n\n\nZhao Tong1111Equal contribution., Chunlin Gong2111Equal contribution., Yiping Zhang3, Qiang Liu4, Xingcheng Xu5, Shu Wu4\nHaichao Shi1, Xiao-Yu Zhang1222Corresponding author.\n\n\n1Institute of Information Engineering, Chinese Academy of Sciences\n2University of Minnesota\n3University of the Chinese Academy of Sciences\n4Institute of Automation, Chinese Academy of Sciences\n5 Shanghai Artificial Intelligence Laboratory\ntongzhao@iie.ac.cn, gong0226@umn.edu, zhangyiping25@mails.ucas.ac.cn,\nshu.wu@nlpr.ia.ac.cn, zhangxiaoyu@iie.ac.cn\n\n\n\n\n1 Introduction\n\nFigure 1: Unsafe CoT Generation. Left: Despite final refusal, Thinking exposes internal traces (red) encoding actionable fake news strategies. Right: Three reasoning LLMs show Thinking raises unsafe rates approach to 80%, confirming latent risks persist despite surface compliance. surface-level refusal.\n\n\nThe rapid deployment of reasoning-capable Large Language Models (LLMs) has fundamentally reshaped news production pipelines (Brigham et al., 2024; Spangher et al., 2024). Central to these systems is the Chain-of-Thought (CoT) mechanism, which enables models to deliberate internally before generating text. However, while CoT enhances output quality(Kim et al., 2023), it simultaneously introduces a new attack surface: malicious actors can exploit this reasoning process through both carefully crafted direct (Wang et al., 2025a) and indirect (Rahman et al., 2025) jailbreak prompts,\nto elicit factually fabricated yet synthetically coherent narratives. In the Fake News Generation (FNG) scenario, this vulnerability allows adversaries to steer the model‚Äôs internal deliberation toward producing high-quality fake news, posing severe threats to social trust well before the final output is even generated (Hu et al., 2025; Wang et al., 2025b, c).\n\n\nHowever, existing safety measures predominantly focus on alignment at the model output level (Li et al., 2025; Chaudhari et al., 2025), detecting merely whether models refuse harmful requests without scrutinizing the logical patterns embedded within the CoT reasoning process. Consequently, since output-layer defenses cannot intervene during intermediate reasoning stages, fake news may be covertly constructed throughout the CoT process, fundamentally undermining the effectiveness of existing safeguards. Recently, studies have begun advocating for systematic investigation of CoT monitoring (Korbak et al., 2025), with approaches generally categorized into self-evaluation (Chen et al., 2025; Meek et al., 2025) and external-supervision (Arnav et al., 2025; Zhou et al., 2024). Nevertheless, these works have not yet explored the specific behaviors and latent risks of CoT reasoning in FNG tasks, where fabricating credible narratives inherently requires exposing and manipulating internal reasoning traces.\n\n\nTo bridge this gap, we conduct a comprehensive analysis across three reasoning LLMs spanning diverse architectures and scales: Llama-8B, Qwen-4B, and Qwen-8B(Dubey et al., 2024; Bai et al., 2023). We construct a dedicated CoT dataset in FNG tasks and evaluate these models during the reasoning phase. Surprisingly, as shown in Fig.¬†1, we find that even when models appear to refuse harmful requests, roughly 80% of their internal reasoning chains still contain security risks. This alarming susceptibility reveals a fundamental fragility: CoT mechanisms can be maliciously exploited to construct harmful content even when final outputs appear compliant. These findings compel us to ask: Is CoT really the chain of truth?\n\n\nTo answer this question, we propose a unified analytical pipeline that systematically deconstructs CoT generation from a coarse-to-fine perspective. First, at the global architectural level, we quantify semantic perception disparities across layers (Jiang et al., 2025) to localize safety-critical layers, where contiguous mid-depth regions for safe and unsafe reasoning trajectories diverge most sharply. Second, within these safety-critical layers, we further capture the specific safety-critical attention heads and attribute divergence by introducing a Jacobian matrix-based spectral analysis framework. Unlike attention heatmaps that merely visualize routing outcomes, the Jacobian of the softmax operator captures how infinitesimal perturbations in attention scores induce probability reallocation, revealing the mechanistic valves that control information flow.\n\n\nSpecifically, we derive three physics-inspired metrics from the Jacobian‚Äôs spectral properties: Stability (spectral norm) quantifies sensitivity to input perturbations; Geometry (principal singular vector alignment) measures consistency of information-flow directions; and Energy (spectral concentration) characterizes how intensely harmful logic embeds in dominant modes. Together, these metrics precisely identify the critical attention heads that drive unsafe reasoning, transforming the abstract question of CoT safety into concrete, measurable routing properties.\n\n\nThe main contributions are summarized as follows:\n\n\n‚Ä¢\n\nWe systematically reveal the phenomenon of unsafe generation within CoT steps in FNG tasks: approximately 80% of reasoning chains harbor latent security risks even when models refuse harmful requests, challenging the assumption that refusal implies safety.\n\n\n\n‚Ä¢\n\nWe establish a coarse-to-fine analysis framework that traces unsafe generation from critical layers to attention heads, providing the mechanistic explanation of how deceptive reasoning patterns structurally diverge from safe routing.\n\n\n\n‚Ä¢\n\nWe introduce a Jacobian-based spectral evaluation method with three interpretable metrics, i.e., stability, geometry, and energy, enabling precise localization and causal measurement of safety-critical routing pathways in reasoning LLMs.\n\n\n\n\n\n\n\n2 Related Work\n\nCoT Monitoring. CoT monitoring has emerged as a critical safety paradigm for detecting deceptive reasoning¬†(Korbak et al., 2025), with existing approaches falling into two categories: self-evaluation methods assessing reasoning traces via faithfulness metrics¬†(Chen et al., 2025; Meek et al., 2025), and external-supervision techniques employing classifiers or adversarial testing¬†(Arnav et al., 2025; Zhou et al., 2024). However, these methods predominantly assume that output-level refusal guarantees safety throughout the reasoning process, failing to recognize that models may covertly construct harmful logic within CoT steps despite final rejection. Our work explores this leaky nature in fake news generation, providing the first fine-grained attribution of such vulnerabilities to specific attention heads via Jacobian-based spectral metrics.\n\n\nMechanistic Interpretability for Safety Analysis.\nWhile prior monitoring approaches operate at the textual or hidden-state level, they lack mechanistic insights into how models route information during CoT generation. Mec"
  },
  {
    "title": "Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say \"I Don't Know\"",
    "url": "https://arxiv.org/abs/2602.04853v1",
    "source": "arxiv",
    "summary": "Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks.",
    "full_text": null
  },
  {
    "title": "The Key to State Reduction in Linear Attention: A Rank-based Perspective",
    "url": "https://arxiv.org/abs/2602.04852v1",
    "source": "arxiv",
    "summary": "Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear atte",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Theoretical Insights\n\n2.1 Background on Linear Attention\n\n2.2 On the Role of the Rank\n\n2.2.1 Rank Utilization\n2.2.2 Why does Rank Utilization Matter?\n\n\n\n\n\n3 The Proposed Pruning Approach\n\n3.1 Invariant Transformations for Linear Attention\n\n3.2 State Size Reduction Requires Structured Pruning\n\nThe Convolution Constraint.\n\n\n\n3.3 Axis-Aligned Methods\n\nWeight-Magnitude-based (ùêãùüè\\mathbf{L^{1}}).\nS-Wanda.\nSensitivity-based.\nRank-based (DRRQR).\n\n\n\n\n\n4 Related Work\n\nRank Considerations.\nPruning Methods.\nQuantization Methods.\n\n\n\n5 Experiments\n\n\n5.1 Results\n\nLanguage Modeling.\nA Note on PCA.\nAblation on the Coupled Selection.\nSpeedup.\n\n\n\n\n6 Discussion\nA Training Details\n\nB Additional Material\n\nB.1 Structured Pruning from the Test-Time Regression Perspective\n\nB.2 Details on Rank Revealing QR (RRQR)\n\nMathematical Formulation.\nGeometric Intuition.\nThe Swap Criterion.\nAlgorithm and Update Rules.\n\n\nB.3 Derivation of Tighter Error Bounds\nB.4 Adapting Convolutions to General Rotations\nB.5 Optimal Diagonal Adaptation\nB.6 Shared Convolutions\n\n\n\nC Additional Experimental Results\n\nC.1 Singular Value Spectrum\n\nC.2 On PCA and Convolutions\n\n\nC.2.1 Shared Convolutions\n\nFilter Similarity.\nPerformance of Shared Convolutions.\n\n\nC.2.2 Impact of Convolutions on PCA-based Pruning\n\n\nC.3 On Coupled Selection\n\n\nD The Effective Rank\n\nE Dynamical Systems Perspective\n\nContinuous-Time Dynamics.\nEuler Discretization.\nSystem Stability.\n\n\n\nF Proofs\n\nF.1 Proof of Proposition¬†2.2\nF.2 Proof of Algebraic Rank\nF.3 Proof of Theorem¬†2.5\nF.4 Proof of Corollary¬†2.6\n\n\n\nG Extended Results\n\nG.1 Throughput Measurements\nG.2 Language Modeling\n\n\nH More Plots\n\n\n\n\n\nThe Key to State Reduction in Linear Attention: A Rank-based Perspective\n\n\nPhilipp Nazari\n\n‚ÄÉ‚ÄÉ\nT. Konstantin Rusch\n\n\n\nAbstract\nLinear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nLinear Attention¬†(Katharopoulos et¬†al., 2020; Schlag et¬†al., 2021; Sun et¬†al., 2023b; Peng et¬†al., 2023; Gu &amp; Dao, 2024; Yang et¬†al., 2024b; Dao &amp; Gu, 2024; Yang et¬†al., 2024a; Team et¬†al., 2025) has emerged as an efficient alternative to softmax attention (Vaswani et¬†al., 2017), enabling high-throughput chunkwise parallel training¬†(Hua et¬†al., 2022; Sun et¬†al., 2023b; Lingle, 2023; Yang et¬†al., 2023) with linear time complexity and constant memory inference. These efficiency gains have recently driven the development of large hybrid models (Lieber et¬†al., 2024; Li et¬†al., 2025; Blakeman et¬†al., 2025; Team, 2025) which predominantly employ linear attention layers, interspersed with only a few softmax attention layers.\n\n\nDespite their impressive performance, prior work indicates that linear attention models still underutilize their capacity in practice¬†(Siems et¬†al., 2025; Parnichkun et¬†al., 2025). In particular, the matrix-valued hidden states that implement the associative memory exhibit a low-rank structure after training. As we show, this structure can increase the model‚Äôs sensitivity to query noise. This finding suggests that the state size can be reduced post-training, yielding models that are both faster and more memory-efficient.\n\n\nTowards this end, we propose a novel structured pruning framework to reduce the size of the hidden states in linear attention models. Within this framework, our experiments reveal that we can consistently remove approximately 50% of the key and query channels at only a minor increase in perplexity, even before recovery fine-tuning. Unlike methods like SliceGPT¬†(Ashkboos et¬†al., 2024a), SpinQuant¬†(Liu et¬†al., 2024) and QuaRot¬†(Ashkboos et¬†al., 2024b), which introduce rotations in the embedding space, our framework operates in the state space. Our framework is thus complementary to these methods.\n\n\nSpecifically, our approach relies on the observation that linear attention models are invariant under orthogonal transformations applied jointly to the queries and keys. Building on this insight, we seek transformations that select the columns of the keys and queries that contribute substantially to model performance. Within this framework, we reformulate several existing pruning strategies, including magnitude-based and gradient-saliency‚Äìbased ones. Motivated by theoretical insights, we furthermore introduce a novel structured pruning approach that selects a subset of columns that maximizes the rank utilization of the remaining memory.\n\n\nIn summary, our main contributions are:\n\n\n‚Ä¢\n\nWe provide theoretical insights into the role of rank in linear attention (Section¬†2.1). We show that rank utilization, a measure of memory isotropy, directly affects retrieval error, and that low rank utilization can amplify query noise.\n\n\n\n‚Ä¢\n\nMotivated by the low rank utilization observed in practice, we formulate a novel post-training state-size reduction framework (Section¬†3). Specifically, we show that the state can be reduced substantially by jointly selecting subsets of channels from the keys and queries.\n\n\n\n‚Ä¢\n\nWe adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on rank-revealing QR decompositions (Section¬†3.3).\n\n\n\n‚Ä¢\n\nWe present extensive empirical results on multiple large, pretrained linear attention models evaluated across a range of zero-shot and recall tasks, demonstrating the effectiveness of our proposed framework (Section¬†5.1).\n\n\n\n\n\nUnstructured sparsityX‚ãÖ\\cdotùêñùêä\\mathbf{W}_{\\mathbf{K}}==Kùêíùê≠\\mathbf{S_{t}}Structured sparsityX‚ãÖ\\cdotùêñùêä‚Äãùêì‚ä§\\mathbf{W}_{\\mathbf{K}}\\mathbf{T^{\\top}}==Kùêíùê≠\\mathbf{S_{t}}\n\nFigure 1: Left: Unstructured pruning yields a sparse weight matrix ùêñùêä\\mathbf{W}_{\\mathbf{K}} yet preserves the column dimension of ùêä\\mathbf{K}, leaving the domain of the state matrix ùêít‚àà‚Ñùdv,dk\\mathbf{S}_{t}\\in\\mathbb{R}^{d_{v},d_{k}} invariant. Right: Structured pruning eliminates basis vectors, mapping keys to a lower-dimensional space ‚Ñùdk‚Ä≤\\mathbb{R}^{d^{\\prime}_{k}} where dk‚Ä≤&lt;dkd^{\\prime}_{k}&lt;d_{k}. This results in a compressed state ùêít‚àà‚Ñùdv,dk‚Ä≤\\mathbf{S}_{t}\\in\\mathbb{R}^{d_{v},d^{\\prime}_{k}}. This reduction strictly decreases the FLOP count required to compute the recurrence. This figure is inspired by Ashkboos et¬†al. (2024a, Figure 1).\n\n\n\n\n2 Theoretical Insights\n\nIn this section, we first introduce linear attention and describe the role of the hidden state as an associative memory, before developing a framework to assess how effectively it is utilized.\n\n\n\n2.1 Background on Linear Attention\n\nSoftmax attention maps an input sequence ùêó‚àà‚ÑùT,h\\mathbf{X}\\in\\mathbb{R}^{T,h} to queries ùêê=ùêóùêñùêê‚àà‚ÑùT,dk\\mathbf{Q}=\\mathbf{X}\\mathbf{W}_{\\mathbf{Q}}\\in\\mathbb{R}^{T,d_{k}}, keys ùêä=ùêóùêñùêä‚àà‚ÑùT,dk\\mathbf{K}=\\mathbf{X}\\mathbf{W}_{\\mathbf{K}}\\in\\mathbb{R}^{T,d_{k}}, and values ùêï=ùêóùêñv‚àà‚ÑùT,dv\\mathbf{V}=\\mathbf{X}\\mathbf{W}_{v}\\in\\mathbb{R}^{T,d_{v}} via linear projections. The output of the attention layer can then be computed in parallel,\n\n\n\nùêé=softmax‚Å°(ùêêùêä‚ä§dk)‚Äãùêï‚àà‚ÑùT,dv.\\mathbf{O}=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^{\\top}}{\\sqrt{d_{k}}}\\right)\\mathbf{V}\\in\\mathbb{R}^{T,d_{v}}.\n\n(1)\n\n\nEquivalently, the same operation can be expressed sequentially: for each time step tt,\n\n\n\nùê®t=‚àës=1texp‚Å°(ùê™t‚Äãùê§s‚ä§/dk)‚àëj=1texp‚Å°(ùê™t‚Äãùê§j‚ä§/dk)‚ÄãùêØs‚àà‚Ñùdv.\\mathbf{o}_{t}=\\sum_{s=1}^{t}\\frac{\\exp(\\mathbf{q}_{t}\\mathbf{k}_{s}^{\\top}/\\sqrt{d_{k}})}{\\sum_{j=1}^{t}\\exp(\\mathbf{q}_{t}\\mathbf{k}_{j}^{\\top}/\\sqrt{d_{k}})}\\mathbf{v}_{s}\\in\\mathbb{R}^{d_{v}}.\n\n(2)\n\n\nThe parallel formulation is commonly used during training and takes advantage of GPU parallelism; however, it imposes quadratic time and memory complexity. The sequential formulation is used at inference and can also be implemented as a KV-cache¬†(Pope et¬†al., 2023). It is worth noting that, even then, the sequential formulation still requires O‚Äã(T)O(T) memory, which is specifically problematic for longer sequences.\n\n\nLinear attention¬†(Schlag et¬†al., 2021; Yang et¬†al., 2024b; Dao &amp; Gu, 2024; Yang et¬†al., 2024a; Siems et¬†al., 2025) addresses this bottleneck by removing the softmax operator in Equation¬†(1) and exploiting the associativity of matrix multiplication:\n\n\n\n(ùêêùêä‚ä§)‚Äãùêï=ùêê‚Äã(ùêä‚ä§‚Äãùêï).\\left(\\mathbf{Q}\\mathbf{K}^{\\top}\\right)\\mathbf{V}=\\mathbf{Q}\\left(\\mathbf{K}^{\\top}\\mathbf{V}\\right).\n\n\n\nBased on this rearrangement, the softmax-free sequential version of Equation¬†(2) can be expressed recurrently,\n\n\n\nùêít=ùêít‚àí1+ùêØt‚Äãùê§t‚ä§,\\mathbf{S}_{t}=\\mathbf{S}_{t-1}+\\mathbf{v}_{t}\\mathbf{k}_{t}^{\\top},\n\n(3)\n\n\nwhere ùêít‚àà‚Ñùdv,dk\\mathbf{S}_{t}\\in\\mathbb{R}^{d_{v},d_{k}} is a matrix-valued hidden state used to compute the output ùê®t=ùêít‚Äãùê™t\\mathbf{o}_{t"
  },
  {
    "title": "El Agente Quntur: A research collaborator agent for quantum chemistry",
    "url": "https://arxiv.org/abs/2602.04850v1",
    "source": "arxiv",
    "summary": "Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap fo",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; physics &gt; arXiv:2602.04850v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Physics  Chemical Physics\n    \n\n    \n      arXiv:2602.04850v1 (physics)\n    \n\n\n  \n    \n  [Submitted on 4 Feb 2026]\n    Title:El Agente Quntur: A research collaborator agent for quantum chemistry\n    Authors:Juan B. P√©rez-S√°nchez, Yunheng Zou, Jorge A. Campos-Gonzalez-Angulo, Marcel M√ºller, Ignacio Gustin, Andrew Wang, Han Hao, Tsz Wai Ko, Changhyeok Choi, Eric S. Isbrandt, Mohammad Ghazi Vakili, Hanyong Xu, Chris Crebolder, Varinia Bernales, Al√°n Aspuru-Guzik            View a PDF of the paper titled El Agente Quntur: A research collaborator agent for quantum chemistry, by Juan B. P\\&#39;erez-S\\&#39;anchez and 14 other authors\n    View PDF\n\n\n\n    \n            Abstract:Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software&#39;s internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Chemical Physics (physics.chem-ph); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)\n        \n          Cite as:\n          arXiv:2602.04850 [physics.chem-ph]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.04850v1 [physics.chem-ph] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.04850\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Yunheng Zou [view email]          [v1]\n        Wed, 4 Feb 2026 18:38:50 UTC (17,492 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled El Agente Quntur: A research collaborator agent for quantum chemistry, by Juan B. P\\&#39;erez-S\\&#39;anchez and 14 other authorsView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: physics.chem-ph\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.MA\n        physics\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborator"
  },
  {
    "title": "El Agente Estructural: An Artificially Intelligent Molecular Editor",
    "url": "https://arxiv.org/abs/2602.04849v1",
    "source": "arxiv",
    "summary": "We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-languag",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; physics &gt; arXiv:2602.04849v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Physics  Chemical Physics\n    \n\n    \n      arXiv:2602.04849v1 (physics)\n    \n\n\n  \n    \n  [Submitted on 4 Feb 2026]\n    Title:El Agente Estructural: An Artificially Intelligent Molecular Editor\n    Authors:Changhyeok Choi, Yunheng Zou, Marcel M√ºller, Han Hao, Yeonghun Kang, Juan B. P√©rez-S√°nchez, Ignacio Gustin, Hanyong Xu, Mohammad Ghazi Vakili, Chris Crebolder, Al√°n Aspuru-Guzik, Varinia Bernales            View a PDF of the paper titled El Agente Estructural: An Artificially Intelligent Molecular Editor, by Changhyeok Choi and 11 other authors\n    View PDF\n\n\n\n    \n            Abstract:We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.\n    \n\n    \n    \n      \n          Subjects:\n          \n            Chemical Physics (physics.chem-ph); Artificial Intelligence (cs.AI); Multiagent Systems (cs.MA)\n        \n          Cite as:\n          arXiv:2602.04849 [physics.chem-ph]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.04849v1 [physics.chem-ph] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.04849\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Changhyeok Choi [view email]          [v1]\n        Wed, 4 Feb 2026 18:38:48 UTC (18,754 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled El Agente Estructural: An Artificially Intelligent Molecular Editor, by Changhyeok Choi and 11 other authorsView PDFTeX Source\n \n      \n          \n          view license\n        \n    \n        \n    Current browse context: physics.chem-ph\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.AI\n        cs.MA\n        physics\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n          \n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these va"
  },
  {
    "title": "Fluid Representations in Reasoning Models",
    "url": "https://arxiv.org/abs/2602.04843v1",
    "source": "arxiv",
    "summary": "Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural informat",
    "full_text": "\n\n\n\n\n1 Introduction\n\nKey Observations.\n\n\n\n2 Background\n\nBlocksWorld.\nMystery BlocksWorld.\nTerminology.\n2.1 Initial Evaluations\n2.2 Mystery Performance Analysis\n\n2.3 Representation Collection\n\nOverview.\nRepresentation extraction.\nIn-naming representations.\nCross-naming representations.\n\n\n\n\n\n3 Representational Studies\n\n3.1 Cross-Naming Representational Convergence\n3.2 Similarity with Average and Original BlocksWorld\n3.3 Base Model Comparison\n\n\n\n4 Causal validation\n\n\n4.1 Positive Steering\n\nExperimental Setup.\nResults.\n\n\n\n4.2 Symbolic Patching\n\nSymbolic Representation Construction.\nExperimental Design.\n\n\n\n4.3 Negative Steering\n\nExperimental Design.\n\n\n\n\n\n5 Related Work\n\nInterpretability of Language Models‚Äô Representations.\nReasoning Interpretability.\n\n\n6 Limitations\n7 Conclusion\nA BlocksWorld prompt example\nB Mystery prompt example\nC Behavior analysis\nD Layer-wise PCA\nE Cross-Model Similarity Analysis\n\nF Hyperparameters and Experimental Configuration\n\nF.1 Representation Collection\nF.2 Positive Steering\nF.3 Symbolic Patching\nF.4 Negative Steering\nF.5 Model Inference\nF.6 Dataset Configuration\n\n\nG Negative steering\nH Mystery BlocksWorld Naming Variants\nI Mystery Performance Analysis\n\nJ Implementation details\n\nJ.1 Steering engine\nJ.2 Hyperparamters\n\n\n\nK Shuffled In-Naming Steering (Exploratory)\n\nSetup.\nFindings.\nInterpretation.\nCaveat.\n\n\n\nL Statistical Analysis of Steering Effects\n\n\nL.1 Test Methodology\n\nData Structure.\nStatistical Test.\n\n\nL.2 Results\nL.3 Discussion\n\n\nM Use of Large Language Models (LLMs)\n\n\n\n\n\nFluid Representations In Reasoning Models\n\n\nDmitrii Kharlapenko, Alessandro Stolfo, Mrinmaya Sachan \nETH Zurich \n\n\n‚ÄÉ‚ÄÉ\nArthur Conmy \n&amp;Zhijing Jin \nUniversity of Toronto \n\n\n\n\nAbstract\nReasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems.\nHowever, the internal model mechanisms that allow this superior performance remain poorly understood.\nWe present a mechanistic analysis of how QwQ-32B ‚Äì a model specifically trained to produce extensive reasoning traces ‚Äì process abstract structural information.\nOn Mystery Blocksworld ‚Äì a semantically obfuscated planning domain ‚Äì we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning.\nThe model develops abstract encodings that focus on structure rather than specific action names.\nThrough steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss.\nWe find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.\n\n\n\n1 Introduction\n\nA fundamental question in understanding reasoning language models is whether these models merely pattern-match against memorized associations, or whether they can dynamically construct new representations during problem-solving. This distinction is critical: true reasoning requires adapting internal representations to fit novel problem structures, not just retrieving precomputed solutions. Several reasoning benchmarks have been introduced to assess whether language models exhibit genuine reasoning capabilities or rely primarily on memorized patterns. For instance, ARC-AGI series of benchmarks (Chollet et al., 2025; Chollet, 2019) test fluid intelligence through novel visual reasoning tasks designed to minimize reliance on prior knowledge and require genuine abstraction and problem-solving capabilities.\n\n\nPlanBench (Valmeekam et al., 2023) takes a complementary approach by creating semantically obfuscated versions of classical planning problems. A particular example is BlocksWorld, a simple planning domain where models must rearrange blocks by applying actions like ‚Äúpick up‚Äù and ‚Äúput down‚Äù according to explicit rules. These action names appear frequently in pretraining corpora with semantics closely aligned to BlocksWorld, enabling even moderately-sized models to solve standard problems with 60-70% accuracy by leveraging preexisting associations. It‚Äôs obfuscated counterpart ‚Äì Mystery BlocksWorld breaks these associations while preserving logical structure: all predicates and actions are replaced with semantically unrelated words (e.g., ‚Äúpick up‚Äù becomes attack‚Äù, ‚Äúon top of‚Äù becomes ‚Äúcraves‚Äù). This isolates structural reasoning from pattern-matching ‚Äì success requires inferring obfuscated terms from context and constructing abstract representations of domain dynamics.\n\n\nValmeekam et al. (2024) demonstrate that this semantic obfuscation creates a striking performance gap between model types. Instruction-tuned large language models drop to near-zero accuracy on Mystery BlocksWorld. In contrast, reasoning models ‚Äì a new class of models trained via reinforcement learning to generate extended chains of thought (Xu et al., 2025) ‚Äì maintain 20-30% accuracy even under complete semantic obfuscation. These reasoning models, including OpenAI‚Äôs o1 (OpenAI, 2024), DeepSeek R1 (DeepSeek-AI, 2025), and QwQ-32B (Qwen Team, 2025), produce long step-by-step reasoning traces that often span tens of thousands of tokens. Their ability to maintain performance under semantic obfuscation suggests they are doing something fundamentally different: dynamically constructing abstract problem representations during reasoning itself, rather than relying on memorized semantic associations.\n\n\nFigure 1: Overview of our pipeline. Left: QwQ-32B‚Äôs accuracy on Standard BlocksWorld is 96%. Center: Mystery BlocksWorld obfuscates semantics (e.g., ‚Äúpick up‚Äù ‚Üí\\to ‚Äúattack‚Äù), reducing QwQ‚Äôs accuracy to 33%. During extended reasoning traces, the model progressively refines internal representations of obfuscated actions, developing abstract symbolic encodings (vectors v0,‚Ä¶,v3,v_{0},\\dots,v_{3}, and u0,‚Ä¶,u3u_{0},\\dots,u_{3} are extracted at different Chain-of-Thought timestamps). Right: Steering experiments inject these refined representations into early reasoning stages, improving accuracy up to 43%, demonstrating that representational adaptations causally contribute to problem-solving performance.\n\n\nDespite the growing interest in understanding these capabilities, mechanistic insights into how extended reasoning traces benefit model performance remain limited. A major section of reasoning interpretability research focuses on identifying universal reasoning circuits through common token or representation-level components (Venhoff et al., 2025; Bogdan et al., 2025; Lee et al., 2025; Galichin et al., 2025). However, another possible approach is to examine the problem representations that these circuits operate on. An example of this approach is a recent work on state tracking in toy reasoning models (Zhang et al., 2025)\n\n\nWe take this representational approach to investigate how reasoning models develop ‚Äúunderstanding‚Äù of the abstract problem structure during their reasoning. Prior work shows that models adapt internal representations when words acquire new meanings in toy in-context learning setups (Park et al., 2025). We extend their methodology to reasoning traces in planning tasks and establish causal relevance through extensive steering experiments that test whether learned representations actually contribute to problem-solving performance.\n\n\nWe focus our analysis on QwQ-32B (Qwen Team, 2025), the most capable open-source reasoning model available, and examine its internal representations while solving Mystery BlocksWorld (Valmeekam et al., 2023) puzzles. Our central hypothesis is that reasoning models progressively refine their internal representations of problem entities during reasoning, developing context-specific semantics that enable abstract structural reasoning independent of surface-level semantics.\n\n\nKey Observations.\n\nOur main findings about the internal mechanisms of reasoning models are:\n\n\n\n\n1.\n\nRepresentational Dynamics (Section¬†3): We observe that QwQ-32B progressively adapts internal representations of actions and predicates during reasoning, with these adaptations converging toward consistent encodings regardless of initial action names.\n\n\n\n2.\n\nCausal Validation (Section¬†4): Through steering experiments, we observe that these representational adaptations causally improve problem-solving performance. Injecting refined representations from successful reasoning traces into early stages of reasoning enhances accuracy on held-out puzzles, with averaged cross-naming representations achieving the strongest effects.\n\n\n\n3.\n\nSymbolic Abstraction (Sections¬†3.2 and¬†4.2): We observe that adapted representations achieve symbolic abstraction, enabling cross-naming transfer. Models can operate effectively when naming-specific representations are replaced with averaged symbolic representations, suggesting convergence toward abstract structural encodings.\n\n\n\n\n\nOur findings suggest that the superior performance of reasoning models on abstract reasoning tasks stems, at least partially, from their ability to dynamically construct problem-specific representational spaces during reasoning. This capability represents a fundamental advance in how language models process and represent abstract structural information, with implications for understanding and improving reasoning capabilities in future system.\n\n\n\n\n\n2 Background\n\nBlocksWorld.\n\nBlocksWorld is a classic planning domain from the International Planning Competitions (IPC, 1998). Each puzzle specifies initial and goal block arrangements, with constraints that agents can hold only one block at a time and cannot pick up blocks with others stacked above them. The domain defines four core actions: pick-up, put-down, stack, and unstack. The state is described using predicates such as on(x,y) (block x is on block y) or on-table(x) (block x is on the table). Full prompt can be found in A. We use PlanBench (Valm"
  },
  {
    "title": "Group-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing",
    "url": "https://arxiv.org/abs/2602.04837v1",
    "source": "arxiv",
    "summary": "Open-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experi",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Method\n\n3.1 Parent Group Selection\n3.2 Open-Ended Group Evolution\n\n\n\n4 Experiments\n\n\n4.1 Benchmarks\n\nSWE-bench.\nPolyglot.\n\n\n4.2 Experimental Settings\n\n4.3 Baselines\n\nOpen-Ended Self-Evolving Baseline.\nHuman-Designed Frameworks.\n\n\n\n\n\n5 Results and Analysis\n\n\n5.1 Main Results\n\nGEA vs. State-of-the-Art Open-Ended Self-Evolving Systems.\nGEA vs. State-of-the-Art Human-Designed Agents.\nAnalysis of evolutionary patterns on two benchmarks.\n\n\n5.2 Evolution Analysis\n5.3 Transferability\n5.4 Robustness\n\n\n6 Conclusion\n\nA Appendix\n\nA.1 Cost Estimate\nA.2 Case Study\n\n\n\n\n\n\n\nGroup-Evolving Agents: Open-Ended Self-Improvement via Experience Sharing\n\n\nZhaotian Weng1 ‚ÄÉ‚ÄäAntonis Antoniades1\nDeepak Nathani1 ‚ÄÉ‚ÄäZhen Zhang1 ‚ÄÉ‚ÄäXiao Pu 1 ‚ÄÉ‚ÄäXin Eric Wang‚Ä†1\n1 University of California, Santa Barbara ‚ÄÉ\n{zhaotian,ericxwang}@ucsb.edu\n\n\n\n\n\nAbstract\nOpen-ended self-improving agents can autonomously modify their own structural designs to advance their capabilities and overcome the limits of pre-defined architectures, thus reducing reliance on human intervention. We introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvements, which treats a group of agents as the fundamental evolutionary unit, enabling explicit experience sharing and reuse within the group throughout evolution. Unlike existing open-ended self-evolving paradigms that adopt tree-structured evolution, GEA overcomes the limitation of inefficient utilization of exploratory diversity caused by isolated evolutionary branches. We evaluate GEA on challenging coding benchmarks, where it significantly outperforms state-of-the-art self-evolving methods (71.0% vs. 56.7% on SWE-bench Verified, 88.3% vs. 68.3% on Polyglot) and matches or exceeds top human-designed agent frameworks (71.8% and 52.0% on two benchmarks, respectively). Analysis reveals that GEA more effectively converts early-stage exploratory diversity into sustained, long-term progress, achieving stronger performance under the same number of evolved agents. Furthermore, GEA exhibits consistent transferability across different coding models and greater robustness, fixing framework-level bugs in 1.4 iterations on average, versus 5 for self-evolving methods.\n\n\nFigure 1: Overview of Group-Evolving Agents (GEA) vs. tree-structured self-evolution for open-endedness.\nGEA treats a group of agents, rather than an individual agent, as the fundamental unit of evolution. At each iteration, a parent group jointly gives rise to an offspring group through explicit intra-group Experience sharing and reuse.\n\n\n\n1 Introduction\n\nOpen-endedness and cumulative progress are key characteristics of scientific breakthroughs [stanley2017open, zhang2025darwin, stanley2015greatness]. However, most existing AI systems rely on pre-defined model architectures designed by humans. Although such systems can accumulate experience through training, they often struggle to transcend the capability boundaries imposed by their initial designs, as they lack the ability to modify their own structural configurations [yin2025godel]. Thus, progress remains heavily dependent on continuous human intervention.\n\n\nExisting open-ended self-improving systems are largely inspired by biological evolution and designed around individual-centric evolutionary processes [zhang2025darwin, yin2025godel, schmidhuber2003godel, robeyns2025self, novikov2025alphaevolve]. At each iteration, a single agent is selected as the parent and refined to produce one or more offspring(Figure 1a). The overall structure follows chain- or tree-structured evolution, where different branches remain strictly isolated. Consequently, although such systems often exhibit substantial exploratory diversity, this diversity rarely serves as effective stepping stones [mouret2015illuminating, pugh2016quality]. Instead, many agents provide only temporary diversity, producing short-lived variants that fail to contribute to long-term cumulative progress.\n\n\nIt is time to rethink agent evolution. AI agents are not biological individuals; why should their evolution remain constrained by biological paradigms?\nIn fact, AI agents can directly share trajectories, tools, and learned artifacts, and they can aggregate complementary skills without the constraints of reproduction or lineage.\n\n\nTherefore, we introduce Group-Evolving Agents (GEA), a new paradigm for open-ended self-improvement that treats a group of agents, rather than an individual agent, as the fundamental unit of evolution (Figure 1b).\nThis shift enables explicit experience sharing and reuse across agents within a group, naturally allowing exploratory discoveries from different agents to be consolidated and accumulated into long-term progress rather than remaining as short-lived variants.\nAt each iteration, GEA first selects a parent group of agents using a Performance-Novelty criterion that balances immediate performance gains with evolutionary diversity. The parent agents then jointly produce a child group through a shared pool of aggregated experience from all members.\n\n\nWe evaluate GEA on challenging coding benchmarks, achieving success rates of 71.0% on SWE-bench Verified and 88.3% on Polyglot, significantly outperforming state-of-the-art open-ended self-evolving methods (56.7% and 68.3%, respectively). Analysis reveals that GEA more effectively consolidates the diversity generated during open-ended exploration, yielding sustained progress and stronger performance given the same number of evolved agents. By leveraging experience from better-performing agents, GEA also exhibits stronger robustness to framework-level perturbations. Furthermore, its improvements stem from workflow and tool enhancements rather than model-specific optimizations, thus transferring consistently across GPT- and Claude-series models.\n\n\nAdditionally, by leveraging meta-learning for self-improvement in open-ended exploration, without any human intervention, GEA achieves performance comparable to or even surpassing human-designed state-of-the-art frameworks on both benchmarks (71.0% vs. 71.8% on SWE-bench Verified, 88.3% vs. 52.0% on Polyglot).\n\n\nIn summary, we propose Group-Evolving Agents, a new paradigm for open-ended self-improvement that:\n\n\n\n\n1.\n\nOvercomes the limitation of inefficient utilization of exploratory diversity caused by branch isolation in existing tree-structured evolution, by enabling explicit experience sharing and reuse within the group during evolution.\n\n\n\n2.\n\nMore effectively consolidates and reuses experience and evolutionary diversity from other agents, achieving significant performance gains and stronger robustness over state-of-the-art open-ended self-evolving methods, with improvements that transfer consistently across different coding models.\n\n\n\n3.\n\nMatches or surpasses human-designed state-of-the-art frameworks through meta-learning-based self-improvement without human intervention.\n\n\n\n\n\n\n\n2 Related Work\n\nRecent years have witnessed growing interest in how AI systems can continuously improve themselves without human intervention [fang2025comprehensive, gao2025survey, wang2025evoagentx]. Most existing self-improving approaches mainly focus on continuous, iterative refinement of the given agent system [shinn2023reflexion, songmind, gao2025survey, zelikman2024self, singhbeyond], typically evolving toward a specific optimization objective and following a linear, chain-based evolutionary structure [zhang2025population, wang2025evoagentx, robeyns2025self] . Such systems achieve self-improvement through mechanisms such as self-play against historical versions or self-generated verification [setlur2024rewarding, wang2024math, huang2025r, wang2025socratic, wei2025toward], supervised fine-tuning [zelikman2022star, gou2023tora, ni2024next] or reinforcement learning on selectively filtered feedback [xin2024deepseek, zhao2025absolute, yuan2024self] , and reflection-based methods [wang2025mobile, yin2025godel, shinn2023reflexion] or in-context learning [hua2024trustagent, sun2023adaplanner]. While this goal-oriented, chain-based evolutionary paradigm enables autonomous improvement along a particular direction, it inherently limits the ability of self-evolving systems to explore diverse evolutionary directions in open-ended solution spaces.\n\n\nA line of work has pointed out that one of the key challenges in enabling unbounded improvement and innovation lies in developing open-ended AI systems that can continuously produce both novel and learnable artifacts [stanley2017open, zhang2025darwin, stanley2015greatness, shapere1964structure]. Building on this insight, open-endedness has been characterized as the capability of systems to continuously generate artifacts that are novel, interesting, and learnable from a human perspective [zhang2025darwin, clune2019ai, hughes2024open, jiang2023general, zhangomni, faldoromni].\n\n\nMotivated by the potential of enabling unbounded evolution through open-ended exploration in self-evolving agents, more recent studies adopt lineage-based, tree-structured evolutionary strategies [antoniadesswe] inspired by biological inheritance and mutation [zhang2025darwin, novikov2025alphaevolve, fernando2024promptbreeder, huang2023large]. In these frameworks, individual parent agents are selected at each iteration to independently produce offspring, enabling various branching exploration across multiple evolutionary directions and helping avoid local optima. However, the strict isolation between evolutionary branches prevents effective information and experience sharing and reuse across lineages. As a result, many promising directions discovered early in evolution persist only as temporary diversity and fail to contribute to long-term cumulative progress.\nTo overcome this limitation, we introduce a group-centric evolutionary paradigm, Group-Evolving Agents (GEA), which explicitly enables intra-group experience sharing and reuse throughout the evolutionary process. By consolidating complementary discov"
  },
  {
    "title": "Are AI Capabilities Increasing Exponentially? A Competing Hypothesis",
    "url": "https://arxiv.org/abs/2602.04836v1",
    "source": "arxiv",
    "summary": "Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims",
    "full_text": "\n\n\n\n1 Introduction\n2 Background on the METR Study\n\n3 Multiplicative Model of AI Progress\n\n3.1 Motivation\n3.2 Regression Model\n3.3 Choice of Link Functions\n3.4 Theoretical Analysis\n\n\n\n4 Our Analysis of the METR Data\n\n4.1 Methodology\n4.2 Results\n\n\n5 Limitations\n6 Conclusion\nA Related Work\nB Proof of Theorem¬†3.1\n\n\n\n\n\nAre AI Capabilities Increasing Exponentially? A Competing Hypothesis\n\n\nHaosen Ge\n\n‚ÄÉ‚ÄÉ\nHamsa Bastani\n\n‚ÄÉ‚ÄÉ\nOsbert Bastani\n\n\n\nAbstract\nRapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation &amp; Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.111Our code is available at https://github.com/obastani/AI_Forecasting.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nA recent report by Model Evaluation &amp; Threat Research (METR)¬†(Kwa et¬†al., 2025) conducted a series of analyses to measure AI capabilities in realistic tasks that require significant effort from human experts. They propose a novel metric: 50% model horizon, which measures the difficulty of tasks that a model can solve successfully 50% of the time. Then, they show that according to this metric, AI capabilities are increasing exponentially over time‚Äîspecifically, model horizons have been doubling every seven months since 2019. Based on these results, they predict that ‚Äúwithin 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.‚Äù\n\n\nSince dissemination, this report has drawn significant attention and started heated discussion on the potential impact of such rapid improvement of AI capabilities. Much of the academic conversation has focused on safety¬†(Barnett &amp; Scher, 2025). However, there are substantial consequences for rapidly increasing AI capabilities beyond safety. Most notably, these results have raised substantial concerns about labor market consequences, raising the potential for large-scale displacement of skilled workers¬†(Brynjolfsson et¬†al., 2025). Importantly, many consequences of these forecasts are immediate, shaping both policy outcomes as well as individual decisions such as choices about education and career paths. Given the substantial consequences of the potential for exponential increase in AI capabilities, there is an urgent need for rigorous methodologies for performing and validating these kinds forecasts.\n\n\n\n\n\n(a) Sigmoid Curve Fit\n\n\n\n\n\n(b) Sigmoid Curve Fit vs METR\n\n\n\nFigure 1: Sigmoid Curve. This sigmoid curve is fit by minimizing the mean-squared error (MSE) of the curve hmodel=Œ≥‚ãÖœÉ‚Äã(Œ¥1‚ãÖdmodel+Œ¥2)h_{\\text{model}}=\\gamma\\cdot\\sigma(\\delta_{1}\\cdot d_{\\text{model}}+\\delta_{2}) to the METR dataset, where hmodelh_{\\text{model}} is METR‚Äôs ‚Äú50% model horizon time‚Äù for the given model, dmodeld_{\\text{model}} is the model release date, and Œ≥,Œ¥1,Œ¥2\\gamma,\\delta_{1},\\delta_{2} are parameters. We use gradient descent in PyTorch for parameter estimation. While it is not clear that progress will plateau, recent progress clearly fits in the linear part of the sigmoid and the inflection point (2025-06-06) is in the past.\n\n\nWe argue to the contrary‚Äîplateauing growth is similarly well supported by the data. While the METR study compares to a small number of alternative hypotheses, aside from linear and super-exponential, none of these alternatives are visualized or discussed in detail. Most notably, they discuss the sigmoid curve as a potential alternative, but claim that their estimate of this model yields an inflection point far in the future (see Appendix¬†D.1 in their paper¬†(Kwa et¬†al., 2025)); they rule out plateauing growth in the near future based on this finding. However, we find that fitting a sigmoid curve results in an inflection point that is actually in the past (specifically, 2025-06-06), as visualized in Figure¬†1.222While this fit is in-sample (due to the small dataset size), our main goal is to estimate the inflection point from the data rather than to provide accurate forecasts into the future.\nThis finding suggests that at the very least, it is plausible that AI capabilities may plateau soon.\n\n\nIn general, it is impossible to rule out either alternative from data alone. Thus, to support our hypothesis, we posit a theoretical model under which the exponential appearance of recent gains in AI capabilities can be interpreted as a consequence of the introduction of reasoning capabilities into base LLMs. Specifically, we model reasoning as a separate technology that contributes multiplicatively to the overall capability of LLMs‚Äîi.e., LLM capabilities can be decomposed into two sigmoids, one for the base LLM and one for reasoning. Indeed, as can be seen from Figure¬†1, the period of progress from o1-preview (released 2024-09-12) to the present forms the linear part of the sigmoid curve. Thus, our hypothesis is that following initial exponential growth due to scaling data and model size, base capabilities plateaued, but overall capabilities continued to grow for a period due to rapid improvements in reasoning.\n\n\nWhile we present a specific alternative analysis, our goal is not to discount the METR study; in fact, we believe continuing exponential improvement is a plausible viewpoint, and it is important to take this potential outcome into consideration. However, we believe that our approach provides a plausible alternative that similarly merits consideration.\n\n\n\n\n2 Background on the METR Study\n\nWe focus on the recent METR study¬†(Kwa et¬†al., 2025), which forecasts that AI capabilities are exponentially increasing. This study was itself critiquing prior work, pointing out that existing metrics such as accuracy are bounded and cannot assess whether growth is exponential. To remedy this issue, they introduce a novel metric, the 50% model horizon time, which quantifies the difficulty of tasks that a model can solve reliably. Unlike prior metrics, this one can increase unboundedly, making it suitable for assessing the possibility of exponential growth. Their analysis concludes that AI capabilities are improving exponentially.\n\n\nTheir experiments include three task families: HCAST, RE-Bench, and SWAA. Specifically, HCAST contains a diverse set of challenges in cybersecurity, machine learning, software engineering, and general reasoning. RE-Bench consists of challenging open-ended machine learning research engineering environments, each of which are intended to take a human expert approximately 8 hours to complete. SWAA comprises 66 small tasks commonly performed in software engineering work. In total, their study includes 170 unique tasks from the three task families. Then, they evaluate 28 popular models on the 170 tasks. Among the models, they label a subset of 15 models as state-of-the-art (SOTA), representing the frontier of AI capabilities. We reuse their experimental results and focus exclusively on the 15 state-of-the-art models to better characterize the scaling behavior of frontier AI capabilities. We include the list of SOTA models and their release dates in Table¬†1.\n\n\nTable 1: Release Dates of the Selected SOTA Models.\n\n\n\nModel\nRelease Date\n\n\n\n\nDavinci-002\n2020-05-28\n\n\nGPT-4\n2023-03-14\n\n\nGrok-4\n2025-07-09\n\n\nClaude 3.5 Sonnet (Oct 2024)\n2024-10-22\n\n\nGPT-5\n2025-08-07\n\n\nClaude 3.7 Sonnet\n2025-02-24\n\n\nGPT-2\n2019-02-14\n\n\nGPT-3.5 Turbo Instruct\n2022-03-15\n\n\nGPT-o1-preview\n2024-09-12\n\n\nGPT-4 (1106)\n2023-11-06\n\n\nGPT-5.1 Codex Max\n2025-11-19\n\n\nGPT-4o\n2024-05-13\n\n\nClaude 3.5 Sonnet\n2024-06-20\n\n\nGPT-o3\n2025-04-16\n\n\nGPT-o1-elicited\n2024-12-05\n\n\n\n\n\nTheir data analysis first estimates the horizon time of each model on each dataset using the following regression:\n\n\n\npmodel=œÉ‚Äã((log‚Å°hmodel‚àílog‚Å°ttask)‚ãÖŒ≤model),\\displaystyle p_{\\text{model}}=\\sigma((\\log h_{\\text{model}}-\\log t_{\\text{task}})\\cdot\\beta_{\\text{model}}),\n\n(1)\n\n\nwhere pmodelp_{\\text{model}} denotes the probability that the model solves a task correctly, ttaskt_{\\text{task}} denotes the difficulty of the task (measured by the amount of time human expert takes to complete the task), hmodelh_{\\text{model}} is the 50% horizon time, Œ≤model\\beta_{\\text{model}} is the parameter they estimate, and œÉ‚Äã(x)=ex/(1+ex)\\sigma(x)=e^{x}/(1+e^{x}) is the sigmoid function. By construction, pmodel=1/2p_{\\text{model}}=1/2 when log‚Å°hmodel=log‚Å°ttask\\log h_{\\text{model}}=\\log t_{\\text{task}}; thus, a model with capability hmodelh_{\\text{model}} attains a success probability of 1/21/2 on tasks with difficulty ttask=hmodelt_{\\text{task}}=h_{\\text{model}}. Thus, hmodelh_{\\text{model}} characterizes the task difficulty threshold at which the model achieves a success rate of 1/21/2.\n\n\nAfter estimating the 50% horizon time, they examine the temporal trend in model capabilities by fitting the following linear regression model:\n\n\n\nlog‚Å°hmodel=Œ≤0+Œ≤1‚ãÖdmodel,\\displaystyle\\log h_{\\text{model}}=\\beta_{0}+\\beta_{1}\\cdot d_{\\text{model}},\n\n\n\nwhere dmodeld_{\\text{model}} denotes the model‚Äôs release date; they fit this data by treating the estimate hmodelh_{\\text{model}} from the previous step as ground truth hmodel‚àóh_{\\text{model}}^{*} and applying linear regression. Note that this model is equivalent to\n\n\n\nhmodel=exp‚Å°(Œ≤0+Œ≤1‚ãÖdmodel),\\displaystyle h_{\\text{model}}=\\exp\\left(\\beta_{0}+\\beta_{"
  },
  {
    "title": "It's not a Lottery, it's a Race: Understanding How Gradient Descent Adapts the Network's Capacity to the Task",
    "url": "https://arxiv.org/abs/2602.04832v1",
    "source": "arxiv",
    "summary": "Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the",
    "full_text": null
  },
  {
    "title": "Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning",
    "url": "https://arxiv.org/abs/2602.04821v1",
    "source": "arxiv",
    "summary": "Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically rewe",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n3 Problem Formulation\n\n4 Methodology\n\n\n4.1 PU-GAT+: Confidence-Monotonic Attention\n\n4.1.1 Motivation: Why Temperature Scaling Fails\n4.1.2 Pairwise Uncertainty Mechanism with Monotonicity Constraint\n4.1.3 Uncertainty Propagation Across Layers\n4.1.4 Dual-Stream Temporal Decomposition\n4.1.5 Spatially-Adaptive Conformal Calibration\n\n\n\n4.2 CRFN-BY: Dependence-Robust Anomaly Detection\n\n4.2.1 Uncertainty-Normalized Residuals\n4.2.2 Context-Conditioned Normalizing Flow\n4.2.3 Conformalized P-Value Construction\n4.2.4 Contamination-Robust Calibration\n4.2.5 Benjamini-Yekutieli FDR Control\n4.2.6 Empirical Dependence Verification\n\n\n\n4.3 LyCon-WRL+: Certified Safe World-Model RL\n\n4.3.1 Explicit Safety Constraint Specification\n4.3.2 Spatial Aggregation: Grid Cells to Intersections\n4.3.3 State Augmentation with Upstream Uncertainty\n4.3.4 Ensemble World Model with Verified Error Bounds\n4.3.5 Lyapunov Function Design\n4.3.6 Certified Lipschitz Bounds via Spectral Normalization\n4.3.7 Uncertainty-Guided Exploration\n4.3.8 Anomaly-Aware Reward\n\n\n\n\n5 Experiments\n\n6 Results\n\n6.1 Forecasting Results\n6.2 Calibration Diagnostics\n6.3 Anomaly Detection Results\n6.4 RL Control Results\n6.5 Ablation Studies\n6.6 Discussion and Limitations\n\n\n7 Conclusion\n\nA Complete Proofs\n\nA.1 Proof of Proposition¬†4.3 (Confidence-Monotonic Attention)\nA.2 Proof of Theorem¬†4.5 (Coverage under Mixing)\nA.3 Proof of Proposition¬†A.1 (P-Value Validity)\nA.4 Proof of Theorem¬†4.8 (FDR Control)\nA.5 Proof of Lemma¬†4.7 (Trimming Validity)\nA.6 Proof of Theorem¬†4.14 (Safety under Bounded Model Error)\n\n\nB Training Details\nC Spatial Aggregation Analysis\nD Model Error Sensitivity\nE Safety Constraints and Lyapunov Details\nF Detailed Algorithm Pseudocode\n\nG Extended Experimental Results\n\nG.1 Forecasting Performance Comparison\nG.2 Anomaly Detection with FDR Control\nG.3 Hyperparameter Sensitivity\nG.4 Constrained vs. Unconstrained Œ≥\\gamma Analysis\nG.5 Œ¥\\delta Sensitivity Analysis\nG.6 Conformalized vs. Parametric P-Values\nG.7 Lyapunov Safety-Performance Trade-off\nG.8 Learning Dynamics\nG.9 Block Dependence Analysis\nG.10 Per-Dataset Results\nG.11 Real Event Detection Details\n\n\nH Computational Resources and Reproducibility\nI Broader Impact\n\nJ Extended Limitations and Future Work\n\nJ.1 Detailed Limitations\nJ.2 Future Work Directions\n\n\n\n\n\n\n\nSafe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and \nWorld-Model Reinforcement Learning\n\n\nJoydeep Chandra\n\n‚ÄÉ‚ÄÉ\nSatyam Kumar Navneet\n\n‚ÄÉ‚ÄÉ\nAleksandr Algazinov\n\n‚ÄÉ‚ÄÉ\nYong Zhang\n\n\n\nAbstract\nUrban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions‚Äîall while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4% coverage efficiency, controls FDR at 4.1% under verified dependence, and improves safety rate to 95.2% compared to 69% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.\nPreliminary work. Under review.\n\nConformal Prediction, Safe Reinforcement Learning, Spatio-Temporal Forecasting\n\n\nFigure 1: Architecture of STREAM-RL. The framework integrates: (1) PU-GAT+ for uncertainty-aware forecasting, (2) CRFN-BY for dependence-robust anomaly detection, and (3) LyCon-WRL+ for certified safe RL. Dashed lines show cross-module uncertainty propagation.\n\n\n\n1 Introduction\n\nUrban traffic congestion costs the U.S. economy $87 billion annually (Schrank et¬†al., 2021), while contributing 29% of transportation-related emissions. Effective management requires: (i) forecasting with calibrated uncertainty (Jiang &amp; Luo, 2022; Li et¬†al., 2018; Guo et¬†al., 2019), (ii) anomaly detection with controlled false discovery rates (Rebjock et¬†al., 2021; Pang et¬†al., 2021), and (iii) safe adaptive control satisfying hard safety constraints (Wei et¬†al., 2021, 2019; Zheng et¬†al., 2019b).\n\n\nExisting integrated approaches suffer from three critical limitations:\n\n\nLimitation 1: Pseudo-uncertainty guidance in graph attention. Prior uncertainty-aware GNNs (Wu et¬†al., 2020; Bai et¬†al., 2020; Shang et¬†al., 2021; Guo et¬†al., 2022) employ temperature scaling Œ±i‚Äãj‚àùexp‚Å°(ei‚Äãj/(1+Œ≤‚ÄãœÉi))\\alpha_{ij}\\propto\\exp(e_{ij}/(1+\\beta\\sigma_{i})), which merely adjusts softmax sharpness‚Äîit cannot preferentially weight confident neighbors over uncertain ones.\n\n\nLimitation 2: Invalid FDR control under dependence. The Benjamini-Hochberg procedure (Benjamini &amp; Hochberg, 1995) requires independence or PRDS among p-values. Spatial autocorrelation and temporal persistence in traffic data violate these assumptions (Fithian &amp; Lei, 2020; Lei &amp; Fithian, 2018), rendering FDR guarantees invalid.\n\n\nLimitation 3: Unverified safe RL assumptions. Lyapunov-based safe RL methods (Chow et¬†al., 2018, 2019; Berkenkamp et¬†al., 2017; Fisac et¬†al., 2018) derive guarantees conditional on assumptions (bounded model error, Lipschitz Lyapunov function) that are rarely verified empirically.\n\n\nWe propose STREAM-RL with three methodologically complete contributions that address these limitations:\n\n\nContribution 1: Constrained Pairwise Uncertainty-Guided Attention (PU-GAT+). We introduce attention coefficients that explicitly compare source and target uncertainties with provably monotonic behavior:\n\n\n\nŒ±i‚Äãj=softmaxj‚Äã(ei‚Äãj+Softplus‚Äã(Œ≥~)‚èüŒ≥‚â•0‚Äã(œÉi‚àíœÉj)+Œ¥‚ãÖùüè‚Äã[j=i])\\alpha_{ij}=\\text{softmax}_{j}\\Big(e_{ij}+\\underbrace{\\text{Softplus}(\\tilde{\\gamma})}_{\\gamma\\geq 0}(\\sigma_{i}-\\sigma_{j})+\\delta\\cdot\\mathbf{1}[j=i]\\Big)\n\n(1)\n\n\nThe Softplus constraint ensures Œ≥‚â•0\\gamma\\geq 0, guaranteeing that when œÉi&gt;œÉj\\sigma_{i}&gt;\\sigma_{j} (source uncertain, neighbor confident), attention to neighbor jj strictly increases. We train via heteroscedastic Gaussian NLL with explicit calibration diagnostics (PIT histograms, reliability diagrams) before conformal wrapping.\n\n\nContribution 2: Conformalized P-Value Construction (CRFN-BY). We employ the Benjamini-Yekutieli procedure (Benjamini &amp; Yekutieli, 2001) with precisely specified p-value construction from flow-based anomaly scores:\n\n\n\npt(i)=1+‚àëj‚ààùíûtrimùüè‚Äã[sj‚â•st(i)]1+|ùíûtrim|p_{t}^{(i)}=\\frac{1+\\sum_{j\\in\\mathcal{C}_{\\text{trim}}}\\mathbf{1}[s_{j}\\geq s_{t}^{(i)}]}{1+|\\mathcal{C}_{\\text{trim}}|}\n\n(2)\n\n\nwhere st(i)=‚àílog‚Å°pŒ∏‚Äã(zt(i)|ct(i))s_{t}^{(i)}=-\\log p_{\\theta}(z_{t}^{(i)}|c_{t}^{(i)}) is the context-conditioned negative log-likelihood and ùíûtrim\\mathcal{C}_{\\text{trim}} is the contamination-trimmed calibration set. This conformalized construction yields valid p-values under exchangeability, which BY then corrects for arbitrary dependence.\n\n\nContribution 3: Complete Lyapunov Certificate Derivation (LyCon-WRL+). We employ spectral normalization (Miyato et¬†al., 2018) to obtain enforced upper bounds L¬ØL\\bar{L}_{L} and J¬ØW\\bar{J}_{W} on Lipschitz constants:\n\n\n\nŒµ‚àó=Œ¥slack+Œ∫‚ãÖd¬ØCL¬ØL‚ãÖ(1+J¬ØW)\\varepsilon^{*}=\\frac{\\delta_{\\text{slack}}+\\kappa\\cdot\\bar{d}_{C}}{\\bar{L}_{L}\\cdot(1+\\bar{J}_{W})}\n\n(3)\n\n\nwhere bounds are enforced via spectral normalization during training (not post-hoc estimates), addressing the gap between theoretical assumptions and practical verification raised in prior work (Gouk et¬†al., 2020; Fazlyab et¬†al., 2019).\n\n\n\n\n2 Related Work\n\nUncertainty-Aware Graph Neural Networks. Bayesian GNNs (Zhang et¬†al., 2019; Hasanzadeh et¬†al., 2020; Pal et¬†al., 2020) propagate uncertainty but incur overhead. Heteroscedastic approaches (Wu et¬†al., 2020; Gal &amp; Ghahramani, 2016; Lakshminarayanan et¬†al., 2017) output uncertainty estimates for loss weighting, not attention modulation. Recent work on confidence-aware message passing (Shen et¬†al., 2021; Zhao et¬†al., 2020; Stadler et¬†al., 2021) addresses robustness but not monotonic uncertainty guidance. Our PU-GAT+ explicitly compares source-target uncertainties with constrained monotonicity.\n\n\nConformal Prediction for Spatio-Temporal Data. STACI (Feng et¬†al., 2025) introduces topology-aware calibration; ACI (Gibbs &amp; Cand√®s, 2021; Zaffran et¬†al., 2022) addresses distribution shift. Recent methods incorporate travel-time adjacency (Patil et¬†al., 2025), weather conditioning (Zhao et¬†al., 2024), and heterogeneous calibration (Romano et¬†al., 2019; Sesia &amp; Romano, 2021; Feldman et¬†al., 2021). These focus on calibration design; our contribution is uncertainty-guided representation learning before conformal wrapping.\n\n\nFDR Control Under Dependence. Beyond BY (Benjamini &amp; Yekutieli, 2001), knockoff filters (Barber &amp; Cand√®s, 2015; Candes et¬†al., 2017), local FDR (Efron, 2010; Storey, 2003), and adaptive procedures (Blanchard et¬†al., 2020; Ramdas et¬†al., 2019; Lei &amp; Fithian, 2018) offer alternatives. Graph-aware FDR procedures (Lei et¬†al., 2021; Ramdas et¬†al., 2017) and e-values (Vovk &amp; Wang, 2021; Gr√ºnwald et¬†al., 2023) improve power under known dependence structure. We adopt BY for simplicity and validity under arbitrary dependence, acknowledging its conservatism as a power-validity trade-off (see Section¬†6.6).\n\n\nSafe Model-Based RL. LAMBDA (As et¬†al., 2022), LBPO (Kang et¬†al., 2022), and CPO (Achiam et¬†al., 2017) combine constraints with learned models. "
  },
  {
    "title": "Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization",
    "url": "https://arxiv.org/abs/2602.04820v1",
    "source": "arxiv",
    "summary": "Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-b",
    "full_text": null
  },
  {
    "title": "XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas",
    "url": "https://arxiv.org/abs/2602.04819v1",
    "source": "arxiv",
    "summary": "Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns asso",
    "full_text": null
  },
  {
    "title": "Horizon-LM: A RAM-Centric Architecture for LLM Training",
    "url": "https://arxiv.org/abs/2602.04816v1",
    "source": "arxiv",
    "summary": "The rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persisten",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background and Related Work\n\n2.1 GPU-Centric Distributed Training Paradigm\n2.2 Memory Extension via Offloading Frameworks\n\n\n\n3 Design Challenges\n\n3.1 Memory Requirement\n3.2 Bandwidth and Streaming Requirement\n3.3 Execution and Scheduling Requirement\n3.4 Design Principles\n3.5 Training Abstraction: CPU-Master, GPU-Cache\n3.6 End-to-End Execution Workflow\n\n3.7 System Architecture\n\n3.7.1 CPU Domain: Authoritative Parameter Store\n3.7.2 GPU Domain: Transient Execution Cache\n3.7.3 Architectural Invariants\n\n\n\n\n\n4 System Design\n\n4.1 CPU Parameter Store Design\n4.2 Explicit Execution Mechanisms\n4.3 GPU Streaming Engine\n4.4 Double Buffering and Multi-stream Scheduling\n4.5 Memory Pool and Resource Partitioning\n\n\n\n5 Implementation\n\n5.1 Authoritative Parameter Storage and Layout\n5.2 Asynchronous Multi-Stream Pipeline\n5.3 SIMD-Accelerated CPU Optimization\n5.4 Memory-Centric Correctness and Robustness\n5.5 Kernel-Level Optimizations\n\n\n\n6 Evaluation\n\n6.1 Experimental Setup\n6.2 Feasibility Boundary\n6.3 Depth Scalability Results.\n6.4 Width Scalability Results.\n6.5 Verification on Different Devices\n\n\n7 Conclusion\n\n\n\n\n\n\nHorizon-LM: A RAM-Centric Architecture for LLM Training \n Single-GPU Training of Hundreds-of-Billions Parameter Language Models with Mixed BF16/FP32 Precision \n\n\n Code: https://github.com/DLYuanGod/Horizon-LM\n\n\n\nZhengqing Yuan1{}^{\\text{1}}‚ÄÇLichao Sun2{}^{\\text{2}}‚ÄÇYanfang (Fanny) Ye1{}^{\\text{1}}\n\n1{}^{\\text{1}}University of Notre Dame ‚ÄÇ2{}^{\\text{2}}Lehigh University \n\n\n\nAbstract.\nThe rapid growth of large language models (LLMs) has outpaced the evolution of single-GPU hardware, making model scale increasingly constrained by memory capacity rather than computation. While modern training systems extend GPU memory through distributed parallelism and offloading across CPU and storage tiers, they fundamentally retain a GPU-centric execution paradigm in which GPUs host persistent model replicas and full autograd graphs. As a result, scaling large models remains tightly coupled to multi-GPU clusters, complex distributed runtimes, and unpredictable host memory consumption, creating substantial barriers for node-scale post-training workloads such as instruction tuning, alignment, and domain adaptation. We present Horizon-LM, a memory-centric training system that redefines the roles of CPU and GPU for large-model optimization. Horizon-LM treats host memory as the authoritative parameter store and uses GPUs solely as transient compute engines through a CPU-master, GPU-template execution model. By eliminating persistent GPU-resident modules and autograd graphs, employing explicit recomputation with manual gradient propagation, and introducing a pipelined double-buffered execution engine, Horizon-LM decouples model scale from GPU count and bounds memory usage to the theoretical parameter footprint. On a single H200 GPU with 1.5‚ÄâTB host RAM, Horizon-LM reliably trains models up to 120B parameters. On a standard single A100 machine, Horizon-LM achieves up to 12.2√ó\\times higher training throughput than DeepSpeed ZeRO-3 with CPU offloading while preserving numerical correctness. Across platforms and scales, Horizon-LM sustains high device utilization and predictable memory growth, demonstrating that host memory, not GPU memory, defines the true feasibility boundary for node-scale large-model training.\n\n\n\n1. Introduction\n\nLarge language models (LLMs) have achieved tremendous advances in recent years¬†(Hurst et al., 2024; Liu et al., 2024; Guo et al., 2025; Team et al., 2023; Singh et al., 2025), powering a new generation of AI systems ranging from coding copilots¬†(Anthropic, 2025; Cursor, 2025) and scientific assistants to multimodal¬†(Li et al., 2024) and multi-agent frameworks¬†(Hong et al., 2023; Wu et al., 2024). A consistent empirical finding is that larger models tend to achieve better performance across a wide spectrum of capabilities, including reasoning, generalization, and long-horizon planning¬†(Jaech et al., 2024). Consequently, training larger foundation models has become a dominant trend, triggering massive investments in computational infrastructure and exposing critical limitations of existing training systems.\n\n\nFigure 1. Sustained TFLOPS across model scales on a singal GH200 (Qwen2.5 for 7B-32B) and H200 (Qwen2.5 72B and GPT-oss 120B). HorizonLM remains efficient while offloading baselines become GPU memory-bound.\n\n\nOver the past few years, large language models have rapidly evolved from hundred-billion-parameter systems to foundation models approaching or exceeding the trillion-parameter scale¬†(Team et al., 2025; Rajbhandari et al., 2020). This explosive growth in model capacity has far outpaced the improvement of single-GPU hardware. For example, mainstream data-center accelerators have only increased from 80 GB HBM on A100 GPUs¬†(NVIDIA Corporation, 2025) to under 180 GB on B200¬†(Kessler and Suresh, 2025), representing a modest growth in per-device memory capacity. At the same time, the economic cost of scaling via GPUs has risen sharply. Individual high-end accelerators now cost on the order of $60,000¬†(Somala, 2025), and fully populated multi-GPU servers can exceed $510,000¬†(Adeboye, 2025), making large-scale GPU clusters increasingly expensive to acquire and operate. Moreover, recent surveys of U.S. universities indicate that among 167 institutions, only two universities achieve an average availability of more than one H100 GPU per student¬†(GPUsPerStudent.org, 2025), highlighting the extreme scarcity and centralization of large-scale GPU resources. As a result, the prevailing approach to large-model training, scaling out through massive GPU parallelism, faces growing memory, cost, and accessibility barriers.\n\n\nAt the same time, the center of innovation is shifting beyond pretraining toward post-training regimes, including instruction tuning, alignment, domain adaptation, and agent specialization¬†(Lai et al., 2025; Tie et al., 2025). Modern AI systems increasingly rely on repeatedly adapting large pretrained models to new tasks, tools, and interaction protocols¬†(Hao et al., 2023). Unlike trillion-parameter pretraining, these workloads are comparatively lightweight in computation and, in principle, could be performed on a single node¬†(Yuan et al., 2025a). But fine-tuning or aligning existing models requires loading full model parameters and optimizer states into memory, rendering hundred-billion-parameter models inaccessible on commodity systems¬†(Yu and Singh, 2025). This creates a fundamental mismatch: while LLM development is transitioning toward memory-bound, node-scale post-training, existing training systems remain optimized for compute-bound, cluster-scale pretraining¬†(Rajbhandari et al., 2021; Wang et al., 2025). As a result, data scientists and application developers are effectively prevented from experimenting with and customizing the most capable foundation models.\n\n\nRecent large-model training systems have converged on offloading-based designs, exemplified by DeepSpeed ZeRO¬†(Rajbhandari et al., 2020) and ZeRO-Infinity¬†(Rajbhandari et al., 2021), which extend GPU memory by dynamically migrating model states across GPU, CPU, and storage tiers. These approaches substantially increase effective capacity and have enabled trillion-parameter training at cluster scale. However, they fundamentally preserve a GPU-centric training paradigm: GPUs remain the owners of long-lived model replicas and full autograd graphs, while CPU and NVMe serve only as auxiliary spill buffers.\n\n\nThis design choice imposes fundamental structural limitations. 1) First, as long as model execution is anchored to persistent GPU-resident modules and full autograd graphs, large-model training inherently requires multi-GPU clusters and complex distributed orchestration¬†(Ilin, 2025). Offloading can alleviate memory pressure, but it does not change the training unit: parameters must still be repeatedly gathered onto GPUs to instantiate long-lived model replicas. Consequently, large-model training remains intrinsically tied to multi-GPU execution and cannot operate in a truly node-scale regime. 2) Second, GPU-centric offloading frameworks do not transform host memory into a true parameter store¬†(Zhao et al., 2023). Instead, CPU memory is repurposed as a distributed runtime heap that simultaneously holds partitioned parameters, optimizer states, gradients, communication buffers, and autograd metadata. As a result, host memory consumption is governed not only by model size, but by the complexity of the training runtime itself, making memory usage difficult to control, predict, or scale linearly with model capacity. 3) Third, because model execution, gradient propagation, and memory management are all entangled within the distributed training runtime, existing systems fundamentally lack the ability to bound and structure memory usage¬†(Duan et al., 2024). Even with abundant host memory, offloading frameworks cannot reliably exploit it as a clean capacity resource, and frequently encounter host-side memory exhaustion when scaling to hundred-billion-parameter models on a single GPU.\n\n\nWe therefore present Horizon-LM, a memory-centric large-model training system that redefines the roles of CPU and GPU in the training pipeline. Horizon-LM treats host memory as the primary parameter store and uses GPUs only as transient compute engines, enabling hundred-billion-parameter models to be trained and adapted on a single GPU with large host memory. 1) First, to eliminate the dependence on persistent GPU-resident models and full autograd graphs, Horizon-LM adopts a CPU-master, GPU-template execution model. All model parameters and optimizer states reside exclusively in host memory, while GPUs host only lightweight, reusable layer templates. During training, parameters are streamed into GPU buffers on demand, used for computation, and immediately released, removing the need for long-lived GPU model replicas and fundamentally breaking the co"
  },
  {
    "title": "Agentic AI in Healthcare & Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents",
    "url": "https://arxiv.org/abs/2602.04813v1",
    "source": "arxiv",
    "summary": "Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., mem",
    "full_text": "\n\n\n\n\nI Introduction\n\nI-A Focus of Our Survey\nI-B Literature Search Process\nI-C Outline\n\n\n\nII Evaluation Dimensions\n\n\nII-A Cognitive Capabilities\n\nII-A1 Planning\nII-A2 Perception (Input Processing)\nII-A3 Action (Output &amp; Execution)\nII-A4 Meta-Capabilities\nII-A5 Consistency &amp; Conflict Resolution\n\n\n\nII-B Knowledge Management\n\nII-B1 External Knowledge Integration\nII-B2 Memory Module\nII-B3 Dynamic Updates &amp; Forgetting\n\n\n\nII-C Interaction Patterns\n\nII-C1 Conversational Mode\nII-C2 Event-Triggered Activation\nII-C3 Human-in-the-Loop\nII-C4 Error Recovery\n\n\n\nII-D Adaptation &amp; Learning\n\nII-D1 Drift Detection &amp; Mitigation\nII-D2 Reinforcement-Based Adaptation\nII-D3 Meta-Learning &amp; Few-Shot\n\n\n\nII-E Safety &amp; Ethics\n\nII-E1 Safety Guardrails &amp; Adversarial Robustness\nII-E2 Bias &amp; Fairness\nII-E3 Privacy-Preserving Mechanism\nII-E4 Regulatory &amp; Compliance Constraints\n\n\n\nII-F Framework Typology\n\nII-F1 Multi-Agent Design\nII-F2 Centralized Orchestration\n\n\n\nII-G Core Tasks &amp; Subtasks\n\nII-G1 Clinical Documentation &amp; EHR Analysis\nII-G2 Medical Question Answering &amp; Decision Support\nII-G3 Triage &amp; Differential Diagnosis\nII-G4 Diagnostic Reasoning\nII-G5 Treatment Planning &amp; Prescription\nII-G6 Drug Discovery &amp; Clinical Trial Design\nII-G7 Patient Interaction &amp; Monitoring\nII-G8 Benchmarking &amp; Simulation Environment\n\n\n\n\n\nIII Empirical Findings and Discussion\n\nIII-A Which Cognitive Sub-Dimensions Remain Underdeveloped in Healthcare Agents?\nIII-B Are Knowledge Management capabilities evenly implemented across healthcare LLM agents?\nIII-C Do Current Agents Balance Conversation, Event Triggers, Human-in-the-Loop and Recovery Adequately?\nIII-D How Well Do Healthcare LLM Agents Adapt?\nIII-E Safety &amp; Ethics Implementation Maturity Remains Limited Among Surveyed Agents\nIII-F Framework Typology Trends Across LLM-Based Healthcare Agents\nIII-G Which Core Tasks Can Healthcare Agents Perform?\n\n\nIV Conclusion\n\n\n\n\n\n\n\\keepXColumns\\history\nReceived 10 December 2025, accepted 31 December 2025, date of publication 5 January 2026, date of current version 9 January 2026.\n10.1109/ACCESS.2026.3651218\n\n\n\\corresp\nCorresponding author: Aditi Singh (e-mail: a.singh22@csuohio.edu).\n\nAgentic AI in Healthcare &amp; Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents\n\n\nSHUBHAM VATSAL1\n\n‚ÄÉ‚ÄÉ\n\nHARSH DUBEY2 AND ADITI SINGH3 (Senior Member\n\n‚ÄÉ‚ÄÉ\n IEEE)\n\nDepartment of Computer Science, New York University, CIMS, New York, USA (e-mail: sv2128@nyu.edu)\n\nDepartment of Computer Science, New York University, CIMS, New York, USA (e-mail: hd2225@nyu.edu)\n\nDepartment of Computer Science, Cleveland State University, Cleveland, USA (e-mail: a.singh22@csuohio.edu)\n\n\n\nAbstract\nLarge Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows.\nYet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation &amp; Learning, Safety &amp; Ethics, Framework Typology and Core Tasks &amp; Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented ‚úì, Partially Implemented Œî\\Delta, Not Implemented ‚úó), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (‚àº\\sim76% ‚úì) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (‚àº\\sim92% ‚úó) and Drift Detection &amp; Mitigation sub-dimension under Adaptation &amp; Learning is rare (‚àº\\sim98% ‚úó). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (‚àº\\sim82% ‚úì) while orchestration layers remain mostly partial. Across Core Tasks &amp; Subtasks, information centric capabilities lead e.g., Medical Question Answering &amp; Decision Support and Benchmarking &amp; Simulation, while action and discovery oriented areas such as Treatment Planning &amp; Prescription still show substantial gaps (‚àº\\sim59% ‚úó). Together, these findings provide an empirical baseline indicating that current agents excel at retrieval-grounded advising but require stronger adaptation and compliance platforms to move from early-stage systems to dependable systems.\n\n\nIndex Terms: \nAgentic AI, Healthcare, Medicine, Prompt Engineering, Large Language Model, Multi-Agent, Taxonomy, Survey, Empirical Analysis, Clinical Trial, Patient Interaction, Diagnostic Reasoning\n\n\n\n\\titlepgskip\n=-15pt\n\n\n\nI Introduction\n\n\nThe advent of LLMs has significantly accelerated advancements across multiple domains, showcasing transformative potential far beyond traditional applications. These models, including GPT-4 [achiam2023gpt] and PaLM [chowdhery2023palm] are trained on extensive datasets comprising billions or even trillions of parameters enabling broad generalization and sophisticated understanding. Empirical studies have showcased that increasing model scale directly correlates with enhanced performance in complex reasoning, multi-step decision making and interactive planning tasks [wei2022chain]. The enhanced cognitive capabilities of modern LLMs have rapidly driven their integration into critical sectors such as healthcare [yang2023large], medicine [thirunavukarasu2023large], finance [li2023large] for risk assessment and investment strategies and education [yan2024practical] for personalized learning and instructional assistance. Recent research has further evolved to emphasize prompt engineering which is a technique involving precise manipulation of natural language inputs to extract and enhance task specific reasoning capabilities of LLMs [schulhoff2024prompt, gu2023systematic, sahoo2024systematic, vatsal2024survey, vatsal2025multilingual]. This shift underscores a critical progression in LLM research highlighting the transition from predictive modeling to sophisticated, context sensitive cognitive interactions. Furthermore, the emergence of LLM-based agents has marked another significant milestone enabling autonomous decision making and interactive task execution across diverse application areas [xi2025rise].\n\n\nIn healthcare and medicine, LLMs have demonstrated considerable promise in streamlining clinical workflows, enhancing diagnostic accuracy and supporting clinical research. For instance, recent studies have utilized LLMs to automate clinical documentation, significantly reducing clinician workload and improving the quality of clinical records [leong2024efficient, baker2024chatgpt]. Similarly, LLM-based decision support systems have been applied to medical question answering, achieving expert level performance in interpreting complex medical literature and clinical guidelines [singhal2023large]. Complementing these advances, LLMs achieve state-of-the-art results on biomedical machine reading comprehension benchmarks [vatsal2024can] and show measurable gains on guideline-based prior-authorization question answering over noisy, real-world records [vatsal2024canprior]. The integration of multimodal capabilities into LLMs enabling the interpretation of radiology scans and clinical images has further advanced diagnostic efficiency as has been seen in frameworks like ChatCAD [wang2024interactive]. Apart from diagnostics, LLMs have been explored for enhancing treatment planning and supporting evidence-based medicine by synthesizing vast corpora of medical research and clinical guidelines [nori2023capabilities]. In drug discovery, LLMs accelerate the identification of novel drug candidates and streamline clinical trial design which has significantly helped in reducing both time and cost [chakraborty2023artificial]. Patient interaction and clinical communication have also benefited from LLM-powered conversational agents thereby improving adherence to treatment plans, facilitating telemedicine and enabling remote patient monitoring [subramanian2024enhancing]. Although these advancements demonstrate substantial impact, they also highlight the need for continuous evaluation of LLMs in healthcare to maintain high standards of reliability and interpretability.\n\n\nRecently, the development of LLM-based agents which are often described under the broader umbrella of Agentic AI has represented an emerging paradigm that capitalizes on the advanced cognitive capabilities of LLMs. These agents integrate multiple cognitive modules including reasoning, planning and memory management allowing them to do sophisticated problem solving and context aware interactions across a variety of domains. Recent research such as Chain-of-Thought prompting [wei2022chain] and Tree-of-Thoughts reasoning [yao2023tree] has highlighted the importance of modular cognitive architectures which help in facilitating the effective execution of complex multi-step reasoning tasks within Agentic AI design space. Additionally, memory-augmented architectures that incorporate episodic and semantic memory systems have further enhanced agents‚Äô capacities to maintain continuity across interactions improving both task accuracy and reliability. Adaptive learning mechanisms including reinforcement learning with human feedback (RLHF) enable these agents to iteratively refine their behavior by integrating user input and environmental signals resulting in dynamic adaptation and alignment with evolving user objectives. As the landscape of LLM-based agents evolves, their impact in decision making i"
  },
  {
    "title": "Robust Generalizable Heterogeneous Legal Link Prediction",
    "url": "https://arxiv.org/abs/2602.04812v1",
    "source": "arxiv",
    "summary": "Recent work has applied link prediction to large heterogeneous legal citation networks \\new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decode",
    "full_text": null
  },
  {
    "title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "url": "https://arxiv.org/abs/2602.04811v1",
    "source": "arxiv",
    "summary": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficult",
    "full_text": "\n\n\n\n1 Introduction\n\n2 SE-Bench\n\n2.1 Benchmark Construction\n2.2 Dataset Splits &amp; Protocol\n2.3 Metrics\n2.4 Statistics and Validation\n\n\n3 Experiment\n\n4 Analysis and Insight\n\n4.1 RQ1: Does SFT Induce True Internalization, or Merely Context Dependence?\n4.2 RQ2: Can RL Internalize Knowledge in the Context?\n4.3 RQ3: Can Self-Play Enable Knowledge Internalization?\n4.4 RQ4: How Knowledge Evolves from SFT to RL?\n4.5 Discussion: Connections to Recent Advancements\n\n\n5 Related Work\n6 Conclusion\nA The Effect of Question and Trajectory Diversity\nB Case Study on Continue RL\n\nC Experiment Details\n\nC.1 Hyper Parameters of Main Experiment\nC.2 Hyper Parameters of RL Ablation\nC.3 Difference between Open and Closed\n\n\n\nD Details of Benchmark\n\nD.1 Examples\nD.2 Selected NumPy Functions\n\n\nE Example of Error Types\n\n\n\n\n\n\nSE-Bench: Benchmarking Self-Evolution with Knowledge Internalization\n\n\nJiarui Yuan\n\n‚ÄÉ‚ÄÉ\nTailin Jin\n\n‚ÄÉ‚ÄÉ\nWeize Chen\n\n‚ÄÉ‚ÄÉ\nZeyuan Liu\n\n‚ÄÉ‚ÄÉ\nZhiyuan Liu\n\n‚ÄÉ‚ÄÉ\nMaosong Sun\n\n\n\nAbstract\nTrue self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ‚Äúnew‚Äù knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring ‚ÄùClosed-Book Training‚Äù to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nSelf-evolution, the capacity for an autonomous agent to recursively improve its own capabilities, is often viewed as a prerequisite for Artificial General Intelligence (AGI)¬†(Goertzel and Pennachin, 2007; Legg and Hutter, 2006).\nAn ideal self-evolving agent acts as a lifelong learner, continuously assimilating information from its environment, optimizing its solutions, and expanding its skill set without human intervention.\nHowever, current approaches often limit the scope of this evolution to transient or localized adaptations, such as inference-time response refinement¬†(Novikov et al., 2025; Wang et al., 2025b) or iterative self-code modification¬†(Zhang et al., 2025a; Wang et al., 2025a). While valuable, these mechanisms differ fundamentally from the expansive definition we explore here: the self-evolution requires agents to actively learn from experience by internalizing novel skills or knowledge, akin to a human expert accumulating domain knowledge over time¬†(Wang et al., 2024; Zhang et al., 2025b; Ouyang et al., 2025).\n\n\nDespite rapid progress in large language model (LLM) reasoning capabilities, we lack a rigorous measurement for this foundational internalization ability.\nExisting benchmarks have made strides in evaluating specific sub-skills related to self-evolution, such as long-horizon information retrieval¬†(Wei et al., 2025a; Li et al., 2025), iterative response refinement¬†(Lee et al., 2025), and complex task execution¬†(Team, 2025c; Jimenez et al., 2024)\nHowever, current evaluations fail to cleanly isolate an agent‚Äôs ability to process and restore experience due to two fundamental obstacles.\nFirst, the entanglement of prior knowledge: when a model solves a task involving ‚Äúnovel‚Äù knowledge, it is indistinguishable whether the agent learned from the relevant experience or merely recalled pre-training data.\nSecond, the entanglement of reasoning complexity: if an agent fails a complex task, it is ambiguous whether it failed to internalize the necessary knowledge or failed to reason over it.\nThis mirrors a student who memorizes a textbook but fails a frontier math problem due to logical difficulty rather than memory gaps.\n\n\nTo address these limitations, we argue that the community needs a ‚ÄúNeedle in a Haystack‚Äù test¬†(Kamradt, 2023) for self-evolution: an environment where tasks are algorithmically trivial if knowledge is internalized, and impossible if it is not.\nTo this end, we introduce SE-Bench. SE-Bench relies on a knowledge obfuscation mechanism to create this clean environment.\nWe employ a knowledge obfuscation mechanism, mapping the core functions of the NumPy¬†(Harris et al., 2020) library to randomized, nonsense identifiers (e.g., numpy.mean ‚Üí\\rightarrow zwc.kocito) and rewriting the documentation to describe a ‚Äúnew‚Äù package.\nAt training time, agents have access to the documentation, but at test time, agents are tasked with solving simple problems using this obfuscated package, with the strict constraint that any use of the original NumPy library is deemed to fail.\nThis design grants SE-Bench three diagnostic properties:\n(1) Impossible without information: Without documentation, the probability of guessing the correct API is mathematically zero, eliminating prior knowledge confounds.\n(2) Trivial with information: Because the underlying logic maps 1-to-1 to standard NumPy, tasks are trivial for any agent that internalizes the mapping. Any ideal self-evolving method should theoretically achieve a near-100% success rate, thus cleanly isolating the internalization capability.\n(3) Compositional generalization: While the training set consists of tasks solvable with single function calls, the test set requires composing multiple internalized functions, assessing generalization beyond simple memorization.\n\n\nBeyond serving as a rigorous metric, SE-Bench functions as a clean testbed that enables us to dissect the fundamental mechanisms of self-evolution. We investigate whether standard parameter-optimization paradigms, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), can genuinely support the internalization capability.\nOur experiments uncover three critical insights: (1) The Open-Book Paradox: We find that the presence of reference material during parameter update inhibits long-term retention. True internalization requires Closed-Book Training: removing the documentation during parameter updates forces the model to compress external logic into its weights, significantly outperforming standard SFT. (2) The RL Gap: While SFT effectively internalizes new knowledge, standard RL fails even under the Closed-Book training setting. We identify that the negative gradient and PPO clipping¬†(Schulman et al., 2017) both are factors that impact the knowledge internalization for RL. (3) Viability of Self-Play: By applying SFT instead of RL to self-generated tasks and corresponding responses, models successfully internalize knowledge from their own noisy, unverified data, proving that self-evolution on knowledge internalization is viable if the correct optimization mechanism is used and that RL is not a one-size-fits-all solution.\n\n\nWe position SE-Bench as a diagnostic testbed for the self-evolving agent community.\nJust as long-context models must at least demonstrate near-perfect retrieval on Needle-in-a-Haystack tests to establish basic competency, we argue that self-evolving agents should also demonstrate the ability to pass SE-Bench before they can be trusted to evolve in complex, open-ended environments.\nAnd because SE-Bench provides a clean, controlled environment, it also serves as an ideal platform for studying the fundamental mechanisms for knowledge internalization, potentially facilitating future research.\n\n\n\n\n2 SE-Bench\n\n\nFigure 1: Overview of the SE-Bench construction pipeline. The process consists of three main stages: (1) Obfuscation, where we implement a wrapper package zwc that renames selected NumPy functions and translates API documentation; (2) Generation, where Claude-4.5-sonnet generates valid tasks and test cases based on the original NumPy library; and (3) Filtering, where tasks are validated through strict consensus between three strong LLMs, followed by human verification.\n\n\nWe argue that a fundamental, yet often overlooked, component of self-evolution is knowledge internalization. While current methods often focus on transient adaptation, optimizing a solution within a single context window¬†(Wang et al., 2025b; Qi et al., 2025; Wei et al., 2025b; Team, 2025b), genuine evolution requires transitioning from a stateless processor to a lifelong learner. A concrete example of such a process is a human software engineer learning a new library: initially relying on documentation, but eventually internalizing the logic to solve problems fluently without external aid through repeated practice.\n\n\nMeasuring such a capability of LLM agents in a similar scenario, however, presents a fundamental dilemma. We cannot evaluate the agent‚Äôs ability to internalize any existing library (like Numpy), as they may have already been embedeed in the LLM‚Äôs pre-training weights¬†(Shao et al., 2025; Wu et al., 2025). Furthermore, simply adopting a newly-released library is a fragile solution: as knowledge cutoff of the LLMs advances, the benchmark quickly becomes obsolete. To rigorously measure the internalization ability of self-evolving methods, we require a domain that is permanently out-of-distribution: one that"
  },
  {
    "title": "Beyond Rewards in Reinforcement Learning for Cyber Defence",
    "url": "https://arxiv.org/abs/2602.04809v1",
    "source": "arxiv",
    "summary": "Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the chal",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Background\n\n2.1 Autonomous Cyber Defence\n\n2.2 Reliability and Risk in RL\n\nDispersion variability across time (DT)\nDispersion variability across runs (DR)\nConditional Value at Risk (CVaR)\nRisk across Fixed-Policy Rollouts (RF)\n\n\n\n\n\n3 Methodology\n\n3.1 Yawning Titan Cyber Gym\n3.2 Cyber Autonomy Gym for Experimentation\n\n3.3 Ground Truth\n\nGround Truth Score (ScoreGT\\mathrm{Score}_{\\mathrm{GT}})\n\n\n3.4 Evaluating reliability across different rewards\n3.5 Experiments\n\n\n4 Results\n5 Discussion\n6 Related Work\n7 Conclusion\nA Complex Dense Negative Reward Function\nB Hyperparameters for training\nC Basic Action Space Results in Yawning Titan\nD DQN Results in Yawning Titan\nE Episodic Rewards with Corresponding ScoreGT\\text{Score}_{\\text{GT}} for Yawning Titan Experiments\nF Policy Analysis of YT Agents\nG Policy analysis of CAGE agents\nH Codebase\nI PPO Agent Training Curves in Yawning Titan\nJ CAGE agents trained using DQN\nK Positive Reward Ablation Study\nL Default CAGE-2 Reward Sources of Bias\nM MiniCAGE Evaluation using the Default CAGE-2 Reward\n\n\n\n\n\nBeyond Rewards in Reinforcement Learning for Cyber Defence\n\n\n\n\n\n\nAbstract\nRecent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.\n\n\n\nElizabeth Bates*, Chris Hicks*, and Vasilios Mavroudis\nThe Alan Turing Institute, London, United Kingdom\nebates, c.hicks, vmavroudis‚Äâ@turing.ac.uk\n*Equal contribution\n\n\n\n1‚ÄÇIntroduction\n\nCyber attacks are increasingly frequent and sophisticated, straining limited cyber defence resources and threatening critical digital systems that people depend upon worldwide. There has been a rising level of interest in using machine learning (ML) methods to improve cyber security; in particular deep reinforcement learning (DRL) which has the ability to learn complex policies from interaction alone, enabling the discovery of strategies unconstrained by flawed system or security models. DRL based autonomous cyber defence (ACD) agents, which have gathered much attention in the literature, could discover novel techniques and provide automation for tasks that currently occupy human analysts.\n\n\nCyber gyms provide efficient and controlled environments for ACD agents. This is particularly important for network security tasks, enabling the large number of interactions required for training without risking production networks or systems. Accordingly, many cyber gyms have been created to enable training agents that defend networked systems¬†[33]. Cyber gyms define one or more Markov Decision Processes (MDPs) in terms of a state space comprising network and host information, an action space of defensive activities, and a reward function aligned to defensive objectives. ACD reward functions are typically highly engineered based on human judgment, combining multiple penalties and incentives determined for a variety of defensive actions and network states¬†[3, 29]. Dense rewards may be preferable because of expedited learning, providing apparently effective solutions using fewer environment steps during training, but they also risk constraining agents to sub-optimal solutions¬†[25]. This is especially concerning for ACD agents which might then contain avoidable weaknesses that are difficult to identify in advance of an attack. Furthermore, dense rewards draw potentially arbitrary numerical equivalences between network states and actions. As the scale and complexity of cyber tasks grow this becomes increasingly challenging to manage and the risks of undesirable agent behaviour are exacerbated.\n\n\nAt the expense of generally requiring more training iterations, sparse rewards place fewer constraints on the solution space and could enable preferable or more effective policies to be discovered. Existing work has not investigated the possibility that dense rewards might limit the performance of ACD agents trained using DRL. To investigate this possibility, and summarising the main contributions of this work, we: (1) propose a ground truth scoring mechanism for network security cyber gyms which allows a direct comparison between agents trained using different reward functions, (2) evaluate a comprehensive range of sparse and dense reward functions using two popular cyber gyms which are adapted to illustrate our ground truth mechanism, and (3) show that sparse reward functions can enhance the effectiveness, reliability and risk-profiles of ACD agents across a variety of network sizes and topologies, action spaces, MDP models and DRL algorithms.\n\n\n\n2‚ÄÇBackground\n\nHere we provide an introduction to ACD, motivate evaluating ACD agents more accurately, and define the key metrics we later build upon to fully evaluate the impact of reward functions in ACD.\n\n\n2.1‚ÄÇAutonomous Cyber Defence\n\nACD agents aim to actively mitigate attacks on computer networks using ML techniques rather than traditional rule-based approaches. By alleviating the bottlenecks of human response speed and information processing, ACD agents could provide a much needed counterbalance to the ever-increasing scale and sophistication of cyber threats.\nReinforcement learning (RL), and particularly DRL given the enormity of data generated by computer networks, is particularly promising as it allows learning defensive strategies from interaction alone without the need for explicit models of how networks, systems, and attackers behave. Such models must continually be updated as attackers evolve, frequently undermining the tools and techniques that derive security proofs or assurances from their correctness. By observing the network state and choosing defensive actions, DRL agents can learn novel, adaptive strategies for defending computer networks that do not depend on potentially incorrect or outdated assumptions.\n\n\n\nSince their learning is guided by maximising long-term rewards, ACD agents critically depend on the rewards provided throughout training. Furthermore, the exploration required for learning from trial-and-error demands a cyber gym allowing extensive experimentation (i.e., risk-taking) without jeopardising valuable production systems. Many cyber gyms have been created¬†[33], provided publicly¬†[19, 22, 3], and even used for competitions seeking the best performing agents¬†[29, 13, 10].\nDespite these promising developments, previous work on ACD is limited to evaluating performance using only mean episodic rewards, and variance of the same, over a number of fixed-policy rollouts. Unlike games (e.g., chess) which correspond relatively naturally to the MDP framework, defending a network of computer hosts does not. Real-world attackers are not confined to turn-based interactions, partial observability affects many aspects of the network, and there is never a state where the defender can be definitively crowned the winner.\n\n\nMost cyber gyms, and prominent ACD competitions, have hand-crafted dense reward functions that are used to train and evaluate agents. Such rewards may misrepresent the true performance of agents and it is impractical for them to accurately represent human knowledge¬†[14], biasing models towards possibly lower-performance and higher-risk strategies. There is a need, which we illustrate and address for the first time to the best of our knowledge, for evaluation methods that accurately represent the ground truth of complex cyber environments. Our ground truth scoring mechanism permits a direct and reproducible comparison between different reward strategies, enabling experiments that empirically quantify the performance and risk characteristics of reward functions in ACD environments.\n\n\n\n2.2‚ÄÇReliability and Risk in RL\n\nThe reliability and risks of RL agents is a critical issue, especially for cyber defence applications where inconsistent performance can be costly or dangerous. Training reliability metrics measure how consistently an RL algorithm performs across multiple training runs, and risk metrics quantify expectations of worst-case performance.\n\n\nTraining reliability\n\nTo evaluate the impact of reward function on training reliability in ACD agents, we build upon the quantitative RL training reliability metrics proposed by¬†Chan et al. [5] based on dispersion variability i.e., the width of the mean episodic rewards distribution.\n\n\nDispersion variability across time (DT)\n\nmeasures the stability of RL training across time. Smooth monotonic policy improvements offer the lowest D‚ÄãTDT scores, indicating high reliability during training and lowered computational costs.\nD‚ÄãTDT is measured by averaging the inter-quartile range (IQR) within a sl"
  },
  {
    "title": "Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning",
    "url": "https://arxiv.org/abs/2602.04807v1",
    "source": "arxiv",
    "summary": "We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) t",
    "full_text": "\n\n\n\n1 Introduction\n2 Related Work\n\n3 Core Architecture for Computational Afferent and Adaptive Learning\n\n3.1 Computational Afferent Traces (CATs)\n3.2 Bi-Level Learning Architecture\n\n\n\n4 Afferent Foundation Model (AFM)\n\n4.1 Environment and Damage Dynamics\n4.2 Afferent Array Architecture\n4.3 Computational Afferent Trace (CAT)\n4.4 Policy Learning with Afferent Signals\n4.5 Evolutionary Optimization of Afferent Parameters\n4.6 Theoretical Analysis\n4.7 Episodic Memory (Artificial Mental Model)\n\n\n\n5 Use Case: Afferent Learning for Biomechanical Digital Twins\n\n5.1 Statistical Analysis\n5.2 Evolution and Analysis of Afferent Arrays\n5.3 Baseline Comparisons\n5.4 Ablation Studies\n5.5 Generalization Across Pathological Conditions\n5.6 Age-Dependent Action Restriction\n\n\n6 Limitations\n7 Conclusion\nA Dataset and Reproducibility\nB Predictive Discrepancy Nociception (Optional)\nC Empirical Smoothness Verification\nD Anatomical Mapping of Nociceptor Arrays\n\nE Implementation Details\n\nE.1 Digital Twin Parameterization\nE.2 NFM Architecture\nE.3 AMM Design\nE.4 Episodic Memory Algorithms\n\nE.5 Data Format Specifications\n\nJSONL Format.\nNPZ Model Format.\n\n\nE.6 Reproducing Experiments\nE.7 Computational Requirements\nE.8 Dataset Statistics\n\n\n\nF Additional Experiments\n\nF.1 Completed Ablation Studies\nF.2 Age-Adaptive Policy Training (500K Steps)\nF.3 Baseline Comparison\nF.4 Pathological Condition Evaluation\nF.5 Future Extensions\n\n\n\n\n\n\n\nEvolving Afferent Architectures: Biologically-inspired Models for\nDamage-Avoidance Learning\n\n\nWolfgang Maass\n\nSaarland University, Saarbr√ºcken, Germany\n\nGerman Research Center for Artificial Intelligence (DFKI), Saarbr√ºcken, Germany\n\n\nSabine Janzen\n\nGerman Research Center for Artificial Intelligence (DFKI), Saarbr√ºcken, Germany\n\n\nPrajvi Saxena\n\nGerman Research Center for Artificial Intelligence (DFKI), Saarbr√ºcken, Germany\n\n\nSach Mukherjee\n\nStatistics and Machine Learning, German Center for Neurodegenerative Diseases (DZNE), Bonn, Germany\n\nMRC Biostatistics Unit, University of Cambridge, Cambridge, UK\n\n\n\nAbstract\nWe introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning.\nInspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.\n\n\n\n1 Introduction\n\nBiological afferents are sensory pathways that convey information from the periphery to the central nervous system, based on a dual-level organization. At the phylogenetic level, afferent architectures are evolved to enable effective damage-avoidance learning; at the ontogenetic level, neural circuitry adapts over an organism‚Äôs lifetime to respond to afferent signals [2, 16]. This bi-level selection and optimization can be viewed as implementing a form of inductive bias with respect to learnability. Sensing architectures are optimized to support efficient policy learning under risk rather than directly minimizing physical damage. This general schema of adaptive sensors coupled with bi-level phylogenetic and ontogenetic optimization is observed not only in the nervous system but across biology, including sensing and fine-tuning mechanisms in immunity and DNA damage sensing and repair.\n\n\nIn contrast, AI systems interacting with models of biological or biomedical systems typically lack internal signals that identify harmful states, integrate temporal stress accumulation, and store this information to bias future decisions. A key application, which motivates this work, is in biomedical digital twins (DTs). DTs in medicine have reached high fidelity [11], and their integration with AI approaches has high potential, but most applications remain reactive and memoryless, preventing adaptive damage-avoidance behaviors and this is an important factor that continues to limit their practical utility.\n\n\nIn this paper, motivated by the foregoing observations about biological systems, we introduce a class of AI architectures in which adaptive sensing mechanisms, termed Computational Afferent Traces (CATs), serve as internal risk signals intended to be operationally equivalent to afferent mechanisms in biological systems. We focus in particular on settings in which an embodied agent operates on tasks involving damage accumulation over time. As we see below, the CATs we learn are related to physiologically-plausible damage signals rather than only abstract considerations. The goal of the (computational) afferent mechanisms we propose is to endow AI systems with an internal risk indicator that guides adaptive behavior. For example, in the specific setting of a biomechanical DT, we would expect CATs to capture aspects such as non-physiological load, instability-driven stress or cumulative strain and thereby allow efficient learning,\n\n\nSpecifically, we propose a learning architecture that mirrors the bi-level (phylogenetic/ontogenetic) organization of biological afferents: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning [10, 19], while reinforcement learning (inner loop) trains damage-avoidance policies using these signals [22]. This separation enables afferents to provide an inductive bias with respect to learnability, where architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage).\n\n\n\n\n2 Related Work\n\nArtificial afferents have been investigated primarily in the context of neurorobotics and bio-inspired sensing, where afferent signals are used to trigger reflexive behaviors or protective responses [24, 7]. In parallel, musculoskeletal digital twins integrate medical imaging, biomechanical modeling, and finite-element simulation to analyze tissue stress, degeneration, and injury risk [11, 6]. While these approaches provide important diagnostic and predictive capabilities, they typically treat afferent inputs or damage as an external quantity to be estimated rather than as an internal, learned risk state that actively shapes behavior and learning over time.\n\n\nA complementary line of work models afferent inputs as an inferential process that integrates sensory evidence, prior expectations, and uncertainty\nproviding an internal belief or risk-related latent state that influences perception and action selection [23, 3, 9]. Although conceptually aligned with the notion of afferent internal signals, these models are often underspecified at the algorithmic level and are rarely grounded in explicit biomechanical damage processes or evaluated in long-horizon learning and control scenarios.\n\n\nEpisodic memory has been shown to improve decision-making, planning, and adaptation in artificial agents [15, 5, 18]. However, episodic mechanisms in reinforcement learning are typically grounded in abstract state‚Äìaction‚Äìreward representations rather than physics-based damage or wear signals [17]. As a result, episodic recall is seldom used to anticipate biomechanical risk or long-term structural degradation.\n\n\nHierarchical and meta-learning frameworks separate architectural optimization from policy learning [20, 21]. In reinforcement learning, evolutionary strategies have been employed as outer-loop optimizers [19, 10], while gradient-based methods such as PPO are commonly used for policy learning [22]. In contrast to approaches that directly evolve policies or rewards, our method evolves afferent sensing architectures whose fitness is evaluated based on downstream learning outcomes.\nWhile our approach shares some motivation with general meta-learning, in contrast to established meta-learning frameworks such as MAML and variants [8, 14, 12] our focus is not on general heuristics concerning learning dynamics but rather specific adaptive sensors and their integration via bi-level phylogenetic/ontogenetic-like learning.\nFurthermore, we focus specifically on\nthe setting of agents operating in tasks\ninvolving damage accumulation over time, where the particular inductive bias we propose is a natural computational analogue to real, biological damage-sensing mechanisms and associated adaptation.\n\n\n\n\n3 Core Architecture for Computational Afferent and Adaptive Learning\n\nThis section outlines the core concepts of the architecture; implementation details are deferred. For concreteness, we use biomechanical digital twins (BDTs) as a running example. We consider an embodied agent in a simulated environment with cumulative damage. At each time step tt, the environment provides observations xt‚àà‚ÑùKx_{t}\\in\\mathbb{R}^{K} under action ata_{t} and context sts_{t}. The agent maintains an internal, unobservable damage state Dt‚àà‚Ñù‚â•0D_{t}\\in\\mathbb{R}_{\\geq 0} that accumulates with mechanical loading, inducing partial observability that is addressed through afferent sensing.\n\n\n\n3.1 Computational Afferent Traces (CATs)\n\n\nDefinition 1 (Computational Afferent Trace (CAT)).\n\n\nLet xt‚àà‚ÑùKx_{t}\\in\\mathbb{R}^{K} denote the feature vector at time tt"
  },
  {
    "title": "Skin Tokens: A Learned Compact Representation for Unified Autoregressive Rigging",
    "url": "https://arxiv.org/abs/2602.04805v1",
    "source": "arxiv",
    "summary": "The rapid proliferation of generative 3D models has created a critical bottleneck in animation pipelines: rigging. Existing automated methods are fundamentally limited by their approach to skinning, treating it as an ill-posed, high-dimensional regression task that is inefficient to optimize and is typically decoupled from skeleton generation. We posit this is a representation problem and introduc",
    "full_text": null
  },
  {
    "title": "OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models",
    "url": "https://arxiv.org/abs/2602.04804v1",
    "source": "arxiv",
    "summary": "Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-graine",
    "full_text": "\n\n\n\n1 Introduction\n\n2 Related Works\n\n2.1 Omni-modal Large Language Models\n2.2 Token Compression in Multimodal Models\n\n\n\n3 Method\n\n3.1 Preliminary\n3.2 OmniSIFT\n3.3 Spatio-Temporal Video Pruning\n3.4 Vision-Guided Audio Selector\n\n\n\n4 Experiment\n\n4.1 Experimental Setting\n4.2 Main Results\n4.3 Efficiency Analysis\n\n4.4 Ablation Study\n\nStructural Ablation: STVP and VGAS.\nToken Compression Paradigm Ablation.\n\n\n4.5 Case Study\n\n\n5 Conclusion\nA Expanded Benchmark Details\n\nB Expanded Implementation Details\n\nInput Configuration and Preprocessing\nConfiguration of Visual and Audio Compression Ratios\nEvaluation Prompts\n\n\n\nC Computing Cost Evaluation\n\nC.1 Parameter Efficiency\nC.2 Computational Complexity\n\n\n\nD More Experimental Results\n\nD.1 Visualization of Attention Sparsity\nD.2 Efficiency Gains across Video Lengths\nD.3 Ablation on Selector Depth\nD.4 Extended Ablation Results\n\n\n\n\n\n\n\n\n¬†OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models\n\n\nYue Ding\n\n‚ÄÉ‚ÄÉ\nYiyan Ji\n\n‚ÄÉ‚ÄÉ\nJungang Li\n\n‚ÄÉ‚ÄÉ\nXuyang Liu\n\n‚ÄÉ‚ÄÉ\nXinlong Chen\n\n‚ÄÉ‚ÄÉ\nJunfei Wu\n\n‚ÄÉ‚ÄÉ\nBozhou Li\n\n‚ÄÉ‚ÄÉ\nBohan Zeng\n\n‚ÄÉ‚ÄÉ\nYang Shi\n\n‚ÄÉ‚ÄÉ\nYushuo Guan\n\n‚ÄÉ‚ÄÉ\nYuanxing Zhang\n\n‚ÄÉ‚ÄÉ\nJiaheng Liu\n\n‚ÄÉ‚ÄÉ\nQiang Liu\n\n‚ÄÉ‚ÄÉ\nPengfei Wan\n\n‚ÄÉ‚ÄÉ\nLiang Wang\n\n\n\nAbstract\nOmni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.\n\nMachine Learning, ICML\n\n\n\n1 Introduction\n\nThe rapid evolution of Omni-LLMs¬†(Cheng et al., 2024; Xu et al., 2025b; Liu et al., 2025a) has significantly advanced holistic audio-video-language understanding¬†(Hong et al., 2025; Zhou et al., 2025; Li et al., 2025). However, video signals are composed of densely sampled consecutive frames¬†(Chen et al., 2024b; Jiang et al., 2025a), and audio streams must be encoded at high temporal resolution to capture acoustic dynamics¬†(Ji et al., 2024). When these high-resolution streams are tokenized and interleaved for joint reasoning, the resulting sequence length grows rapidly. For example, a typical 20-second multimodal clip can yield more than 20K tokens¬†(Xu et al., 2025a). Such long token sequences significantly increase computational cost, particularly for long video understanding¬†(Fu et al., 2025).\n\n\nFigure 1: Performance comparison across five audio‚Äìvideo benchmarks.\nResults are obtained using Qwen2.5-Omni-7B with a 35% token retained ratio, comparing OmniSIFT against three baseline token compression methods and the full-token baseline.\n\n\nFigure 2: \nCompression paradigm comparison for Omni-LLMs.\nToken compression for Omni-LLMs can be categorized into three paradigms:\n(a) modality-decoupled compression (left top), which applies audio and video compression independently;\n(b) modality-symmetric compression (right top), which treats the two modalities equally informative;\nand (c) modality-asymmetric compression (bottom, ours), which first prunes visual redundancy and then performs visually guided audio compression.\n\n\n\nToken compression¬†(Chen et al., 2024a; Liu et al., 2025d, b; Ye et al., 2025) has emerged as a practical solution to mitigate the prohibitive computational cost caused by excessive token sequences. In the context of vision-centric MLLMs, a substantial body of work has explored effective strategies for pruning redundant visual tokens¬†(Chen et al., 2024a; Tao et al., 2025a; Yao et al., 2025), demonstrating that significant efficiency gains can be achieved with minimal performance degradation. However, directly extending these approaches to audio‚Äìvideo understanding in Omni-LLMs is far from straightforward. As illustrated in Fig.¬†2, the modality-decoupled compression method directly transfers vision-only techniques to both video and audio streams. While simple, this strategy completely ignores cross-modal semantic dependencies¬†(Seo et al., 2023) and may discard tokens that are jointly informative.\n\n\nA recent line of work adopts a modality-symmetric token compression paradigm.\nOmniZip¬†(Tao et al., 2025b) follows this paradigm by first compressing audio tokens using attention scores from the audio encoder, and then guiding video token pruning with audio-derived saliency.\nIts reliance on attention-based saliency limits compatibility with efficient operators such as FlashAttention¬†(Shah et al., 2024). In addition, treating the two modalities as equally informative collapses the compression process into selecting salient temporal positions, rather than capturing modality-specific semantic cues.\nEchoingPixels¬†(Gong et al., 2025) also adopts a modality-symmetric design, performing global cross-modal contextualization over all audio and video tokens via four additional LLM decoder layers before compression. This compression method delays compression to a late stage and introduces substantial computational overhead.\n\n\nIn practice, humans process audio‚Äìvideo content asymmetrically¬†(Koppen et al., 2008). Visual redundancy can typically be resolved using visual cues alone, whereas the saliency of audio signals depends on whether the visual scene provides a semantic anchor¬†(Zhao et al., 2018; Arandjelovic and Zisserman, 2017), such as a visible speaker or a visually grounded event¬†(Chowdhury et al., 2025). This perceptual asymmetry suggests that effective omni-modal token compression should be guided by visual semantics rather than treated symmetrically across modalities.\n\n\nTaken together, these observations suggest three design principles for Omni-LLM token compression: (1) Modality-asymmetric, vision-guided compression; (2) Lightweight compression; (3) Compatibility with efficient operators.\n\n\nBased on the above analysis, we present OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric framework for visually guided token compression. As illustrated in Figure¬†2, OmniSIFT first prunes spatial and temporal redundancy in video to produce a compact set of visual anchors, and then uses these anchors to select the audio tokens that are most informative for the scene. This two-stage pipeline removes uninformative signals while preserving the key multimodal cues required for reasoning.\n\n\nWith only 4.85M additional parameters, OmniSIFT achieves lower latency than training-free baselines such as OmniZip on Qwen2.5-Omni-7B. Moreover, with only 25% of the original tokens retained, it consistently outperforms all compression baselines and even surpasses the full-token model on several settings, as illustrated in Figure¬†1.\n\n\nOur main contributions are summarized as follows:\n\n\n‚Ä¢\n\nBased on the asymmetric dependency between audio and video, we derive practical design principles for omni-modal token compression.\n\n\n\n‚Ä¢\n\nWe present OmniSIFT, a modality-asymmetric framework that first removes spatial and temporal redundancy in video tokens and then uses the resulting visual anchors to select informative audio tokens.\n\n\n\n‚Ä¢\n\nExtensive experiments across five benchmarks show that OmniSIFT delivers strong performance‚Äìefficiency gains, achieving higher accuracy even with only 25% of the original tokens.\n\n\n\n\n\nFigure 3: \nArchitecture of OmniSIFT, a modality-asymmetric compression framework.\nThe framework operates in two stages.\nIn the first stage, STVP removes spatial and temporal redundancy in video tokens to obtain a compact set of visual anchors.\nIn the second stage, VGAS selects audio tokens conditioned on these visual anchors.\nThe resulting compressed multimodal sequence is then fed into the LLM backbone for downstream reasoning.\n\n\n\n\n\n2 Related Works\n\n\n2.1 Omni-modal Large Language Models\n\nOmni-LLMs¬†(Jiang et al., 2025b) extend large language models to process heterogeneous modalities within a unified autoregressive framework. Unlike conventional Video-LLMs¬†(An et al., 2025; Bai et al., 2025; Wu et al., 2025a; Chen et al., 2025b; Wu et al., 2025b), which primarily focus on the interaction between visual sequences and textual instructions, Omni-LLMs additionally incorporate audio signals¬†(Cheng et al., 2024; Tang et al., 2025; Liu et al., 2025a; Chen et al., 2026). Proprietary systems such as GPT-4o¬†(Hurst et al., 2024) and Gemini¬†(Comanici et al., 2025) further demonstrate strong performance on audio‚Äìvisual understanding tasks¬†(Li et al., 2025; Hong et al., 2025). In the open-source community, models like Qwen2.5-Omni¬†(Xu et al., 2025a) adopt a typical architecture that aligns modality-specific encoders with an LLM through learned projection layers.\n\n\n\n\n2.2 Token Compression in Multimodal Models\n\nIn the video domain, token compression methods such as VisionZip¬†(Yang et al., 2025), VidCom2¬†(Liu et al., 2025c), TimeChat-Online¬†(Yao et al., 2025), and DyCoke¬†(Tao et al., 2025a) estimate token importance through various saliency or similarity metrics. Recent work has begun to explore compression in the audio‚Äìvideo setting. OmniZip¬†(Tao et al., 2025b) represents an early attempt, selec"
  },
  {
    "title": "Maximum-Volume Nonnegative Matrix Factorization",
    "url": "https://arxiv.org/abs/2602.04795v1",
    "source": "arxiv",
    "summary": "Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, m",
    "full_text": "  class=\"with-cu-identity\"\n  \n  \n  \n    \n      Skip to main content\n      \n      \n        \n          \n        \n          We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.\n          Donate\n        \n      \n\n      \n\n  \n     &gt; cs &gt; arXiv:2602.04795v1\n  \n\n        \n        \n\n          \n    \n      \n        \n          \n          Help | Advanced Search\n        \n        \n          \n            \n              All fields\n              Title\n              Author\n              Abstract\n              Comments\n              Journal reference\n              ACM classification\n              MSC classification\n              Report number\n              arXiv identifier\n              DOI\n              ORCID\n              arXiv author ID\n              Help pages\n              Full text\n            \n          \n        \n        \n        Search\n      \n    \n  \n     \n\n      \n        \n          \n          \n            \n              \n              \n              \n            \n          \n          \n            open search\n            \n              \n                \n                  \n                  \n                  \n                  GO\n                \n              \n            \n\n            open navigation menu\n            \n              \n                quick links\n                \n                    Login\n                    Help Pages\n                    About\n                \n              \n            \n          \n        \n      \n    \n\n    \n      \n\n    \n    \n--\n\n  \n    \n      Computer Science  Machine Learning\n    \n\n    \n      arXiv:2602.04795v1 (cs)\n    \n\n\n  \n    \n  [Submitted on 4 Feb 2026]\n    Title:Maximum-Volume Nonnegative Matrix Factorization\n    Authors:Olivier Vu Thanh, Nicolas Gillis            View a PDF of the paper titled Maximum-Volume Nonnegative Matrix Factorization, by Olivier Vu Thanh and Nicolas Gillis\n    View PDF\n\n\n\n    \n            Abstract:Nonnegative matrix factorization (NMF) is a popular data embedding technique. Given a nonnegative data matrix $X$, it aims at finding two lower dimensional matrices, $W$ and $H$, such that $X\\approx WH$, where the factors $W$ and $H$ are constrained to be element-wise nonnegative. The factor $W$ serves as a basis for the columns of $X$. In order to obtain more interpretable and unique solutions, minimum-volume NMF (MinVol NMF) minimizes the volume of $W$. In this paper, we consider the dual approach, where the volume of $H$ is maximized instead; this is referred to as maximum-volume NMF (MaxVol NMF). MaxVol NMF is identifiable under the same conditions as MinVol NMF in the noiseless case, but it behaves rather differently in the presence of noise. In practice, MaxVol NMF is much more effective to extract a sparse decomposition and does not generate rank-deficient solutions. In fact, we prove that the solutions of MaxVol NMF with the largest volume correspond to clustering the columns of $X$ in disjoint clusters, while the solutions of MinVol NMF with smallest volume are rank deficient. We propose two algorithms to solve MaxVol NMF. We also present a normalized variant of MaxVol NMF that exhibits better performance than MinVol NMF and MaxVol NMF, and can be interpreted as a continuum between standard NMF and orthogonal NMF. We illustrate our results in the context of hyperspectral unmixing.\n    \n\n    \n    \n              \n          Comments:\n          arXiv admin note: substantial text overlap with arXiv:2412.06380\n        \n\n          Subjects:\n          \n            Machine Learning (cs.LG); Signal Processing (eess.SP); Numerical Analysis (math.NA); Machine Learning (stat.ML)\n        \n          Cite as:\n          arXiv:2602.04795 [cs.LG]\n        \n        \n          &nbsp;\n          (or \n              arXiv:2602.04795v1 [cs.LG] for this version)\n          \n        \n        \n          &nbsp;\n                        https://doi.org/10.48550/arXiv.2602.04795\n              \n                \n                Focus to learn more\n              \n              \n              \n                                  arXiv-issued DOI via DataCite (pending registration)\n            \n          \n        \n    \n  \n\n    \n      Submission history From: Olivier Vu Thanh [view email]          [v1]\n        Wed, 4 Feb 2026 17:43:25 UTC (6,281 KB)\n\n  \n  \n    \n      \n      Full-text links:\n      Access Paper:\n      \n  \nView a PDF of the paper titled Maximum-Volume Nonnegative Matrix Factorization, by Olivier Vu Thanh and Nicolas GillisView PDFTeX Source\n \n      view license\n    \n        \n    Current browse context: cs.LG\n\n  \n\n      &lt;&nbsp;prev\n    \n    &nbsp; | &nbsp;    \n      next&nbsp;&gt;\n    \n  \n    new\n     | \n    recent\n     | 2026-02\n  \n    Change to browse by:\n    \n        cs\n        cs.NA\n        eess\n        eess.SP\n        math\n        math.NA\n        stat\n        stat.ML\n    \n  \n\n    \n      \n        References &amp; Citations\n        \n          NASA ADSGoogle Scholar\n          Semantic Scholar\n        \n        \n      \n\n\n    export BibTeX citation\n    Loading...\n\n\n\n    \n        \n            BibTeX formatted citation\n            &times;\n        \n        \n            loading...\n        \n        \n            Data provided by: \n            \n        \n    \n\n  Bookmark\n    \n  \n  \n    \n  \n  \n  \n\n\n  \n    Bibliographic Tools\n    \n      Bibliographic and Citation Tools\n      \n        \n          \n            \n              \n              \n              Bibliographic Explorer Toggle\n            \n          \n          \n            Bibliographic Explorer (What is the Explorer?)\n          \n        \n        \n          \n            \n              \n              \n              Connected Papers Toggle\n            \n          \n          \n            Connected Papers (What is Connected Papers?)\n          \n        \n          \n            \n              \n              \n              Litmaps Toggle\n            \n          \n          \n            Litmaps (What is Litmaps?)\n          \n        \n        \n          \n            \n              \n              \n              scite.ai Toggle\n            \n          \n          \n            scite Smart Citations (What are Smart Citations?)\n          \n        \n      \n        \n        \n        \n        \n    \n\n\n    \n    Code, Data, Media\n    \n      Code, Data and Media Associated with this Article\n      \n        \n          \n            \n              \n              \n              alphaXiv Toggle\n            \n          \n          \n            alphaXiv (What is alphaXiv?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            CatalyzeX Code Finder for Papers (What is CatalyzeX?)\n          \n        \n\n        \n          \n            \n              \n              \n              DagsHub Toggle\n            \n          \n          \n            DagsHub (What is DagsHub?)\n          \n        \n  \n        \n          \n            \n              \n              \n              GotitPub Toggle\n            \n          \n          \n            Gotit.pub (What is GotitPub?)\n          \n        \n\n        \n          \n            \n              \n              \n              Huggingface Toggle\n            \n          \n          \n            Hugging Face (What is Huggingface?)\n          \n        \n\n        \n          \n            \n              \n              \n              Links to Code Toggle\n            \n          \n          \n            Papers with Code (What is Papers with Code?)\n          \n        \n\n\n        \n          \n            \n              \n              \n              ScienceCast Toggle\n            \n          \n          \n            ScienceCast (What is ScienceCast?)\n          \n        \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n    \n\n\n      \n      Demos\n      \n        Demos\n        \n          \n            \n              \n                \n                \n                Replicate Toggle\n              \n            \n            \n              Replicate (What is Replicate?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              Hugging Face Spaces (What is Spaces?)\n            \n          \n          \n            \n              \n                \n                \n                Spaces Toggle\n              \n            \n            \n              TXYZ.AI (What is TXYZ.AI?)\n            \n          \n        \n        \n        \n        \n      \n      \n      Related Papers\n      \n        Recommenders and Search Tools\n        \n          \n            \n              \n                \n                \n                Link to Influence Flower\n              \n            \n            \n              Influence Flower (What are Influence Flowers?)\n            \n          \n          \n            \n              \n                \n                \n                Core recommender toggle\n              \n            \n            \n              CORE Recommender (What is CORE?)\n            \n                    \n            \n              \n                \n                \n                IArxiv recommender toggle\n              \n            \n            \n              IArxiv Recommender\n              (What is IArxiv?)\n            \n          \n\n        \n        \n          \n            Author\n            Venue\n            Institution\n            Topic\n          \n          \n            \n            \n            \n            \n          \n        \n        \n        \n      \n\n      \n      \n        About arXivLabs\n      \n      \n        \n          \n            arXivLabs: experimental projects with community collaborators\n            arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\n            Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is comm"
  }
]